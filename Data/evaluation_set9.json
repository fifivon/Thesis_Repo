[
  {
    "abstract": "The level-dependent representation of simple speech sounds in cat primary auditory cortex (AI) is explored in naive cats and in animals that have been exposed to these sounds in behavioral detection and discrimination tasks. Population analyses of multiple unit responses in the form of post-stimulus time histograms (PSTHs), neurograms, and spatial distribution were made for synthetic consonant-vowel sounds across AI. The temporal profile of cortical responses was robust across neurons, characterized by brief phasic responses at the onset of consonantal burst and voicing. The spectral profile of the sounds, i.e., the formant structure, was only weakly expressed in the response magnitude across characteristic frequency. The spatial response distribution across AI was discontinuous, and consisted of several patches of activation. Intensity-dependence in the spatial activity distribution was more strongly expressed than in population PSTHs and neurograms. Differences attributable to behavioral training were observed for rate-encoding and temporal encoding of speech sounds.",
    "actual_venue": "Speech Communication"
  },
  {
    "abstract": "This installment of Notes from the Community covers the first animated feature film rendered entirely in a game engine, bad design methods, customizable tattoos, debugging paper jams, and other topics to take your mind off your worries.",
    "actual_venue": "Ieee Pervasive Computing"
  },
  {
    "abstract": "Deep neural networks (DNNs) have risen to prominence, in the last few years, thanks to their very good performance on different classification and recognition tasks. However, their implementations suffer from long latency caused by the complexity of the network. Recently, many hardware implementations were introduced to accelerate the processing time of DNNs, and in particular of convolutional layers. While they can easily meet the timing constraint of real-time applications for small networks, complex models such as VGG and VGG-like networks are still out of reach. In this paper, we propose an technique to prune the neurons of each convolutional layer, also called activations, which directly contribute to the latency. Comparing networks with the same number of activations, we show that the activation-pruned networks perform better than the unpruned networks in terms of misclassification error.",
    "actual_venue": "Ieee Global Conference On Signal And Information Processing"
  },
  {
    "abstract": "A combination of multiple interconnected Hopfield networks is proposed to identify and control a class of control affine nonlinear systems, and is compared with two other single Hopfield networks each with different number of neurons. The control strategy is based on global linearising control in which the network models is used to synthesise a state feedback that will cancel out the nonlinearities yielding a linear model on which standard PID control can then be applied",
    "actual_venue": "Ijcnn"
  },
  {
    "abstract": "Concurrent programs are known to be difficult to test and maintain. These programs often fail because of concurrency bugs caused by non-deterministic interleavings among shared memory accesses. Even though a concurrency bug can be detected, it is still hard to isolate the root cause of the bug, due to the challenge in understanding the complex thread interleavings or schedules. In this paper, we propose a practical and precise isolation technique for concurrent bugs called Pinso that seeks to exploit the non-deterministic nature of concurrency bugs and accurately find the root causes of program error, to further help developers maintain concurrent programs. Pinso profiles runtime inter-thread interleavings based on a set of summarized memory access patterns, and then, isolates suspicious interleaving patterns in the triaging phase. Using a filtration-oriented scheduler, Pinso effectively eliminates false positives that are irrelevant to the bug. We evaluate Pinso with 11 real-world concurrency bugs, including single- and multi-variable violation, from sever/desktop concurrent applications (MySQL, Apache, and several others). Experiments indicate that our tool accurately isolates the root causes of all the bugs.",
    "actual_venue": "Icsme"
  },
  {
    "abstract": "Consider a finite t + r − 1 dimensional projective space PG(t + r − 1, s) over a Galois field GF(s) of order s = ϱh, where ϱ and h are positive integers and ϱ is the prime characteristic of the field. A collection of k points in PG (t + r − 1, s) constitutes an L(t, k)-set if no t of them are linearly dependent. An L(t, k)-set is maximal if there exists no other L(t, k′)-set with k′ > k. The largest k for which an L(t, k)-set exists is denoted by Mt(t + r, s). K. A. Bush [3] established that Mt(t, s) = t + 1 for t ⩾ s. The purpose of this paper is to generalize this result and study Mt(t + r, s) for t, r, and s in certain relationships.",
    "actual_venue": "Journal Of Combinatorial Theory Series A"
  },
  {
    "abstract": "This paper proposes a low power tone recognition suitable for automatic tonal speech recognizer (ATSR). The tone recognition estimates fundamental frequency (F-0) only from vowels by using a new magnitude difference function (MDF), called vowel-MDF. Accordingly, the number of operations is considerably reduced. In order to apply the tone recognition in portable electronic equipment, the tone recognition is designed using parallel and pipeline architecture.. Due to the pipeline and parallel computations, the architecture achieves high throughput and consumes low power. In addition, the architecture is able to reduce the number of input frames depending on vowels, making it moire adaptable depending on the maximum number of frames. The proposed architecture is evaluated with words selected from voice activation for GPS systems, phone dialing options, and words having the same phoneme but different tones. In comparison with the autocorrelation method, the experimental results show 35.7% reduction in power consumption and 27.1% improvement of tone recognition accuracy (110 words comprising 187 syllables). In comparison with ATSR without the tone recognition, the speech recognition accuracy indicates 25.0% improvement of ATSR with tone recogntion (2,250 training data and 45 testing words).",
    "actual_venue": "Ieice Transactions On Fundamentals Of Electronics Communications And Computer Sciences"
  },
  {
    "abstract": "The flow-structure interactions streaming from the motion of an immersed solid body are investigated through an Arbitrary Lagrangian-Eulerian (ALE) approach applied to the Lattice Boltzmann method (LBM). The method is based on the use of a moving grid to describe the flow around the solid body, while the physical domain is resolved by the use of an Eulerian frame fixed grid. The moving grid displacements follows the same moving law of the body, and its shape does not change during the simulation. The communication between the moving grid and the fixed grid is performed at the beginning of each time step through interpolation.The ALE-LBM approach has been derived from the discretized Boltzmann equation by a Chapman-Enskog expansion procedure, the equivalence of the proposed method with the Navier-Stokes equations for a weakly compressible athermal flow being recovered.Numerical simulations of academical test cases have been performed in order to assess the method and to investigate the sensitivity of the error to the simulation parameters. Three different test cases have been considered, in order to perform a robust assessment of the ALE-LBM approach. More specifically, the Uniform Flow, the Poiseuille Flow and the Plane Wave test cases have been studied and the limits of application of the approach have been defined and discussed.Finally, the case of a rotating two dimensional square cylinder immersed in a Poiseuille Flow, Re = 80 , has been numerically investigated. The results confirm that the ALE-LBM approach is able to correctly represent the physical flow features and, in particular, the transition zone between the two grids used is smooth and continuous, a sign that the error due to the interpolation process is bounded and does not diverge in time.",
    "actual_venue": "J Comput Physics"
  },
  {
    "abstract": "Describes a concurrent programming language, named IAda, for robot applications. IAda is based on the Ada language and includes two programming levels: the first one to perform concurrent applications with tasks or procedures written in Ada, and the second one to control the execution of the previous. To reach this second goal, the authors include, in the definition of this language, a constraint scheme whose semantic is to verify properties during the execution of the task level program. The authors describe this language and its construction to show the high portability of IAda",
    "actual_venue": "Iros"
  },
  {
    "abstract": "In this paper, we propose a Trustworthy Service Evaluation (TSE) system to enable users to share service reviews in service-oriented mobile social networks (S-MSNs). Each service provider independently maintains a TSE for itself, which collects and stores users' reviews about its services without requiring any third trusted authority. The service reviews can then be made available to interested users in making wise service selection decisions. We identify three unique service review attacks, i.e., linkability, rejection, and modification attacks, and develop sophisticated security mechanisms for the TSE to deal with these attacks. Specifically, the basic TSE (bTSE) enables users to distributedly and cooperatively submit their reviews in an integrated chain form by using hierarchical and aggregate signature techniques. It restricts the service providers to reject, modify, or delete the reviews. Thus, the integrity and authenticity of reviews are improved. Further, we extend the bTSE to a Sybil-resisted TSE (SrTSE) to enable the detection of two typical sybil attacks. In the SrTSE, if a user generates multiple reviews toward a vendor in a predefined time slot with different pseudonyms, the real identity of that user will be revealed. Through security analysis and numerical results, we show that the bTSE and the SrTSE effectively resist the service review attacks and the SrTSE additionally detects the sybil attacks in an efficient manner. Through performance evaluation, we show that the bTSE achieves better performance in terms of submission rate and delay than a service review system that does not adopt user cooperation.",
    "actual_venue": "Ieee Trans Parallel Distrib Syst"
  },
  {
    "abstract": "Many information tasks on the web require users to make use of multiple websites, for both content as well as information processing or visualization features. Programmers have created \"mashups,\" which customize or combine the functionality of multiple websites by extracting information from web pages or accessing web services APIs. Non-programmers lack the tools or skills create such customizations because of the programming obstacles involved. Marmite is a tool that empowers non-programmers to create functionality similar to those found in mashups.",
    "actual_venue": "Vl/Hcc"
  },
  {
    "abstract": "In this paper, an adaptive prescribed performance tracking control scheme is investigated for a class of output feedback nonlinear systems with input unmodeled dynamics based on dynamic surface control method. A transformation with respect to tracking error is introduced to implement prescribed performance tracking control. A novel description based on Lyapunov function for unmodeled dynamics is presented to deal with state unmodeled dynamics. The negative impact of nonlinear input unmodeled dynamics is offset by using a normalization signal. With the aid of Nussbaum function, the unknown control gain sign is effectively handled in the controller design. By the theoretical analysis, not only the signals in the closed-loop system are proved to be semi-globally uniformly ultimately bounded, but also the transient tracking performance can be guaranteed. Two simulation examples are provided to show the effectiveness of the proposed approach.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "The 5G mobile wireless networks are expected to provision distinct delay-bounded QoS guarantees for a wide range of multimedia services, applications, and users with extremely diverse requirements. On the other hand, how to efficiently support multimedia services over 5G wireless networks has imposed many new challenging issues not encountered before in 4G wireless networks. Bringing data contents...",
    "actual_venue": "Ieee Wireless Communications"
  },
  {
    "abstract": "Social Surroundings is an application that uses smartphones and online social networks to help eliminate social barriers and encourage natural communication in public places.",
    "actual_venue": "Multimedia, Ieee"
  },
  {
    "abstract": "We propose a navigation framework using the place recognition technique. We represent the world environment as linked images with visual line words. Navigation in the proposed system works by traversing a sequence of images. In attached video, we show our visual navigation system.",
    "actual_venue": "Urai"
  },
  {
    "abstract": "The executions of operating system services based on smart cards allow one to personalize some functionalities of the operating system by using the secret information stored in a smart card and the basic computations that a smart card can perform. However, current solutions for integrating smart card features in operating system services require at least a partial execution of the operating system functionalities at “user level”. Such executions decrease the security and the performance of the system as they are less robust compared to the kernel-level ones. In this paper we present the design and implementation of SmartK, a kernel module that integrates directly in the Linux kernel the support of smart cards. The use of SmartK allows one to securely personalize an operating system service still maintaining its execution at kernel level.",
    "actual_venue": "Trustbus"
  },
  {
    "abstract": "PTS is highly efficient in peak-to-average power ratio (PAPR) reduction of OFDM systems, but the considerable computational complexity for the required search through a high-dimensional vector space is potential problem for a practical implementation. In this paper, we formulate the search problem of PTS as a combinatorial optimization (CO) problem, then propose an algorithm based on improved discrete particle swarm optimization (I-DPSO), which reduces the number of multiplications required for PTS. Numerical results show that, the proposed algorithm can achieve performance-complexity tradeoff for moderate PAPR reduction.",
    "actual_venue": "Icnsc"
  },
  {
    "abstract": "This paper is concerned with the problem of determining the optimal size and composition of a permanent workforce needed to run a facility when demand is specified by a workstation group (WSG) for up to 24 hours a day, 7 days a week. For full-time employees, a solution is characterized by a bid job, which consists of a five-day-a-week schedule, a lunch break for all shifts, and a set of WSG task assignments for each of the half-hour periods in a shift. In contrast, each part-time employee may be given anywhere from one to six shifts during the week, and each shift may vary from four to eight hours in length. To facilitate supervision, all employees must be assigned to a home WSG, but when idle time exists in their schedules, they can be redeployed to other WSGs for a portion of the day. One of the complicating and unique factors addressed in this paper is the existence of nonsymmetric movement restrictions between WSGs. For example, an employee whose home base is A may be permitted to perform tasks at B, but not vice versa. Because the full problem could not be reduced to a single model, a multistage solution approach was developed. In the first stage, an extended shift-scheduling problem is solved to determine the optimal number of employees and their shifts. The results are postprocessed in subsequent stages to obtain lunch breaks, days off, and task assignments under WSG movement restrictions. In the implementation of the multistage approach, two alternatives were explored. The first was based on the idea of partitioning the WSGs into manageable clusters and then solving them in series. The second involved the direct solution of an integer programming formulation of the task assignment problem with home-base restrictions and WSG movement restrictions, but for a fixed workforce. An iterative scheme was used to adjust the size of the workforce until all constraints were satisfied and overall optimality was achieved. Testing was done with data provided by the U.S. Postal Service (USPS) mail processing and distribution center (P&DC) in Dallas. The computations showed that the second alternative always yielded the smaller workforce and was always able to find good solutions within 30 minutes.",
    "actual_venue": "Andsom-Manufacturing And Service Operations Management"
  },
  {
    "abstract": "We consider various single machine scheduling problems in which the processing time of a job depends either on its position in a processing sequence or on its start time. We focus on problems of minimizing the makespan or the sum of (weighted) completion times of the jobs. In many situations we show that the objective function is priority-generating, and therefore the corresponding scheduling problem under series-parallel precedence constraints is polynomially solvable. In other situations we provide counter-examples that show that the objective function is not priority-generating.",
    "actual_venue": "Journal Of Scheduling"
  },
  {
    "abstract": "Tensor-variate regression approaches have been spotlighted over the past years, due to the fact that many challenging regression tasks in the real world involve in high-order tensorial data. However, these approaches are often computationally prohibitive, which limits the predictive performance for large data sets. In this paper, we propose a computationally-efficient tensor-variate regression approach in which the latent function is flexibly modeled by using online local Gaussian process (OLGP). By doing so, the large data set is efficiently processed by constructing a number of small-sized GP experts in an online fashion. Furthermore, we introduce two efficient search strategies to find local GP experts to make accurate predictions with a Gaussian mixture representation. Finally, we evaluate our approach on a real-life regression task, reconstruction of limb movements from brain signal, to show its effectiveness and scalability for large data sets.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "This paper proposes a novel method for classifying six categories of patterns of fluorescence staining of a HEp-2 cell. The proposed method is constructed as a combination of the powerful rotation invariant co-occurrence among adjacent local binary pattern (RIC-LBP) image feature and a linear support vector machine (SVM). RIC-LBP provides high descriptive ability and robustness against local rotations of an input cell image. To further deal with global rotation, we synthesize many training images by rotating the original training images and constructing the SVM using both the original and synthesized images. The proposed method has the following advantages: (1) robustness against uniform changes in intensity of an input cell image, (2) invariance under local and global rotation of the image, (3) low computational cost, and (4) easy implementation. The proposed method was demonstrated to be effective through evaluation experiments using the MIVIA HEp-2 images dataset and comparison with typical state-of-the-art methods.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "Remote-sensing (RS)-based agricultural drought indicators, such as the normalized difference vegetation index (NDVI), normalized difference water index (NDWI), enhanced vegetation index (EVI), and normalized difference drought index (NDDI), have been popularly used in agricultural drought monitoring, analysis and related applications. Understanding the relationships between these indicators and root zone soil moisture under different canopies will help reduce uncertainties and enhance reliability of estimating soil moisture levels over a large area, and thus improve the accuracy in drought monitoring, analysis, and forecasting. This research aims to investigate such relationships and choose a drought indicator that best correlates with soil moisture observed at various depths from the soil climate analysis network (SCAN) sites, with 0-64 days of lagging periods. Results from the study show that the indicators applied to corn led to statistically significant correlations with soil moisture at deeper depths and also the soil moisture memory can be kept for a long period of time (32-48 days), while the indicators applied to soybeans correlated best with soil moisture at shallower depths and the soil moisture memory can only be kept for a short time (concurrent to 16 days).",
    "actual_venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of  "
  },
  {
    "abstract": "In GIS-based data-driven modeling of mineral prospectivity, a suitably fine unit cell size is used for spatial representation of known occurrences of mineral deposits of the type sought (D) in a study area (T). However, until now, the unit cell size is chosen subjectively. In this paper, a methodology is proposed for objective selection of the most suitable unit cell size for data-driven modeling of mineral prospectivity using a raster-based GIS. A set of choices of suitable unit cell sizes is first derived via point pattern analysis of a set of known occurrences of mineral deposits of the type sought. Then, (a) the lower limit of a set of choices of suitable unit cell sizes is considered and defined according to the map scales from which spatial data for mineral prospectivity mapping were derived, and (b) the upper limit of the same set of choices of suitable unit cell sizes is considered (and revised as necessary) based on knowledge of spatial extents of mineral deposits of the type sought or via analysis of reflexive nearest neighbour points. Finally, it is shown that fractal analysis of spatial contrast between unit cells containing D and unit cells not containing D in T provides for objective selection of the most suitable unit cell size. In a case study application of the weight-of-evidence method to mineral prospectivity mapping, using the most suitable unit cell size, found via the proposed methodology, results in spatial evidence weights and weight uncertainties that are nearly identical to those derived by using the finest (i.e., lower limit) unit cell size. In contrast to using the most suitable unit cell size, using coarser unit cell sizes result in higher positive weights, lower negative weights and higher weight uncertainties of spatial evidence of mineral prospectivity. The proposed methodology for objective selection of the most suitable unit cell size in data-driven modeling of mineral prospectivity using a raster-based GIS is robust and can easily be implemented.",
    "actual_venue": "Computers And Geosciences"
  },
  {
    "abstract": "In this paper, we propose a Scale and Rotation Invariant Implicit Shape Model (SRIISM), and develop a local feature matching based system using the model to accurately locate and identify large numbers of object instances in an image. Due to repeated instances and cluttered background, conventional methods for multiple object instance identification suffer from poor identification results. In the proposed SRIISM, we model the joint distribution of object centers, scale, and orientation computed from local feature matches in Hough voting, which is not only invariant to scale changes and rotation of objects, but also robust to false feature matches. In the multiple object instance identification system using SRIISM, we apply a fast 4D bin search method in Hough space with complexity O(n), where n is the number of feature matches, in order to segment and locate each instance. Furthermore, we apply maximum likelihood estimation (MLE) for accurate object pose detection. In the evaluation, we created datasets simulating various industrial applications such as pick-and-place and inventory management. Experiment results on the datasets show that our method outperforms conventional methods in both accuracy (5%-30% gain) and speed (2x speed up).",
    "actual_venue": "Computer Vision - Eccv Workshops, Pt"
  },
  {
    "abstract": "We consider a low data rate non-coherent impulse radio ultra-wideband (IR-UWB) system based on binary pulse position modulation (2-PPM), which applies sparse codes to enable code division multiple access (CDMA). The suitability of sparse codes (i.e., codes with a low code weight) is investigated considering multi-user interference (MUI) and multipath propagation. The decoding of the particular CDMA code takes place after non-coherent combining. Different sparse codes such as time hopping (TH) random codes, TH codes constructed from M-sequences, and optical orthogonal codes are employed. We propose a semi-analytical performance analysis method using Gaussian approximation and the statistics of the code collisions to obtain the bit error rate expressions, which is much more accurate than the code correlation function. The multiple access performance is analyzed in terms of the signal-to-noise ratio (SNR) as well as the number of supported users.",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "Adaptive algorithms are an important technique to achieve portable high performance. They choose among solution methods and optimizations according to expected performance on a particular machine. Grid environments make the adaptation probleln harder, because The optimal decision inay change across runs and even during runtime. Therefore, the performance model used by an adaptive algorithm must be able to change decisions without high overhead. In this paper, we present work that is modifying previous research into rapid performance modeling to support adaptive grid applications through sampling and high granularity modeling. We also outline preliminary results that show the ability to predict differences in performance among algorithms in the same program.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": "Abstractó Hybrid Wireless-Optical Broadband Access Net- work (WOBAN) is a combination of wireless and optical network to optimize the cost and performance of an access network. Wireless nodes collect trafc from end users and carry them to the optical part of a WOBAN using multiple hops accumulating delay at each wireless node. Moreover, the radio-capacity on each wireless link limits the capacity on each outgoing link from the node in a single-radio Wireless Mesh Network (WMN) of a WOBAN. Thus, delay and capacity limitation in the WMN of a WOBAN is a major bottleneck. We design a capacity and delay aware routing scheme, CaDAR, to minimize the delay and increase network support in the WMN of a WOBAN. Our analysis shows that CaDAR is an efcient routing scheme for a simgle-radio WMN for aWOBANthat can support much higher load and has lower system delay than other approaches [1] because of better load balanced routing. Keywords: Wireless-optical hybrid network, routing, delay,",
    "actual_venue": "Beijing"
  },
  {
    "abstract": "It is studied that several constructive heuristics for solving the sequence dependent setup time flowshop problem with the objective of minimizing makespan. Three priority rules imbedded in the heuristics are tested and a tie-breaking strategy is examined. The experimental results on benchmarks show that the priority rules are helpful to improve the performance, especially for the instances in which setup times are averagely smaller than the average processing time. The results also show that the setup times have a large effect on the performance of the heuristics.",
    "actual_venue": "IRI"
  },
  {
    "abstract": "Ontology serves as the blueprint for knowledge dissemination within and across domains and applications. In general, acquisition of ontological knowledge can be automated in many domains. This paper presents a scenario where we semiautomatically instantiate an ontology associated with meta-cognitive skills, with assistance from human experts. We argue why such semi-automatic instantiation of ontologies is a practicable approach to knowledge acquisition.",
    "actual_venue": "Ieee International Conference On Advanced Learning Technologies, Proceedings"
  },
  {
    "abstract": "An increasing number of biological machines have been revealed to have more than two macroscopic states. Quantifying the underlying multiple-basin functional landscape is essential for understanding their functions. However, the present models seem to be insufficient to describe such multiple-state systems. To meet this challenge, we have developed a coarse grained triple-basin structure-based model with implicit ligand. Based on our model, the constructed functional landscape is sufficiently sampled by the brute-force molecular dynamics simulation. We explored maltose-binding protein (MBP) which undergoes large-scale domain motion between open, apo-closed (partially closed) and holo-closed (fully closed) states responding to ligand binding. We revealed an underlying mechanism whereby major induced fit and minor population shift pathways co-exist by quantitative flux analysis. We found that the hinge regions play an important role in the functional dynamics as well as that increases in its flexibility promote population shifts. This finding provides a theoretical explanation of the mechanistic discrepancies in PBP protein family. We also found a functional \"backtracking\" behavior that favors conformational change. We further explored the underlying folding landscape in response to ligand binding. Consistent with earlier experimental findings, the presence of ligand increases the cooperativity and stability of MBP. This work provides the first study to explore the folding dynamics and functional dynamics under the same theoretical framework using our triple-basin functional model.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "We present a method for specialising the constraints in constrained Horn clauses with respect to a goal. We use abstract interpretation to compute a model of a query-answer transformation of a given set of clauses and a goal. The effect is to propagate the constraints from the goal top-down and propagate answer constraints bottom-up. Our approach does not unfold the clauses at all; we use the constraints from the model to compute a specialised version of each clause in the program. The approach is independent of the abstract domain and the constraints theory underlying the clauses. Experimental results on verification problems show that this is an effective transformation, both in our own verification tools (convex polyhedra analyser) and as a pre-processor to other Horn clause verification tools.",
    "actual_venue": "Sci Comput Program"
  },
  {
    "abstract": "A method is proposed to protect MPEG video quality from packet loss for real-time transmission over the Internet. Because MPEG uses inter-frame coding, relatively small packet loss rates in IP transmission can dramatically reduce the quality of the received MPEG video. In the proposed high-priority protection (HiPP) method, the MPEG video stream is split into high- and low-priority partitions, using a technique similar to MPEG-2 data partitioning. Overhead resilient data for the MPEG video stream is created by applying forward error correction coding to only the high-priority portion of the video stream. The high- and low-priority data, and resilient data, are sent over a single channel, using a packetization method that maximizes resistance to burst losses, while minimizing delay and overhead. Because the proposed method has low delay and does not require re-transmission, it is well suited for interactive and multicast applications. Simulations were performed comparing the improvement in video quality using the HiPP method, using experimental Internet packet loss traces with loss rates in the range of 0-8.5%. Overhead resiliency data rates of 0%, 12.5%, 25%, and 37.5% were studied, with different compositions of the overhead data for the 25% and 37.5% overhead rates, in an attempt to find the \"best\" composition of the overhead data. In the presence of packet loss, the received video quality, as measured by PSNR and the Negsob measure, was significantly improved when the HiPP method was applied. (C) 1999 Elsevier Science B.V. All rights reserved.",
    "actual_venue": "Signal Processing: Image Communication"
  },
  {
    "abstract": "We consider an efficient Bayesian approach to estimating integration-based posterior summaries from a separate Bayesian application. In Bayesian quadrature we model an intractable posterior density function f(·) as a Gaussian process, using an approximating function g(·), and find a posterior distribution for the integral of f(·), conditional on a few evaluations of f (·) at selected design points. Bayesian quadrature using normal g (·) is called Bayes-Hermite quadrature. We extend this theory by allowing g(·) to be chosen from two wider classes of functions. One is a family of skew densities and the other is the family of finite mixtures of normal densities. For the family of skew densities we describe an iterative updating procedure to select the most suitable approximation and apply the method to two simulated posterior density functions.",
    "actual_venue": "Statistics And Computing"
  },
  {
    "abstract": "LetV be a set inRn consisting of finitely many hyperplanes. The linear recognition problem given byV is to determine, using ternary comparisons of the form “f(x):0” wheref:Rn →R is a linear function, whether a pointxεRn is inV. We consider lower bounds on the number of comparisons whenV corresponds to some NP-complete problems. A technique is proposed for proving such bounds. If the tests “f(x):0” are restricted so thatf always defines some hyperplane inV, then some NP-complete problems are shown to have exponential lower bounds inn. Examples of larger classes of linear test functions are found such that the exponential lower bounds are still valid.",
    "actual_venue": "BIT"
  },
  {
    "abstract": "In this manuscript, we analyze the dynamical anomalies of a parametric family of iterative schemes designed by Kou et al. It is known that its order of convergence is three for any arbitrary value of the parameter, but it has order four (and it is optimal in the sense of Kung–Traub’s conjecture) when a specific value is selected. Among all the elements of this family, one can choose this fourth-order element or any of the infinite members of third order of convergence, if only the speed of convergence is considered. However, the stability of the methods plays an important role in their reliability when they are applied on different problems. This is the reason why we analyze in this paper the dynamical behavior on quadratic polynomials of the mentioned family. The study of fixed points and their stability, joint with the critical points and their associated parameter planes, show the richness of the class and allow us to find members of it with excellent numerical properties, as well as other ones with very unstable behavior. Some test functions are analyzed for confirming the theoretical results.",
    "actual_venue": "Journal Of Computational And Applied Mathematics"
  },
  {
    "abstract": "Navigation satellites are a core component of navigation satellite-based systems such as Global Positioning System, Global Navigation Satellite System and Galileo, which provide location and timing information for a variety of uses. Such satellites are designed for operating on orbit to perform tasks and have lifetimes of 10years or more. Reliability, availability and maintainability analysis of systems has been indispensable in the design phase of satellites in order to achieve minimum failures or to increase mean time between failures and thus to plan maintenance strategies, optimise reliability and maximise availability. In this paper, we present formal models of both a single satellite and a navigation satellite constellation and logical specification of their reliability, availability and maintainability properties, respectively. The probabilistic model checker PRISM has been used to perform automated analysis of these quantitative properties. Copyright (c) 2014 John Wiley & Sons, Ltd.",
    "actual_venue": "Quality And Reliability Engineering International"
  },
  {
    "abstract": "This paper presents a novel Interaction Reproducing Model (IRM) for the purpose of adjusting computer user support to match the state of the user, and it consists of a set of Interaction Finite State Machines (I-FSMs). Each I-FSM is trained using actual interaction records, and it represents an ideal interaction pattern for a user state. It can choose an appropriate system action by reproducing the interaction pattern of the I-FSM most similar to the current interaction. We developed a prototype teaching system, and conducted preliminary experiments. The results show that user impressions using our approach were better than when using the system without our approach.",
    "actual_venue": "PCM"
  },
  {
    "abstract": "Recently non-coding RNA (ncRNA) genes have been found to serve many important functions in the cell such as regulation of gene expression at the transcriptional level. Potentially there are more ncRNA molecules yet to be found and their possible functions are to be revealed. The discovery of ncRNAs is a difficult task because they lack sequence indicators such as the start and stop codons displayed by protein-coding RNAs. Current methods utilize either sequence motifs or structural parameters to detect novel ncRNAs within genomes. Here, we present an ab initio ncRNA finder, named ncRNAscout, by utilizing both sequence motifs and structural parameters. Specifically, our method has three components: (i) a measure of the frequency of a sequence, (ii) a measure of the structural stability of a sequence contained in a t-score, and (iii) a measure of the frequency of certain patterns within a sequence that may indicate the presence of ncRNA. Experimental results show that, given a genome and a set of known ncRNAs, our method is able to accurately identify and locate a significant number of ncRNA sequences in the genome. The ncRNAscout tool is available for downloading at http://bioinformatics.njit.edu/ncRNAscout.",
    "actual_venue": "Genomics, Proteomics And Bioinformatics"
  },
  {
    "abstract": "Effective and robust visual tracking is one of the most important tasks for the intelligent visual surveillance. In this paper, we proposed a novel method for detecting and tracking moving people using the spatiotemporal latent semantic cues and the incremental eigenspace tracking techniques. During tracking process, the target appearance model is incrementally learned in low dimensional tensor eigenspace by adaptively updating the eigenbasis and sample mean. At the same time, the spatiotemporal latent semantic cues calibrate the estimation of tracking and detect new moving people coming in the same surveillance scene. Experiment results show that with the calibration based on spatiotemporal latent semantic cues, the proposed method can track the moving people automatically and effectively.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "Three-dimensional (3-D) digital images and patterns under transformations are facilitated by the splitting- shooting method (SSM) and the splitting- integration method (SIM), The combination (CSIM) of using both SSM and SIM and two combinations (CIIM) of using SIM only are proposed for a cycle conversion T-1T, where T is a nonlinear transformation, and T-1 is its inverse transformation. This paper focuses on exploitation of accuracy of obtained image greyness. In our discrete algorithms, letting a 3-D pixel be split into N-3 subpixels, the convergence rates, O(1/N), O(1/N-2), and O(1/N-3); of sequential error can be achieved by the three combinations respectively. High convergence rates indicate less CPU time needed. Both error bounds and computation of pixel greyness have shown the significance of the proposed new algorithms.",
    "actual_venue": "Ieee Transactions On Systems, Man, And Cybernetics, B: Cybernetics"
  },
  {
    "abstract": "PageRank problem is the cornerstone of Google search engine and is usually stated as solving a huge linear system. Moreover, when the damping factor approaches 1, the spectrum properties of this system deteriorate rapidly and this system becomes difficult to solve. In this paper, we demonstrate that the coefficient matrix of this system can be transferred into a block form by partitioning its rows into special sets. In particular, the off-diagonal part of the block coefficient matrix can be compressed by a simple low-rank factorization, which can be beneficial for solving the PageRank problem. Hence, a matrix partition method is proposed to discover the special sets of rows for supporting the low-rank factorization. Then a preconditioner based on the low-rank factorization is proposed for solving difficult PageRank problems. Numerical experiments are presented to support the discussions and to illustrate the effectiveness of the proposed methods.",
    "actual_venue": "Journal Of Computational And Applied Mathematics"
  },
  {
    "abstract": "The accessibility of project knowledge obtained from experiences is an important and crucial issue in enterprises. This information need about project knowledge can be different from one person to another depending on the different roles he or she has. Therefore, a new ontology-based case-based reasoning OBCBR approach that utilises an enterprise ontology is introduced in this article to improve the accessibility of this project knowledge. Utilising an enterprise ontology improves the case-based reasoning CBR system through the systematic inclusion of enterprise-specific knowledge. This enterprise-specific knowledge is captured using the overall structure given by the enterprise ontology named ArchiMEO, which is a partial ontological realisation of the enterprise architecture framework EAF ArchiMate. This ontological representation, containing historical cases and specific enterprise domain knowledge, is applied in a new OBCBR approach. To support the different information needs of different stakeholders, this OBCBR approach has been built in such a way that different views, viewpoints, concerns and stakeholders can be considered. This is realised using a case viewpoint model derived from the ISO/IEC/IEEE 42010 standard. The introduced approach was implemented as a demonstrator and evaluated using an application case that has been elicited from a business partner in the Swiss research project.",
    "actual_venue": "Enterprise Is"
  },
  {
    "abstract": "We present an approach to automatically extract a pertinent subset of soft output classifiers, and to aggregate them into a global decision rule using the Choquet integral. This approach relies on two key points. The first is a learning algorithm that uses a measure of the confusion between the categories to be recognized. The second is a selection scheme that discards weak or redundant decision rules, keeping only the most relevant subset. An experimental study, based on real world data, is then described. It analyzes the improvements achieved by these points first when used independently, then when combined together.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "Today's power grid is facing many challenges due to increasing load growth, aging of existing power infrastructures, high penetration of renewable, and lack of fast monitoring and control. Utilizing recent developments in Information and Communication Technologies (ICT) at the power-distribution level, various smart-grid applications can be realized to achieve reliable, efficient, and green power. Interoperable exchange of information is already standardized in the globally accepted smart-grid standard, IEC 61850, over the local area networks (LANs). Due to low installation cost, sufficient data rates, and ease of deployment, the industrial wireless LAN technologies are gaining interest among power utilities, especially for less critical smart distribution network applications. Extensive work is carried out to examine the wireless LAN (WLAN) technology within a power distribution substation. The first phase of the work is initiated with the radio noise interference measurements at 27.6- and 13.8-kV distribution substations, including circuit breaker switching operations. For a detailed investigation, the hardware prototypes of WLAN-enabled IEC 61850 devices are developed using industrial embedded systems, and the performance of smart distribution substation monitoring, control, and protection applications is analyzed for various scenarios using a round trip-time of IEC 61850 application messages. Finally, to examine the real-world field performance, the developed prototype devices are installed in the switchyard and control room of 27.6 power distribution substation, and testing results of various applications are discussed.",
    "actual_venue": "Ieee Trans Industrial Informatics"
  },
  {
    "abstract": "We discuss a way to set up a reliable 3D office scene recognition and interpretation scheme for a home robot using artificial vision only. It is a very difficult problem which has been studied by numerous scientists for many years. We do not pretend to describe the solution of this problem in the present paper, but simply to express our state of mind. Our efforts are concentrated on the manner to produce a well posed problem and on the use of an analysis/synthesis feedback loop in order to solve it. It is expected to produce a more robust solution than the visual open loop solutions already available in the literature",
    "actual_venue": "Graphics And Robotics"
  },
  {
    "abstract": "In this paper, we propose a novel zero-correlation zone (ZCZ) code, called m-ZCZ code. Both auto-correlation function (ACF) side lobes and cross-correlation function (CCF) of the code are zero within ZCZ, whose length can be adjusted in a very.&#39;flexible manner. The m-ZCZ code can be generated using a simple method. If compared with conventional binary ZCZ codes, the m-ZCZ code can support more use...",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "Iron and steel industry is an essential and sizable sector for industrialized economies. Since it is capital and energy extensive, companies have been putting consistent emphasis on technology advances in the production process to increase productivity and to save energy. The modern integrated process of steelmaking, continuous casting and hot rolling (SM–CC–HR) directly connects the steelmaking furnace, the continuous caster and the hot rolling mill with hot metal flow and makes a synchronized production. Such a process has many advantages over the traditional cold charge process. However, it also brings new challenges for production planning and scheduling. In this paper we first give a comparative analysis of the production processes and production management problems for the SM–CC–HR and the traditional cold charge process. We then review planning and scheduling systems developed and methods used for SM–CC–HR production. Finally some key issues for further research in this field are discussed.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Let a given collection of sets have size N measured by the sum ofthe cardinalities. Yellin and Jutla presented an algorithm which constructedthe partial order induced by the subset relation (a &quot;subsetgraph&quot;) in O(N2= log N) operations over a dictionary ADT, and exhibiteda collection whose subset graph had \\Theta(N2= log2N) edges. Thispaper establishes a matching upper bound on the number of edges ina subset graph, shows that the known bound on Yellin and Jutla'salgorithm is...",
    "actual_venue": "J Algorithms"
  },
  {
    "abstract": "The interactive simulation of deformable solids has become a major working area in Computer Graphics. We present a sophisticated material law, better suited for dynamical computations than the standard approaches. As an important example, it is employed to reproduce measured material data from biological soft tissue. We embed it into a state-of-the-art finite element setting employing an adaptive basis. For time integration the use of an explicit stabilized Runge-Kutta method is proposed.",
    "actual_venue": "Symposium On Computer Animation"
  },
  {
    "abstract": "Information and Communications Technologies (ICT) penetration is growing at exponential rates and affecting societies, countries and organizations, which has led to a need for understanding whether they contribute to development. To ascertain whether ICT are contributing to development, the example of a current ICT, Twitter is used, along with the aim of this research: To understand and explain how public sector organizations are adopting and using online social networks; namely twitter, for the delivery of e-government services that will provide a better world to live in the Omani public sector. By considering this aim, we attempt to explain whether Twitter, contributes towards the creation of a 'better world' to live in, or leads to diverse outcomes in a developing country, Oman. To achieve the aim, we used two public sector organizations workforces' experiences and applied the Choice Framework (CF) developed by Kleine [1]. For the research approach, we employed a qualitative approach and the data collection techniques, reference to archival documents, interviews, photographic evidence and observations. The analysis was completed using the lens of interpretivism, socio-materiality along with grounded theory concepts. The study reveals that ICT4D is providing a better world for most of the citizens, but for the providers of the improved e-government services, it implies aligning local practices to the technology, which affects their home/work life balance. The contributions of this research lie in emphasising largely how the use of Twitter in Oman will lead to development. The Choice Framework selected for our understanding was adapted and led to diverse results to those mentioned in previous ICT4D studies; therefore, our research makes a contribution of understanding ICT4D in an e-government context, which was amiss in the previous frameworks. For businesses, our findings inform practitioners on the ICT Technologies areas that need attention while implementing them within an environment similar to Oman's public sector. For policymakers, this research informs of the areas that require policymakers' attention when placing their efforts where they are best served.",
    "actual_venue": "Information And Communication Technologies And Development"
  },
  {
    "abstract": "Poor sitting postures influence one's health and can cause upper limb and neck disorder. Current solutions for siting posture recognition, however, are impractical due to intrusiveness, high cost or low generalization capability. Particularly, most of the existing solutions are chair-dependent, which are highly coupled with certain types of chairs. In this paper, we design Postureware, a smart cushion, which is a low-cost, non-intrusive and general sitting posture recognition system. In particular, Postureware incorporates very thin pressure sensors to offer non-intrusive experience, an effective sensor placement solution to reduce cost, a set of user-invariant features and an ensemble learning classifier to improve generalization ability. We implement a prototype system and conduct extensive experiments. The results show that Postureware can classify fifteen fine-grained postures with high accuracy.",
    "actual_venue": "Ieee International Conference On Pervasive Computing And Communications Workshops (Percom Workshops"
  },
  {
    "abstract": "The portable computer or notebook has become an integral and even essential aspect of modern life. Year-in-year its price to the consumer falls while its performance grows, yet recent analysis suggests that pound-for-pound its environmental costs are amongst the highest of any product on the planet. In this paper we explore the market price of ''sustainable'' notebooks. Drawing on the framework of the 'tragedy of the commons' we postulate that as manufacturers shift costs away from the commons to comply with sustainability standards, the cost to the consumer will inevitably rise. We test our hypothesis by comparing the prices of EPEAT Gold certified notebooks with uncertified portable computers. The results are discussed, alternative hypotheses explored and further research outlined.",
    "actual_venue": "J Strategic Inf Sys"
  },
  {
    "abstract": "We study cooperating distributed systems (CD-systems) of stateless deterministic restarting automata with window size 1 that are equipped with an external pushdown store. In this way we obtain an automata-theoretical characterization for the class of word languages that; are linearizations of context-free trace languages.",
    "actual_venue": "Theoretical Informatics And Applications"
  },
  {
    "abstract": "Under the development of the Digital Society and with the aim of achieving a true transition from the Information Society to the Knowledge Society, ICTs play a capital role in educational and knowledge management processes in any kind of entity, from Small and Medium-sized Enterprises to the Public Administration. The Spanish Public Administration is composed by a huge amount of the heterogeneous public organisms that range from research centres to public companies. The purpose of this paper is to provide an analysis about the knowledge management in the Spanish Public Administration through several real study cases developed in different public organisms. The analysis shows how the case studies follow the Suricata Architecture in order to identify success factors to replicate them in other public contexts.",
    "actual_venue": "Teem"
  },
  {
    "abstract": "The emerging cloud computing service market aims at delivering computing resources as a utility over the Internet with a high quality. It has evolving unknown demand that is typically highly uncertain. Traditional provisioning methods either make idealized assumption of the demand distribution or rely on extensive offline statistical analysis of historical data. In this paper, we present an online adaptive learning approach to address the optimal resource provisioning problem. Based on a stochastic loss model of the cloud services, we formulate the provisioning problem from a revenue management perspective, and present a stochastic gradient-based learning algorithm that adaptively adjusts the provisioning solution as observations of the demand are continuously made. We show that our adaptive learning algorithm guarantees optimality and demonstrate through simulation that they can adapt quickly to non-stationary demand.",
    "actual_venue": "Sigmetrics Performance Evaluation Review"
  },
  {
    "abstract": "The nearest neighbor searching problem (also called the post office problem) calls for organizing the set P of N points in k-space so that the nearest neighbor in P to a new point can be quickly found. Friedman, Baskett and Shustek describe an algorithm for nearest neighbor searching based on projecting the points onto the various coordinate axes; their analysis of this method shows that a nearest neighbor search can be performed in O(N1–1/k) expected time, for any fixed dimension k>1 under a variety of probability distributions. In this paper we shall prove the stronger (worst-case) result that the total time required by (an extension of) their method to find the nearest neighbor of every point in any fixed k-dimensional point set is O(N2–1/k), which immediately implies a result similar to theirs. The above results hold only for the L metric; we also investigate the Euclidean (L2) metric. Our first result for that metric shows that the above analysis does not hold in general, and our second result then goes on to show that the analysis does in fact apply in practice, because of the finite word-length restrictions of real computers.",
    "actual_venue": "Icalp"
  },
  {
    "abstract": "Arguably, the most effective technique to ensure wide adoption of a concept (or product) is by repeatedly exposing individuals to messages that reinforce the concept (or promote the product). Recognizing the role of repeated exposure to a message, in this paper we propose a novel framework for the effective placement of content: Given the navigational patterns of users in a network, e.g., web graph, hyperlinked corpus, or road network, and given a model of the relationship between content-adoption and frequency of exposition, we define the repetition-aware content-placement (RACP) problem as that of identifying the set of B nodes on which content should be placed so that the expected number of users adopting that content is maximized. The key contribution of our work is the introduction of memory into the navigation process, by making user conversion dependent on the number of her exposures to that content. This dependency is captured using a conversion model that is general enough to capture arbitrary dependencies. Our solution to this general problem builds upon the notion of absorbing random walks, which we extend appropriately in order to address the technicalities of our definitions. Although we show the RACP problem to be NP-hard, we propose a general and efficient algorithmic solution. Our experimental results demonstrate the efficacy and the efficiency of our methods in multiple real-world datasets obtained from different application domains.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "Systems combining an interval narrowing solver and a linear programming solver can tackle constraints over the reals that none of these solvers can handle on their own. In this paper we introduce a cooperating scheme where an interval narrowing solver and a linear programming solver work concurrently. Information exchanged by the solvers is therefore handled as soon as it becomes available. Moreover, to improve the pruning, the linear programming solver computes the actual range of values of each variable with respect to the subset of linear constraints. To validate the proposed architecture a prototype system-named CCC-has been developed. Several examples are given to illustrate the gain in speed and precision we can expect with CCC.",
    "actual_venue": "Reliable Computing"
  },
  {
    "abstract": "Although the initiative of Autonomic Computing was introduced a dozen years ago, several challenges remain open. One of these challenges is the efficient monitoring at runtime oriented to the detection, diagnosis, and repair of problems that result from failures or bugs in software and/or hardware components. For this purpose, Communication-induced Checkpointing (CIC) can be a useful tool. Communication-induced Checkpointing has been used to attack a wide range of problems that arise in distributed systems, such as rollback recovery, software debugging and software verification, among others. In CIC algorithms, an autonomic component (process) asynchronously cooperates by exchanging information on the application messages about saved local states called checkpoints. CIC aims to form global consistent snapshots by grouping checkpoints (one by each component) in a non-coordinated way. To achieve this, CIC solutions continuously monitor the exchanged control information to identify possible dangerous checkpointing patterns. When a dangerous pattern is identified, it is broken by locally triggering a forced checkpoint. Nevertheless, as we will show, not all forced checkpoints triggered by current solutions are necessary. In this paper, we present a delayed checkpoint approach suitable for autonomic computing that reduces forced checkpoints by establishing certain triggering rules that we call safe checkpoint conditions. Finally, some results are presented which show that our proposal is more efficient than other current solutions.",
    "actual_venue": "Wetice"
  },
  {
    "abstract": "We prove that every two-dimensional permutive cellular automaton is conjugate to a one-sided shift with compact set of states.",
    "actual_venue": "J Comput Syst Sci"
  },
  {
    "abstract": "Large-scale artificial neural networks (ANNs) have been used to mimic the information processing function of the brain. Spiking neural networks (SNNs) are a kind of ANN, which mimic real biological neural networks, conveying information through the communication of short pulses between neurons. Since each neuron in these networks is connected to thousands of others, high bandwidth is required. Moreover, since the spike times are used to encode information in SNN, very low communication latency is also required. The 2D-NoC was used as a solution to provide a scalable interconnection fabric in large-scale parallel SNN systems. The 3D-ICs have also attracted a lot of attention as a potential solution to resolve the interconnect bottleneck. The combination of these two emerging technologies provides a new horizon for IC designs to satisfy the high requirements of low-power and small footprint in emerging AI applications. This paper first presents an analytical model to analyze the performance of different neural network topologies and compare it with a system-level simulation. Second, we present an architecture and a low-latency routing algorithm for spike traffic routing in 3D-NoC of spiking neurons (3DNoC-SNN). The 3DNoC-SNN is validated based on an RTL-level implementation, while area/power analysis is performed using 45-nm CMOS technology.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "BackgroundMany algorithms have been developed to infer the topology of gene regulatory networks from gene expression data. These methods typically produce a ranking of links between genes with associated confidence scores, after which a certain threshold is chosen to produce the inferred topology. However, the structural properties of the predicted network do not resemble those typical for a gene regulatory network, as most algorithms only take into account connections found in the data and do not include known graph properties in their inference process. This lowers the prediction accuracy of these methods, limiting their usability in practice.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "We construct a finite element like scheme for fully nonlinear integro-partial differential equations arising in optimal control of jump-processes. Special cases of these equations include optimal portfolio and option pricing equations in finance. The schemes are monotone and robust. We prove that they converge in very general situations, including degenerate equations, multiple dimensions, relatively low regularity of the data, and for most (if not all) types of jump-models used in finance. In all cases we provide (probably optimal) error bounds. These bounds apply when grids are unstructured and integral terms are very singular, two features that are new or highly unusual in this setting.",
    "actual_venue": "Siam J Numerical Analysis"
  },
  {
    "abstract": "In this work we present a novel design for theater-sized interactive fulldome system, the EONVision Idome. For edutainment, it combines Hollywood style story telling with cutting edge technologies, providing a group of audience the feast of immersive 4D theater experience. For industrial training, the fully immersive and interactive dome delivers real-time VR tutorials for enhanced training experience. Compared with traditional VR training facilities like CAVE and EON Icube, it hosts larger group of trainees for lower training cost per capita, and provides the means to conduct collaborative training with multiple trainees.",
    "actual_venue": "Vric"
  },
  {
    "abstract": "In this work we study the electron density localization/delocalization in HF clusters. In order to do this, we have employed two methods, the electron localization function (ELF) and the quantum mechanics local moment representation. The first one is the most widely used method to study electronic localization/delocalization in molecular systems; whereas, the local moment representation provides a tridimensional representation of the volumes in the molecular space where electronic localization occurs. Our results show the existence of correlation between both, ELF and local moment representation results. Also, it is found that at the HB critical point, the electron density localization increases with the cluster size, which is associated with the cooperative effects observed in the cyclic HF clusters. The set of results discussed in this work lead us to conclude that the enlargement in the intermolecular electronic localization goes along with an increase in the HB covalency and in the electronic delocalization throughout the HB cluster, which leads to larger system stabilization: Larger the intermolecular electronic localization, greater the cooperative effects observed in the HB clusters.",
    "actual_venue": "Journal Of Computational Methods In Sciences And Engineering"
  },
  {
    "abstract": "Prediction of extreme events is a highly important and challenging problem in science, engineering, finance, and many other areas. The observed extreme events in these areas are often associated with complex nonlinear dynamics with intermittent instability. However, due to lack of resolution or incomplete knowledge of the dynamics of nature, these instabilities are typically hidden. To describe nature with hidden instability, a stochastic parameterized model is used as the low-order reduced model. Bayesian inference incorporating data augmentation, regarding the missing path of the hidden processes as the augmented variables, is adopted in a Markov chain Monte Carlo (MCMC) algorithm to estimate the parameters in this reduced model from the partially observed signal. Howerver, direct application of this algorithm leads to an extremely low acceptance rate of the missing path. To overcome this shortcoming, an efficient MCMC algorithm which includes a pre-estimation of hidden processes is developed. This algorithm greatly increases the acceptance rate and provides the low-order reduced model with a high skill in capturing the extreme events due to intermittency.",
    "actual_venue": "Siam-Asa Journal On Uncertainty Quantification"
  },
  {
    "abstract": "The LINC transmitter is an architecture that provides linear amplification using nonlinear but power efficient amplifiers. The realization of the high power efficiency potential is the most critical issue of LINC. We have investigated a promising LINC amplifier topology that, using ideal bilateral PA devices, theoretically achieves linear amplification with 100% efficiency at all output levels. Circuit simulations based on integrated circuit (IC) techniques show good linearity and power efficiency results up to a few hundred MHz but gradually degrading performance as we approach the GHz range. With the advance of IC technology, this LINC amplifier topology offers a good possibility for obtaining highly efficient linear RF power amplifiers",
    "actual_venue": "Vtc Fall"
  },
  {
    "abstract": "Jamming is a typical attack by exploiting the nature of wireless communication. Lots of researchers are working on improving energy-efficiency of jamming attack from the attacker’s view. Whereas, in the low-duty-cycle wireless sensor networks where nodes stay asleep most of time, the design of jamming attack becomes even more challenging especially when considering the stochastic transmission patt...",
    "actual_venue": "Ieee Transactions On Dependable And Secure Computing"
  },
  {
    "abstract": "Semiconductor industry currently utilizes copper wafer bonding as one of key technologies for 3D integration. This review paper describes both science and technology of copper wafer bonding with regard to present applications. The classification of Cu bonding, bonding mechanisms, process developments, its microstructure evolution, as well as other characterizations are reviewed. Researches about patterned Cu bonding, future prospects, and 3D integration using Cu bonding are discussed in this paper.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Besides the proposed methods feasibility in a clinical environment, evaluation has shown good accuracy and high robustness indicating that it could be applied in image-guided interventions.",
    "actual_venue": "Int J Computer Assisted Radiology And Surgery"
  },
  {
    "abstract": "This article is concerned with optimizing human-machine turn-taking. In particular, the article covers an in-depth analysis of the timings when users respond to system query in spoken dialog systems. The goal of this work is to obtain a broad understanding of such timing patterns independent of dialog system type and dialog state context. Therefore, the analysis was based on a large volume of data both from a number of deployed spoken dialog system and an experimental study. The data from the experimental study showed that too short timeout settings can cause the system to interrupt a user and thus cause turn-taking problems. Next, the response timing patterns both during a system prompt as well as after prompt-end were analyzed for a number of different question types. It is shown that user responses while the system is playing a prompt (aka `barge-in') tend to occur in the range of 10---25 % of all user responses, where the exact percentage of barge-in is context-dependent. It was also found that the timing of user responses after a system finishes speaking always follows the same uni-modal pattern independent of system domain and question type. This pattern can be modeled with a rational distribution. Based on these findings, a probabilistic response time model is presented, that allows calculating the likelihood of a user response at any time in a system. This response timing model can be used for multiple purposes, among them timeout setting optimization.",
    "actual_venue": "International Journal Of Speech Technology"
  },
  {
    "abstract": "The management of uncertainty in expert systems has usually been left to ad hoc representations and rules of combinations lacking either a sound theory or clear semantics. The objective of this paper is to establish a theoretical basis for defining the syntax and semantics of a small subset of calculi of uncertainty operating on a given term set of linguistic statements of likelihood. Each calculus is defined by specifying a negation, a conjunction and a disjunction operator. Families of Triangular norms and conorms constitute the most general representations of conjunction and disjunction operators. These families provide us with a formalism for defining an infinite number of different calculi of uncertainty. The term set will define the uncertainty granularity, i.e. the finest level of distinction among different quantifications of uncertainty. This granularity will limit the ability to differentiate between two similar operators. Therefore, only a small finite subset of the infinite number of calculi will produce notably different results. This result is illustrated by two experiments where nine and eleven different calculi of uncertainty are used with three term sets containing five, nine, and thirteen elements, respectively. Finally, the use of context dependent rule set is proposed to select the most appropriate calculus for any given situation. Such a rule set will be relatively small since it must only describe the selection policies for a small number of calculi (resulting from the analyzed trade-off between complexity and precision).",
    "actual_venue": "Uncertainty In Artificial Intelligence"
  },
  {
    "abstract": "Analysis of execution traces plays a fundamental role in many program analysis approaches, such as runtime verification, testing, monitoring, and specification mining. Execution traces are frequently parametric, i.e., they contain events with parameter bindings. Each parametric trace usually consists of many meaningful trace slices merged together, each slice corresponding to one parameter binding. For example, a Java program creating iterator objects i(1) and i(2) over collection object c(1) may yield a trace createlter < c(1) i(1)> next < i(1)> createlter < c(1) i(2)> updateColl < c(1)> next < i(1)> parametric in collection c and iterator i, whose slices corresponding to instances \"c, i bar right arrow c(1), i(1)\" and \"c, i bar right arrow c(1), i(2)\" are createlter < c(1) i(1)> next < i(1)> updateColl < c(1)> next < i(1)> and, respectively, createlter < c(1) i(2)> updateColl < c(1)>. Several approaches have been proposed to specify and dynamically analyze parametric properties, but they have limitations: some in the specification formalism, others in the type of trace they support. Not unexpectedly, the existing approaches share common notions, intuitions, and even techniques and algorithms, suggesting that a fundamental study and understanding of parametric trace analysis is necessary. This foundational paper aims at giving a semantics-based solution to parametric trace analysis that is unrestricted by the type of parametric property or trace that can be analyzed. Our approach is based on a rigorous understanding of what a parametric trace/property/monitor is and how it relates to its non-parametric counter-part. A general-purpose parametric trace slicing technique is introduced, which takes each event in the parametric trace and dispatches it to its corresponding trace slices. This parametric trace slicing technique can be used in combination with any conventional, non-parametric trace analysis technique, by applying the later on each trace slice. As an instance, a parametric property monitoring technique is then presented, which processes each trace slice online. Thanks to the generality of parametric trace slicing, the parametric property monitoring technique reduces to encapsulating and indexing unrestricted and well-understood non-parametric property monitors (e.g., finite or push-down automata). The presented parametric trace slicing and monitoring techniques have been implemented and extensively evaluated. Measurements of runtime overhead confirm that the generality of the discussed techniques does not come at a performance expense when compared with existing parametric trace monitoring systems.",
    "actual_venue": "Logical Methods In Computer Science"
  },
  {
    "abstract": "Several kinds of event log data produced in daily clinical activities have yet to be used for secure and efficient improvement of hospital activities. Data Warehouse systems in Hospital Information Systems used for the analysis of structured data such as disease, lab-tests, and medications, have also shown efficient outcomes. This article is focused on two kinds of essential functions: process mining using log data and non-structured data analysis via Natural Language Processing.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "We establish in this paper a new two-stage supply chainwith onemanufacturer and two retailers which have a fixedmarket share in themature and stable market with specific reference to consumer electronics industry. This paper offers insights into how the three forecasting methods affect the bullwhip effect considering the market share under the ARMA(1, 1) demand process and the orderup- to inventory policy. We also discuss the stability of the order with the theory of entropy. In particular, we derive the expressions of bullwhip effectmeasure under the MMSE, MA, and ES methods and compare them by numerical simulations. Results show that the MA is always better in contrast to the ES for reducing the bullwhip effect in our supply chain model. When moving average coefficient is lower than a certain value, the MMSE method is the best for reducing the bullwhip effect; otherwise, theMA method is the best. Moreover, the larger the market share of the retailer with a long lead time is, the greater the bullwhip effect is, no matter what the forecasting method is.",
    "actual_venue": "Complexity"
  },
  {
    "abstract": "This paper presents a novel combined energy functional based on edge and region information for active contour model, which can be applied to segment textured images containing low contrast and high illumination variations. In the proposed method, the edge features are calculated based on the phase-based approaches derived from the monogenic signal, which are robust to illumination variations in the image. These feature values are used in an edge energy functional to assist the active contour evolving towards the true object boundaries. To extract the region features, at first, we compute the normalized accumulated short-term autocorrelation (NASTA) values for the image, which suppress background clutter and enhance dissimilarities between objects and background. Next, a local cumulative distribution function (LCDF) of NASTA is calculated for every pixel in a local window around it and is used as a region feature for that pixel. Then, the obtained features are employed to define a new localized region-based energy functional that can correctly segment regions with same intensity mean and variance. The proposed edge and region energy terms are integrated and a regularization term is added to them to form our combined active contour. Experimental results indicate remarkable advantages of proposed method comparing to existing combination models.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Sensor nodes are small devices that \"measure\" their environment and communicate feeds of low-level data values to a base station for further processing and archiving. Dissemination of these multi-valued feeds is challenging because of the limited resources (processing, bandwidth, energy) available in the nodes of the network. In this paper, we first describe the SBR algorithm for compressing multi-valued feeds containing historical data from each sensor. The key to our technique is the base signal, a series of values extracted from the real measurements that is used to provide piece-wise approximation of the measurements. While our basic technique exploits correlations among measurements taken on a single node, we further show how it can be adapted to exploit correlations among multiple nodes in a localized setting. Sensor nodes may form clusters and, within a cluster, a group leader identifies and coalesces similar measurements taken by different nodes. This localized mode of operation further improves the accuracy of the approximation, typically by a factor from 5 to 15. We provide detailed experiments of our algorithms and make direct comparisons against standard approximation techniques like Wavelets, Histograms and the Discrete Cosine Transform, on a variety of error metrics and for real data sets from different domains.",
    "actual_venue": "Vldb J"
  },
  {
    "abstract": "In this paper, we propose a method for cloud removal in a cloud-contaminated high-resolution (HR) optical satellite image with two kinds of auxiliary images of different types: a low-resolution (LR) optical satellite composite image and a synthetic aperture radar (SAR) image. In the proposed method, we assume that cloud-contaminated and cloud-free regions have been detected accurately, then dictio...",
    "actual_venue": "Ieee Journal Of Selected Topics In Applied Earth Observations And Remote Sensing"
  },
  {
    "abstract": "In this paper we consider the problem of universal prediction of individual\ncontinuous sequences with square-error loss, using a deterministic finite-state\nmachine (FSM). The goal is to attain universally the performance of the best\nconstant predictor tuned to the sequence, which predicts the empirical mean and\nincurs the empirical variance as the loss. The paper analyzes the tradeoff\nbetween the number of states of the universal FSM and the excess loss (regret).\nWe first present a machine, termed Exponential Decaying Memory (EDM) machine,\nused in the past for predicting binary sequences, and show bounds on its\nperformance. Then we consider a new class of machines, Degenerated Tracking\nMemory (DTM) machines, find the optimal DTM machine and show that it\noutperforms the EDM machine for a small number of states. Incidentally, we\nprove a lower bound indicating that even with large number of states the regret\nof the DTM machine does not vanish. Finally, we show a lower bound on the\nachievable regret of any FSM, and suggest a new machine, the Enhanced\nExponential Decaying Memory, which attains the bound and outperforms the EDM\nfor any number of states.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "Small non-coding RNAs (21 to 24 nucleotides) regulate a number of developmental processes in plants and animals by silencing genes using multiple mechanisms. Among these, the most conserved classes are microRNAs (miRNAs) and small interfering RNAs (siRNAs), both of which are produced by RNase III-like enzymes called Dicers. Many plant miRNAs play critical roles in nutrient homeostasis, developmental processes, abiotic stress and pathogen responses. Currently, only 70 miRNA have been identified in soybean.We utilized Illumina's SBS sequencing technology to generate high-quality small RNA (sRNA) data from four soybean (Glycine max) tissues, including root, seed, flower, and nodules, to expand the collection of currently known soybean miRNAs. We developed a bioinformatics pipeline using in-house scripts and publicly available structure prediction tools to differentiate the authentic mature miRNA sequences from other sRNAs and short RNA fragments represented in the public sequencing data.The combined sequencing and bioinformatics analyses identified 129 miRNAs based on hairpin secondary structure features in the predicted precursors. Out of these, 42 miRNAs matched known miRNAs in soybean or other species, while 87 novel miRNAs were identified. We also predicted the putative target genes of all identified miRNAs with computational methods and verified the predicted cleavage sites in vivo for a subset of these targets using the 5' RACE method. Finally, we also studied the relationship between the abundance of miRNA and that of the respective target genes by comparison to Solexa cDNA sequencing data.Our study significantly increased the number of miRNAs known to be expressed in soybean. The bioinformatics analysis provided insight on regulation patterns between the miRNAs and their predicted target genes expression. We also deposited the data in a soybean genome browser based on the UCSC Genome Browser architecture. Using the browser, we annotated the soybean data with miRNA sequences from four tissues and cDNA sequencing data. Overlaying these two datasets in the browser allows researchers to analyze the miRNA expression levels relative to that of the associated target genes. The browser can be accessed at http://digbio.missouri.edu/soybean_mirna/.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "A comprehensive understanding of the ways in which fish create and control forces is fundamental to engineering underwater vehicles that maneuver with the agility of fish. In this study the sunfish was selected as a biological model from which to understand pectoral fin motions and forces during hover. The kinematic patterns of the biological fin were identified and implemented on a biorobotic model of the fin. The effects of fin patterns and mechanical properties on force were evaluated. Pressure was measured at multiple points on the fin's surface and assessed for use in the closed loop control of fin force. The study revealed that a wide range of motions are used during hover, and that forces are significantly different from those found previously for steady swimming. However as fin speeds increase, the fin's dynamic motions, and the magnitude and direction of the forces become more similar to those of steady swimming. Collective measures of pressure over the fin's surface exhibited trends that correlated well with fin forces in relative magnitudes and directions. Results strongly suggest that distributed measures of pressure are useful for force prediction and control.",
    "actual_venue": "Intelligent Robots And Systems"
  },
  {
    "abstract": "We consider the problem of configuration formation in modular robot systems where a set of modules that are initially in different configurations and located at different locations are required to assume appropriate positions so that they can get into a new, user-specified, target configuration. We propose a novel algorithm based on graph isomorphism, where the modules select locations or spots in the target configuration using a utility-based framework, while retaining their original configuration to the greatest extent possible, to reduce the time and energy required by the modules to assume the target configuration. We have shown analytically that our proposed algorithm is complete and guarantees a Pareto-optimal allocation. Experimental simulations of our algorithm with different number of modules in different initial configurations and located initially at different locations, show that the planning time of our algorithm is nominal (order of msec. for 100 modules). We have also compared our algorithm against a market-based allocation algorithm and shown that our proposed algorithm performs better in terms of time and number of messages exchanged.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "High frequency signal injection is considered a parameter independent sensorless speed and position estimation technique which operates efficiently at low and zero speed. That is due to the saliency presence in the machine, which gives information about the rotor speed and position. Therefore the high frequency signal injected into the motor is modulated by the rotor speed and position information. Conventionally, to extract the useful information from the spectrum of the resultant signal, Phase-Locked Loop structure based demodulation schemes have been used. However, the PLL dynamics can be affected by the variation of inductances, PI parameters, and filter characteristics. In addition the error between the actual and estimated angle has to be small at the beginning of the estimation process. These restrictions are essential to maintain stability and to track the rotor speed and position accurately. However, such conditions are not always satisfied and are dependent on the operating conditions of the machine. Recently the concept of instantaneous frequency has become useful in many engineering applications where it is used to describe time varying signals. In this paper an instantaneous frequency estimation scheme based on the short time Fourier Transform ridges algorithm is proposed to detect the rotor speed and position. Theoretical and simulation aspects of the conventional and proposed methods are discussed and the obtained results are compared.",
    "actual_venue": "Engineering Letters"
  },
  {
    "abstract": "The new generation of smart devices, equipped with a large variety of sensors, enhances the Participatory Sensing of data. However, many issues arise when selecting participants to perform the sensing tasks. These issues are necessarily related to the limited energetic resources of devices, the impact of users mobility as well as the quality of collected data, recently defined as “Quality of Information” (QoI). In this context, we propose QEMSS (QoI and Energy aware Mobile Sensing Scheme) as a selection scheme for participatory sensing tasks, taking into consideration the quality of sensed data, QoI, and the dedicated energy for their acquisition. The aim of our model QEMSS is to select, among all participants in the sensing campaigns, the subset of users who maximizes the QoI of non redundant information while minimizing the overall energy consumption. To do so, we illustrate our selection scheme based on the Tabu Search algorithm in order to achieve a sub-optimal solution. Simulation results were compared to two other State of The art schemes: the Random Selection (RS) and a method based on a greedy search (DPS). Our scheme is proved to be as performing as the two other methods. Particularly, our scheme achieves a very high quality of information in challenging scenarios such as low dense areas and/or low energetic resources.",
    "actual_venue": "International Conference On Protocol Engineering And International Conference On New Technologies Of Distributed Systems"
  },
  {
    "abstract": "Anesthesiologists are taught to carefully manage patient vital signs during surgery. Unfortunately, there is little empirical evidence that vital sign management, as currently practiced, is correlated with patient outcomes. We seek to validate or repudiate current clinical practice. Using a database of over 90,000 cases, we attempt to determine whether those cases that an anesthesiologist would subjectively decide are “low quality” are more likely to result in negative outcomes. The problem reduces to one of multidimensional time series classification. Our approach is to have an expert anesthesiologist label a small number of training cases, from which we can train a classifier to use to label all 90,000 cases. We then use the labeling to search for correlation with outcomes. We consider several standard classification methods, such as dynamic time warping in conjunction with a kNN classifier, as well as the recently proposed complexity invariant distance, and a regression based upon the feature extraction methods outlined by Mao et al. (using features such as time series mean, standard deviation, skew, approximate entropy, etc.). We also propose a feature selection mechanism that learns a hidden Markov model to segment the time series; the fraction of time that each series spends in each state is used to label the series using a regression based classifier. In the end, we are able to obtain strong, empirical evidence that current best practice is correlated with reduced negative patient outcomes.",
    "actual_venue": "SDM"
  },
  {
    "abstract": "We consider two-way amplify-and-forward relaying in a multichannel system with two terminal nodes and one relay nodes, where the time-division broadcast (TDBC)-based two-way relaying strategy is used. Given power alloation, we investigate the design of subchannel pairing at the relay to maximize the sum rate of the system. With broadband systems of large number of subchannels, our focus is on designing efficient subchannel pairing strategies. We first show that, unlike in the one-way case, there exists no explicit SNR-based pairing strategy leading to sum-rate maximization. Instead, we formulate the pairing optimization as an axial 3-D assignment problem which is NPhard, and propose an iterative optimization method to solve it with complexity O(N-3). Based on SNR over each subchannel, we also propose sorting-based algorithms for scenarios with and without direct link, with a low complexity of O(N log N). The simulation results also show the proposed algorithm offers the performance very close to the iterative optimization method.",
    "actual_venue": "Asilomar Conference On Signals, Systems, And Computers"
  },
  {
    "abstract": "Mobile user equipments supporting Long Term Evolution (LTE) - carrier aggregation (CA) face several new unpredicted issues when operating in frequency division duplex mode. This paper addresses the transmit (Tx) modulated spur problem, a new issue, that appears in LTE-CA transceivers. Due to the presence of several mixers and dividers to support different bands and CA scenarios, many harmonics are generated on the chip. Unavoidable crosstalk between these harmonics can generate an unwanted continuous wave spur around the Tx frequency. Once this spur appears at the chip area where the receive (Rx) local oscillator resides, it downconverts the undesired portion of the Tx signal which leaked through the duplexer to the Rx baseband. This interference, referred to as Tx modulated spur, causes a severe degradation of the signal-to-noise ratio (SNR) of the wanted signal. To mitigate such, we propose an active digital cancellation architecture which utilizes the known information of the Tx signal. An equivalent baseband model of the Tx modulated spur is presented from which the digital cancellation architecture is derived. Simulations show that the proposed architecture cancels the Tx interference and improves the SNR of the wanted signal significantly.",
    "actual_venue": "Ieee Vehicular Technology Conference"
  },
  {
    "abstract": "Tamper localization and recovery watermarking scheme can be used to protect the integrity and authenticity of medical images. In this paper, a simple tamper localization and recovery scheme that uses lossless compression was proposed. Lossy compression may also be applied when necessary. The watermarked image has the PSNR of 47.4 dB and the results show that tampering was successfully localized and tampered area was exactly recovered.",
    "actual_venue": "Communications In Computer And Information Science"
  },
  {
    "abstract": "We present a novel method for producing convincing pictures of shaded objects based entirely on 2D image operations. This approach, which we call image-based shading design, offers direct artistic control in the picture plane by deforming image primitives so that they appear to conform to specific 3D shapes. Using a differential analysis of reflected radiance, we identify the two types of surface flows involved in the depiction of shaded objects, which are consistent with recent perceptual studies. We then introduce two novel deformation operators that closely mimic surface flows while providing direct artistic controls in real-time.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "We propose a framework for extracting structure from stereo which represents the scene as a collection of approx- imately planar layers. Each layer consists of an explicit 3D plane equation, a colored image with per-pixel opac- ity (a sprite), and a per-pixel depth offset relative to the plane. Initial estimates of the layers are recovered using techniques taken from parametric motion estimation. These initial estimates are then refined using a re-synthesis algo- rithm which takes into account both occlusions and mixed pixels. Reasoning about such effects allows the recovery of depth and color information with high accuracy, even in partially occluded regions. Another important benefit of our framework is that the output consists of a collection of approximately planar regions, a representation which is far more appropriate than a dense depth map for many appli- cations such as rendering and video parsing.",
    "actual_venue": "Cvpr"
  },
  {
    "abstract": "Entertainment systems promise to be a significant application for Mixed Reality. Recently, a growing number of Mixed Reality applications have included interaction with synthetic characters and storytelling. However, AI-based Interactive Storytelling techniques have not yet been explored in the context of Mixed Reality. In this paper, we describe a first experiment in the adaptation of an Interactive Storytelling technique to a Mixed Reality system. After a description of the real time image processing techniques that support the creation of a hybrid environment, we introduce the storytelling technique and the specificities of user interaction in the Mixed Reality context. We illustrate these experiments by discussing examples obtained from the system.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The paper presents the main issues that usually appear in the development of a legislative digital library. The great number of legislative documents which accumulates over the time raises the need for electronic management of this content and the meta-information associated with it. The preparation, the management and the distribution to end users are explained in detail in this paper, offering in the same time an architectural solution for the development of a similar library. A big emphasis was putted on the legislative documents automatic reference linking mechanisms.",
    "actual_venue": "Ecdl"
  },
  {
    "abstract": "State-of-the-art linear programming (LP) solvers give solutions without any warranty. Solutions are not guaranteed to be optimal or even close to optimal. Of course, it is generally believed that the solvers produce optimal or at least close to optimal solutions.We have implemented a system LPex which allows us to check this belief. More precisely, given an LP and a basis B, it determines whether the basis is primal feasible and/or dual feasible. It can also find the optimum starting from an arbitrary basis (or from scratch). It uses exact arithmetic to guarantee correctness of the results. The system is efficient enough to be applied to medium- to large-scale LPs. We present results from the netlib benchmark suite.",
    "actual_venue": "Soda"
  },
  {
    "abstract": "In a cognitive radio network, selfish secondary users may not voluntarily contribute to desired cooperative sensing. We design the first fully distributed scheme to incentivize participation of nodes in cooperative sensing, by connecting sensing and spectrum allocation, and offering incentive from latter to the former. Secondary users that are more active and report more accurate sensing values will be given higher reputation values, which results in lower prices in the spectrum allocation phase. Theoretical analysis and simulation results indicate that the proposed method effectively incentivizes sensing participation, and rewards truthful and accurate reporting. Our proposed system is fully distributed and does not rely on a central authority, and so is more applicable in dynamic cognitive radio networks in practice. We also show how to improve the robustness of reputation when malicious nodes report spurious reputation.",
    "actual_venue": "Infocom"
  },
  {
    "abstract": "A new algorithm is presented for the determination of the coefficients of an n-dimensional (n-D) transfer function. The n-D state-space system is described by the n-D Fornasini–Marchesini models. The proposed algorithm is theoretically attractive and computationally fast and it is based on the discrete Fourier transform (DFT). A step-by-step example is given to illustrate the application of the proposed algorithm.",
    "actual_venue": "Journal Of The Franklin Institute"
  },
  {
    "abstract": "Professional musicians constitute a model par excellence for understanding experience-dependent plasticity in the human brain, particularly in the auditory domain. Their intensive sensorimotor experience with musical instruments has been shown to entail plastic brain alterations in cortical perceptual and motor maps. It remains an important question whether this neuroplasticity might extend beyond basic perceptual and motor functions and even shape higher-level conceptualizations by which we conceive our physical and social world. Here we show using functional magnetic resonance imaging (fMRI) that conceptual processing of visually presented musical instruments activates auditory association cortex encompassing right posterior superior temporal gyrus, as well as adjacent areas in the superior temporal sulcus and the upper part of middle temporal gyrus (pSTG/MTG) only in musicians, but not in musical laypersons. These areas in and adjacent to auditory association cortex were not only recruited by conceptual processing of musical instruments during visual object recognition, but also by auditory perception of real sounds. Hence, the unique intensive experience of musicians with musical instruments establishes a link between auditory perceptual and conceptual brain systems. Experience-driven neuroplasticity in musicians is thus not confined to alterations of perceptual and motor maps, but even leads to the establishment of higher-level semantic representations for musical instruments in and adjacent to auditory association cortex. These findings highlight the eminent importance of sensory and motor experience for acquiring rich concepts.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Dolphins have biological sonar abilities that exceed those of any man-made system in an aquatic environment. One problem of particular importance, and for which only limited capabilities exist, is the detection and recognition of targets buried under sediment. This paper reviews dolphin echolocation capabilities and describes a system that uses a dolphin-like signal and biomimetic signal processing mechanisms to emulate the performance of the dolpin on such targets. The system employed a digitized dolphin click with a center frequency of 120 kHz and a 3dB bandwidth of 39 kHz, 50 μs duration. This signal was transmitted through seawater into mud and the echoes reflected from the objects were recorded and digitized. Two spectral estimators were used to extract a time-frequency representation of the echo. One was based on short-time fast Fourier transforms, and the other was based on an autoregressive estimator. The time-frequency representation was then processed by a separate backpropagation neural network for each estimator, designed to derive independent identifications of the targets. These identifications were then combined in a modified probabilistic neural network that used a linear transfer function rather than a binary function for its output. Finally the output of the probabilistic network was processed symbolically by a simple expert system. Three experiments are described in which the system was used to discriminate a small stainless-steel cylinder from cyliners of the same size made of hollow aluminium, foam-filled aluminium, or coral rock embedded in resin. Each of the targets was presented buried in mud at a depth of several centimeters. The system proved highly effectively at recognizing these buried targets.",
    "actual_venue": "Neural Networks"
  },
  {
    "abstract": "This paper describes the work of an on-going project investigating the incorporation of physics-based modeling for simulating dynamic interactions in robotic aided manipulations. The research aims at achieving better simulation of robot behaviors in real situations making it a useful tool for off-line programming. By allowing manipulation strategy simulation and performance visualization of a designed operation to be carried out within a virtual environment the system will be a valuable tool for handling strategy design, testing, modification, and optimization without fear of damaging the physical systems. As an environment for demonstrating tacks to robots, the systems will also be a useful plarform far robot teaching and training. The system has particular promise in flexible assembly where it is more economical to perform the reconfiguration of a work cell and to generate or modify handling strategies for new tasks without having to shut down the real system. This paper reports the initial work carried out with the preliminary results given.",
    "actual_venue": "Computer Graphics International"
  },
  {
    "abstract": "Polarimetric measurements of scattering coefficient sigma0 of a bare soil were performed changing a roughness of a soil surface and a soil moisture content using the 35 GHz polarimetric scatterometer. At Ka band, a few experimental results of sigma0 data are available. One of the purpose of this experiment is to obtain the surface backscattering characteristics to evaluate surface clutter interference with precipitation measurement from space using the Dual frequency Precipitation Radar (DPR, 13.8 GHz and 35.5 GHz), which is planned to be onboard the Global Precipitation Measurement (GPM) Mission core satellite to observe precipitation globally. In evaluation of surface clutter interference, sigma0 data for various surface conditions are needed, especially sigma0 dependence on soil moisture content and surface roughness. Another purpose of this experiment is to apply the measured sigma0 data to estimate a soil moisture content globally after the launch of the GPM core satellite. An angular scan range of the DPR is from nadir direction though the incidence angle of 8.4 degrees (with the 35.5 GHz radar) or 17 degrees (with 13.6 GHz radar), so the measured data by the DPR is useful to observe the earth surface condition globally. In this study, the roughness of the soil surface was measured with a laser profile meter to determine the roughness dependence of the scattering coefficient. A soil moisture content was measured comparing the weight of the soil before and after the heating of the soil. The soil was heated enough to contain no water. To increase the number of the independent samples for each experimental conditions (soil moisture content/surface roughness/incidence angle), azimuthal angle is changed like clockwork using the turntable of 2 m diameter. The system of the scatterometer is network analyzer based polarimeter",
    "actual_venue": "Igarss"
  },
  {
    "abstract": "Static drawings of biological pathways are still an important research tool for biologists. Gerhard Michal created his seminal drawings of metabolic networks in the 1960s and thus defined canonical representations of some key pathways. The Kyoto Encyclopedia of Genes and Genomes (KEGG) provides the most popular static drawings of biological networks of different types, used in a huge number of publications. These drawings are so widely known that they are immediately recognizable to most biologists. This enables collaborative work and simplifies the communication of analysis results. Automatic layout of these pathway maps is complicated by the fact that the information available from KEGG does not contain the entire layout information of the reference maps. Here we present a fully automated algorithm for interactive KEGG layout construction. The algorithm conserves the original KEGG layout to the extent possible while improving readability by removing unnecessary elements (in organism-specific maps). Multiple pathway maps can be laid out simultaneously to facilitate the navigation of larger networks. The algorithm supports the hierarchical layout of sub networks and thus supports interactive exploration of large datasets.",
    "actual_venue": "Pacific Visualization Symposium"
  }
]