[
  {
    "abstract": "We are investigating the representation of simple visual objects by groups of retinal ganglion cells and are simultaneously recording the responses of ganglion cells in the isolated turtle retina with 15 out of an array of 100 penetrating microelectrodes. Stimulation is with circular spots of light of various intensities, diameters and colors. We have trained a three layer artificial neural network to estimate the stimulus parameters and have challenged it to classify the color, size and intensity of test stimuli. Individual ganglion cells are poor encoders of stimulus features, but the 15 cells in our sample allow one to classify intensity, color and spot diameter to within 0.6 log units, 61 nm, and 0.68 mm, respectively.",
    "actual_venue": "Iwann"
  },
  {
    "abstract": "To enable automated landmine detection a number of practical problems must be overcome. One of them is the mapping of data from landmine detection sensors using a mobile demining robot in an unstructured outdoor environment. The odometry of the robot used for mapping can be improved by using an additional vision system. It is proposed in this paper to utilize natural landmarks which can be found on the ground for this purpose. New simple algorithms for detection and association of natural landmarks are developed and described. Finally, the experimental results of applying these algorithms on a set of images obtained from a camera mounted on the robot are presented. The results show their robustness for different environmental conditions and robot motions.",
    "actual_venue": "Ieee/Rsj International Conference On Intelligent Robots And Systems"
  },
  {
    "abstract": "Abstract Delta Modeling (ADM) offers an algebraic description of how a (software) product line may be built so that every product can be automatically derived by structured reuse of code. In traditional application engineering a single valid feature configuration is chosen, which does not change during the lifetime of the product. However, there are many useful applications for product lines that change their configuration at run time. We present a new technique for generating efficient dynamic product lines from their static counterparts. We use Mealy machines for their dynamic reconfiguration. Furthermore, we posit that monitoring some features will be more expensive than monitoring others, and present techniques for minimizing the cost of monitoring the system. We stay in the abstract setting of ADM but the techniques can be instantiated to any concrete domain. We illustrate them through the example of a mobile application for Android, which dynamically reconfigures a devices operating profile based on environmental factors.",
    "actual_venue": "Splc"
  },
  {
    "abstract": "While cognitive radio networks (CRNs) present a promising solution to solve the scarcity of the radio spectrum, they are still susceptible to security threats. Until now, only a few researchers considered the use of intrusion detection systems (IDSs) to combat these threats against CRNs. In this article we describe a CRN based on IEEE wireless regional area network (WRAN) and describe some of the security threats against it. For the secondary users in the CRN to quickly detect whether they are being attacked, a simple yet effective IDS is then presented. Our proposal uses non-parametric cumulative sum (cusum) as the change point detection algorithm to discover the abnormal behavior due to attacks. Our proposed IDS adopts an anomaly detection approach and it profiles the CRN system parameters through a learning phase. So, our proposal is also able to detect new types of attacks. As an example, we present the case of detection of a jamming attack, which was not known to the IDS beforehand. The proposed IDS is evaluated through computer based simulations, and the simulation results clearly indicate the effectiveness of our proposal.",
    "actual_venue": "Network, Ieee"
  },
  {
    "abstract": "The \"virtual wall\" is the most common building block used in constructing haptic virtual environments. A virtual wall is typically based on a simple spring model, with unilateral constraints that allow the user to make and break contact with a surface. There are a number of factors (sample-and-hold, device dynamics, sensor quantization, etc.) that cause virtual walls to demonstrate active (nonpassive) behavior, destroying the illusion of reality. In this paper, we find an explicit upper bound on virtual wall stiffness that is a necessary and sufficient condition for virtual wall passivity. We consider a haptic display that can be modeled as a mass with Coulomb-plus-viscous friction, being acted upon by two external forces: an actuator and a human user. The system is equipped with only one sensor, an optical encoder measuring the position of the mass. We explicitly model the effects of position resolution, which has not been done in previous work. We make no assumptions about the human user, and we consider arbitrary constant sampling rates. The main result of our analysis is a necessary and sufficient condition for passivity that relies on the Coulomb friction in the haptic device, as well as the encoder resolution. We experimentally verify our results with a one-degree-of-freedom haptic display, and find that the system can display nonpassive behavior in two decoupled modes that are predicted by the necessary and sufficient condition. One mode represents instability, while the other mode results in active tactile sensations.",
    "actual_venue": "Ieee Transactions On Robotics"
  },
  {
    "abstract": "We present a tableau method for inconsistency-adaptive logics and illustrate it in terms of the two best studied systems.\n The method is new in that adaptive logics require a more complex structure of the tableaus and of some rules and conditions.\n As there is no positive test for derivability in inconsistency-adaptive logics, the tableau method is important for providing\n criteria for derivability.",
    "actual_venue": "Tableaux"
  },
  {
    "abstract": "We present a homogenization technique for rarefied gas flow over a microstructured surface consisting of patterns of periodic features. The length scale of the model domain is comparable to the mean free path of the molecules, while the scale of the surface patterns is much smaller. The flow is modeled by a system of linear Boltzmann equations with a diffusive boundary condition at the patterned surface. The resulting homogenized boundary condition holds at a virtual. at surface and incorporates the microscopic geometry information about the surface structure on the macroscopic level. Numerical results validate the approach. The setup models low pressure chemical vapor deposition processes in the manufacturing of integrated circuits.",
    "actual_venue": "Siam Journal On Applied Mathematics"
  },
  {
    "abstract": "This paper presents a new construction of matrices with no singular square submatrix. This construction allows designing erasure codes over finite fied with fast encoding and decoding algorithms.",
    "actual_venue": "Contemporary Mathematics Series"
  },
  {
    "abstract": "Real-world things are increasingly becoming fully qualified members of the Web. From, pacemakers and medical records to children's toys and sneakers, things are connected over the Web and publish information that is available for the whole world to see. It is crucial that there is secure access to this Web of Things (WoT) and to the related information published by things on the Web. In this paper, we introduce an architecture that encompasses Web-enabled things in a secure and scalable manner. Our architecture utilizes the features of the well-known role-based access control (RBAC) to specify the access control policies to the WoT, and we use cryptographic keys to enforce such policies. This approach enables prescribers to WoT services to control who can access what things and how access can continue or should terminate, thereby enabling privacy and security of large amount of data that these things are poised to flood the future Web with.",
    "actual_venue": "Codes, Cryptology, And Information Security, C2Si"
  },
  {
    "abstract": "This paper presents the novel concept of the fully differential operational floating conveyor (FD-OFC) for the first time, to the best of the authors knowledge. A CMOS design for the proposed FD-OFC is introduced as an 8 (4x4) port general purpose analog building block. The proposed design has the advantage of low power consumption as it can operate under biasing conditions of only 1.5 V, while its wide bandwidth exceeds 100 MHz. These operating conditions recommend the proposed device to be integrated to a wide range of low power-wide high speed applications. The terminal behavior of the proposed device is mathematically modeled and its operation is simulated using the UMC 130 nm technology kit in Cadence environment.",
    "actual_venue": "Isvlsi"
  },
  {
    "abstract": "Big Data has become the new ubiquitous term used to describe massive collection of datasets that are difficult to process using traditional database and software techniques. Most of this data is inaccessible to users, as we need technology and tools to find, transform, analyze, and visualize data in order to make it consumable for decision-making. One aspect of Big Data research is dealing with the Variety of data that includes various formats such as structured, numeric, unstructured text data, email, video, audio, stock ticker, etc. Managing, merging, and governing a variety of data is the focus of this paper. This paper proposes a semantic Extract-Transform-Load (ETL) framework that uses semantic technologies to integrate and publish data from multiple sources as open linked data. This includes - creation of a semantic data model to provide a basis for integration and understanding of knowledge from multiple sources, creation of a distributed Web of data using Resource Description Framework (RDF) as the graph data model, extraction of useful knowledge and information from the combined data using SPARQL as the semantic query language.",
    "actual_venue": "Bigdata Congress"
  },
  {
    "abstract": "The control of energy consumption in mobile devices is very important and is being paid more and more attentions to. An effective way to reduce the energy consumption of video decoding is to reduce the computational complexity of the video decoder. This paper aims at the tradeoff between complexity and video quality, and proposes a complexity scalable AVS video decoder. AVS is the recent video coding standard developed by the Audio and Video Coding Standard Workgroup of China, which has similar performance with H.264/AVC but a more succinct technical plan. With little information of the characteristics of video sequences added to the bitstream by the encoder, the decoder provides complexity scalable output. Given a percentage K, the decoder can accurately reduce K% computational complexity with slight video quality degradation. Our experimental studies show that, for typical video, using the complexity scalable technology, the quality of video sequences remains acceptable.",
    "actual_venue": "Momm"
  },
  {
    "abstract": "This paper gives an overview of an empirical cross-calibration technique developed for the Surface Water Ocean Topography mission (SWOT). The method is here used to detect and to mitigate two spatially coherent errors in SWOT topography data: the baseline roll error whose signature is linear across track, and the baseline length error whose signature is quadratic across track. Assuming that topography data are corrupted by coherent error signatures that we can model, we extract the signatures, and we empirically use the error estimates to correct SWOT data. The cross-calibration is tackled with a two-step scheme. The first step is to get local estimates over cross-calibration zones, and the second step is to perform a global interpolation of local error estimates and to mitigate the error everywhere. Three methods are used to get local error estimates: 1) we remove a static first guess reference such as a digital elevation model, 2) we exploit overlapping diamonds between SWOT swaths, and 3) we exploit overlapping segments with traditional pulse-limited altimetry sensors. Then, the along-track propagation is performed taking the local estimates as an input, and an optimal interpolator (1-D objective analysis) constrained with a priori statistical knowledge of the problem. The rationale of this paper is to assume that SWOT's scientific requirements are met on all errors but the ones being cross-calibrated. In other words, the algorithms presented in this paper are not needed at this stage of the mission definition, and they are able to deal with higher error levels (e.g., if hardware constraints are relaxed and replaced by additional ground processing). Even in our most pessimistic theoretical scenarios of baseline roll and baseline length errors (up to 70 cm RMS of uncorrected topography error), the cross-calibration algorithm reduces coherent errors to less than 2 cm (outer edges of the swath). Residual errors are subcentimetric for very low-frequency errors (e.g- , orbital revolution). Sensitivity tests highlight the benefits of using additional pulse-limited altimeters and optimal inversion schemes when the problem is more difficult to solve (e.g., wavelengths of less than 1000 km), but also to provide a geographically homogeneous correction that cannot be obtained with SWOT's sampling alone.",
    "actual_venue": "Geoscience And Remote Sensing, Ieee Transactions"
  },
  {
    "abstract": "We investigate the prospect of applying runtime verification to cheat detection. Game implementation bugs are extensively exploited by cheaters, especially in massively multiplayer games. As games are implemented on larger scales and game object interactions become more complex, it becomes increasingly difficult to guarantee that high-level game rules are enforced correctly in the implementation. We observe that although implementing high-level rules in code is complex because of interference between rules, checking for rule compliance at runtime is simple because only a single rule is involved in each check. We demonstrate our idea by applying the Java-MaC runtime verification system to a simple game to detect a transaction bug that is common in massively multiplayer games.",
    "actual_venue": "Netgames"
  },
  {
    "abstract": "Cloud radio access network (C-RAN) is emerging as a transformative paradigmatic architecture for the next generation of cellular networks. In this paper, a novel resource allocation solution that optimizes the energy consumption of a C-RAN is proposed. First, an energy consumption model that characterizes the computation energy of the base band unit (BBU) pool is introduced based on the empirical ...",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "TVLA (Three-Valued-Logic Analyzer) is a \"YACC\"-like framework for automatically constructing abstract interpreters from an operational semantics. The operational semantics is specified as a generic transition system based on first-order logic. TVLA was implemented in Java and successfully used to prove interesting properties of (concurrent) Java programs manipulating dynamically allocated linked data structures.",
    "actual_venue": "International Federation For Information Processing"
  },
  {
    "abstract": "The field of evolutionary robotics has demonstrated the ability to automatically design the morphology and controller of simple physical robots through synthetic evolutionary processes. However, it is not clear if variation-based search processes can attain the complexity of design necessary for practical engineering of robots. Here., we demonstrate an automatic design system that produces complex robots by exploiting the principles of regularity, modularity, hierarchy, and reuse. These techniques are already established principles of scaling in engineering design and have been observed in nature, but have not been broadly used in artificial evolution. We gain these advantages through the use of a generative representation, which combines a programmatic representation with an algorithmic process that compiles the representation into a detailed construction plan. This approach is shown to have two benefits: it can reuse components in regular and hierarchical ways, providing a systematic way to create more complex modules from simpler ones; and the evolved representations can capture intrinsic properties of the design space, so that variations in the representations move through the design space more effectively than equivalent-sized changes in a nongenerative representation. Using this system, we demonstrate for the first time the evolution and construction of modular, three-dimensional, physically locomoting robots, comprising many more components than previous work on body-brain evolution.",
    "actual_venue": "Ieee Transactions On Robotics And Automation"
  },
  {
    "abstract": "The estimation of soil moisture and crop biomass based on differential interferometry is questioned by the influence of intercepted rain (i.e., plant surface moisture) on repeat-pass observables. The magnitude, the origin of this effect, as well as its dependence on system and crop biophysical parameters have been only marginally addressed so far. This paper intends to investigate these aspects wi...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "Existing approaches to object encapsulation either rely on ad hoc syntactic restrictions or require the use of specialised type systems. Syntactic restrictions are difficult to scale and to prove correct, while specialised type systems require extensive changes to programming languages. We demonstrate that confinement can be enforced cheaply in Featherweight Generic Java, with no essential change to the underlying language or type system. This result demonstrates that polymorphic type parameters can simultaneously act as ownership parameters and should facilitate the adoption of confinement and ownership type systems in general-purpose programming languages.",
    "actual_venue": "J Funct Program"
  },
  {
    "abstract": "We consider two-dimensional languages, called here 2d transducer languages, generated by iterative applications of transducers (finite state automata with output). To each transducer a two-dimensional language consisting of blocks of symbols is associated: the bottom row of a block is an input string accepted by the transducer and, by iterative application of the transducer, each row of the block is an output of the transducer on the preceding row. We observe that this class of languages is a proper subclass of recognizable picture languages containing the class of all factorial local 2d languages. By taking the average growth rate of the number of blocks in the language as a measure of its complexity, also known as the entropy of the language, we show that every entropy value of a one-dimensional regular language can be obtained as an entropy value of a 2d transducer language.",
    "actual_venue": "Ciaa"
  },
  {
    "abstract": "Brownfield redevelopment (BR) is an ongoing issue for governments, communities, and consultants around the world. It is also an increasingly popular research topic in several academic fields. Strategic decision support that is now available for BR is surveyed and assessed. Then a dominance-based rough-set approach is developed and used to classify cities facing BR issues according to the level of two characteristics, BR effectiveness and BR future needs. The data for the classification are based on the widely available results of a survey of US cities. The unique features of the method are its reduced requirement for preference information, its ability to handle missing information effectively, and the easily understood linguistic decision rules that it generates, based on a training classification provided by experts. The resulting classification should be a valuable aid to cities and governments as they plan their BR projects and budgets.",
    "actual_venue": "Environmental Modelling And Software"
  },
  {
    "abstract": "Security bugs are critical programming errors that can lead to serious vulnerabilities in software. Examining their behaviour and characteristics within a software ecosystem can provide the research community with data regarding their evolution, persistence and others. We present a dataset that we produced by applying static analysis to the Maven Central Repository (approximately 265GB of data) in order to detect potential security bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics' results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version in our data repository, we also store specific metadata, such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results involving security bugs, as we show in specific examples.",
    "actual_venue": "Third International Workshop On Building Analysis Datasets And Gathering Experience Returns For Security"
  },
  {
    "abstract": "This paper presents a deterministic O (nm log n + n2log2n) = Õ (nm) time algorithm for splitting off all edges incident to a vertex s of even degree in a multigraph G, where n and m are the numbers of vertices and links (= vertex pairs between which G has an edge) in G, respectively. Based on this, many graph algorithms using edge-splitting can run faster. For example, the edge-connectivity augmentation problem in an undirected multigraph can be solved in Õ (nm) time, which is an improvement over the previously known randomized Õ (n3) bound and deterministic Õ (n2m) bound.",
    "actual_venue": "Acm Symposium On Theory Of Computing"
  },
  {
    "abstract": "Synthetic genetic transistors are vital for signal amplification and switching in genetic circuits. However, it is still problematic to efficiently select the adequate promoters, Ribosome Binding Sides (RBSs) and inducer concentrations to construct a genetic transistor with the desired linear amplification or switching in the Input/Output (I/O) characteristics for practical applications.Three kinds of promoter-RBS libraries, i.e., a constitutive promoter-RBS library, a repressor-regulated promoter-RBS library and an activator-regulated promoter-RBS library, are constructed for systematic genetic circuit design using the identified kinetic strengths of their promoter-RBS components.According to the dynamic model of genetic transistors, a design methodology for genetic transistors via a Genetic Algorithm (GA)-based searching algorithm is developed to search for a set of promoter-RBS components and adequate concentrations of inducers to achieve the prescribed I/O characteristics of a genetic transistor. Furthermore, according to design specifications for different types of genetic transistors, a look-up table is built for genetic transistor design, from which we could easily select an adequate set of promoter-RBS components and adequate concentrations of external inducers for a specific genetic transistor.This systematic design method will reduce the time spent using trial-and-error methods in the experimental procedure for a genetic transistor with a desired I/O characteristic. We demonstrate the applicability of our design methodology to genetic transistors that have desirable linear amplification or switching by employing promoter-RBS library searching.",
    "actual_venue": "Bmc Systems Biology"
  },
  {
    "abstract": "•The Capacitated Location-Routing Problem is solved.•Two heuristics based on the simulated annealing method are proposed.•The local search reallocates customers and next improves the routes cost.•There is a diversification procedure to explore facility locations.•Solutions of nine large instances (with 150 and 200 customers) were improved.",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "Radio frequency identification (RFID) technology is considered to be the next step in the revolution of supply-chain management, retail, and beyond. To derive real benefit from RFID, the applications must incorporate functions to process the enormous event data generated quickly by RFID operations. For this reason, many RFID middleware systems have been developed. Although RFID middleware assists in the management of the flow of event data, developers require real time knowledge of meaningful events, which are more actionable not simple data collection. Determining meaningful events requires a context, which typically comes from reference data. In this paper, we present a contextual event framework (CEF) that transforms RFID events into contextual events",
    "actual_venue": "Itng"
  },
  {
    "abstract": "The availability of BIG molecular databases derived from quantum mechanics computations represent an opportunity for computational intelligence practitioners to develop new tools with same accuracy but much lower computational complexity compared to the costly Schrodinger equation. In this study, unsupervised and supervised learning methods are applied to investigate the internal structure of the data and to learn the mapping between the atomic coordinates of molecules and their properties. Low dimensional spaces revealed a well defined clustering structure as defined by the measures used for comparing molecules based their atom distributions and chemical composition. Supervised learning techniques were applied on the original predictor variables, as well as on a subset of selected variables found using evolutionary algorithms guided by residual variance analysis (Gamma Test). Black and white box modeling approaches were used (random forests, neural networks and model trees and adaptive regression respectively). All of them delivered good performance, error and correlation-wise, with neural networks producing the best results. In particular white box techniques obtained explicit functional dependencies, some of them achieving considerably reduction of the feature set and expressed as simple models.",
    "actual_venue": "Artificial Neural Networks And Machine Learning - Icann : Workshop And Special Sessions"
  },
  {
    "abstract": "The objective of this paper was to explore the role of a Knowledge Management professional in today's blue chip companies, to examine if and how the KM professionals differ from each other, and if Knowledge Management professionals on the whole match the earlier proposed models of an 'ideal' KM professional. The findings suggested that employing organisations did not have a clear idea of the KM role. This ambiguity was picked up the newly appointed KM professionals, who as a result felt uncertain and insecure in their KM role. Further, the findings supported earlier studies, in what the participants perceived the KM role to be (2/3 change agent, 1/3 information systems technology) although they perceived themselves primarily as management consultants/entrepreneurs rather than technicians/managers.",
    "actual_venue": "Pakm"
  },
  {
    "abstract": "Reusability is one of the principal software quality factors. In the context of model driven development (MDD), reuse of model transformations is also considered a key activity to achieve productivity and quality. It is necessary to devote important research efforts to find out appropriate reusability mechanisms for transformation tools and languages. In this paper we present two approaches for reusing model transformation definitions. Firstly, we tackle the creation of related model transformations, showing how the factorization of common parts can be achieved. Secondly, we describe a proposal on the composition of existing, separated transformation definitions so that they can be used to solve a concrete transformation problem. We illustrate both proposals with examples taken from the development of a software product line for adventure games, which has been implemented using the modularization mechanisms of the RubyTL transformation language.",
    "actual_venue": "Icmt"
  },
  {
    "abstract": "A new method for modeling correlated noise in receiving antenna arrays for direction-of-arrival (DOA) estimation is introduced. The array noise is divided into a coupled and an uncoupled components which originate from the array environment and the internal circuitry of the antenna elements, respectively. While the uncoupled noise power can be determined a priori and be removed from the array rece...",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "Today’s power systems become more prone to cyber-attacks due to the high integration of information technologies. In this paper, we demonstrate that the outages of some lines can be masked by injecting false data into a set of measurements. The success of the topology attack can be guaranteed by making that: 1) the injected false data obeys Kirchhoff current law and Kirchhoff voltage law to avoid being detected by the bad data detection program in the state estimation and 2) the residual in the line outage detection is increased such that the line outage cannot be detected by phasor measurement unit data. A bilevel optimization problem is set up to determine the optimal attack vector that can maximize the residual of the outaged line. The IEEE 39-bus and 118-bus systems are used to demonstrate the masking scheme.",
    "actual_venue": "Ieee Trans Information Forensics And Security"
  },
  {
    "abstract": "•We propose an accurate diffeomorphic ST method for 3D strain estimation in EC images.•We provide a formulation that generalize the previous speckle models used in US.•It is introduced a formulation which considers an adaptive temporal correlation.•A probabilistic tissue characterization is used to distinguish relevant motions.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "We propose and evaluate a simple yet effective technique of combining nonadaptive and adaptive beamforming methods aimed to achieve high quality of ultrasound images at low computational cost. Our hybrid beamformer automatically switches between nonadaptive and adaptive beamforming of input data vectors, based on the outcome of the comparison of the input coherence factor against a certain threshold. For illustrative purposes, we used the delay-and-sum (DAS) beamformer as an example of a nonadaptive method, while the Generalized Sidelobe Canceller (GSC) and Adaptive Single Snapshot Beamformer (ASSB) served as two examples of an adaptive method. We have applied our technique to simulated ultrasound images of a 12-point phantom and a point-scattering-cyst phantom, demonstrating substantial computational savings without a significant degradation in the image resolution and contrast, in comparison to the standard GSC-based or ASSB-based beamforming methods.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "Mutual information is one of the most widespread similarity criteria for multi-modal image registration but is limited to low dimensional feature spaces when calculated using histogram and kernel based entropy estimators. In the present article we propose the use of the Kozachenko-Leonenko entropy estimator (KLE) to calculate higher order regional mutual information using local features. The use of local information overcomes the two most prominent problems of nearest neighbor based entropy estimation in image registration: the presence of strong interpolation artifacts and noise. The performance of the proposed criterion is compared to standard MI on data with a known ground truth using a protocol for the evaluation of image registration similarity measures. Finally, we show how the use of the KLE with local features improves the robustness and accuracy of the registration of color colposcopy images.",
    "actual_venue": "Cvpr Workshops"
  },
  {
    "abstract": "Approximately one-third of all independently living elderly people are not compliant with their drug therapy. This lack of medication management results either in undermedication or overmedication, causing unnecessary and often serious health risks. This problem will worsen in the future with the change of demographics and cost constraints in the health sector. Therefore there is a need for (cost-) effective reliable approaches to compliance monitoring. To date numerous care schemes, retrospective assessment procedures and compliance supports tools have been introduced, but none of them has fully solved the problem of medication non-compliance yet. This paper will address some of the factors that need to be considered when designing such systems and will showcase DigiSpenser, a recently developed compliance monitoring and drug management system.",
    "actual_venue": "Conference Proceedings : Annual International Conference Of The Ieee Engineering In Medicine And Biology Society Ieee Engineering In Medicine And Biology Society Annual Conference"
  },
  {
    "abstract": "Hierarchical clustering algorithms can be agglomerative or divisive, depending on how partitions are formed. Such algorithms have advantages mainly related to the desired level of granularity the partition should have. The work described in this paper approaches two hierarchical algorithms, one agglomerative (and three of its variants) and the other divisive, focusing on their performance in unsupervised learning tasks related to gestalt clusters. Taking into account that the point sets considered are representative of gestalt clusters, the experiments show that the best results have been obtained when the agglomerative approach was used.",
    "actual_venue": "Intelligent Systems Design And Applications"
  },
  {
    "abstract": "The behavior of a class of hybrid systems in discrete-time can be represented by nonlinear difference equations with a Markov input. The analysis of such a system usually starts by establishing the Markov property of the joint process formed by combining the system's state and input. There are, however, no complete proofs of this property. This paper aims to address this problem by presenting a complete and explicit proof that uses only fundamental measure-theoretical concepts.",
    "actual_venue": "Journal Of The Franklin Institute"
  },
  {
    "abstract": "Shearlets have emerged in recent years as one of the most successful methods for the multiscale analysis of multidimensional signals. Unlike wavelets, shearlets form a pyramid of well-localized functions defined not only over a range of scales and locations, but also over a range of orientations and with highly anisotropic supports. As a result, shearlets are much more effective than traditional wavelets in handling the geometry of multidimensional data, and this was exploited in a wide range of applications from image and signal processing. However, despite their desirable properties, the wider applicability of shearlets is limited by the computational complexity of current software implementations. For example, denoising a single 512 × 512 image using a current implementation of the shearlet-based shrinkage algorithm can take between 10 s and 2 min, depending on the number of CPU cores, and much longer processing times are required for video denoising. On the other hand, due to the parallel nature of the shearlet transform, it is possible to use graphics processing units (GPU) to accelerate its implementation. In this paper, we present an open source stand-alone implementation of the 2D discrete shearlet transform using CUDA C++ as well as GPU-accelerated MATLAB implementations of the 2D and 3D shearlet transforms. We have instrumented the code so that we can analyze the running time of each kernel under different GPU hardware. In addition to denoising, we describe a novel application of shearlets for detecting anomalies in textured images. In this application, computation times can be reduced by a factor of 50 or more, compared to multicore CPU implementations.",
    "actual_venue": "Eurasip J Adv Sig Proc"
  },
  {
    "abstract": "For a two-variable formula B(X,Y) of Monadic Logic of Order (MLO) the Church synthesis problem concerns the existence and construction of a finite-state operator Y=F(X) such that B(X,F(X)) is universally valid over Nat.",
    "actual_venue": "Information And Computation"
  },
  {
    "abstract": "Given a set of experimentally determined lower and upper bounds on the distances between the atoms of a molecule, we study the minimum and maximum values that any one distance can attain when all of the remaining distances are confined between their lower and upper bounds. The triangle inequality may be used to derive a first approximation to these ‘distance limits’, and a complete characterization of these ‘triangle limits’, together with an efficient alogrithm for computing them, is presented. A four-point relation known as the ‘tetrangle inequality’ is then discussed as a possible means of obtaining an improved approximation.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "Genome assembly using high throughput data with short reads, arguably, remains an unresolvable task in repetitive genomes, since when the length of a repeat exceeds the read length, it becomes difficult to unambiguously connect the flanking regions. The emergence of third generation sequencing (Pacific Biosciences) with long reads enables the opportunity to resolve complicated repeats that could not be resolved by the short read data. However, these long reads have high error rate and it is an uphill task to assemble the genome without using additional high quality short reads. Recently, Koren et al. 2012 proposed an approach to use high quality short reads data to correct these long reads and, thus, make the assembly from long reads possible. However, due to the large size of both dataset (short and long reads), error-correction of these long reads requires excessively high computational resources, even on small bacterial genomes. In this work, instead of error correction of long reads, we first assemble the short reads and later map these long reads on the assembly graph to resolve repeats.   Contribution: We present a hybrid assembly approach that is both computationally effective and produces high quality assemblies. Our algorithm first operates with a simplified version of the assembly graph consisting only of long contigs and gradually improves the assembly by adding smaller contigs in each iteration. In contrast to the state-of-the-art long reads error correction technique, which requires high computational resources and long running time on a supercomputer even for bacterial genome datasets, our software can produce comparable assembly using only a standard desktop in a short running time.",
    "actual_venue": "Wabi"
  },
  {
    "abstract": "This paper analyzes the relationship among research collaboration, number of documents and number of citations of computer science research activity. It analyzes the number of documents and citations and how they vary by number of authors. They are also analyzed (according to author set cardinality) under different circumstances, that is, when documents are written in different types of collaboration, when documents are published in different document types, when documents are published in different computer science subdisciplines, and, finally, when documents are published by journals with different impact factor quartiles. To investigate the above relationships, this paper analyzes the publications listed in the Web of Science and produced by active Spanish university professors between 2000 and 2009, working in the computer science field. Analyzing all documents, we show that the highest percentage of documents are published by three authors, whereas single-authored documents account for the lowest percentage. By number of citations, there is no positive association between the author cardinality and citation impact. Statistical tests show that documents written by two authors receive more citations per document and year than documents published by more authors. In contrast, results do not show statistically significant differences between documents published by two authors and one author. The research findings suggest that international collaboration results on average in publications with higher citation rates than national and institutional collaborations. We also find differences regarding citation rates between journals and conferences, across different computer science subdisciplines and journal quartiles as expected. Finally, our impression is that the collaborative level (number of authors per document) will increase in the coming years, and documents published by three or four authors will be the trend in computer science literature.",
    "actual_venue": "Scientometrics"
  },
  {
    "abstract": "The Topology and Orchestration Specification for Cloud Applications (TOSCA) enables the description, provisioning, and management of complex cloud applications in a portable way. TOSCA, therefore, provides a comprehensive yet complex set of mechanisms that may hinder users from unleashing its power due to misusing or neglecting parts of those mechanisms. TOSCA has just been standardized and, although it seems to be highly adopted in industry, there is a lack of systematic research of its features and capabilities. In this work we discuss the design of basic building blocks for cloud applications, called node types, and show how they can benefit from a deep integration with TOSCA. We developed a generic architecture for the realization of TOSCA node types, show an implementation of this architecture and validate it based on a sample cloud application. Our work gives an insight into the capabilities of TOSCA with respect to enable the creation of portable cloud services based on a set of composable building blocks.",
    "actual_venue": "Cloud Computing"
  },
  {
    "abstract": "In this paper, a time-delayed impulsive controller is constructed and the uniformly asymptotic stability of chaotic systems based on a Takagi-Sugeno (TS) fuzzy model is investigated and some new and useful criteria are derived. In addition, the uniformly asymptotic synchronization of chaotic systems based on the TS fuzzy model is discussed using similar techniques. Finally, some numerical simulations are presented to illustrate the effectiveness and feasibility of the derived results.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "Very small instruction caches have been shown to greatly reduce fetch energy. However, for many appli- cations the use of a small filter cache can lead to an unacceptable increase in execution time. In this paper, we propose the Tagless Hit Instruction Cache (TH-IC), a technique for completely eliminating the performance penalty associated with filter caches, as well as a fur- ther reduction in energy consumption due to not having to access the tag array on cache hits. Using a few meta- data bits per line, we are able to more efficiently track the cache contents and guarantee when hits will occur in our small TH-IC. When a hit is not guaranteed, we can instead fetch directly from the L1 instruction cache, eliminating any additional cycles due to a TH-IC miss. Experimental results show that the overall processor en- ergy consumption can be significantly reduced due to the faster application running time and the elimination of tag comparisons for most of the accesses.",
    "actual_venue": "Micro"
  },
  {
    "abstract": "Optimization of the replacement policy used for Shared Last-Level Cache (SLLC) management in a Chip-MultiProcessor (CMP) is critical for avoiding off-chip accesses. Temporal locality, while being exploited by first levels of private cache memories, is only slightly exhibited by the stream of references arriving at the SLLC. Thus, traditional replacement algorithms based on recency are bad choices for governing SLLC replacement. Recent proposals involve SLLC replacement policies that attempt to exploit reuse either by segmenting the replacement list or improving the rereference interval prediction. On the other hand, inclusive SLLCs are commonplace in the CMP market, but the interaction between replacement policy and the enforcement of inclusion has barely been discussed. After analyzing that interaction, this article introduces two simple replacement policies exploiting reuse locality and targeting inclusive SLLCs: Least Recently Reused (LRR) and Not Recently Reused (NRR). NRR has the same implementation cost as NRU, and LRR only adds one bit per line to the LRU cost. After considering reuse locality and its interaction with the invalidations induced by inclusion, the proposals are evaluated by simulating multiprogrammed workloads in an 8-core system with two private cache levels and an SLLC. LRR outperforms LRU by 4.5&percnt; (performing better in 97 out of 100 mixes) and NRR outperforms NRU by 4.2&percnt; (performing better in 99 out of 100 mixes). We also show that our mechanisms outperform rereference interval prediction, a recently proposed SLLC replacement policy and that similar conclusions can be drawn by varying the associativity or the SLLC size.",
    "actual_venue": "Taco"
  },
  {
    "abstract": "Cloud Service Brokers (CSBs) rose in importance due to the diverse number of public and private cloud providers, and the need to manage the consumption of those services within an enterprise. As you can see in Figure 1, CSBs sit between the service consumers, which is typically an application, and the cloud service provider. CSBs provide the following services.",
    "actual_venue": "Ieee Cloud Computing"
  },
  {
    "abstract": "Binary neural networks (BNNs) are promising to deliver accuracy comparable to conventional deep neural networks at a fraction of the cost in terms of memory and energy. In this paper, we introduce the XNOR neural engine (XNE), a fully digital configurable hardware accelerator IP for BNNs, integrated within a microcontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid SRAM/stand...",
    "actual_venue": "Ieee Transactions On Computer-Aided Design Of Integrated Circuits And Systems"
  },
  {
    "abstract": "Mobile Geographic Information Systems are a versatile extension of traditional GIS and can provide GIS data between the field and the office. This paper addresses the challenges imposed on telecommunications infrastructure management by disaster events. MGIS are presented as tools that provide meaningful real-time communication between field workers and management.",
    "actual_venue": "Ijmc"
  },
  {
    "abstract": "The unscented Kalman filter (UKF) was recently introduced in literature for simultaneous multi-tensor estimation and tractography. This UKF however was not intrinsic to the space of diffusion tensors. Lack of this key property leads to inaccuracies in the multi-tensor estimation as well as in tractography. In this paper, we propose an novel intrinsic unscented Kalman filter (IUKF) in the space of symmetric positive definite matrices, which can be used for simultaneous recursive estimation of multi-tensors and tractography from diffusion weighted MR data. In addition to being more accurate, IUKF retains all the advantages of UKF for instance, multi-tensor estimation is only performed in the places where it is needed for tractography, which would be much more efficient than the two stage process involved in methods that do tracking post diffusion tensor estimation. The accuracy and effectiveness of the proposed method is demonstrated via real data experiments.",
    "actual_venue": "Biomedical Imaging"
  },
  {
    "abstract": "We propose a hybrid face recognition method that combines holistic and feature analysis-based approaches using a Markov random field (MRF) model. The face images are divided into small patches, and the MRF model is used to represent the relationship between the image patches and the patch ID's. The MRF model is first learned from the training image patches, given a test image. The most probable patch ID's is then inferred using the belief propagation (BP) algorithm. Finally, the ID of the test image is determined by a voting scheme from the estimated patch ID's. Experimental results on several face datasets indicate the significant potential of our method.",
    "actual_venue": "Icpr"
  },
  {
    "abstract": "The problem of matching a forensic sketch to a gallery of mug shot images is addressed in this paper. Previous research in sketch matching only offered solutions to matching highly accurate sketches that were drawn while looking at the subject (viewed sketches). Forensic sketches differ from viewed sketches in that they are drawn by a police sketch artist using the description of the subject provided by an eyewitness. To identify forensic sketches, we present a framework called local feature-based discriminant analysis (LFDA). In LFDA, we individually represent both sketches and photos using SIFT feature descriptors and multiscale local binary patterns (MLBP). Multiple discriminant projections are then used on partitioned vectors of the feature-based representation for minimum distance matching. We apply this method to match a data set of 159 forensic sketches against a mug shot gallery containing 10,159 images. Compared to a leading commercial face recognition system, LFDA offers substantial improvements in matching forensic sketches to the corresponding face images. We were able to further improve the matching performance using race and gender information to reduce the target gallery size. Additional experiments demonstrate that the proposed framework leads to state-of-the-art accuracys when matching viewed sketches.",
    "actual_venue": "Ieee Transactions On Pattern Analysis And Machine Intelligence"
  },
  {
    "abstract": "Outfit recommendation automatically pairs user-specified reference clothing with the most suitable complement from online shops. Wearing aesthetically is a criterion for matching such fashion items. Fashion style tells a lot about one's personality and emerges from how people assemble clothing outfit from seemingly disjoint items into a cohesive concept. Experts share fashion tips showcasing their compositions to public where each item has both an image and textual meta-data. Also, retrieving products from online shopping catalogs in response to such real-world image query is essential for outfit recommendation. Our earlier tutorial focused on style and compatibility in fashion recommendation mostly based on metric and deep learning approaches. Herein, we cover several other aspects of fashion recommendation using visual signals (e.g., cross-scenario retrieval, attribute classification) and combine text input (e.g., interpretable embedding) as well. Each section concludes walking through programs executed on Jupyter workstation using real-world data sets.",
    "actual_venue": "Proceedings Of The International Conference On Multimedia"
  },
  {
    "abstract": "Recently, the Industrial Internet of Things (IIoT) is attracting growing attention from both academia and industry. Meanwhile, trust-based communication is widely utilized in various systems. In this article, studying the performance of IIoT, we investigate trust-based communication for IIoT. In particular, devoting attention to sensor- cloud, which is a paradigm of IIoT, we propose three types of...",
    "actual_venue": "Ieee Communications Magazine"
  },
  {
    "abstract": "In this paper we examine a number of concepts and issues concerning variable rate coding of speech. We formulate the problem as a multistate coder (i.e. a coder that can operate at several bit rates) coupled with a time buffer. We first analyze the theoretical aspects of the problem by examining it in the context of a block processing formulation. We also allude to a multiple user configuration of variable rate coding for TASI type applications. A practical example of a variable rate ADPCM coder is presented and applied to speech coding. It is shown that by careful design the algorithm can be made to be as robust to channel errors as that of a fixed rate ADPCM coder.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Ieee International Conference Icassp"
  },
  {
    "abstract": "With the development of RGB-D sensors, the high-quality color point cloud can be obtained conveniently. Besides the geometrical information of point cloud, the color has great potential to assist the point cloud registration. In this paper, we propose a registration method by adaptively combining with color moment information to improve the registration accuracy. Firstly, three kinds of central moments are used to characterize the color distribution of two point clouds. And then, we build the correspondence between point clouds by dynamically combining the geometric feature and color moment feature of each point, hence the corresponding points satisfy both geometric similarity and color similarity. Finally, for the partial registration problem in practice, we apply the improved Trimmed Iterative Closest Point (TrICP) algorithm framework to calculate the rigid transformation. Experimental results demonstrate that our algorithm is more robust and accurate in dealing with point cloud geometry defects, missing data and poor initial position.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "Cache-enabled heterogeneous cellular networks (HCNs) have been investigated extensively to alleviate backhaul congestion and reduce content delivery delay. In this paper, we jointly optimize content placement and user association to minimize the average content delivery delay in cache-enabled HCNs based on flow-level models. This formulation considers (1) different timescales of content placement and content delivery, (2) locality of content popularity, and (3) the heterogeneity of spatial traffic distribution, which are often neglected in existing researches. The joint optimization problem is formulated as a mixed integer nonlinear programming problem in load-non-coupled and load-coupledmodels, respectively. We decouple this problem into two interrelated subproblems and resolve them individually. For the user association problem under a given content placement situation, we propose a content-level selective association algorithm, which allows the requests for different contents at the same location to connect to different base stations (BSs). In addition, we propose a greedy content caching algorithm to add contents to the caches of BSs in an iterative manner. These two algorithms are alternately executed until the caches of all the BSs are filled to capacity. Simulation results show that the proposed algorithm achieves better performance in terms of average delay and backhaul usage compared with traditional content placement and user association approaches.",
    "actual_venue": "Wireless Communications And Mobile Computing"
  },
  {
    "abstract": "A synthetic fiber rope, which is lightweight and has a high tensile strength and flexibility, is receiving considerable attention as a replacement for a stainless steel wire rope. This letter describes its ability to endure repetitive bending. We performed experiments in conformity with the ISO 2020-2 standard using ten synthetic fiber ropes made of different materials and with different compositi...",
    "actual_venue": "Ieee Robotics And Automation Letters"
  },
  {
    "abstract": "Many long-running network analytics applications impose a high-throughput and high reliability requirements on stream processing systems. However, previous stream processing systems cannot sustain high-speed traffic at the core router level. Furthermore, their fault-tolerant schemes cannot provide strong consistency which is essential for network analytics. In this paper, we present the design and implementation of SAND, a fault-tolerant distributed stream processing system for network analytics. SAND is designed to operate under high-speed network traffic, and it uses a novel check pointing protocol which can perform failure recovery based on upstream backup and check pointing. We prove our fault-tolerant scheme provides strong consistency even under multiple node failure. We implement several real-world network analytics applications on SAND, evaluate their performance using network traffic captured from commercial cellular core networks, and demonstrate that SAND can sustain high-speed network traffic and that our fault-tolerant scheme is efficient.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "To better protect information systems, computation time data protection needs to be considered such that even when the system is partially compromised, the security of the system can still be assured. All existing approaches incur high overhead. Some do not support full computation power. In this paper, we propose a secure computation algorithm with the goals of reducing the communication overhead and providing full computation power. We reduce the communication overhead by aggregating multiple operations. Our algorithm has full computation power, and yields better performance than existing approaches. The proposed algorithm focuses on confidentiality assurance. Replication schemes can be combined with the algorithm to assure system integrity.",
    "actual_venue": "Hase"
  },
  {
    "abstract": "Waves have long been thought to be a fundamental mechanism for communicating information within a medium and are widely observed in biological systems. However, a quantitative analysis of biological waves is confounded by the variability and complexity of the response. This paper proposes a robust technique for extracting wave structure from experimental data by calculating “wave subspaces” from the KL decomposition of the data set. If a wave subspace contains a substantial portion of the data set energy during a particular time interval, one can deduce the structure of the wave and potentially isolate its information content. This paper uses the wave subspace technique to extract and compare wave structure in data from three different preparations of the turtle visual cortex. The paper demonstrates that wave subspace caricatures from the three cortical preparations have qualitative similarities. In the numerical model, where information about the underlying dynamics is available, wave subspace landmarks are related to activation and changes in behavior of other dynamic variables besides membrane potential.",
    "actual_venue": "Journal Of Computational Neuroscience"
  },
  {
    "abstract": "Although some papers argued that multi-sensor fusion could improve performances and robustness of fingerprint verification systems, no previous work explicitly dealt with such topic, and no experimental evidence has been reported. In this paper, we show by experiments that a significant performance improvement can be obtained by decision-level fusion of two well-known fingerprint capture devices. As, to the best of our knowledge, this is the first work on multi-sensor fingerprint fusion, we believe that it can contribute to stimulate further researches on this promising topic.",
    "actual_venue": "Biometric Authentication, Proceedings"
  },
  {
    "abstract": "Level-one data cache (L1 DC) and data translation lookaside buffer (DTLB) accesses impact energy usage as they frequently occur and each L1 DC and DTLB access uses significantly more energy than a register file access. Often, multiple memory operations will reference the same cache line using the same register, such as when iterating through an array. We propose to memoize L1 DC access information, such as the L1 DC data array way and the DTLB way, by associating this information with the register used to access it. When a load or store calculates the memory address, we detect whether the calculated address shares the cache line memoized with the base register. If so, we avoid the L1 DC tag array access and the DTLB access to determine the L1 DC way and instead use the memoized information. In addition, only a single data array way in a set-associative L1 DC needs to be accessed during a load instruction when the L1 DC way has been memoized. Our nonspeculative memoization approach can be applied before a speculative approach, allowing a significant reduction in data access energy usage for existing executables with no ISA modifications.",
    "actual_venue": "Ieee/Acm International Symposium On Low Power Electronics And Design"
  },
  {
    "abstract": "An algorithm based on information retrieval that applies the lexical database WordNet together with a linear discriminant function is proposed. It calculates the degree of similarity between words and their relative importance to support the development of distributed applications based on web services. The algorithm uses the semantic information contained in the Web Service Description Language specifications and ranks web services based on their similarity to the one the developer is searching for. It is applied to a set of 48 real web services in five categories, then compared them to four other algorithms based on information retrieval, showing an averaged improvement over all data between 0.6% and 1.9% in precision and 0.7% and 3.1% in recall for the top 15 ranked web services. The objective was to reduce the burden and time spent searching web services during the development of distributed applications, and it can be used as an alternative to current web service discovery systems such as brokers in the Universal Description, Discovery, and Integration (UDDI) platform.",
    "actual_venue": "Journal Of Advanced Computational Intelligence And Intelligent Informatics"
  },
  {
    "abstract": "Supermodular functions can be represented as minima of suitable modular functions under certain constraints. A general order-theoretic condition allowing such representations for functions on orders and lattices respectively will be introduced. The underlying idea is redistribution (movement) of “mass” given by the Möbius inverse of the function to be expressed. The representation of a (Choquet) capacity as a minimum of probability distributions — a so-called lower probability — is an important special case.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "The memory architecture has the huge impact on the performance of embedded systems. Most of the current on-chip memory techniques focused on the optimization of timing performance, power consumption, and area. Thermal issue of memory subsystem has not been fully considered in these techniques. For on-chip memory architectures, the high temperature may cause the exponential increase of leakage, which becomes one of the major factors of power consumption. With the shrinkage of feature size and the great demand of silicon area, on-chip memory is the dominate contributor of the total leakage. Therefore, it is a challenging issue to manage the thermal cost of the on-chip memory architectures. In this paper, we focus on the optimization of the on-chip memory architecture that consists of cache and scratchpad memory (SPM). Our objective is to optimize the thermal behavior of the memory components for a target application with loops, while keeping the timing performance. We propose a thermal-aware memory architecture exploration algorithm TAME, and a thermal-aware data allocation algorithm TADA. These two algorithms collaborate to perform the memory architecture exploration, considering memory components' type, size, power, area and timing performance. Experimental results show that our method reduces the peak temperature of on-chip memory subsystem significantly, and at the same time the timing performance is even improved by making fully use of SPM.",
    "actual_venue": "Trustcom/Ispa/Iucc"
  },
  {
    "abstract": "This paper presents a texture descriptor based on wavelet frame transforms. At each position in the image, and for each resolution level, we consider both vertical and horizontal wavelet detail coefficients as the components of a bivariate random vector. The magnitudes and angles of these vectors are computed. At each level the empirical histogram of magnitudes is modeled by a Generalized Gamma distribution, and the empirical histogram of angles is modeled by a different version of the von Mises distribution that accounts for histograms with 2 modes. Each texture is characterized by few parameters. A new distance is presented (based on the Kullback–Leibler divergence) that allows giving relative importance to each model and to each resolution level. This distance is later conveniently adapted to provide for rotation invariance, by establishing equivalence classes over distributions of angles. Through a broad set of experiments on three different image databases, we demonstrate that our new descriptor and distance measure can be successfully applied in the context of texture retrieval. We compare our system to several relevant methods in this field in terms of retrieval performance and number of parameters used by each method. We also include some classification tests. In all the tests, we obtain superior retrieval rates for a set of fewer parameters involved.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": ".This paper analyses the 2D motion field on the image plane produced by the 3D motion of a planeundergoing simple deformations. When the deformation can be represented by a planar linear vectorfield, the projected vector field, i.e. the 2D motion field of the deformation, is at most quadratic. This2D motion field has one singular point, with eigenvalues identical to those of the singular point describingthe deformation. As a consequence, the nature of the singular point of the deformation ...",
    "actual_venue": "International Journal Of Computer Vision"
  },
  {
    "abstract": "Many apps for mobile devices show lists of elements that the user can sort by choosing a single metric, e.g. price or date or alphabetical order. Often, none of the different sorting is ideal, in fact elements that are more interesting to the user are often spread along the list as there is no single sorting that can group them together at the top. We propose that the user can sort the list by including two or more metrics to create a personalized ranking. Sorting is thus multidimensional and the order of the chosen metrics defines a different ranking according to predefined weights. We designed and developed an iOS framework that implements a multidimensional sortable list, with a drag-and-drop interface to let the user choose the personal order of metrics. We performed qualitative user tests with 8 users.",
    "actual_venue": "AVI"
  },
  {
    "abstract": "In the last years, with the increase of the available data from social networks and the rise of big data technologies, social data has emerged as one of the most profitable market for companies to increase their benefits. Besides, social computation scientists see such data as a vast ocean of information to study modern human societies. Nowadays, enterprises and researchers are developing their own mining tools in house, or they are outsourcing their social media mining needs to specialised companies with its consequent economical cost. In this paper, we present the first cloud computing service to facilitate the deployment of social media analytics applications to allow data practitioners to use social mining tools as a service. The main advantage of this service is the possibility to run different queries at the same time and combine their results in real time. Additionally, we also introduce twearch, a prototype to develop twitter mining algorithms as services in the cloud.",
    "actual_venue": "Lecture Notes In Artificial Intelligence"
  },
  {
    "abstract": "This paper defines, for use in design, rules for propagating \"distribution constraints\" through relationships such as algebraic or vector equations. Distribution constraints are predicate logic statements about the values that physical system parameters may assume. The propagation rules take into account \"variation source causality\": information about when and how the values are assigned during the design, manufacturing, and operation of the system.",
    "actual_venue": "Ai Edam Artificial Intelligence For Engineering Design, Analysis And Manufacturing"
  },
  {
    "abstract": "While multiple alignment is the first step of usual classification schemes for biological sequences, alignment-free methods are being increasingly used as alternatives when multiple alignments fail. Subword-based combinatorial methods are popular for their low algorithmic complexity (suffix trees ...) or exhaustivity (motif search), in general with fixed length word and/or number of mismatches. We developed previously a method to detect local similarities (the N-local decoding) based on the occurrences of repeated subwords of fixed length, which does not impose a fixed number of mismatches. The resulting similarities are, for some \"good\" values of N, sufficiently relevant to form the basis of a reliable alignment-free classification. The aim of this paper is to develop a method that uses the similarities detected by N-local decoding while not imposing a fixed value of N. We present a procedure that selects for every position in the sequences an adaptive value of N, and we implement it as the MS4 classification tool.Among the equivalence classes produced by the N-local decodings for all N, we select a (relatively) small number of \"relevant\" classes corresponding to variable length subwords that carry enough information to perform the classification. The parameter N, for which correct values are data-dependent and thus hard to guess, is here replaced by the average repetitivity kappa of the sequences. We show that our approach yields classifications of several sets of HIV/SIV sequences that agree with the accepted taxonomy, even on usually discarded repetitive regions (like the non-coding part of LTR).The method MS4 satisfactorily classifies a set of sequences that are notoriously hard to align. This suggests that our approach forms the basis of a reliable alignment-free classification tool. The only parameter kappa of MS4 seems to give reasonable results even for its default value, which can be a great advantage for sequence sets for which little information is available.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "We for the first time combine generated adversarial network (GAN) with wide-field light microscopy to achieve deep learning super-resolution under a large field of view (FOV). By appropriately adopting prior microscopy data in an adversarial training, the network can recover a high-resolution, accurate image of new specimen from its single low-resolution measurement. This capacity has been adequately demonstrated by imaging various types of samples, such as USAF resolution target, human pathological slides and fluorescence-labelled fibroblast cells. Their gigapixel, multi-color reconstructions verify a successful GAN-based single image super-resolution procedure. Furthermore, this deep learning-based imaging approach doesn;t necessarily introduce any change to the setup of a conventional wide-filed microscope, reconstructing large FOV (about 95 mm^2), high-resolution (about 1.7 {mu}m) image at a high speed (in 1 second). As a result, GAN-microscopy opens a new way to computationally overcome the general challenge of high-throughput, high-resolution microscopy that is originally coupled to the physical limitation of systemu0027s optics.",
    "actual_venue": "Arxiv: Image And Video Processing"
  },
  {
    "abstract": "Citation analysis was traditionally based on data from the ISI Citation indexes. Now with the appearance of Scopus, and with the free citation tool Google Scholar methods and measures are need for comparing these tools. In this paper we propose a set of measures for computing the similarity between rankings induced by ordering the retrieved publications in decreasing order of the number of citations as reported by the specific tools. The applicability of these measures is demonstrated and the results show high similarities between the rankings of the ISI Web of Science and Scopus and lower similarities between Google Scholar and the other tools.",
    "actual_venue": "Journal Of Informetrics"
  },
  {
    "abstract": "This paper mainly studies the stability and Hopf bifurcation criteria of hub-based genetic regulatory networks with multiple delays and bidirectional couplings. The hub-structured network is an important motif in complex networks, which provides a new view angle on structure to describe the regulation mechanism between genes (including both mRNAs and proteins). It is well known that hubs play a le...",
    "actual_venue": "Ieee Transactions On Circuits And Systems -Regular Papers"
  },
  {
    "abstract": "With the speed and power bottleneck in the conventional Von Neumann architecture, the interest in the neuromorphic systems has greatly increased in recent years. To create a highly dense communication network between the pre- and post-neurons, RRAM devices are used as synapses in the neuromorphic systems due to many advantages including their small sizes and low-power operations. However, due to RRAM reliability issues, in particular soft-errors, the performance of the RRAM-based neuromorphic systems are significantly degraded. In this article, we propose a novel framework for detecting and resolving the degradation in the system performance due to the RRAM reliability soft-errors. The read and write circuits modifications to implement the framework, and their impact on the delay and energy consumption of the neuromorphic system are also discussed in this article. Using a combination of BRIAN and SPICE simulations, we demonstrate that the proposed framework can restore the accuracy of the example RRAM-based neuromorphic system from 43% back to its target value of 91.6% with a minimal impact on the read (",
    "actual_venue": "Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "The computational performance of Network-on-Chip (NoC) and Multi-Processor System-on-Chip (MPSoC) for implementing cryptographic block ciphers can be improved by exploiting parallel and pipeline execution. In this paper, we present a parallel and pipeline processing method for block cipher algorithms: Data Encryption Standard (DES), Triple-DES Algorithm (TDEA), and Advanced Encryption Standard (AES) based on pure software implementation on an NoC. The algorithms are decomposed into task loops, functions, and data flow for parallel and pipeline execution. The tasks are allocated by the proposed mapping strategy to each Processing Element (PE) which consists of a 32-bit Reduced Instruction Set Computer (RISC) core, internal memory, router, and Network Interface (NI) to communicate between PEs.The proposed approach is simulated by using Networked Processor Array (NePA), the cycle-accurate SystemC and Hardware Description Language (HDL) model platform. We show that our method has the advantage of flexibility as compared to previous implementations of cryptographic algorithms based on hardware and software co-design or traditional hardwired ASIC design. In addition, the simulation result presents that the parallel and pipeline processing approach for software block ciphers can be implemented on various NoC platforms which have different complexities and constraints.",
    "actual_venue": "Itng"
  },
  {
    "abstract": "We present a method for reconstruction of the visual hull (VH) of an object in real-time from multiple video streams. A state of the art polyhedral reconstruction algorithm is accelerated by implementing it for parallel execution on a multi-core graphics processor (GPU). The time taken to reconstruct the VH is measured for both the accelerated and non-accelerated implementations of the algorithm, over a range of image resolutions and number of cameras. The results presented are of relevance to researchers in the field of 3D reconstruction at interactive frame rates (real-time), for applications such as telepresence.",
    "actual_venue": "VR"
  },
  {
    "abstract": "Driver fatigue has become an important factor to traffic accidents worldwide, and effective detection of driver fatigue has major significance for public health. The purpose method employs entropy measures for feature extraction from a single electroencephalogram (EEG) channel. Four types of entropies measures, sample entropy (SE), fuzzy entropy (FE), approximate entropy (AE), and spectral entropy (PE), were deployed for the analysis of original EEG signal and compared by ten state-of-the-art classifiers. Results indicate that optimal performance of single channel is achieved using a combination of channel CP4, feature FE, and classifier Random Forest (RF). The highest accuracy can be up to 96.6%, which has been able to meet the needs of real applications. The best combination of channel + features + classifier is subject-specific. In this work, the accuracy of FE as the feature is far greater than the Acc of other features. The accuracy using classifier RF is the best, while that of classifier SVM with linear kernel is the worst. The impact of channel selection on the Acc is larger. The performance of various channels is very different.",
    "actual_venue": "Computational And Mathematical Methods In Medicine"
  },
  {
    "abstract": "We present a novel attack named \"Tap 'n Ghost\", which aims to attack the touchscreens of NFC-enabled mobile devices such as smartphones. Tap 'n Ghost consists of two striking attack techniques - \"Tag-based Adaptive Ploy (TAP)\" and \"Ghost Touch Generator.\" First, using a NFC card emulator embedded in a common object such as table, a TAP system performs tailored attacks on the victim's smartphone by employing device fingerprinting; e.g., popping up a customized dialogue box asking whether or not to connect to an attacker's Bluetooth mouse. Further, Ghost Touch Generator forces the victim to connect to the mouse even if she or he aimed to cancel the dialogue by touching the \"cancel\" button; i.e., it alters the selection of a button on a screen. After the connection is established, the attacker can remotely take control of the smartphone, with the knowledge about the layout of the screen derived from the device fingerprinting. To evaluate the reality of the attack, we perform an online survey with 300 respondents and a user study involving 16 participants. The results demonstrate that the attack is realistic. We additionally discuss the possible countermeasures against the threats posed by Tap 'n Ghost.",
    "actual_venue": "Ieee Symposium On Security And Privacy"
  },
  {
    "abstract": "We present a new algorithm to compute a topologically and geometrically accurate triangulation of an implicit surface. Our approach uses spatial subdivision techniques to decompose a manifold implicit surface into star-shaped patches and computes a visibilty map for each patch. Based on these maps, we compute a homeomorphic and watertight triangulation as well as a parameterization of the implicit surface. Our algorithm is general and makes no assumption about the smoothness of the implicit surface. It can be easily implemented using linear programming, interval arithmetic, and ray shooting techniques. We highlight its application to many complex implicit models and boundary evaluation of CSG primitives.",
    "actual_venue": "Symposium On Geometry Processing"
  },
  {
    "abstract": "In recent years the scientific research has undergone substantial changes. In particular, there is a greater collaboration between research groups, which leads to an increase in the use of information processing techniques, and, therefore, the need to share results and observations among participants of a research. This work has as main goal to propose an architecture to support distributed processing of scientific experiments, as an implementation of so-called collaborative laboratories.",
    "actual_venue": "Ieee International Conference On Computer Supported Cooperative Work In Design"
  },
  {
    "abstract": "Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more effective overall than those produced by the white-box techniques considered; however, the faults detected by the two classes of techniques differ, suggesting that the techniques are complementary.",
    "actual_venue": "Ieee Trans Software Eng"
  },
  {
    "abstract": "•We propose a combined variational SR model in a well posed framework based on the bilateral total variation and second order term.•The existence of a solution to the proposed minimization problem is assured.•The improved regularization is efficient in degraded image super-resolution task using primal-dual algorithm.•The proposed method gives better performance comparing with competitive approaches.",
    "actual_venue": "Computer Vision And Image Understanding"
  },
  {
    "abstract": "Using articulatory features for speech recognition improves the performance of low-resource languages. One way to obtain articulatory features is by using an articulatory classifier (pseudo articulatory features). The performance of the articulatory features depends on the efficacy of this classifier. But, training such a robust classifier for a low-resource language is constrained due to the limited amount of training data. We can overcome this by training the articulatory classifier using a high resource language. This classifier can then he used to generate articulatory features for the low-resource language. However, this technique fails when high and low-resource languages have mismatches in their environmental conditions. In this paper, we address both the aforementioned problems by jointly estimating the articulatory features and low-resource acoustic model. The experiments were performed on two low-resource Indian languages namely, Hindi and Tamil. English was used as the high-resource language. A relative improvement of 23% and 10% were obtained for Hindi and Tamil, respectively.",
    "actual_venue": "Annual Conference Of The International Speech Communication Association , Vols -: Situated Interaction"
  },
  {
    "abstract": "A deep belief network (DBN) is effective to create a powerful generative model by using training data. However, it is difficult to fast determine its optimal structure given specific applications. In this paper, a growing DBN with transfer learning (TL-GDBN) is proposed to automatically decide its structure size, which can accelerate its learning process and improve model accuracy. First, a basic DBN structure with single hidden layer is initialized and then pretrained, and the learned weight parameters are frozen. Second, TL-GDBN uses TL to transfer the knowledge from the learned weight parameters to newly added neurons and hidden layers, which can achieve a growing structure until the stopping criterion for pretraining is satisfied. Third, the weight parameters derived from pretraining of TL-GDBN are further fine-tuned by using layer-by-layer partial least square regression from top to bottom, which can avoid many problems of traditional backpropagation algorithm-based fine-tuning. Moreover, the convergence analysis of the TL-GDBN is presented. Finally, TL-GDBN is tested on two benchmark data sets and a practical wastewater treatment system. The simulation results show that it has better modeling performance, faster learning speed, and more robust structure than existing models. \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Note to Practitioners</italic>\n—Transfer learning (TL) aims to improve training effectiveness by transferring knowledge from a source domain to target domain. This paper presents a growing deep belief network (DBN) with TL to improve the training effectiveness and determine the optimal model size. Facing a complex process and real-world workflow, DBN tends to require long time for its successful training. The proposed growing DBN with TL (TL-GDBN) accelerates the learning process by instantaneously transferring the knowledge from a source domain to each new deeper or wider substructure. The experimental results show that the proposed TL-GDBN model has a great potential to deal with complex system, especially the systems with high nonlinearity. As a result, it can be readily applicable to some industrial nonlinear systems.",
    "actual_venue": "Ieee Transactions On Automation Science And Engineering"
  },
  {
    "abstract": "This paper develops two theories of data abstraction and refinement: one for applicative types, as they are found in functional programming languages, and one for state-based types found in imperative languages. The former are modelled by algebraic structures, the latter by automata. The automaton-theoretic model covers not just data types but distributed systems in general. Within each theory two examples of data refinement are presented and formally verified with the theorem prover Isabelle. The examples are an abstract specification and two implementations of a memory system, and a mutual exclusion algorithm.",
    "actual_venue": "Rex Workshop Proceedings On Stepwise Refinement Of Distributed Systems: Models, Formalisms, Correctness"
  },
  {
    "abstract": "This paper describes a general method for modifying conventional binary asynchronous counters such that the counting register advances by any desired odd integer for each received count. The pertinent design features of conventional additive and subtractive asynchronous counters are reviewed. Simplification of the design of a counter which advances by an odd integer is achieved through use of a set of alternately additive and subtractive subcounters. An example of the logical design of a counter which advances by 13 is presented.",
    "actual_venue": "Electronic Computers, Ieee Transactions"
  },
  {
    "abstract": "A processor architecture combining high-performance and low-power is presented. A prototype chip, Xetal-II, has been realized in 90 nm CMOS technology based on the proposed architecture. Recent experimental results show a compute performance of up to 140 GOPS at 785 mW when operating at 110 MHz. The main architectural feature that allows high computational efficiency is the massively-parallel single-instruction multiple-data (MP-SIMD) compute paradigm. Due to the high data-level parallelism, applications like video scene analysis can efficiently exploit the proposed architecture. The chip has an internal 16-bit datapath and 10 Mbit of on-chip video memory facilitating energy efficient implementation of video processing kernels.",
    "actual_venue": "Signal Processing Systems"
  },
  {
    "abstract": "The quality-of-service (QoS) communication that supports mobile applications to guarantee bandwidth utilization is an important issue for Bluetooth wireless personal area networks (WPANs). In this paper, we address the problem of on-demand QoS routing with interpiconet scheduling in Bluetooth WPANs. A credit-based QoS (CQ) routing protocol is developed which considers different Bluetooth packet types because of different types of Bluetooth packets have different bandwidth utilization levels. This work improves the bandwidth utilization of Bluetooth scatternets by providing a new interpiconet scheduling scheme. Centralized and distributed algorithms are investigated in this work to improve the bandwidth utilization for the on-demand QoS routing protocol The performance analysis illustrates that our credit-based QoS routing protocol achieves enhanced performances, compared to existing QoS routing protocols.",
    "actual_venue": "International Symposium On Computers And Communications"
  },
  {
    "abstract": "The monitoring of agricultural areas is one of the most important topics for remote sensing data analysis, especially to assist food security in the future. To improve the quality and quantify uncertainties, it is of high relevance to understand the spectral reflectivity regarding the structural and spectral properties of the canopy. The importance of understanding the influence of plant and canopy structure is well established, but, due to the difficulty of acquiring reflectance data from numerous differently structured canopies, there is still a need to study the structural and spectral dependencies affecting top-of-canopy reflectance and reflectance anisotropy. This paper presents a detailed study dealing with two fundamental issues: (1) the influence of plant and canopy architecture changes due to crop phenology on nadir acquired cereal top-of-canopy reflectance, and (2) the anisotropic reflectance of cereal top-of-canopy reflectance and its inter-annual variations as affected by varying contents of biochemical constituents and changes on canopy structure across green phenological stages between tillering and inflorescence emergence. All of the investigations are based on HySimCaR, a computer-based approach using 3D canopy models and Monte Carlo ray tracing (drat). The achieved results show that the canopy architecture significantly influences top-of-canopy reflectance and the bidirectional reflectance function (BRDF) in the VNIR (visible and near infrared), and SWIR (shortwave infrared) wavelength ranges. In summary, it can be said that the larger the fraction of the radiation reflected by the plants, the stronger is the influence of the canopy structure on the reflectance signal. A significant finding for the anisotropic reflectance is that the relative row orientation of the cereal canopies is mapped in the 3D-shape of the BRDF. Summarised, this study provides fundamental knowledge for improving the retrieval of biophysical vegetation parameters of agricultural areas for current and upcoming sensors with large FOV (field of view) with respect to the quantification of uncertainties.",
    "actual_venue": "Remote Sensing"
  },
  {
    "abstract": "Carpooling is a means of vehicle sharing by which drivers share their cars with one or more riders whose travel itineraries are similar to their own. As such, carpooling can be an effective way to ease traffic congestion. In this paper, we first present an intelligent carpool system based on the service-oriented architecture. Second, we propose a fuzzy-controlled genetic-based carpool algorithm by...",
    "actual_venue": "Ieee Transactions On Fuzzy Systems"
  },
  {
    "abstract": "We describe a \"factoring\" method which constructs all twenty-seven Hadamard (16, 6, 2) difference sets. The method involves identifying perfect ternary arrays of energy 4 (PTA(4)) in homomorphic images of a group G, studying the image of difference sets under such homomorphisms and using the preimages of the PTA(4)s to find the \"factors\" of difference sets in G. This \"factoring\" technique generalizes to other parameters, offering a general mechanism for creating Hadamard difference sets.",
    "actual_venue": "The Electronic Journal Of Combinatorics"
  },
  {
    "abstract": "Dynamic programming recursive equations are used to develop a procedure to obtain the set of efficient solutions to the multicriteria integer linear programming problem. An alternate method is produced by combining this procedure with branch and bound rules. Computational results are reported.",
    "actual_venue": "Math Program"
  },
  {
    "abstract": "In this paper, we propose a novel framework for enhancement of very low-light video. For noise reduction, motion adaptive temporal filtering based on the Kalman structured updating is presented. Dynamic range of denoised video is increased by adaptive adjustment of RGB histograms. Finally, remaining noise is removed using Non-local means (NLM) denoising. The proposed method exploits color filter array (CFA) raw data for achieving low memory consumption.",
    "actual_venue": "Icce"
  },
  {
    "abstract": "In sign language recognition, one of the problems is to collect enough training data. Almost all of the statistical methods used in sign language recognition suffer from this problem. Inspired by the crossover of genetic algorithms, this paper presents a method to expand Chinese sign language (CSL) database through re-sampling from existing sign samples. Two original samples of the same sign are regarded as parents. They can reproduce their children by crossover. To verify the validity of the proposed method, some experiments are carried out on a vocabulary of 2435 gestures in Chinese sign language. Each gesture has 4 samples. Three samples are used to be the original generation. These three original samples and their offspring are used to construct the training set, and the remaining sample is used for test. The experimental results show that the new samples generated by the proposed method are effective",
    "actual_venue": "FG"
  },
  {
    "abstract": "Common group key agreement protocols are not applicable in ad hoc networks because the dynamic and multi-hop nature. Clustering is a method by which nodes are hierarchically organized based on their relative proximity to one another. Driven by this insight, a hierarchical key agreement protocol is proposed to weaken the 1-hop assumption in common group key agreement protocols. We employ Joux's tripartite protocol and a generalized Diffie-Hellman protocol as the basic building block for group key agreement. The protocol can handle efficiently the dynamic events in ad hoc networks. Moreover, in order to authenticate the messages, a provable ID-based signature scheme is presented. The analysis results indicate that the proposed protocol is secure in withstanding many common attacks and is extremely efficient and feasible to ad hoc networks with large size.",
    "actual_venue": "Cans"
  },
  {
    "abstract": "Maps of high-resolution surface shortwave net radiation (SSNR) are important for resolving differences in the surface energy budget at the ecosystem level. The maps can also bridge the gap between existing coarse-resolution SSNR products and point-based field measurements. This study presents a modified hybrid method to estimate both instantaneous and daily SSNR from Landsat data. SSNR values are directly linked to Landsat top-of-atmosphere reflectance by extensive radiative transfer simulation. Regression coefficients are pre-calculated and stored in a look-up table (LUT). Atmospheric water vapor is a key parameter affecting SSNR, and three methods of treating water vapor are evaluated in this study. Comparison between Landsat retrievals and field measurements at six AmeriFlux sites shows that the hybrid method with water vapor as a dimension of LUT can estimate SSNR with a root mean square error of 77.5 W/m2 (instantaneous) and 36.1 W/m2 (daily). The method of water vapor correction produces similar results. However, a generic LUT that covers all levels of water vapor results in much larger errors.",
    "actual_venue": "Ieee Geosci Remote Sensing Lett"
  },
  {
    "abstract": "The scenarios opened by the increasing availability, sharing and dissemination of music across the Web is pushing for fast, effective and abstract ways of organizing and retrieving music material. Automatic classification is a central activity to model most of these processes, thus its design plays a relevant role in advanced Music Information Retrieval. In this paper, we adopted a state-of-the-art machine learning algorithm, i.e. Support Vector Machines, to design an automatic classifier of music genres. In order to optimize classification accuracy, we implemented some already proposed features and engineered new ones to capture aspects of songs that have been neglected in previous studies. The classification results on two datasets suggest that our model based on very simple features reaches the state-of-art accuracy (on the ISMIR dataset) and very high performance on a music corpus collected locally.",
    "actual_venue": "Riao"
  },
  {
    "abstract": "In this paper, we propose a new method for searching and browsing news videos, based on multi-modal approach. In the proposed scheme, we use closed caption (CC) data to index the contents of TV news articles effectively. To achieve time alignment between the CC texts and video data, which is necessary for multi-modal search and visualization, supervised speech recognition technique is employed. In our implementations, we provide two different mechanisms for news video browsing. One is to use a textual query based search engine, and the other is to use topic based browser which acts as an assistant tool for finding the desired news articles. Compared to other systems mainly dependent on visual features, the proposed scheme could retrieve more semantically relevant articles quite well.",
    "actual_venue": "Pacific Rim Conference On Multimedia"
  }
]