[
  {
    "abstract": "Libraries incessantly undergo change determined by evolving user needs. These are often induced by the emergence of previously unavailable tools. Web 2.0 represents an example of such a need-shifting technology, which has led to an embrace of new user interactivity services for many library websites, thus coined Library 2.0. This paradigm shift calls for new evaluation models to include the implementation of Web 2.0 technologies. The aim of this paper is to present such a model, and to evaluate the quality of Library 2.0 functionalities, measuring the quality of the 2.0 services offered through the websites based on user perception. We adopt fuzzy linguistic modeling to represent user perception, and apply aggregation operations to linguistic labels in order to evaluate the quality of the new services. Furthermore, our model subsumes the LibQUAL+methodology, allowing for the identification of specific 2.0 functionalities in need of improvement and of those outstandingly satisfied by the system. (C) 2013 Elsevier Ltd. All rights reserved.",
    "actual_venue": "International Journal Of Information Management"
  },
  {
    "abstract": "Future broadband networks will provide end-to-end connections which span multiple networks, operators, domains and technologies. The abil- ity to supply end-to-end connections depends on the ability to satisfy (in near real-time) end-user topology, schedule (availability of resources in the present or future), quality of service and cost constraints. This paper examines the problems imposed by schedule constraints, as they allow operators and service providers the ability to differentiate their offerings above the traditional con- nectivity, QoS, and cost basis. Such constraints are strongly linked to QoS is- sues, and also to maximizing the use of shared network resources and are there- fore quite important for management of future broadband networks. In order to satisfy temporal constraints imposed by scheduled connections, both the man- agement systems and managed resources involved must provide appropriate support. This paper presents an overview of several important management is- sues that are involved in the support of scheduled connections.",
    "actual_venue": "Isandn"
  },
  {
    "abstract": "Response latencies were recorded during a continuous performance task using four standard microcomputer response media: the standard QWERTY keyboard, a numeric keypad, a light-pen and a touch-screen. There were significant differences among these media, the fastest responses being made with the touch-screen, and the slowest with the light-pen. The keypad was superior to the full keyboard. Within devices, the physical layout of the key-based devices accounted for the differences among responses with specific keys. A common pattern of response using the touch-screen and light-pen was found, with a constant offset of 309 ms for the light-pen. A tentative model of response execution with these screen-based devices is presented, with implications for the nature of response parameters in unconstrained response environments, and for the design of human-computer interfaces.",
    "actual_venue": "International Journal Of Man-Machine Studies"
  },
  {
    "abstract": "We propose a new real-time packet scheduling algorithm for streaming scalable H.264. Our algorithm makes use of a packet importance measure, which we define, that takes into consideration transmission history, channel conditions, and the unique decoding dependencies due to the temporal wavelet encoding. Our algorithm utilizes this importance measure to minimize the expected reconstruction distortion at the decoder under a certain rate constraint. In our experimental results we show gains of more than 3 dB in decoded video quality when transmissions are controlled with our algorithm as compared to existing schedulers",
    "actual_venue": "Icme"
  },
  {
    "abstract": "An interface circuit for differential capacitive sensing applications with tunable dynamic range is presented. Capacitive microsensors are ubiquitously employed in many applications pertaining to all aspects of modern life. The proposed circuit requires a small chip area and offers a good signal-to-noise ratio as well as adjustable sensitivity and dynamic range. Reference signals are produced on chip and used for the synchronous demodulation of current signals through a differential capacitive sensor. In addition to the normal open-loop operation mode of the circuit, it can be operated within a novel closed-loop configuration in order to extend its dynamic range. The circuit was designed and fabricated in a standard 0.35-Î¼m CMOS technology from Austriamicrosystems. Experimental and simulation results are presented and discussed. The circuit is capable of resolving 0.4 fF of variation in capacitance with a 50-kHz measurement bandwidth. Reducing the bandwidth to 1 kHz for signal frequencies around 10 kHz increases the dynamic range of the closed-loop circuit to 99 and 110 dB for open-loop and closed-loop circuits, respectively.",
    "actual_venue": "Circuits And Systems : Express Briefs, Ieee Transactions"
  },
  {
    "abstract": "Magnetic resonance imaging (MRI) measures of brain atrophy are often considered to be a marker of axonal loss in multiple sclerosis (MS) but evidence is limited. Optic neuritis is a common manifestation of MS and results in optic nerve atrophy. Retinal nerve fibre layer (RNFL) imaging is a non-invasive way of detecting axonal loss following optic neuritis. We hypothesise that if the optic nerve atrophy that develops following optic neuritis is contributed to by axonal loss, it will correlate with thinning of the RNFL. Twenty-five patients were studied at least 1 year after a single unilateral attack of optic neuritis without recurrence, with a selection bias towards incomplete recovery. They had MR quantification of optic nerve cross-sectional area and optic nerve lesion length, as well as optical coherence tomography (OCT) measurement of mean RNFL thickness and macular volume, quantitative visual testing, and visual evoked potentials (VEPs). Fifteen controls were also studied. Significant optic nerve atrophy (mean decrease 30% versus controls), RNFL thinning (mean decrease 33% versus controls), and macular volume loss occurred in patients' affected eyes when compared with patients' unaffected eyes and healthy controls. The optic nerve atrophy was correlated with the RNFL thinning, macular volume loss, visual acuity, visual field mean deviation, and whole field VEP amplitude but not latency.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "In this paper, we derive analytical expressions for the central and side quantizers which, under high-resolution assumptions, minimize the expected distortion of a symmetric multiple-description lattice vector quantization (MD-LVQ) system subject to entropy constraints on the side descriptions for given packet-loss probabilities. We consider a special case of the general n-channel symmetric multiple-description problem where only a single parameter controls the redundancy tradeoffs between the central and the side distortions. Previous work on two-channel MD-LVQ showed that the distortions of the side quantizers can be expressed through the normalized second moment of a sphere. We show here that this is also the case for three-channel MD-LVQ. Furthermore, we conjecture that this is true for the general n-channel MD-LVQ. For given source, target rate, and packet-loss probabilities we find the optimal number of descriptions and construct the MD-LVQ system that minimizes the expected distortion. We verify theoretical expressions by numerical simulations and show in a practical setup that significant performance improvements can be achieved over state-of-the-art two-channel MD-LVQ by using three-channel MD-LVQ.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "The gesture-based human-computer interface requires new user authentication technique because it does not have traditional input devices like keyboard and mouse. In this paper, we propose a new finger-gesture-based authentication method, where the in-air-handwriting of each user is captured by wearable inertial sensors. Our approach is featured with the utilization of both the content and the writing convention, which are proven to be essential for the user identification problem by the experiments. A support vector machine (SVM) classifier is built based on the features extracted from the hand motion signals. To quantitatively benchmark the proposed framework, we build a prototype system with a custom data glove device. The experiment result shows our system achieve a 0.1% equal error rate (EER) on a dataset containing 200 accounts that are created by 116 users. Compared to the existing gesture-based biometric authentication systems, the proposed method delivers a significant performance improvement.",
    "actual_venue": "Ieee International Joint Conference On Biometrics"
  },
  {
    "abstract": "In in online environment, an E-Procurement process should be able to react and adapt in near real-time to changes in suppliers, requirements, and regulations. WS-BPEL is an emerging standard for process automation, but is oriented towards design-time binding of services. This missing issue can be resolved through designing an extension to WS-BPEL to support automation of flexible e-Procurement processes. Our proposed framework will support dynamic acquisition of procurement services from different suppliers dealing with changing procurement requirements. The proposed framework is illustrated by applying it to health care where different health insurance providers Could be involved to procure the medication for patients.",
    "actual_venue": "Lecture Notes In Business Information Processing"
  },
  {
    "abstract": "The aim of this paper is to study the use of Evolutionary Multiobjective Techniques to improve the performance of Neural Networks (NN). In particular, we will focus on classification problems where classes are imbalanced. We propose an evolutionary multiobjective approach where the accuracy rate of all the classes is optimized at the same time. Thus, all classes will be treated equally independently of their presence in the training data set. The chromosome of the evolutionary algorithm encodes only the weights of the training patterns missclassified by the NN. Results show that the multiobjective approach is able to consider all classes at the same time, disregarding to some extent their abundance in the training set or other biases that restrain some of the classes of being learned properly.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "Direction finders such as MUSIC experience loss of resolution and increased bias in the presence of nonwhite noise. This paper presents two versions of a steepest descent gradient algorithm that prewhiten the signal received by an arbitrarily oriented volumetric sensor array, minimizing these undesirable effects. The algorithms optimize a whiteness functional over a surface with desirable properties including low dimensionality, unimodality, and concavity. Two ambient noise models facilitate algorithm development through succinct parameterization. The first, which is a novel linear matricial ambient noise model based on a spherical harmonic expansion, places no constraints on array geometry. The second model requires a spatially uniformly sampled sensor array and reduces problem dimensionality associated with exact multidimensional autoregressive modeling. The algorithms estimate a stacked vector arrangement of the model parameters. Application with MUSIC demonstrates enhanced performance in terms of angular resolution and detection of low signal-to-noise ratio (SNR) sources",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "In this paper, we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a hand-crafted lexical resource. Following this, we derive a dependency-based structure representation from texts, which aims to provide a proper base for the inference rule application. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements.",
    "actual_venue": "Eacl"
  },
  {
    "abstract": "This paper presents the design and deployment experience of an air-dropped wireless sensor network for volcano hazard monitoring. The deployment of five stations into the rugged crater of Mount St. Helens only took one hour with a helicopter. The stations communicate with each other through an amplified 802.15.4 radio and establish a self-forming and self-healing multi-hop wireless network. The distance between stations is up to 2 km. Each sensor station collects and delivers real-time continuous seismic, infrasonic, lightning, GPS raw data to a gateway. The main contribution of this paper is the design and evaluation of a robust sensor network to replace data loggers and provide real-time long-term volcano monitoring. The system supports UTC-time synchronized data acquisition with 1ms accuracy, and is online configurable. It has been tested in the lab environment, the outdoor campus and the volcano crater. Despite the heavy rain, snow, and ice as well as gusts exceeding 120 miles per hour, the sensor network has achieved a remarkable packet delivery ratio above 99% with an overall system uptime of about 93.8% over the 1.5 months evaluation period after deployment. Our initial deployment experiences with the system have alleviated the doubts of domain scientists and prove to them that a low-cost sensor network system can support real-time monitoring in extremely harsh environments.",
    "actual_venue": "Mobisys"
  },
  {
    "abstract": "Recent theoretical and experimental studies indicate that spatial multiplexing (SM) systems have enormous potential for increasing the capacity of corresponding multiple input multiple output (MIMO) channels in rich scattering environments. In this paper, we describe two fast detection algorithms for the standard BLAST receiver. The first algorithm covers the nulling and cancellation (NC) detection interface that applies inflation and deflation recursion to the initialization and iteration stages, respectively. The second algorithm utilizes the QR decomposition (QRD) detection interface that reuses Cholesky decomposition and back substitution, leading to an alternative low complexity detector. Compared with the conventional fastest method, our new algorithms reduce the complexity of the preprocessing by a factor of about 2. Furthermore, they are also implementation friendly by avoiding the recursive permutation and permitting the function reuse.",
    "actual_venue": "Ieee Vehicular Technology Conference Fall, Vols"
  },
  {
    "abstract": "Conventional database management systems are not suitable for real-time applications. To address this need, we have designed, implemented and evaluated a novel architecture of a real-time database management system. The architecture includes a real-time control layer which provides sophisticated admission control, scheduling and overload management. We consider a multi-class transaction workload consisting of high, medium and low importance classes. The proposed algorithms are dynamic and make transactions with higher importance have small miss deadline percentage. Simulation results show that our architecture can achieve a significant performance even in overload situations.",
    "actual_venue": "Aiccsa"
  },
  {
    "abstract": "Gaze tracking technology is increasingly common in desktop, laptop and mobile scenarios. Most previous research on eye gaze patterns during human-computer interaction has been confined to controlled laboratory studies. In this paper we present an in situ study of gaze and mouse coordination as participants went about their normal activities. We analyze the coordination between gaze and mouse, showing that gaze often leads the mouse, but not as much as previously reported, and in ways that depend on the type of target. Characterizing the relationship between the eyes and mouse in realistic multi-task settings highlights some new challenges we face in designing robust gaze-enhanced interaction techniques.",
    "actual_venue": "Ubicomp Adjunct"
  },
  {
    "abstract": "Recently, many parallel computing models using dynamically reconfigurable electrical buses have been proposed in the literature. The underlying characteristics are similar among these models, but they do have certain differences that can take form of restrictions on configurations allowed. This paper presents a constant time simulation of an R-mesh on an LR-mesh (a restricted model of the R-mesh), proving that in spite of the differences, the two models possess the same complexity. In other words, the LR-mesh can simulate a step of the R-mesh in constant time with a polynomial increase in size. This simulation is based on Rein-gold's algorithm to solve USTCON in log-space. The simulation is also the first to be executed in constant time.",
    "actual_venue": "Long Beach, Ca"
  },
  {
    "abstract": "Occluded face detection is a challenging detection task due to the large appearance variations incurred by various real-world occlusions. This paper introduces an Adversarial Occlusion-aware Face Detector (AOFD) by simultaneously detecting occluded faces and segmenting occluded areas. Specifically, we employ an adversarial training strategy to generate occlusion-like face features that are difficult for a face detector to recognize. Occlusion mask is predicted simultaneously while detecting occluded faces and the occluded area is utilized as an auxiliary instead of being regarded as a hindrance. Moreover, the supervisory signals from the segmentation branch will reversely affect the features, aiding in detecting heavily-occluded faces accordingly. Consequently, AOFD is able to find the faces with few exposed facial landmarks with very high confidences and keeps high detection accuracy even for masked faces. Extensive experiments demonstrate that AOFD not only significantly outperforms state-of-the-art methods on the MAFA occluded face detection dataset, but also achieves competitive detection accuracy on benchmark dataset for general face detection such as FDDB.",
    "actual_venue": "Arxiv: Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "We performed a user study that measured the effectiveness of our new 3D selection technique, Scope, which dynamically adapts to the environment by altering its activation area and visual appearance with relation to cursor velocity. Users tested our new technique against existing techniques Raycast, Bendcast, and Hook across a variety of different 3D scenarios which featured three different levels of object density and three different levels of object velocity. Our two dependent variables were completion time and total attempts per scenario. Users also completed a post-questionnaire which yielded qualitative insights on their experience. Our study shows that Bendcast, Scope, and Hook all performed similarly across all scenarios, yet were all significantly faster and less error-prone than Raycast. Despite this similar performance, users strongly favored Scope over the other three techniques, and over the second most preferred technique nearly two to one.",
    "actual_venue": "D User Interfaces"
  },
  {
    "abstract": "The lower and average spectral radii measure, respectively, the minimal and average growth rates of long products of matrices taken from a finite set. The logarithm of the average spectral radius is tradition- ally called Lyapunov exponent. When one performs these products in the max-algebra, we obtain quantities that measure the performance of Dis- crete Event Systems. We show that approximating the lower and average max-algebraic spectral radii is NP-hard.",
    "actual_venue": "Ieee Trans Automat Contr"
  },
  {
    "abstract": "ECM across a CP in Korean poses difficulties from the standpoint of the locality of A-movement/agreement. A phase-based analysis is developed which requires two steps: (i) in the embedded CP, VP/vP containing its VP-intemal subject first moves to Spec-CP, which renders the subject accessible to the matrix v, in accordance with Chomsky's Phase Impenetrability Condition; (ii) ECM takes place in a local relation between the matrix v and the embedded subject. It is shown that the otherwise puzzling fact that ECM across a CP, but not Passivization across a CP, is affected by the type of the embedded verb in Korean is accounted for in a principled way, based on the assumption that CP and vP, but not TP and VP, are phases.",
    "actual_venue": "Paclic : Language, Information, And Computation, Proceedings"
  },
  {
    "abstract": "We propose a novel attention framework called attentive linear transformation (ALT) for automatic generation of image captions. Instead of learning the spatial or channel-wise attention in existing models, ALT learns to attend to the high-dimensional transformation matrix from the image feature space to the context vector space. Thus ALT can learn various relevant feature abstractions, including s...",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "The scalability of SDH/SONET to high speeds places strict performance requirements on ATM systems. Throughput preservation of link speed through protocols to a higher layer application is a known problem in high-speed communication systems, which becomes more acute as link speed increases and is being addressed with designs that offer high speed data paths and high embedded processing power.We introduce a specialized, high-speed, scalable and re-usable Queue Manager (QM) for ATM systems, which enables high-speed data transfer to/from system memory and management of logical data structures. We describe its architecture, and then we present implementations in hardware as well as in software for embedded systems. We evaluate the implementations, demonstrating the performance improvement and the system scalability.",
    "actual_venue": "Antibes-Juan Les Pins"
  },
  {
    "abstract": "In the current era of internet, information related to crime is scattered across many sources namely news media, social networks, blogs, and video repositories, etc. Crime reports published in online newspapers are often considered as reliable compared to crowdsourced data like social media and contain crime information not only in the form of unstructured text but also in the form of images. Given the volume and availability of crime-related information present in online newspapers, gathering and integrating crime entities from multiple modalities and representing them as a knowledge base in machine-readable form will be useful for any law enforcement agencies to analyze and prevent criminal activities. Extant research works to generate the crime knowledge base, does not address extraction of all non-redundant entities from text and image data present in multiple newspapers. Hence, this work proposes Crime Base, an entity relationship based system to extract and integrate crime related text and image data from online newspapers with a focus towards reducing duplicity and loss of information in the knowledge base. The proposed system uses a rule-based approach to extract the entities from text and image captions. The entities extracted from text data are correlated using contextual as-well-as semantic similarity measures and image entities are correlated using low-level and high-level image features. The proposed system also presents an integrated view of these entities and their relations in the form of a knowledge base using OWL. The system is tested for a collection of crime related articles from popular Indian online newspapers.",
    "actual_venue": "Information Processing And Management"
  },
  {
    "abstract": "AbstractAs the network of a power distribution company can be very large, maintaining such a network is not only very time-consuming but also very costly, especially when failures occur. A huge amount of effort could be saved by monitoring the substations remotely, which allows, amongst others, to trace failures more efficiently. In this study, such a monitoring network is designed based on WiMAX in 'Flanders', Belgium. To the best of the authors' knowledge, this study differs from previous studies by combining both coverage and cost calculations. Firstly, the number of base stations needed to cover the considered area is determined. Therefore, link budget calculations are performed to obtain the ranges of the base stations. Secondly, the cost to install and maintain the network is determined using the results of the first step.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "We propose a novel discriminative model for semantic labeling in videos by incorporating a prior to model both the shape and temporal dependencies of an object in video. A typical approach for this task is the conditional random field (CRF), which can model local interactions among adjacent regions in a video frame. Recent work has shown how to incorporate a shape prior into a CRF for improving labeling performance, but it may be difficult to model temporal dependencies present in video by using this prior. The conditional restricted Boltzmann machine (CRBM) can model both shape and temporal dependencies, and has been used to learn walking styles from motion- capture data. In this work, we incorporate a CRBM prior into a CRF framework and present a new state-of-the-art model for the task of semantic labeling in videos. In particular, we explore the task of labeling parts of complex face scenes from videos in the YouTube Faces Database (YFDB). Our combined model outperforms competitive baselines both qualitatively and quantitatively.",
    "actual_venue": "Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "This paper presents a high-throughput, energy-efficient, and scalable low-density parity-check (LDPC) decoder with time-domain (TD) signal processing. The proposed arbiter-based minimum value finder is able to support practical long codes. The latency for determining the first two minimum values required in the check node unit is significantly reduced through TD processing. A layered Q-based decod...",
    "actual_venue": "Ieee Journal Of Solid-State Circuits"
  },
  {
    "abstract": "As smart devices proliferate in the home and become universally networked, users will experience an increase in both the number of the devices they can control and the available interaction modalities for controlling them. Researchers have explored the possibilities of speech- and gesture-based interfaces in desktop scenarios, but it is also necessary to explore these technologies in the context of an intelligent environment, one in which the user's focus moves off of the desktop and into a physically disparate set of networked smart devices. We report results from a lab study where six families tried various interfaces for controlling lights in the home. Results indicate a preference for speech-based interfaces and illuminate the importance of location awareness.",
    "actual_venue": "Human-Computer Interaction - Interact , Pt"
  },
  {
    "abstract": "Term extraction methods based on linguistic rules have been proposed to help the terminology building from corpora. As they face the difficulty of identifying the relevant terms among the noun phrases extracted, statistical measures have been proposed. However, the term selection results may depend on corpus and strong assumptions reflecting specific terminological practice. We tackle this problem by proposing a parametrised C-Value which optimally considers the length and the syntactic roles of the nested terms thanks to a genetic algorithm. We compare its impact on the ranking of terms extracted from three corpora. Results show average precision increased by 9% above the frequency-based ranking and by 12% above the C-Value-based ranking.",
    "actual_venue": "Advances In Natural Language Processing"
  },
  {
    "abstract": "Execution of GPGPU workloads consists of different stages including data I/O on the CPU, memory copy between the CPU and GPU, and kernel execution. While GPU can remain idle during I/O and memory copy, prior work has shown that overlapping data movement (I/O and memory copies) with kernel execution can improve performance. However, when there are multiple dependent kernels, the execution of the kernels is serialized and the benefit of overlapping data movement can be limited. In order to improve the performance of workloads that have multiple dependent kernels, we propose to automatically overlap the execution of kernels by exploiting implicit pipeline parallelism. We first propose Coarse-grained Reference Counting-based Scoreboarding (CRCS) to guarantee correctness during overlapped execution of multiple kernels. However, CRCS alone does not necessarily improve overall performance if the thread blocks (or CTAs) are scheduled sequentially. Thus, we propose an alternative CTA scheduler -- Pipeline Parallelism-aware CTA Scheduler (PPCS) that takes available pipeline parallelism into account in CTA scheduling to maximize pipeline parallelism and improve overall performance. Our evaluation results show that the proposed mechanisms can improve performance by up to 67% (33% on average). To the best of our knowledge, this is one of the first work that enables overlapped execution of multiple dependent kernels without any kernel modification or explicitly expressing dependency by the programmer.",
    "actual_venue": "Pact"
  },
  {
    "abstract": "Medical image analysis is shifting from current film-oriented light screen environments to computer environments that involve viewing and analyzing large sets of images on a computer screen. Magnetic Resonance Imaging (MRI) studies, in particular, can involve many images. This paper examines how best to meet the needs of radiologists in a computational environment. To this end, a field study was conducted to observe radiologists' interactions during MRI analysis in the traditional light screen environment. Key issues uncovered involve control over focus and context, dynamic grouping of images and retrieval of images and image groups. To address the problem of focus and context, existing layout adjustment and magnification techniques are explored to provide the most appropriate solution. Our interest is in combining the methodologies of human computer interaction studies with computational presentation possibilities to design a visual environment for the crucial field of medical image analysis.",
    "actual_venue": "Ieee Visualization"
  },
  {
    "abstract": "Authentication of audio signals is an important problem in multimedia forensics. Tampering is typically followed by postprocessing that aims to remove the traces of the forgery. The variety of possible postprocessing operations makes tampering detection even more challenging. In this letter, we propose the amplitude cooccurrence vector features, which exploit cooccurrence patterns in audio signals...",
    "actual_venue": "Ieee Signal Processing Letters"
  },
  {
    "abstract": "n audio recording is subject to a number of possible distortions and artifacts. Consider, for example, artifacts due to acoustic reverberation and background noise. The acoustic reverberation depends on the shape and the composition of the room, and it causes temporal and spectral smearing of the recorded sound. The background noise, on the other hand, depends on the secondary audio source activities present in the evidentiary recording. Extraction of acoustic cues from an audio recording is an important but challenging task. Temporal changes in the estimated reverberation and background noise can be used for dynamic acoustic environment identification (AEI), audio forensics, and ballistic settings. We describe a statistical technique based on spectral subtraction to estimate the amount of reverberation and nonlinear filtering based on particle filtering to estimate the background noise. The effectiveness of the proposed method is tested using a data set consisting of speech recordings of two human speakers (one male and one female) made in eight acoustic environments using four commercial grade microphones. Performance of the proposed method is evaluated for various experimental settings such as microphone independent, semi- and full-blind AEI, and robustness to MP3 compression. Performance of the proposed framework is also evaluated using Temporal Derivative-based Spectrum and Mel-Cepstrum (TDSM)-based features. Experimental results show that the proposed method improves AEI performance compared with the direct method (i.e., feature vector is extracted from the audio recording directly). In addition, experimental results also show that the proposed scheme is robust to MP3 compression attack.",
    "actual_venue": "Ieee Transactions On Information Forensics And Security"
  },
  {
    "abstract": "Corpus-based approaches to CLIR have been studied for many years. However, using commercial MT systems for CLEF has been considered easier and better performing. Our goal is to be one of the CLEF participants who show that the hypothetical performance drop is not large enough to justify the loss of control and transparency, especially for research systems. We participated in two bilingual runs and the small multilingual run using software and data that are free to obtain, transparent and modifiable.",
    "actual_venue": "Clef"
  },
  {
    "abstract": "This work proposes using Wavelet-Packet Cepstral coefficients (WPPCs) as an alternative way to do filter-bank energy-based feature extraction (FE) for automatic speech recognition (ASR). The rich coverage of time-frequency properties of Wavelet Packets (WPs) is used to obtain new sets of acoustic features, in which competitive and better performances are obtained with respect to the widely adopted Mel-Frequency Cepstral coefficients (MFCCs) in the TIMIT corpus. In the analysis, concrete filter-bank design considerations are stipulated to obtain most of the phone-discriminating information embedded in the speech signal, where the filter-bank frequency selectivity, and better discrimination in the lower frequency range [200Hz-1kHz] of the acoustic spectrum are important aspects to consider.",
    "actual_venue": "Speech Communication"
  },
  {
    "abstract": "This paper considers cooperative sensing in cognitive networks under Spectrum Sensing Data Falsification attack (SSDF) in which malicious users can intentionally send false sensing information. One effective method to deal with the SSDF attack is the q-out-of-m scheme, where the sensing decision is based on q sensing reports out of m polled nodes. The major limitation with the q-out-of-m scheme is its high computational complexity due to exhaustive search. In this paper, we prove that for a fixed percentage of malicious users, the detection accuracy increases almost exponentially as the network size increases. Motivated by this observation, as well as the linear relationship between the scheme parameters and the network size, we propose a simple but accurate approach that significantly reduces the complexity of the q-out-of-m scheme. The proposed approach can easily be applied to the large scale networks, which can be much more reliable under malicious attacks.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "The Yeast Intron DataBase (YIDB) contains currently available information about all introns encoded in the nuclear and mitochondrial genomes of the yeast Saccharomyces cerevisiae. Introns are divided according: to their mechanism of excision: group I and group II introns, pre-mRNA introns, tRNA introns and the HAC1 intron. Information about the host genome, the type of RNA in which they are inserted and their primary structure are provided together with references. For nuclear pre-mRNA introns, transcription frequencies, as determined by microarray experiments, have also been included. This updated database is accessible at: http://www.embl-heidelberg.de/ExternalInfo/seraphin/yidb.html.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "Linear codes have been an interesting topic in both theory and practice for many years. In this paper, for an odd prime $p$, we determine the explicit complete weight enumerators of two classes of linear codes over $\\mathbb{F}_p$ and they may have applications in cryptography and secret sharing schemes. Moreover, some examples are included to illustrate our results.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "Human localization is fundamental in human centered computing and human-robot interaction (HRI), as human operators should be localized by robots before being actively serviced. This paper proposes a simple and efficient approach for estimating the distance and orientation of an human, from a single robot-acquired image. We adopt a simple combination of multiple Haar feature-based classifiers to compute face scores, that represent the probability that the detected face is acquired from each of a predefined set of poses. Using the Locally Weighted Projectron Regression (LWPR), an online incremental regression-based learning scheme, we can reliably learn and predict the pose of a human face in real-time at a low computational cost. The accuracy, robustness, and scalability of the obtained solutions have been verified through emulation experiments performed on a large data set of real data acquired by a networked swarm of robots.",
    "actual_venue": "Image Processing"
  },
  {
    "abstract": "Conventional mega datacenter (DC) lacks flexibility and has poor service connectivity for users faraway. Therefore, the metro-embeded DC architecture has been introduced to deliver more reliable and flexible services with less access latency. For fine-grained all-optical switching ability and high bandwidth connection, optical burst switching (OBS) ring is deployed inside the micro-DCs (mDC) and OBS over wavelength switched optical network paradigm is used for inter-mDC communication. Meanwhile, hierarchical SDN control is employed to virtualize and holistically coordinate the distributed mDCs and the metro network slices into reconfigurable virtual DC (VDC). Optimal VDC provisioning based on an integer linear programming (ILP) formulation is also developed in the SDN control plane to provide services with minimum latency and traffic load. A dynamic VDC provisioning algorithm based on partial ILP is further proposed to reconfigure the VDC with reduced complexity considering the user and resource dynamics. Experimental demonstrations conducted on the implemented metro-embedded DC prototype testbed show that optimal and fast VDC provisioning is realized based on the proposed ILP, while dynamic VDC reconfiguration makes sure that VDC is adjusted accordingly with the change of user location and resource requirement.",
    "actual_venue": "Optical Switching And Networking"
  },
  {
    "abstract": "Dynamic Voltage and Frequency Scaling (DVFS) on General-Purpose Graphics Processing Units (GPGPUs) is now becoming one of the most significant techniques to balance computational performance and energy consumption. However, there are still few fast and accurate models for predicting GPU kernel execution time under different core and memory frequency settings, which is important to determine the best frequency configuration for energy saving. Accordingly, a novel GPGPU performance estimation model with both core and memory frequency scaling is herein proposed. We design a cross-benchmarking suite, which simulates kernels with a wide range of instruction distributions. The synthetic kernels generated by this suite can be used for model pre-training or as supplementary training samples. Then we apply two different machine learning algorithms, Support Vector Regression (SVR) and Gradient Boosting Decision Tree (GBDT), to study the correlation between kernel performance counters and kernel performance. The models trained only with our cross-benchmarking suite achieve satisfying accuracy (16%similar to 22% mean absolute error) on 24 unseen real application kernels. Validated on three modern GPUs with a wide frequency scaling range, by using a collection of 24 real application kernels, the proposed model is able to achieve accurate results (5.1%, 2.8%, 6.5% mean absolute error) for the target GPUs (GTX 980, Titan X Pascal and Tesla P100).",
    "actual_venue": "Gpgpu: Proceedings Of The Annual Workshop On General Purpose Processing Using Graphics Processing Unit"
  },
  {
    "abstract": "We consider large scale Publish/Subscribe systems deployed across multiple organizations. However, such cross organizational deployment is often hindered by firewalls and Network Address Translators (NATs). Several workarounds have been proposed to allow firewall and NAT traversal, e.g. VPN, connection reversal, relay routers. However, each traversal mechanism in turn leads to trade-offs with respect to implementation complexity, infrastructure overhead, latency, etc. We focus on the latency aspect in this work. We propose a cost-performance model that allows quantitative evaluation of the performance latency induced by the different firewall traversal mechanisms. The utility of the model is that for a given network configuration, it is able to provide a (close) approximation of the performance latencies based on simulation results, without actually having to deploy them in practice. This also allows selecting the best traversal mechanism for a given configuration. Finally, experimental results are given to show the validity of the proposed model.",
    "actual_venue": "Icpe"
  },
  {
    "abstract": "â¢In sequential programs, high performance is a proxy for low energy consumption.â¢In concurrent programs, some variants are fast but very energy consuming.â¢We provide guidelines for developers to improve energy efficiency of Haskell programs.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Wireless localization technology has become an important part of pervasive computing applications. This paper presents DE2, a wireless localization approach that is responsive to single beacon conditions. Unlike previous work that needs a priori knowledge of the scenario during the pre-deployment phase with significant human effort, or uses a certain number of beacons during the localization phase with high energy consumption, to accomplish localization, DE2 leverages a single beacon without much prior human effort to locate multiple targets. The key challenge is how to utilize the limited information caught by a single beacon and little human effort to obtain the positions of multiple unknowns. The intuition underlying DE2 is that, direction and distance constraints between an unknown position and the single beacon are adequate to determine the unknown position. When a user rotates around an RF receiver (the single beacon), the human body acts as a signal-blocking obstacle. It causes the signal from a transmitter (position-unknown) to the single receiver to attenuate in a certain scope. The blocking effect caused by a human body can be utilized to obtain the direction constraint between the unknown position and the single beacon. Moreover, a corresponding distance constraint is also concluded in the rotating RSS according to the RF propagation model. DE2 pushes the limit of minimum beacons needed for localization without much pre-configuration effort. To demonstrate the utility of DE2, we implement DE2 in realworld single beacon wireless networks. The results show that these applications can significantly benefit from DE2.",
    "actual_venue": "Uic/Atc"
  },
  {
    "abstract": "This study investigates a new biobjective lane-reservation problem, which is to exclusively reserve lanes from an existing transportation network for special transport tasks with given deadlines. The objectives are to minimize the total negative impact on normal traffic due to the reduction of available lanes for general-purpose vehicles and to maximize the robustness of the lane-reservation solution against the uncertainty in link travel times. We first define the robustness for the lane-reservation problem and formulate a biobjective mixed-integer linear program. Then, we develop an improved exact epsilon-constraint and a cut-and-solve combined method to generate its Pareto front. Computational results for an instance based on a real network topology and 220 randomly generated instances with up to 150 nodes, 600 arcs, and 50 tasks demonstrate that the proposed method is able to find the Pareto front and that the proposed cut-and-solve method is more efficient than the direct use of optimization software CPLEX.",
    "actual_venue": "Ieee Transactions Intelligent Transportation Systems"
  },
  {
    "abstract": "In distributed transactional database systems deployed over cloud servers, entities cooperate to form proofs of authorizations that are justified by collections of certified credentials. These proofs and credentials may be evaluated and collected over extended time periods under the risk of having the underlying authorization policies or the user credentials being in inconsistent states. It therefore becomes possible for a policy-based authorization systems to make unsafe decisions that might threaten sensitive resources. In this paper, we highlight the criticality of the problem. We then present the first formalization of the concept of trusted transactions when dealing with proofs of authorizations. Accordingly, we define different levels of policy consistency constraints and present different enforcement approaches to guarantee the trustworthiness of transactions executing on cloud servers. We propose a Two-Phase Validation Commit protocol as a solution, that is a modified version of the basic Two-Phase Commit protocols. We finally provide performance analysis of the different presented approaches to guide the decision makers in which approach to use.",
    "actual_venue": "Distributed Computing Systems Workshops"
  },
  {
    "abstract": "Data mining deals with finding hidden knowledge patterns in often huge data sets. The work presented in this paper elaborates on defining data mining tasks in terms of fine-grained composable operators instead of coarse-grained black box algorithms. Data mining tasks in the knowledge discovery process typically need one relational table as input and data preprocessing and integration beforehand. The possible combination of different kind of operators (relational, data mining and data preprocessing operators) represents a novel holistic view on the knowledge discovery process. Initially, as described in this paper, for the low-level execution phase but yielding the potential for rich optimization similar to relational query optimization. We argue that such macro-optimization embracing the overall KDD process leads to improved performance instead of focusing on just a small part of it via micro-optimization.",
    "actual_venue": "Kdir : Proceedings Of The International Conference On Knowledge Discovery And Information Retrieval"
  },
  {
    "abstract": "We discuss the impact of cultural differences on usability evaluations that are based on the thinking-aloud method (TA). The term 'cultural differences' helps distinguish differences in the perception and thinking of Westerners (people from Western Europe and US citizens with European origins) and Easterners (people from China and the countries heavily influenced by its culture). We illustrate the impact of cultural cognition on four central elements of TA: (1) instructions and tasks, (2) the user's verbalizations, (3) the evaluator's reading of the user, and (4) the overall relationship between user and evaluator. In conclusion, we point to the importance of matching the task presentation to users' cultural background, the different effects of thinking aloud on task performance between Easterners and Westerners, the differences in nonverbal behaviour that affect usability problem detection, and, finally, the complexity of the overall relationship between a user and an evaluator with different cultural backgrounds.",
    "actual_venue": "Interacting With Computers"
  },
  {
    "abstract": "Let G be a t-uniform s-regular linear hypergraph with r vertices. It is shown that the number of independent sets $\\IS(\\hgraph)$ in $\\hgraph$ satisfies \\[ \\log_2 \\IS(\\hgraph) \\le \\frac{r}{t} \\left( 1 + O \\biggl( \\frac{\\log^2(ts)}{s} \\biggr) \\right) . \\] This leads to an improvement of a previous bound by Alon obtained for t = 2 (i.e., for regular ordinary graphs). It is also shown that for the Hamming graph $\\Hamming(n,q)$ (with vertices consisting of all n-tuples over an alphabet of size q and edges connecting pairs of vertices with Hamming distance $1$), \\[ \\frac{\\log_2 \\IS(\\Hamming(n,q))}{q^n} = \\frac{1}{q} + O \\biggl(\\frac{\\log^2 (q n)}{q n} \\biggr). \\] The latter result is then applied to show that the Shannon capacity of the n-dimensional $(d,\\infty)$-runlength-limited (RLL) constraint converges to 1/(d+1) as n goes to infinity.",
    "actual_venue": "Siam J Discrete Math"
  },
  {
    "abstract": "Diffusion is a well-known algorithm for load-balancing in which tasks move from heavily-loaded processors to lightly-loaded neighbors. This paper presents a rigorous analysis of the performance of the diffusion algorithm on arbitrary networks.It is shown that the running time of the diffusion algorithm is bounded by: &OHgr;(log &sgr;/&Ggr;) â¤ Time â¤ O(N&sgr;/&Ggr;) and &OHgr;(log &sgr;/&Fgr;) â¤ Time â¤ O(&sgr;/&Fgr;2), where N is the number of nodes in the network, &sgr; is the standard deviation of the initial load distribution (which represents how imbalanced the load is initially), and &Ggr; and &Fgr; are the network's electrical and fluid conductances respectively (which are measures of the network's bandwidth).For the case of the generalized mesh with wrap-around (which includes common networks like the ring, 2D-torus, 3D-torus and hypercube), we derive tighter bounds and conclude that the diffusion algorithm is inefficient for lower dimensional meshes.",
    "actual_venue": "Spaa"
  },
  {
    "abstract": "We describe a uniform deduction mechanism called surface deduction, for Horn clauses with equality in a special form called flat. In this flat form arguments of non-equality predicates must be variables, and each equality has at most one non-variable subterm: this subterm can occur only on the right hand side of the equality predicate, and itself has only variables as subterms. S-deduction consists of three rules which rely on a very simple form of unification that manipulates only variables and flat terms. These rules preserve flatness. We present a method for converting arbitrary sets of Horn clauses S into a flat form Sâ² in such a way that S-deduction is sound and complete with respect to unsatisfiability. It is also shown that S together with the usual axioms for equality is unsatisfiable if there is an S-refutation from Sâ². In the case when an S-deduction terminates with a clause consisting of equalities (a residuum), an appropriate interpretation for the nonunifiability is provided. S-deduction is compared with recent related results (Colmerauer, van Emden and Lloyd).",
    "actual_venue": "Annals Of Pure And Applied Logic"
  },
  {
    "abstract": "The introduction and acceptance of any new technology by the general public goes through a stage during which the capabilities of the new technology are tested and measured against traditional procedures. During this process the new technology is often used in a manner similar to and with the constraints characteristic of the traditional procedures, even if the new technology is not bound by them. It takes time and experimentation for the new technology to be adopted and exploited to its full potential. Today, we find ourselves in a similar situation in respect to the introduction of the digital medium. Initially, the interactions that took place in this medium mimicked the ones we were used to conducting in other media. Thus, the first electronic exchanges were conceptualized as \"e-mail\" although they differ from traditional mail in almost every respect: the exchange is almost instantaneous and is not affected by geographic distance, the \"mail\" can stay forever in one's \"mail box\" and now one can even use the same procedure to \"mail\" voice messages, pictures and movie clips. With increasing familiarity with the medium, the uses specific to the new medium emerge: tele-presence, e-commerce, digital paging, collaborative document creation, etc. In this paper 1 will use the example of documenting the art creation process to explore the implications of the history logging potential of the digital medium for creative processes in general.",
    "actual_venue": "Cultural Heritage Informatics"
  },
  {
    "abstract": "Bees are very important for terrestrial ecosystems and, above all, for the subsistence of many crops, due to their ability to pollinate flowers. Currently, the honey bee populations are decreasing due to colony collapse disorder (CCD). The reasons for CCD are not fully known, and as a result, it is essential to obtain all possible information on the environmental conditions surrounding the beehives. On the other hand, it is important to carry out such information gathering as non-intrusively as possible to avoid modifying the bees' work conditions and to obtain more reliable data. We designed a wireless-sensor networks meet these requirements. We designed a remote monitoring system (called WBee) based on a hierarchical three-level model formed by the wireless node, a local data server, and a cloud data server. WBee is a low-cost, fully scalable, easily deployable system with regard to the number and types of sensors and the number of hives and their geographical distribution. WBee saves the data in each of the levels if there are failures in communication. In addition, the nodes include a backup battery, which allows for further data acquisition and storage in the event of a power outage. Unlike other systems that monitor a single point of a hive, the system we present monitors and stores the temperature and relative humidity of the beehive in three different spots. Additionally, the hive is continuously weighed on a weighing scale. Real-time weight measurement is an innovation in wireless beehive-monitoring systems. We designed an adaptation board to facilitate the connection of the sensors to the node. Through the Internet, researchers and beekeepers can access the cloud data server to find out the condition of their hives in real time.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "A wide range of low level vision problems have been formulated in terms of finding the most probable assignment of a Markov Random Field (or equivalently the lowest energy configuration). Perhaps the most successful example is stereo vision. For the stereo problem, it has been shown that finding the global optimum is NP hard but good results have been obtained using a number of approximate optimization algorithms. In this paper we show that for standard benchmark stereo pairs, the global optimum can be found in about 30 minutes using a variant of the belief propagation (BP) algorithm. We extend previous theoretical results on reweighted belief propagation to account for possible ties in the beliefs and using these results we obtain easily checkable conditions that guarantee that the BP disparities are the global optima. We verify experimentally that these conditions are typically met for the standard benchmark stereo pairs and discuss the implications of our results for further progress in stereo.",
    "actual_venue": "Iccv"
  },
  {
    "abstract": "This paper aims at optimally adjusting a set of green times for traffic lights in a single intersection with the purpose of minimizing travel delay time and traffic congestion. Fuzzy logic system (FLS) is the method applied to develop the intelligent traffic timing controller. For this purpose, an intersection is considered and simulated as an intelligent agent that learns how to set green times in each cycle based on the traffic information. The FLS controller (FLC) uses genetic algorithm to tune its parameters during learning phase. Finally, The performance of the intelligent FLC is compared with the performance of a FLC with predefined parameters and three simple fixed-time controller. Simulation results indicate that intelligent FLC significantly reduces the total delay in the network compared to the fixed-time method and FLC with manual parameter setting.",
    "actual_venue": "Systems, Man And Cybernetics"
  },
  {
    "abstract": "Providing an adequate long-term user participation incentive is important for a participatory sensing system to maintain enough number of active users (sensors), so as to collect a sufficient number of data samples and support a desired level of service quality. In this work, we consider the sensor selection problem in a general time-dependent and location-aware participatory sensing system, taking the long-term user participation incentive into explicit consideration. We study the problem systematically under different information scenarios, regarding both future information and current information (realization). In particular, we propose a Lyapunov-based VCG auction policy for the on-line sensor selection, which converges asymptotically to the optimal off-line benchmark performance, even with no future information and under asymmetry of current information. Extensive numerical results show that our proposed policy outperforms the state-of-art policies in the literature, in terms of both user participation (e.g., reducing the user dropping probability by 25% similar to 90%) and social performance (e.g., increasing the social welfare by 15% similar to 80%).",
    "actual_venue": "Ieee Conference On Computer Communications"
  },
  {
    "abstract": "An open problem in the numerical solution of fractional partial differential equations (FPDEs) is how to obtain high-order accuracy for singular solutions; even for smooth right-hand sides solutions of FPDEs are singular. Here, we consider the one-dimensional diffusion equation with general two-sided fractional derivative characterized by a parameter p is an element of [0, 1]; for p = 1/2 we recover the Riesz fractional derivative, while for p = 1, 0 we obtain the one-sided fractional derivative. We employ a Petrov Galerkin projection in a properly weighted Sobolev space with (two-sided) Jacobi polyfracnomials as basis and test functions. In particular, we derive these two-sided Jacobi polyfractonomials as eigenfunctions of a Sturm-Liouville problem with weights uniquely determined by the parameter p. We provide a rigorous analysis and obtain optimal error estimates that depend on the regularity of the forcing term, i.e., for smooth data (corresponding to singular solutions) we obtain exponential convergence, while for smooth solutions we obtain algebraic convergence. We demonstrate the sharpness of our error estimates with numerical examples, and we present comparisons with a competitive spectral collocation method of tunable accuracy. We also investigate numerically deviations from the theory for inhomogeneous Dirichlet boundary conditions as well as for a fractional diffusion-reaction equation.",
    "actual_venue": "Siam Journal On Numerical Analysis"
  },
  {
    "abstract": "The immense popularity in mobile internet has driven part of classic web information and services to transform and shrink themselves to match the diversity of web-capable mobile devices. This paper will present a Web-based mashup tool to work with range of Japanese domestic mobile handsets and their variety of limited functionalities. Our purpose is to provide users an alternative option to customize information to be displayed and to integrate available services for their mobile phone. Three major working components, functionality tester, service aggregator and output simulator will aid users in understanding the capability of their mobile device and selecting corresponding functionalities. Successfully simulated and simplified information services will be accessible via user's mobile phones with correct configurations.",
    "actual_venue": "Icwe"
  },
  {
    "abstract": "The problem of recovering a low-rank matrix from a set of observations corrupted with gross sparse error is known as the robust principal component analysis (RPCA) and has many applications in computer vision, image processing and web data ranking. It has been shown that under certain conditions, the solution to the NP-hard RPCA problem can be obtained by solving a convex optimization problem, namely the robust principal component pursuit (RPCP). Moreover, if the observed data matrix has also been corrupted by a dense noise matrix in addition to gross sparse error, then the stable principal component pursuit (SPCP) problem is solved to recover the low-rank matrix. In this paper, we develop efficient algorithms with provable iteration complexity bounds for solving RPCP and SPCP. Numerical results on problems with millions of variables and constraints such as foreground extraction from surveillance video, shadow and specularity removal from face images and video denoising from heavily corrupted data show that our algorithms are competitive to current state-of-the-art solvers for RPCP and SPCP in terms of accuracy and speed.",
    "actual_venue": "Comp Opt And Appl"
  },
  {
    "abstract": "We study generalizations of the Tower of Hanoi (ToH) puzzle with relaxed placement rules. In 1981, D. Wood suggested a variant, where a bigger disk may be placed higher than a smaller one if their size difference is less than k. In 1992, D. Poole suggested a natural disk-moving strategy, and computed the length of the shortest move sequence (algorithm) under its framework. However, other strategies were not considered, so the lower bound/optimality question remained open. In 1998, Beneditkis, Berend, and Safro were able to prove the optimality of Poole's algorithm for the first non-trivial case k=2 only. We prove it be optimal in the general case. Besides, we prove a tight bound for the diameter of the configuration graph of the problem suggested by Wood. Further, we consider a generalized setting, where the disk sizes should not form a continuous interval of integers. To this end, we describe a finite family of potentially optimal algorithms and prove that for any set of disk sizes, the best one among those algorithms is optimal. Finally, a setting with the ultimate relaxed placement rule (suggested by D. Berend) is defined. We show that it is not more general, by finding a reduction to the second setting.",
    "actual_venue": "Isaac"
  },
  {
    "abstract": "Recently, there has been a growing interest in the Agent-oriented paradigm to cope with the needs imposed by nowadays complex and networked systems. Developing Multi-Agent Systems (MAS) calls for addressing aspects such as interaction, autonomy, collaboration and pro-activeness. One way to cope with these needs is to have agency properties as well as intentionality in the center of the software development process. In this work a proposal is presented to bring intentionality and agency properties to the early stages of software development. The proposal is based on Strategic Dependency Situations (SDsituations) as a simple technique for helping requirements elicitation. Strategic Dependency Situations applies the Agent-Oriented approach based on intentionality to face the complexity of MAS developing.",
    "actual_venue": "WER"
  },
  {
    "abstract": "This paper examines \"language processing\" approach to paging where the of the programming language compiler or interpreter is responsible for generating the necessary control code for the page management of a program. We explore this idea for APL and describe an approach to incorporating in a program the necessary paging functions. The semantics of APL computation are examined to observe how paging operations can be incorporated into the computation. We discuss a model of data access in APL that exhibits storage use for both scalar and page references. A data structure that encodes the logical use of data from an array is introduced. We find that ordering computations efficiently and computing paging needs can be determined by simple transformations on this structure. This analysis leads us to an efficient method for paging an APL computation. Our approach builds on previous studies for efficiently executing APL.",
    "actual_venue": "Proceedings Of The Acm Sigplan-Sigact Symposium On Principles Of Programming Languages"
  },
  {
    "abstract": "Browsing, search and retrieval in digital video libraries depend on the ability of the system to match, classify and group video shots by their visual contents. However, similarity of shots cannot always be settled by using only one key frame per shot as commonly practiced. In this paper we propose a scheme to match video shots and to cluster them by taking into account the temporal variations within individual shots. A much reduced representation for a video shot is used. These images still capture the dynamics of visual contents for matching and clustering process. Experimental results are reported.",
    "actual_venue": "Icip"
  },
  {
    "abstract": "This paper reports how OptBees, an algorithm inspired by the collective decision-making of bee colonies, performed in the test bed developed for the Special Session & Competition on Real-Parameter Single Objective Optimization at CEC-2014. The test bed includes 30 scalable functions, many of which are both non-separable and highly multi-modal. Results include OptBees' performance on the 10, 30, 50 and 100-dimensional versions of each function.",
    "actual_venue": "Ieee Congress On Evolutionary Computation"
  },
  {
    "abstract": "In this paper channel capacity analysis of spread spectrum audio watermarking (SSW) system for transmission of hidden information through a noisy environment is performed. Channel capacity analysis of algorithm enables us to determine the most reliable transmission rate of hidden information by the watermarking system, i.e. the analysis makes a relation between capacity and robustness as two conflicting features of watermarking algorithms. Examining transmission and detection of hidden information shows that SSW behaves like a binary symmetric channel. It is proved that well known Shannon channel capacity formula for calculating channel capacity is not applicable for SSW system. Instead, analysis of channel capacity for additive color noise channels should be used. Obtaining channel capacity for SSW system using pseudo-noise random sequences with different period, different power of additive white Gaussian noise channel and different power of host signal is performed in simulation results.",
    "actual_venue": "Iih-Msp"
  },
  {
    "abstract": "Process modeling is one of the most significant tasks software process improvement teams perform. Conventionally modeling is performed by teams composed of domain experts and process engineers and it takes considerable effort and time. In recent years there have been studies which use the data extracted from the actual events that took place to determine process models. In this paper we present the results of a case study we conducted to determine the effectiveness of four process discovery and process mining algorithms. After applying process discovery algorithms we compare the results by the actual process and process definitions. We discuss the discrepancies between the actual flow and the process definitions, and the weaknesses and strong aspects of the algorithms.",
    "actual_venue": "Euromicro-Seaa"
  },
  {
    "abstract": "This paper presents the design of the robot AILA, a mobile dual-arm robot system developed as a research platform for investigating aspects of the currently booming multidisciplinary area of mobile manipulation. The robot integrates and allows in a single platform to perform research in most of the areas involved in autonomous robotics: navigation, mobile and dual-arm manipulation planning, active compliance and force control strategies, object recognition, scene representation, and semantic perception. AILA has 32 degrees of freedom, including 7-DOF arms, 4-DOF torso, 2-DOF head, and a mobile base equipped with six wheels, each of them with two degrees of freedom. The primary design goal was to achieve a lightweight arm construction with a payload-to-weight ratio greater than one. Besides, an adjustable body should sustain the dual-arm system providing an extended workspace. In addition, mobility is provided by means of a wheel-based mobile base. As a result, AILA's arms can lift 8kg and weigh 5.5kg, thus achieving a payload-to-weight ratio of 1.45. The paper will provide an overview of the design, especially in the mechatronics area, as well as of its realization, the sensors incorporated in the system, and its control software.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "Recent works on brain functional analysis have highlighted the importance of distributed functional networks and synchronized activity between networks in mediating cognitive functions. The network perspective is fundamental to relate mechanisms of brain functions and the basis for classifying brain states. This work analyzes the network mechanisms related to motor imagery tasks based on synchronization measure (PLV (phase-locking value)) in EEG alpha-band for the BCI Competition IV Data Set. Based on network dissimilarities between motor imagery and rest tasks, important nodes and important channel pairs corresponding to tasks for all subjects are identified. The identified important channel pairs corresponding to tasks demonstrate significant PLV variation in line with the experiment protocol. With the selection of subject-specific reactive band, these channel pairs provide even more higher variation corresponding to tasks. This paper demonstrates the potential of these identified channel pairs in task classification for future BCI applications.",
    "actual_venue": "Annual International Conference Of The Ieee Engineering In Medicine And Biology Society, Vols"
  },
  {
    "abstract": "Pull requests form a new method for collaborating in distributed software development. To study the pull request distributed development model, we constructed a dataset of almost 900 projects and 350,000 pull requests, including some of the largest users of pull requests on Github. In this paper, we describe how the project selection was done, we analyze the selected features and present a machine learning tool set for the R statistics environment.",
    "actual_venue": "MSR"
  },
  {
    "abstract": "This paper is concerned with coordinated collaboration of multiagent systems in which there exist multiple agents which have their own set of skills to perform some tasks, multiple external resources which can be either used exclusively by an agent or shared by the specified number of agents at a time, and a set of tasks which consist of a collection of subtasks each of which can be carried out by an agent. Even though a subtask can be carried out by several agents, its processing cost may be different depending on which agent performs it. To process tasks, some coordination work is required such as allocating their constituent subtasks among competent agents and scheduling the allocated subtasks to determine their processing order at each agent. This paper proposes a genetic algorithm-based method to coordinate the agents to process tasks in the considered multiagent environments. It also presents some experiment results for the proposed method and discusses the pros and cons of the proposed method.",
    "actual_venue": "Lecture Notes In Artificial Intelligence"
  },
  {
    "abstract": "The coarse-grained reconfigurable architecture (CGRA) is proven to be energy efficient in several specific domains. In CGRAs, the on-chip memory hierarchy, which contains the context memory and the data memory organizations, should be well considered to achieve appropriate tradeoffs among three aspects: 1) performance; 2) area; and 3) power. In this paper, two techniques called the hierarchical configuration context (HCC) and the lifetime-based data-memory organization (LDO) focusing on the context memory and the data memory organizations are proposed to compress the on-chip memory space and to reduce the reconfiguration time and the data-reference time. In the HCC, the contexts are constructed in a hierarchical fashion to completely eliminate the repetitive portions of the contexts, not only reducing the overall context storage, but also alleviating the context transportation overhead. A fast context-indexing mechanism in the HCC is proposed to achieve fast reconfiguration, as the hierarchically organized contexts can be located and accessed conveniently. In the LDO, the on-chip data are classified into two types, based on the lifetime of data. The short-lifetime data are stored in the first in first out to increase the reuse ratio of memory space automatically, whereas the long-lifetime data are stored in the radom access memory for several time references. The HCC and the LDO are used in a CGRA core called as reconfigurable processing unit (RPU). Two RPUs are integrated in a reconfigurable computing processor (RCP) called as REconfigurable MUlti-media System, High-Performance Processor (REMUS_HPP). Because of the HCC, compared with a traditional nonhierarchical system, the total context storage required in H.264 decoding is reduced by 77%. Because of the LDO, the normalized on-chip data memory size at same performance level in the REMUS_HPP is only 23.8% and 14.8% of those in XPP-III (a high-performance RCP) and ADRES (a low-power RCP). REMUS_HPP is implemented - n a 48.9-mm2 silicon with TSMC 65-nm technology, using a 200-MHz working frequency to achieve 1920 Ã 1088 at 30 fps H.264 high-profile decoding. Compared with XPP-III, the performance of the REMUS_HPP is 1.81Ã boosted, whereas the energy efficiency is 4.75Ã higher.",
    "actual_venue": "Ieee Transactions On Very Large Scale Integration Systems"
  },
  {
    "abstract": "Though the relationship between the investment in information systems (IS) and a firm's performance continues to be important; conclusive evidence that information technology (IT) contributes to a firm's effectiveness is rare. This study tests the relationship between the integration of IS during mergers and acquisitions and their effectiveness. The findings point to a positive relationship between IS integration and effectiveness only when controlling for (a) IT intensity, and (b) organizational culture differences between the joining firms. Thus, managers are advised to take into account IT intensity and cultural differences during the pre-merger negotiations and during the post-merger integration process.",
    "actual_venue": "Information And Management"
  },
  {
    "abstract": "Along with recent experiments in the design of communication or computer tools for supporting various kinds of group working, the development ofcollaborative drawing systems has emerged as a notable research area within the field of Computer-Supported Cooperative Work. This paper reports a survey of the experiments in collaborative drawing support tools with an objective of reviewing how the issues of supportingcollaborative design have been addressed by the research prototypes. The survey is presented in three parts: (1) findings from the observations of group interaction in drawing and design activities, (2) a framework for classifying the design issues experimented with by prototypes developers, and (3) a categorisation of the current prototype systems by interrelating the patterns of group use observed with the system features classified. The survey indicates that there are currently at least three different strategies of developing collaborative drawing support tools, which reflect the existence of diversified understanding and technological responses to what and how human collaboration in design may be supported.",
    "actual_venue": "Computer Supported Cooperative Work"
  },
  {
    "abstract": "Metering of the energy supplied to consumers is an important component of operations for utility providers. Several schemes have been employed for this purpose, including traditional postpaid and prepaid metering, and more advanced smart metering technology. Analysis of the data generated by these meters has the potential to provide insights into consumer characteristics and power consumption patterns, including consumer segmentation and anomaly detection. We describe the different types of power purchase and consumption data, as well as the analytics algorithms that can be applied to them. Most applications developed for energy meter data require high resolution information of the type provided by smart meters, thus leaving aggregate prepaid or postpaid meter schemes at a disadvantage. In this paper, we present analytics-based methodologies to upgrade aggregate prepaid and postpaid meter data resolution, which will allow smart meter analytics to be applied without expensive infrastructure upgrades.",
    "actual_venue": "E-Energy"
  },
  {
    "abstract": "We define some new polynomials associatedto a linear binary code and a harmonic function of degree k.The case k=0 is the usual weight enumerator of thecode. When divided by (xy)^k, they satisfy a MacWilliamstype equality. When applied to certain harmonic functions constructedfrom Hahn polynomials, they can compute some information on theintersection numbers of the code. As an application, we classifythe extremal even formally self-dual codes of length 12.",
    "actual_venue": "Des Codes Cryptography"
  },
  {
    "abstract": "In recent years we have seen numerous proof-of-concept attacks on implantable medical devices such as pacemakers. Attackers aim to breach the strict operational constraints that these devices operate within, with the end-goal of compromising patient safety and health. Most efforts to prevent these kinds of attacks are informal, and focus on application- and system-level security --- for instance, using encrypted communications and digital certificates for program verification. However, these approaches will struggle to prevent all classes of attacks. Runtime verification has been proposed as a formal methodology for monitoring the status of implantable medical devices. Here, if an attack is detected a warning is generated. This leaves open the risk that the attack can succeed before intervention can occur. In this paper, we propose a runtime-enforcement based approach for ensuring patient security. Custom hardware is constructed for individual patients to ensure a safe minimum quality of service at all times. To ensure correctness we formally verify the hardware using a model-checker. We present our approach through a pacemaker case study and demonstrate that it incurs minimal overhead in terms of execution time and power consumption.",
    "actual_venue": "Proceedings Of The Acm-Ieee International Conference On Formal Methods And Models For System Design"
  },
  {
    "abstract": "A Simulated Annealing Genetic Algorithm (SAGA) with greedy mapping mechanism is developed to solve task partitioning problems in coarse grain reconfigurable systems. A fitness function combined multiple objectives (communication cost, number of partitions and number of bypass nodes) is constructed to optimize the execution time. Experimental results show that SAGA produces better solutions than traditional level-based or clustering-based partitioning algorithm. The operation time saved is up to 5% compared with level-based algorithm, and critical parameters such as communication cost and number of partitions are reduced by 10% in average.",
    "actual_venue": "Asicon"
  },
  {
    "abstract": "Several estimation problems in vision involve the minimization of cumulative geometric error using non-linear least-squares fitting. Typically, this error is characterized by the lack of interdependence among certain subgroups of the parameters to be estimated, which leads to minimization problems possessing a sparse structure. Taking advantage of this sparseness during minimization is known to achieve enormous computational savings. Nevertheless, since the underlying sparsity pattern is problem-dependent, its exploitation for a particular estimation problem requires non-trivial implementation effort, which often discourages its pursuance in practice. Based on recent developments in sparse linear solvers, this paper provides an overview of sparseLM, a generalpurpose software package for sparse non-linear least squares that can exhibit arbitrary sparseness and presents results from its application to important sparse estimation problems in geometric vision.",
    "actual_venue": "Eccv"
  },
  {
    "abstract": "With the advancement of mobile technology, mobile phones can store significant amount of sensitive and private information. The security issue of mobile phones becomes an important field to investigate. This paper proposes a prototype of fingerprint authentication mobile phone based on sweep sensor MBF310. The prototype is composed of the front-end fingerprint capture sub-system and the back-end fingerprint recognition system. A sweep fingerprint sensor MBF310 is used to fit the request of the mobile phone in the field of the size, cost, and power consumption. The performance of the proposed prototype is evaluated on the database built by the sweep fingerprint sensor. The EER is 4.23%, and the average match time of the prototype is about 4.5 seconds.",
    "actual_venue": "Icapr"
  },
  {
    "abstract": "In this paper, a semi-blind algorithm is proposed for the detection of most significant tap (MST) in the sparse channel estimation of OFDM systems. Based on an analysis of the second-order statistics of the signal received through a noise-free sparse channel, a direct relationship between the positions of the most significant taps (MST) of the sparse channel and the lags of the nonzero correlation functions is revealled, leading to an efficient semi-blind MST detection algorithm. By using the acquired MST position, a sparse least square channel estimate is then obtained. A number of computer simulation-based experiments are carried out to confirm the effectiveness of the proposed semi-blind MST detection algorithm and the associated sparse LS channel estimation method.",
    "actual_venue": "Iscas"
  },
  {
    "abstract": "The MPEG-4 Scalable to Lossless (SLS) audio coding is recently being developed to provide a unified solution for high compression perceptual audio coding and high-quality lossless audio coding. SLS provides efficient Fine Granular Scalable (FGS) coding from AAC core layer to lossless, and achieves reasonable perceptual quality at its scalable coding range using a sequential bit-plane scanning method, which minimizes the audio distortion according to the spectral shape of the core layer quantization errors. In this paper, it is shown that the perceptual quality performance of SLS at intermediate rates can be further improved by incorporating psychoacoustic model into the bit-plane coding process. In addition, it is also found that such an improvement can be achieved by slightly tweaking the original bit-plane coding process of SLS and hence preserving its nice features such as compatibility to lossless coding and low complexity.",
    "actual_venue": "Ieee International Conference On Multimedia And Expo - Icme , Vols -, Proceedings"
  },
  {
    "abstract": "Providing travel time information to travelers on available route alternatives in traffic networks is widely believed to yield positive effects on both individual drive and (route/departure time) choice behavior as well as on collective traffic operations in terms of for example overall time savings and - if nothing else - on the reliability of travel times. As such there is an increasing need for fast and reliable online travel time prediction models. In an operational context, also adaptivity of such models is a crucial property. This paper describes a method to calibrate (train) a data driven travel time prediction model (a so-called state-space neural network - SSNN) in an incremental fashion. Since travel times are available only for realized trips, travel time prediction is not a one-step prediction task, and thus online incremental learning methods such as the extended Kalman filter (EKF) can not be applied directly. We propose a delayed EKF method which can be applied online. By constraining the model parameters within particular bounds, an automatic regularization scheme is incorporated, which guarantees a smooth mapping",
    "actual_venue": "Itsc"
  },
  {
    "abstract": "The Pacific Gas and Electric Company, the largest investor-owned energy utility in the United States, obtains a significant fraction of its electric energy and capacity from hydrogeneration. Although hydro provides valuable flexibility, it is subject to usage limits and must be carefully scheduled. In addition, the amount of energy available from hydro varies widely from year to year, depending on precipitation and streamflows. Optimal scheduling of hydrogeneration, in coordination with other energy sources, is a stochastic problem of practical significance to PG&E. SOCRATES is a system for the optimal scheduling of PG&E's various energy sources over a one- to two-year horizon. This paper concentrates on the component of SOCRATES that schedules hydro. The core is a stochastic optimization model, solved using Benders decomposition. Additional components are streamflow forecasting models and a database containing hydrological information. The stochastic hydro scheduling module of SOCRATES is undergoing testing in the user's environment, and we expect PG&E hydrologists and hydro schedulers to place progressively more reliance upon it.",
    "actual_venue": "Annals Of Operations Research"
  },
  {
    "abstract": "In this paper, we propose a novel type of Impulse Radio Ultra-Wideband (IR-UWB) beamforming structure that we named a chirp beamformer. It represents an alternative to the well-known correlation beamforming paradigm. The chirp beamformer uses a chirp pulse as a symbol waveform. In comparison with a correlation beamformer, the chirp beamformer requires considerably lower timing resolution in cases where the IR-UWB symbol consists of a relatively long single uninterrupted waveform, as in the mandatory modes of two of the IEEE 802.15 IR-UWB standards, namely IEEE 802.15.4a-2007 and IEEE 802.15.6-2012. Furthermore, the bandwidth of the signal that the chirp beamformer generates in the baseband is considerably lower than in the case of the correlation beamformer. Hence, in the case of digital baseband signal generation, the chirp beamformer requires a considerably lower sampling rate than the correlation beamformer. A method for shaping the beam pattern of the chirp beamformer is also described. All concepts introduced are illustrated by numerical examples.",
    "actual_venue": "Pimrc"
  },
  {
    "abstract": "Set-associative caches are widely used in CPU memory hierarchies, I/O subsystems, and file systems to reduce average access times. This article proposes an efficient simulation technique for simulating a group of set-associative caches in a single pass through the address trace, where all caches have the same line size but varying associativities and varying number of sets. The article also introduces a generalization of the ordinary binomial tree and presents a representation of caches in this class using the Generalized Binomial Tree (gbt). The tree representation permits efficient search and update of the caches. Theoretically, the new algorithm, GBF_LS, based on the gbt structure, always takes fewer comparisons than the two earlier algorithms for the same class of caches: all-associativity and generalized forest simulation. Experimentally, the new algorithm shows performance gains in the range of 1.2 to 3.8 over the earlier algorithms on address traces of the SPEC benchmarks. A related algorithm for simulating multiple alternative direct-mapped caches with fixed cache size, but varying line size, is also presented.",
    "actual_venue": "Acm Trans Comput Syst"
  },
  {
    "abstract": "We introduce the Bayesian Expansion (BE), an approximate numerical technique for passage time distribution analysis in queueing networks. BE uses a class of Bayesian networks to approximate the exact joint probability density of the model by a product of conditional marginal probabilities that scales efficiently with the model's size. We show that this naturally leads to decomposing a queueing network into a set of Markov processes that jointly approximate the dynamics of the model and from which passage times are easily computed. Approximation accuracy of BE depends on the specific Bayesian network used to decompose the joint probability density. Hence, we propose a selection algorithm based on the Kullback-Leibler divergence to search for the Bayesian network that provides the most accurate results. Random models and case studies of increasing complexity show the significant accuracy gain of distribution estimates returned by BE compared to Markov and Chebyshev inequalities that are frequently used for percentile estimation in queueing networks.",
    "actual_venue": "Perform Eval"
  },
  {
    "abstract": "The goal of producing a general purpose, semantically moti- vated, and computationally tractable deductive reasoning ser- vice remains surprisingly elusive. By and large, approaches that come equipped with a perspicuous model theory either result in reasoners that are too limited from a practical point of view or fall off the computational cliff. In this paper, we propose a new logic of belief called SL which lies between the two extremes. We show that query evaluation based on SL for a certain form of knowledge bases with disjunctive information is tractable in the propositional case and decidable in the first-order case. Also, we present a sound and complete axiomatization for propositional SL.",
    "actual_venue": "KR"
  },
  {
    "abstract": "The users of mobile devices increasingly use networked services to address their information needs. Questions asked by mobile users are strongly influenced by contextual factors such as location, conversation and activity. We report on a diary study performed to better understand mobile information needs. We find that the type of questions recorded by participants varies across their locations, with differences between home, shopping and in-car contexts. These variations occur both in the query terms and in the form of desired answers. Both the location of queries and the participants' activities affected participants' questions. When information needs were affected by both location and activity, they tended to be strongly affected by both factors. The overall picture that emerges is one of multiple contextual influences interacting to shape mobile information needs. Mobile devices that attempt to adapt to users' context will need to account for a rich variety of situational factors.",
    "actual_venue": "Mobile Hci"
  },
  {
    "abstract": "Clinical pathways are widely adopted by many large hospitals around the world in order to provide high-quality patient treatment and reduce the length and cost of hospital stay. However, nowadays most of them are static and non-personalized. Our objective ...",
    "actual_venue": "Iscsct"
  },
  {
    "abstract": "Prosody is a kind of cues that are critical to human speech perception and comprehension, so it is plausible to integrate prosodic information into machine speech recognition. However, as a result of the supra-segmental nature, it is hard to integrate prosodic information with conventional acoustic features. Recently, RNNLMs have shown to be the state-of-the-art language model in many tasks. We thus attempt to integrate prosodic information into RNNLMs for improving speech recognition performance based on rescoring strategy. Firstly, three word-level prosodic features are extracted from speech and then passed to RNNLMs separately. Therefore RNNLMs predict the next word based on prosodic features and word history. Experiments conducted on LibriSpeech Corpus show that the word error rate decreases from 8.07% to 7.96%. Secondly, prosodic information is combined on feature-level and model-level for further improvements and word error rate decreases 4.71% relatively.",
    "actual_venue": "Asia-Pacific Signal And Information Processing Association Annual Summit And Conference"
  },
  {
    "abstract": "A new method of recognition the fabric defects features is proposed for alleviating the difficulties of extracting to complicated fabric defects features. First, fast Fourier transform and self-adaptive power spectrum decomposition are performed. Sector-regional energy of spectrum is extracted and its mean and standard deviations is calculated as fabric features. Then, the spectral energy distribution was objected to direction Y, and local peak was extracted as defects recognition features after objection. Fabric defects recognition using the proposed method shows a high performance in the on-line detection.",
    "actual_venue": "Computer Science And Software Engineering, International Conference"
  },
  {
    "abstract": "This work reports two different characteristic patterns detected in IGBT chips failed in real operation (railway application) by failure analysis procedures. The analysed chips have been recovered from the rheostatic chopper leg and from the three legs which supplies the traction motor. It is observed that depending on the location and characteristics of the detected default (burn-out spot), this failure can be attributed to a latch-up process or a secondary breakdown mechanism. These results are corroborated with tests at limit, obtaining the same result. Consequently, each failure can be linked to overcurrent (latch-up) or overtemperature (secondary breakdown) events, which makes possible to distinguish between problems coming from driving strategies or thermal issues (uneven temperature distribution inside the module or packaging wear-out).",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Three decades ago, Rockel et al. proposed that neuronal surface densities (number of neurons under a square millimeter of surface) of primary visual cortices (V1s) in primates is 2.5 times higher than the neuronal density of V1s in nonprimates or many other cortical regions in primates and nonprimates. This claim has remained controversial and much debated. We replicated the study of Rockel et al. with attention to modern stereological precepts and show that indeed primate V1 is 2.5 times denser (number of neurons per square millimeter) than many other cortical regions and nonprimate V1s; we also show that V2 is 1.7 times as dense. As primate V1s are denser, they have more neurons and thus more pinwheels than similar-sized nonprimate V1s, which explains why primates have better visual acuity.",
    "actual_venue": "Proceedings Of The National Academy Of Sciences Of The United States Of America"
  },
  {
    "abstract": "Motivation: Data collection in spreadsheets is ubiquitous, but current solutions lack support for collaborative semantic annotation that would promote shared and interdisciplinary annotation practices, supporting geographically distributed players. Results: OntoMaton is an open source solution that brings ontology lookup and tagging capabilities into a cloud-based collaborative editing environment, harnessing Google Spreadsheets and the NCBO Web services. It is a general purpose, format-agnostic tool that may serve as a component of the ISA software suite. OntoMaton can also be used to assist the ontology development process.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "The application of a family of single parity-check (SPC) product codes in the fiber-optic code-division multiple-access (CDMA) systems with binary pulse position modulation (BPPM) is considered in this paper. We evaluate the bit-error probabilities of the coded systems, taking into account the photodetector noise, thermal noise, and the multiple-access interference. In this paper, the performance of the employed coding schemes over a binary-input output-symmetric channel is analyzed, based on the density evolution approach. The validity of the analytically obtained bit-error rates (BERs) over binary symmetric channels (BSCs) for each of the decoding iterations is verified by the system simulation. It is then shown that the application of these coding schemes in the optical BPPM-CDMA communication system, which can be modeled as a BSC, significantly improves the system performance. Particularly, for a certain BER (less than 10-7) and bandwidth, the coded systems not only permit a higher number (more than twice) of active users, compared with the uncoded systems, but also can operate at higher channel bit rate (up to four times) with more than 10% energy saving. Furthermore, by increasing the number of code dimensions, less power (less than 40%) is required to achieve the desired performance",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Since its inception, the field of Data Mining and Knowledge Discovery from Databases has been driven by the need to solve practical problems [4]. From scaling to large databases and handling noisy and high-dimensional data to finding associational patterns in grocery store transaction data, data mining is a research area rich in application [1]. Despite its practical roots few case studies of data mining applications have been published. The industrial track of the annual SIGKDD conference has provided one such forum, but rarely do these papers present complete descriptions of deployed systems [2]. This special issue attempts to address the gap by showcasing the choices, strategies, and lessons learned from building a real-world data mining application. In a sense this collection is a follow-up to the first workshop on data mining case studies held during ICDM-2006 [3]. This issue however introduces several new papers. Of the 29 papers reviewed 10 papers were accepted. The papers come from a broad range of application areas including Customer Relationship Management, Medicine, Taxation, and Software Development.",
    "actual_venue": "Sigkdd Explorations"
  },
  {
    "abstract": "In a dynamic heterogeneous environment, such as pervasive and ubiquitous computing, context-aware adaptation is a key concept to meet the varying requirements of different users. Context-awareness is the most promising way to manage the user information and to provide the means of communication at the right time in the right way. Connectivity and quality of service (QoS) of applications are two most important considerations that should be taken into account for designing a context-aware system. This paper presents connectivity from the view point of context awareness, identifies various relevant raw connectivity contexts, and discusses how high-level context information can be abstracted from the raw context information. It also presents a QoS-and context-aware algorithm for supporting mobile applications in a heterogeneous network environment. The unified approach towards connectivity information and QoS-awareness makes the algorithm more practical than most of the currently existing algorithms which consider connectivity and QoS separately. Simulation results show that the use of context information helps to improve the delivered application QoS.",
    "actual_venue": "Bangalore Compute Conf"
  },
  {
    "abstract": "The demand for humanitarian aid is extraordinarily large and it is increasing. In contrast, the funding for humanitarian operations does not seem to be increasing at the same rate. Humanitarian logistics has the challenge of allocating scarce resources to complex operations in an efficient way. After acquiring sufficient contextual knowledge, academics can use operations research (OR) to adapt successful supply chain management best practices to humanitarian logistics. We present two cases of OR applications to field vehicle fleet management in humanitarian operations. Our research shows that by using OR to adapt supply chain best practices to humanitarian logistics, significant improvements can be achieved.",
    "actual_venue": "International Transactions In Operational Research"
  },
  {
    "abstract": "If you're still recovering from the barrage of ads, news, emails, Facebook posts, and newspaper articles that were giving you the latest poll numbers, asking you to volunteer, donate money, and vote, this talk will give you a look behind the scenes on why you were seeing what you were seeing. I will talk about how machine learning and data mining along with randomized experiments were used to target and influence tens of millions of people. Beyond the presidential elections, these methodologies for targeting and influence have the power to solve big problems in education, healthcare, energy, transportation, and related areas. I will talk about some recent work we're doing at the University of Chicago Data Science for Social Good summer fellowship program working with non-profits and government organizations to tackle some of these challenges.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "Researchers have been investigating shape-changing interfaces, however technologies for thin, reversible shape change remain complicated to fabricate. uniMorph is an enabling technology for rapid digital fabrication of customized thin-film shape-changing interfaces. By combining the thermoelectric characteristics of copper with the high thermal expansion rate of ultra-high molecular weight polyethylene, we are able to actuate the shape of flexible circuit composites directly. The shape-changing actuation is enabled by a temperature driven mechanism and reduces the complexity of fabrication for thin shape-changing interfaces. In this paper we describe how to design and fabricate thin uniMorph composites. We present composites that are actuated by either environmental temperature changes or active heating of embedded structures and provide a systematic overview of shape-changing primitives. Finally, we present different sensing techniques that leverage the existing copper structures or can be seamlessly embedded into the uniMorph composite. To demonstrate the wide applicability of uniMorph, we present several applications in ubiquitous and mobile computing.",
    "actual_venue": "Acm Symposium On User Interface Software And Technology"
  },
  {
    "abstract": "Structured data acquisition is a common task that is widely performed in biomedicine. However, current solutions for this task are far from providing a means to structure data in such a way that it can be automatically employed in decision making (e.g., in our example application domain of clinical functional assessment, for determining eligibility for disability benefits) based on conclusions derived from acquired data (e.g., assessment of impaired motor function). To use data in these settings, we need it structured in a way that can be exploited by automated reasoning systems, for instance, in the Web Ontology Language (OWL); the de facto ontology language for the Web.We tackle the problem of generating Web-based assessment forms from OWL ontologies, and aggregating input gathered through these forms as an ontology of \"semantically-enriched\" form data that can be queried using an RDF query language, such as SPARQL. We developed an ontology-based structured data acquisition system, which we present through its specific application to the clinical functional assessment domain. We found that data gathered through our system is highly amenable to automatic analysis using queries.We demonstrated how ontologies can be used to help structuring Web-based forms and to semantically enrich the data elements of the acquired structured data. The ontologies associated with the enriched data elements enable automated inferences and provide a rich vocabulary for performing queries.",
    "actual_venue": "J Biomedical Semantics"
  },
  {
    "abstract": "Morphologically rich languages pose a challenge to the annotators of treebanks with respect to the status of orthographic (space-delimited) words in the syntactic parse trees. In such languages an orthographic word may carry various, distinct, sorts of information and the question arises whether we should represent such words as a sequence of their constituent morphemes (i.e., a Morpheme-Based annotation strategy) or whether we should preserve their special orthographic status within the trees (i.e., a Word-Based annotation strategy). In this paper we empirically address this challenge in the context of the development of Language Resources for Modern Hebrew. We compare and contrast the Morpheme-Based and Word-Based annotation strategies of pronominal clitics in Modern Hebrew and we show that the Word-Based strategy is more adequate for the purpose of training statistical parsers as it provides a better PP-attachment disambiguation capacity and a better alignment with initial surface forms. Our findings in turn raise new questions concerning the interaction of morphological and syntactic processing of which investigation is facilitated by the parallel treebank we made available.",
    "actual_venue": "Sixth International Conference On Language Resources And Evaluation, Lrec"
  },
  {
    "abstract": "This paper explores methods and representations that allow two perceptually heterogeneous robots, each of which represents concepts via grounded properties, to transfer knowledge despite their differences. This is an important issue, as it will be increasingly important for robots to communicate and effectively share knowledge to speed up learning as they become more ubiquitous.We use GÃ¤rdenfors' conceptual spaces to represent objects as a fuzzy combination of properties such as color and texture, where properties themselves are represented as Gaussian Mixture Models in a metric space. We then use confusion matrices that are built using instances from each robot, obtained in a shared context, in order to learn mappings between the properties of each robot. These mappings are then used to transfer a concept from one robot to another, where the receiving robot was not previously trained on instances of the objects. We show in a 3D simulation environment that these models can be successfully learned and concepts can be transferred between a ground robot and an aerial quadrotor robot.",
    "actual_venue": "Iros"
  },
  {
    "abstract": "The goal of the paper is to provide a vague summary of currently existing blockchain use cases in the information technology industry. Respective use cases have been examined in already existing scientific papers, Master Theses, industry white papers and blogs of industry experts. The paper also contains a description of blockchain main technological aspects and working principles, which allows making the assessment of the presented use cases. For each use case respective companies or organisations are added that are applying or testing the given solution. Due to research limitations the paper should not be considered an exhaustive blockchain use case description. The paper also provides short introduction into a feasibility analysis of specific blockchain use case. The authors describe the basic steps of potential idea evaluation with regards to blockchain main aspects. It helps understand the necessity for development of a detailed blockchain feasibility model.",
    "actual_venue": "Applied Computer Systems"
  },
  {
    "abstract": "Biosequences typically have a small alphabet, a long length, and patterns containing gaps (i.e., \"don't care\") of arbitrary size. Mining frequent patterns in such sequences faces a different type of explosion than in transaction sequences primarily motivated in market-basket analysis. In this paper, we study how this explosion affects the classic sequential pattern mining, and present a scalable two-phase algorithm to deal with this new explosion. The Segment Phase first searches for short patterns containing no gaps, called segments. This phase is efficient. The Pattern Phase searches for long patterns containing multiple segments separated by variable length gaps. This phase is time consuming. The purpose of two phases is to exploit the information obtained from the first phase to speed up the pattern growth and matching and to prune the search space in the second phase. We evaluate this approach on synthetic and real life data sets.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "Devices in mobile ad hoc networks depend on the cooperation of other nodes for relaying of packets in the network. Medium Access Control protocols such as IEEE 802.11 are efficient with upper layer protocols only if the nodes adhere to the protocol and cooperate. Non-cooperating nodes may delay forwarding of frames or drop the frames altogether. This might be advantageous for individual nodes (from the point of view of saving energy) but it hampers the network as a whole. Such non-cooperating nodes disrupt communication between the cooperating nodes. We present a solution with a part modification of the IEEE 802.11 protocol to detect and penalize such non-cooperating nodes and thus making it unattractive to deny cooperation. For this we associate each node with an index, the Fairness Index, which is dynamically mapped to the behavior of the associated node. We not only penalize them but also ensure reliable routing throughout. In our scheme we also allow a convicted node to return to the mainstream if it shows its eagerness to cooperate. We prove the same using simulation results.",
    "actual_venue": "Lecture Notes In Engineering And Computer Science"
  },
  {
    "abstract": "Because of the increasing availability of multi-core machines, clusters, Grids, and combinations of these there is now plenty of computational power, but today's programmers are not fully prepared to exploit parallelism. In particular, Java has helped in handling the heterogeneity of such environments. However, there is a lot of ground to cover regarding facilities to easily and elegantly parallelizing applications. One path to this end seems to be the synthesis of semi-automatic parallelism and Parallelism as a Concern (PaaC). The former allows users to be mostly unaware of parallel exploitation problems and at the same time manually optimize parallelized applications whenever necessary, while the latter allows applications to be separated from parallel-related code. In this paper, we present EasyFJP, an approach that implicitly exploits parallelism in Java applications based on the concept of fork-join synchronization pattern, a simple but effective abstraction for creating and coordinating parallel tasks. In addition, EasyFJP lets users to explicitly optimize applications through policies, or user-provided rules to dynamically regulate task granularity. Finally, EasyFJP relies on PaaC by means of source code generation techniques to wire applications and parallel-specific code together. Experiments with real-world applications on an emulated Grid and a cluster evidence that EasyFJP delivers competitive performance compared to state-of-the-art Java parallel programming tools.",
    "actual_venue": "Computer Science And Information Systems"
  },
  {
    "abstract": "This paper explores an application-specific customization technique for the data cache, one of the foremost area/power consuming and performance determining microarchitectural features of modern embedded processors. The automated methodology for customizing the processor microarchitecture that we propose results in increased performance reduced power consumption and improved determinism of critical system parts while the fixed design ensures processor standardization. The resulting improvements help to enlarge the significant role of embedded processors in modern hardware/software codesign techniques by leading to increased processor utilization and reduced hardware cost. A novel methodology for static analysis and a field-reprogrammable implementation of a customizable cache controller that implements a partitioned cache structure is proposed. The simulation results show significant decrease of miss ratio compared to traditional cache organizations",
    "actual_venue": "Codes"
  },
  {
    "abstract": "This work explores the feasibility of steering a drone with a (recurrent) neural network, based on input from a forward looking camera, in the context of a high-level navigation task. We set up a generic framework for training a network to perform navigation tasks based on imitation learning. It can be applied to both aerial and land vehicles. As a proof of concept we apply it to a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a room containing a number of obstacles. So far only feedforward neural networks (FNNs) have been used to train UAV control. To cope with more complex tasks, we propose the use of recurrent neural networks (RNN) instead and successfully train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision based control is a sequential prediction problem, known for its highly correlated input data. The correlation makes training a network hard, especially an RNN. To overcome this issue, we investigate an alternative sampling method during training, namely window-wise truncated backpropagation through time (WW-TBPTT). Further, end-to-end training requires a lot of data which often is not available. Therefore, we compare the performance of retraining only the Fully Connected (FC) and LSTM control layers with networks which are trained end-to-end. Performing the relatively simple task of crossing a room already reveals important guidelines and good practices for training neural control networks. Different visualizations help to explain the behavior learned.",
    "actual_venue": "Arxiv: Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "There are increasing applications that require precise calibration of cameras to perform accurate measurements on objects located within images, and an automatic algorithm would reduce this time consuming calibration procedure. The method proposed in this article uses a pattern similar to that of a chess board, which is found automatically in each image, when no information regarding the number of rows or columns is supplied to aid its detection. This is carried out by means of a combined analysis of two Hough transforms, image corners and invariant properties of the perspective transformation. Comparative analysis with more commonly used algorithms demonstrate the viability of the algorithm proposed, as a valuable tool for camera calibration.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "We study the structure of a social network of strong ties (trust network) investigating its property of connectedness versus fragmentation. To this purpose we analyse an extensive set of census data, about marrying or having children with immigrants, collected by Italian national statistical institute for all Italian municipalities from 2001 to 2011. Not using neither obtaining personal local information but only average ones, our method fully complies with privacy and confidentiality. Our findings show that large cities display the behaviour of highly fragmented trust networks where individuals face possible phenomena of alienation. Smaller cities and villages instead behave like fully connected social systems with a rich tie structure, where isolation is rare or completely absent. While confirming classical sociological theories on alienation in large urban areas our approach provides a quantitative method to test them and a predictive tool for policy makers.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "The control of a new kind of airship is presented. By restricting its flight to a vertical plane, the mathematical model is reduced. The simplified model is proved to be minimum phase, and a nonlinear controller based on input-output linearization is designed. Since the performance of the controller is significantly impacted by the choice of parameters, simulations of three different pole placement strategies are presented. The nonlinear controller shows better performances than a linear LQR controller when the initial condition is significantly away from the desired equilibrium.",
    "actual_venue": "CDC"
  },
  {
    "abstract": "This paper analyzes the minimization of Î±-divergences in the context of multi-class Gaussian process classification. For this task, several methods are explored, including memory and computationally efficient variants of the Power Expectation Propagation algorithm, which allow for efficient training using stochastic gradients and mini-batches. When these methods are used for training, very large datasets (several millions of instances) can be considered. The proposed methods are also very general as they can interpolate between other popular approaches for approximate inference based on Expectation Propagation (EP) (Î±â¯ââ¯1) and Variational Bayes (VB) (Î±â¯ââ¯0) simply by varying the Î± parameter. An exhaustive empirical evaluation analyzes the generalization properties of each of the proposed methods for different values of the Î± parameter. The results obtained show that one can do better than EP and VB by considering intermediate values of Î±.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Database caching at proxy servers enables dynamic content to be generated at the edge of the network, thereby improving the scalability and response time of web applications. The scale of deployment of edge servers coupled with the rising costs of their administration demand that such caching middleware be adaptive and self-managing. To achieve this, a cache must be dynamically populated and pruned based on the application query stream and access pattern. In this paper, we describe such a cache which maintains a large number of materialized views of previous query results. Cached \"views\" share physical storage to avoid redundancy, and are usually added and evicted dynamically to adapt to the current workload and to available resources. These two properties of large scale (large number of cached views) and overlapping storage introduce several challenges to query matching and storage management which are not addressed by traditional approaches. In this paper, we describe an edge data cache architecture with a flexible query matching algorithm and a novel storage management policy which work well in such an environment. We perform an evaluation of a prototype of such an architecture using the TPC-W benchmark and find that it reduces query response times by up to 75%, while reducing network and server load.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "Electron localization function (ELF) theory is used to characterize changes that occur upon excitation from ground singlet to first excited triplet states in a series of isoelectronic 16-electron molecules including H2CCH2, HNCH2, H2CO, HNNH, HNO, and O-2 (,ground triplet to excited singlet). ELF allows one to visualize lone pair or nonbonding electrons, and in these cases the pi --> pi* or n --> pi excitation processes involved lead to an effective 90 degrees rotation of the electronic structure about one heavy atom center and consequent distortion towards pyramidal symmetry about both heavy atom centers. The heavy atom bond lengths change very little in those cases where effectively two-center three-electron bonds can be formed (HNNH, HNO, and O-2) while a significant lengthening occurs in those cases where hydrogen atoms prevent such interactions (H2CCH2, HNCH2, and H2CO). It is shown that both ELF basin populations and atoms-in-molecules (AIM) delocalization indices reflect expected bond orders for conventional single and double bonds provided one compares the ratio of the molecular quantities rather than their absolute magnitudes. (C) 2001 John Wiley & Sons, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "This report presents empirical results of fine-grain communication on the 80-processor EM-X distributed-memory multiprocessor. EM-X has hardware support for low latency, high throughput fine-grain communication -- this hardware support includes packet generation integrated into the instruction execution pipeline for single-cycle communication overhead, direct memory access for remote references, and rapid context switching for latency tolerance. We study the fine-grain communication performance of integer radix sort, a code with irregular communication, on EM-X, and compare it to the Fujitsu AP1000+ and the Cray Server CS6400. Our experimental results indicate that EM-X achieves high throughput and low overhead for fine-grain communication. Whereas EM-X's communication performance scales perfectly as we increase the number of processors, other coarse-grain message-passing machines exhibit fluctuation and performance degradation for larger configurations due to network contention.",
    "actual_venue": "Ieee Pact"
  },
  {
    "abstract": "We study the effect of uncertain feature measurements and show how classification and learning rules should be adjusted to compensate for it. Our approach is particularly fruitful in multimodal fusion scenarios, such as audio-visual speech recognition, where multiple streams of complementary features whose reliability is time-varying are integrated. For such applications, by taking the measurement noise uncertainty of each feature stream into account, the proposed framework leads to highly adaptive multimodal fusion rules for classification and learning which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme under certain assumptions; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. The potential of our approach is demonstrated in audio-visual speech recognition experiments.",
    "actual_venue": "Crete"
  },
  {
    "abstract": "We describe HARC, a system for speech understanding that integrates speech recognition techniques with natural language processing. The integrated system uses statistical pattern recognition to build a lattice of potential words in the input speech. This word lattice is passed to a unification parser to derive all possible associated syntactic structures for these words. The resulting parse structures are passed to a multi-level semantics component for interpretation.",
    "actual_venue": "HLT"
  },
  {
    "abstract": "The set of relevantly balanced formulas is introduced in implicational fragment of BCK-logic. It is shown that any relevantly balanced formula has unique normal form proof. Such formulas are defined by the relevance relation between type variables in a formula. The set of balanced formulas (or equivalently one-two-formulas) is included in the relevantly balanced formulas. The uniqueness of normal form proofs is known for balanced formulas as the coherence theorem. Thus the result extends the theorem with respect to implicational formulas. The set of relevantly balanced formulas is characterized as the set of irrelevant substitution instances of principal type-schemes of BCK--terms.",
    "actual_venue": "Category Theory And Computer Science"
  },
  {
    "abstract": "This paper investigates the velocity and temperature field in the thermal boundary layer over a semi-infinite flat plate. The differential transformation method is used to determine some solutions of these velocity and thermal boundary-layer problems. Numerical results for the dimensionless temperature profiles are presented graphically for different values of Prandtl number. It is found that the results obtained by the present method are in good agreement with those provided by other numerical methods.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "A major objective in this paper is the application of new connectionist structures for fast and robust online learning of internal robot dynamic relations used as part of impedance control strategies in the case of robot contact tasks. Using proposed connectionist structures, stabilization of robot motion and interaction force with environment is achieved. The proposed neural network models with their special topology are integrated in position-based impedance control, force-based impedance control and stabilizing impedance control. In this way, efficient dynamic compensation and fast learning properties of the control algorithm for contact tasks are enabled. The effectiveness of the learning method is shown by simulation experiments of robot deburring process",
    "actual_venue": "San Diego, Ca"
  },
  {
    "abstract": "Science Citation Index SCI, The Engineering Index EI, and Index to Scientific & Technical Proceeding ISTP are widely accepted and used to evaluate the scientific research level of higher learning institutions by many countries' science and technology field currently. After research, the authors point out the blemishes in this method and put forward the problems that need to be noticed, and then, under current conditions, bring forward brand-new standards and methods to estimate research level, efficiency, and fund exploitation. One shouldn't over-emphasize the total amount of papers collected in SCI, EI & ISTP when evaluating the scientific research level of higher learning institutions, whereas using 'comprehensive factor' analysis method can make it more scientific and efficient.",
    "actual_venue": "Nature Precedings"
  },
  {
    "abstract": "We define the degenerate weighted Stirling numbers of the first and second kinds, S 1 ( n , k , Î» â¥ Î¸ ) and S ( n , k , Î» â¥ Î¸ ). By specializing Î» and Î¸ we can obtain the Stirling numbers, the weighted Stirling numbers and the degenerate Stirling numbers. Basic properties of S 1 ( n , k , Î» â¥ Î¸ ) and S ( n , k , Î» â¥ Î¸ ), such as recurrence formulas and combinatorial interpretations, are presented, and a theorem which relates S 1 ( n , k , Î» â¥ Î¸ ) and S ( n , k , Î» â¥ Î¸ ) to each other, and to other special numbers, is proved. This theorem provides a unified approach to a number of special cases which have recently appeared in the literature.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "This article is concerned with the video signal obtained when a coherent pulsed radar scans through moving scintillating scatrefers (clutter). In particular, a formula is obtained for the autocorrelation function of the clutter, assuming Gaussian distributions of doppler and scintillation for the scatterers. Results are included on a computer model developed to simulate clutter with predefined characteristics.",
    "actual_venue": "Information Theory, IEEE Transactions Â "
  },
  {
    "abstract": "We provide the tool 'TICO' (Translation Initiation site COrrection) for improving the results of conventional gene finders for prokaryotic genomes with regard to exact localization of the translation initiation site (TIS). At the current state TICO provides an interface for direct post processing of the predictions obtained from the widely used program GLIMMER. Our program is based on a clustering algorithm for completely unsupervised scoring of potential TIS locations.Our tool can be freely accessed through a web interface at http://tico.gobics.de/maike@gobics.de",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "Model based frameworks like Simulink/Stateflow for developing software control algorithms advocate the analysis of an abstract discrete state model of the controller before developing the actual code for the implementation. Though some of the existing tools support automatic code generation from the model, in actual practice the code is developed manually with the model as a reference, and independently validated against the safety requirements. If the safety properties can be guaranteed by the code then we reach verification closure, but if this is not the case, then we must debug the actual source of error in the code. We propose an approach for using the abstract model as a reference in this debugging task.",
    "actual_venue": "Isec"
  },
  {
    "abstract": "Are managers discouraging IT professionals from exhibiting behaviors the organization desperately needs?",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "In order to obtain better learning results in supervised learning, it is important to choose model parameters appropriately. Model selection is usually carried out by preparing a finite set of model candidates, estimating a generalization error for each candidate, and choosing the best one from the candidates. If the number of candidates is increased in this procedure, the optimization quality may be improved. However, this in turn increases the computational cost. In this paper, we focus on a generalization error estimator called the regularized subspace information criterion and derive an analytic form of the optimal model parameter over a set of infinitely many model candidates. This allows us to maximize the optimization quality while the computational cost is kept moderate.",
    "actual_venue": "Iie Transactions"
  },
  {
    "abstract": "\"Summary data\" is a representation of \"groups of facts.\" Statistics are a typical example of summary data, which is often a major component of databases that deal with huge domains, such as objects in a whole country or events that occurred over a long time range. Although any summary can be reproduced from the corresponding originals, these are often unavailable and the required data may or may not be derivable from the given summary data. A schema of summary data is defined as a relationship between classifications of object types and domains for attributes. Reclassification rules are introduced as semantic relations among classifications. Set theoretical lemmata provide an inference mechanism that judges derivability of required summary data from collected summary data, and derives the former, if it is derivable, from the latter. It is also shown how this inference mechanism improves a summary database in usability and logical data independence. Discussions are made with examples in the statistical field.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "In this paper, synchronization of memristor-based neural networks (MNNs) with time-varying delays is investigated. By employing the NewtonâLeibniz formulation and inequality technique, the controller with state or output coupling is designed to obtain global exponential synchronization of MNNs. The obtained delay-dependent conditions can be checked easily and they also enrich and improve the results in earlier publications. Finally, one numerical example is given to demonstrate the effectiveness of the obtained results.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "In this article, we develop an integrated supplierâbuyer inventory model with the assumption that the market demand is sensitive to the retail price and the supplier adopts a trade credit policy. The trade credit policy discussed in this paper is a âtwo-partâ strategy: cash discount and delayed payment. That is, if the buyer pays within M1, the buyer receives a cash discount; otherwise, the full purchasing price must be paid before M2, where M2>M1â©¾0. The objective of this research is to determine the optimal pricing, ordering, shipping, and payment policy to maximize the joint expected total profit per unit time. An iterative algorithm is established to obtain the optimal strategy. Furthermore, numerical examples and sensitivity analysis are presented to illustrate the results of the proposed model and to draw managerial insights.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "The Dichotomy Conjecture for constraint satisfaction problems (CSPs) states that every CSP is in P or is NP-complete (Feder-Vardi, 1993). It has been verified for conservative problems (also known as list homomorphism problems) by A. Bulatov (2003). Egri et al. (SODA 2014) augmented this result by showing that for digraph templates H, every conservative CSP, denoted LHOM(H), is solvable in log space or is hard for NL. A conjecture of Larose and Tesson from 2007 forecasts that when LHOM(H) is in log space, then in fact, it falls in a small subclass of log space, the set of problems expressible in symmetric Data log. The present work verifies the conjecture for LHOM(H) (and, indeed, for the wider class of conservative CSPs with binary constraints), and by so doing sharpens the aforementioned dichotomy. A combinatorial characterization of symmetric Data log provides the language in which the algorithmic ideas of the paper, quite different from the ones in Egri et al., are formalized.",
    "actual_venue": "Annual Acm/Ieee Symposium On Logic In Computer Science"
  },
  {
    "abstract": "Several long-standing problems in software engineering are concerned with inadequate requirements elicitation, analysis, specification, validation, and management. This deficit is a major cause of project failure and as such several techniques and frameworks have been developed to assist developers in handling requirements. Methods for handling functional requirements have been in existence for many decades, however methods for handling non-functional requirements are a more recent development. The Non-Functional Requirements (NFR) Framework is one such method that models non-functional requirements and associated implementation methods. This paper extends the previous quantitative reasoning extension into a single objective optimisation model. The model aims to selectively choose operationalizations in order to increase the overall satisfaction of non-functional requirements. Additionally, the optimisation model will be able to handle larger and more complicated graphs than the original framework.",
    "actual_venue": "Apccm"
  },
  {
    "abstract": "In recent years, Robust Capon Beamformer with Spatial Smoothing (RCB-SS) has improved ultrasound imaging. The RCB-SS is implemented in the time domain and spatial smoothing has the disadvantage of reducing the effective aperture leading to lower lateral resolution. In this paper, we propose a new adaptive beamformer in the frequency domain using frequency smoothing. We investigate the performance of our method using experimental data and compare its performance with RCB-SS and Delay and Sum (DAS) beamformer. Our method has a narrower mainlobe width, lower sidelobes, stronger interference suppression, less speckle and better reconstruction at higher depths than RCB-SS and DAS beamforming. The proposed method also shows better performance when the aperture size is reduced, thus allowing higher frame rates and lower data storage.",
    "actual_venue": "SSP"
  },
  {
    "abstract": "Wi-Fi fingerprinting without site surveys is one interesting approach for indoor localization. Current approaches in this field either achieve high accuracy with a large fingerprint database, or yield lower accuracy when the database size is small. In this paper, we propose a novel RSS (Received Signal Strength)-range based approach for fingerprint building, which optimizes the size of the fingerprint database while maintaining the accuracy at the same level. In this approach, a fingerprint is a low-dimensional vector of RSS-ranges, which are extracted from a high-dimensional vector of Wi-Fi scans in the process of fingerprint building. The proposed approach is used and evaluated in the autonomous localization system, which we call WHERE. The evaluation results show the system can optimize the size of the fingerprint database while maintaining an accuracy of room-level.",
    "actual_venue": "Context"
  },
  {
    "abstract": "A fast calculation of the most-significant carry in an addition is required in several applications, such as comparisons of two operands by performing their difference, sign detection, and overflow detection. It has been proposed to calculate this carry by detecting the most-significant carry chain and collecting the carry after this chain. The detection can be implemented by a prefix tree of AND gates and the collecting by a multi-input OR or by a connection with tri-state buffers.We have performed an estimate of the delay of this implementation for a data-path width of 64 bits and conclude that it is not significantly faster than the traditional carry-look-ahead based method.We propose a multilevel implementation, which allows the overlap of successive levels thereby reducing the overall delay. For 64-bit operands we estimate a delay reduction of about 15% with respect to the traditional carry-look-ahead based method, with a similar number of gates and number and length of interconnections.",
    "actual_venue": "Iccd"
  },
  {
    "abstract": "Hyperspectral sensors collect hundreds of narrow and contiguously spaced spectral bands of data. Such sensors provide fully registered high resolution spatial and spectral images that are invaluable in discriminating between man-made objects and natural clutter backgrounds. The price paid for this high resolution data is extremely large data sets, several hundred of Mbytes for a single scene, that make storage and transmission difficult, thus requiring fast onboard processing techniques to reduce the data being transmitted. Attempts to apply traditional maximum likelihood detection techniques for in-flight processing of these massive amounts of hyperspectral data suffer from two limitations: first, they neglect the spatial correlation of the clutter by treating it as spatially white noise; second, their computational cost renders them prohibitive without significant data reduction like by grouping the spectral bands into clusters, with a consequent loss of spectral resolution. This paper presents a maximum likelihood detector that successfully confronts both problems: rather than ignoring the spatial and spectral correlations, our detector exploits them to its advantage; and it is computationally expedient, its complexity increasing only linearly with the number of spectral bands available. Our approach is based on a Gauss-Markov random field (GMRF) modeling of the clutter, which has the advantage of providing a direct parameterization of the inverse of the clutter covariance, the quantity of interest in the test statistic. We discuss in detail two alternative GMRF detectors: one based on a binary hypothesis approach, and the other on a âsingleâ hypothesis formulation. We analyze extensively with real hyperspectral imagery data (HYDICE and SEBASS) the performance of the detectors, comparing them to a benchmark detector, the RX-algorithm. Our results show that the GMRF âsingleâ hypothesis detector outperforms significantly in computational cost the RX-algorithm, while delivering noticeable detection performance improvement",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "In this paper, we derive a generic closed-form approximation (CFA) of the energy efficiency-spectral efficiency (EE-SE) trade-off for the uplink of coordinated multi-point (CoMP) system and demonstrate its accuracy for both idealistic and realistic power consumption models (PCMs). We utilize our CFA to compare CoMP against conventional non-cooperative system with orthogonal multiple access. In the idealistic PCM, CoMP is more energy efficient than non-cooperative system due to a reduction in power consumption; whereas in the realistic PCM, CoMP can also be more energy efficient but due to an improvement in SE and mainly for cell-edge communication and small cell deployment.",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "Recent approaches to crowdsourcing entity matching (EM) are limited in that they crowdsource only parts of the EM workflow, requiring a developer to execute the remaining parts. Consequently, these approaches do not scale to the growing EM need at enterprises and crowdsourcing startups, and cannot handle scenarios where ordinary users (i.e., the masses) want to leverage crowdsourcing to match entities. In response, we propose the notion of hands-off crowdsourcing (HOC)}, which crowdsources the entire workflow of a task, thus requiring no developers. We show how HOC can represent a next logical direction for crowdsourcing research, scale up EM at enterprises and crowdsourcing startups, and open up crowdsourcing for the masses. We describe Corleone, a HOC solution for EM, which uses the crowd in all major steps of the EM process. Finally, we discuss the implications of our work to executing crowdsourced RDBMS joins, cleaning learning models, and soliciting complex information types from crowd workers.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "Electronic control units (ECU) play a more and more important role in the development of road vehicles. Forecasts lead up to 25% of the total vehicle value in 2000. The increasing complexity and stringent quality and cost requirements mandate tremendous improvements in the specification and design process. This paper presents the cooperative activities at Daimler-Benz research and Mercedes-Benz development departments to install an optimized design process",
    "actual_venue": "Euro-Dac"
  },
  {
    "abstract": "This paper presents a comparative analysis of group interaction around two display types, shared and individual, using a 'new media' arts application as a way to explore the physical technology setup for an intensive care unit in a hospital. We propose this method for laboratory settings when the research questions derive from socially complex environments, but real-world interventions are not possible. While users solve an 'interaction problem' that is posed through the 'new media' arts application for their own expressive purposes, researchers can analyse and collate the results to understand the solution space. We present a study with the bodyPaint application to address a design issue that we discovered when assessing the merits of an electronic patient record system.",
    "actual_venue": "Bcs Hci"
  },
  {
    "abstract": "A leading edge 14 nm SoC platform technology based upon the 2nd generation Tri-Gate transistor technology [5] has been optimized for density, low power and wide dynamic range. 70 nm gate pitch, 52 nm metal pitch and 0.0499 um(2) HDC SRAM cells are the most aggressive design rules reported for 14/16 nm node SoC process to achieve Moore's Law 2x density scaling over 22 nm node. High performance NMOS/PMOS drive currents of 1.3/1.2 mA/um, respectively, have been achieved at 0.7 V and 100 nA/um off-state leakage, 37%/50% improvement over 22 nm node. Ultra-low power NMOS/PMOS drives are 0.50/0.32 mA/um at 0.7 V and 15pA/um Ioff. This technology also deploys high voltage I/O transistors to support up to 3.3 V I/O. A full suite of analog, mixed-signal and RF features are also supported.",
    "actual_venue": "Symposium On Vlsi Technology-Digest Of Technical Papers"
  },
  {
    "abstract": "We present previously unknown high severity vulnerabilities in Android. The first is in the Android Platform and Google Play Services. The Platform instance affects Android 4.3-5.1, M (Preview 1) or 55% of Android devices at the time of writing. This vulnerability allows for arbitrary code execution in the context of many apps and services and results in elevation of privileges. In this paper we also demonstrate a Proof-of-Concept exploit against the Google Nexus 5 device, that achieves code execution inside the highly privileged system_server process, and then either replaces an existing arbitrary application on the device with our own malware app or changes the device's SELinux policy. For some other devices, we are also able to gain kernel code execution by loading an arbitrary kernel module. We had responsibly disclosed the vulnerability to Android Security Team which tagged it as CVE-2015-3825 (internally as ANDROID-21437603/21583894) and patched Android 4.4/5.x/M and Google Play Services. For the sake of completeness we also made a large-scale experiment over 32,701 of Android applications, finding similar previously unknown deserialization vulnerabilities, identified by CVE-2015-2000/1/2/3/4/20, in 6 SDKs affecting multiple apps. We responsibly (privately) contacted the SDKs' vendors or code maintainers so they would provide patches. Further analysis showed that many of the SDKs were vulnerable due to weak code generated by SWIG, an interoperability tool that connects C/C++ with a variety of languages, when fed with some bad configuration given by the developer. We therefore worked closely with the SWIG team to make sure it would generate more robust code -- patches are available.",
    "actual_venue": "Woot"
  },
  {
    "abstract": "The World Summit on Sustainable Development (Johannesburg, 2002) encouraged the application of the ecosystem approach by 2010. However, at the same summit, the signatory States undertook to restore and exploit their stocks at maximum sustainable yield (MSY), a concept and practice without ecosystemic dimension, since MSY is computed species by species, on the basis of a monospecific model. Acknowledging this gap, we propose a definition of âecosystem viable yieldsâ (EVY) as yields compatible (a) with biological safety levels (over which biomasses can be maintained for all times) and (b) with an ecosystem dynamics. The difference from MSY is that this notion is not based on equilibrium but on viability theory, which offers advantages for robustness. For a generic class of multispecies models with harvesting, we provide explicit expressions for the EVY. We apply our approach to the anchovyâhake couple in the Peruvian upwelling ecosystem.",
    "actual_venue": "Environmental Modeling And Assessment"
  },
  {
    "abstract": "We live in a world permeated by signs that belong to multiple areas of knowledge. Signs that have been filtered and stored by our natural and artificial interfaces can determine our perceptions and conceptions about the complexities of the world. The proposal of this study is to present poetic experiments that sustain a consistency between what is real and what can be defined as semiotically real, and that sustain an efficient behavior but, in being poetic, exerts tension on this close relationship. The exercise of exploring the limits of conformed thought - understood here as a composition of codes, standards, patterns and cultural representations - expands and reveals a variety of settings that are not readily perceived in the world.",
    "actual_venue": "Universal Access In Human-Computer Interaction: Access To Interaction, Pt"
  },
  {
    "abstract": "Verified and validated security policies are essential components of high assurance computer systems. The design and implementation of security policies are fundamental processes in the development, deployment, and maintenance of such systems. In this paper, we introduce an expert system that helps with the design and implementation of security policies. We show how Prolog is used to verify system correctness with respect to policies using a theorem prover. Managing and visualizing information in high assurance computer systems are challenging tasks. To simplify these tasks, we show how a graph-based visualization tool is used to validate policies and provide system security managers with a process that enables policy reviews and visualizes interactions between the system's entities. The tool provides not only a representation of the formal model, but also its execution. The introduced executable model is a formal specification and knowledge representation method.",
    "actual_venue": "Journal Of Computers"
  },
  {
    "abstract": "The current agile software development methods do not seem to address usability and interaction design issues enough, i.e., the interaction design process may remain implicit. However, few studies with positive results have been conducted concerning integrating explicit interaction design process into agile software development. In this study, the interaction design process of Mobile-Dâ¢ is extended with the personas approach. Empirical evaluation of the resulting model is performed in a case project. The results provide view points for both industrial and scientific purposes on the applications of interaction design activities in different stages of agile development process.",
    "actual_venue": "XP"
  },
  {
    "abstract": "We demonstrate how a new data structure for sparse distributed polynomials in the Maple kernel significantly accelerates a large subset of Maple library routines. The POLY data structure and its associated kernel operations (degree, coeff, subs, has, diff, eval, ...) are programmed for high scalability, allowing polynomials to have hundreds of millions of terms, and very low overhead, increasing parallel speedup in existing routines and improving the performance of high level Maple library routines.",
    "actual_venue": "Acm Comm Computer Algebra"
  },
  {
    "abstract": "Clustering algorithm is an essential element to implement a hierarchical routing protocol, especially for a large-scale wireless sensor network. In this paper, we propose a new type of energy-efficient clustering algorithm, which maximizes the physical distance between cluster head and gateway by a neighbor node discovery mechanism. Furthermore, a slave/master patching scheme is introduced as a useful means of further improving the energy-efficiency. It has been shown that the number of cluster heads can be reduced by as many as 21% as compared with the existing clustering algorithms.",
    "actual_venue": "Euc Workshops"
  },
  {
    "abstract": "We introduce a new methodology for signal-to-noise ratio (SNR) video scalability based on the partitioning of the DCT coefficients. The DCT coefficients of the displaced frame difference (DFD) for inter-blocks or the intensity for intra-blocks are partitioned into a base layer and one or more enhancement layers, thus, producing an embedded bitstream. Subsets of this bitstream can be transmitted with increasing video quality as measured by the SNR. Given a bit budget for the base and enhancement layers the partitioning of the DCT coefficients is done in a way that is optimal in the operational rate-distortion sense. The optimization is performed using Lagrangian relaxation and dynamic programming (DP). Experimental results are presented and conclusions are drawn",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "The paper presents a Coarse-Grained Multicomputer algorithm that solves the Longest Common Subsequence Problem. This algorithm can be implemented in the CGM with processors in O( ) in time and O( ) communica- tion steps. It is the first CGM algorithm for this problem. We present also experimental results showing that the CGM algorithm is very efficient.",
    "actual_venue": "PDP"
  },
  {
    "abstract": "In this paper, a one-parameter eight-point orthogonal transform suitable for image compression is proposed. An algorithm for its fast computation is developed and an efficient structure for a simple implementation valid for all possible values of its independent parameter is proposed. It is shown that an appropriate selection of the values of the parameter results in a number of new multiplication-free transforms having a good compromise between the computational complexity and performance. Applying the proposed transform to image compression, we show that it outperforms the existing transforms having complexities similar to that of the proposed one.",
    "actual_venue": "Circuits And Systems"
  },
  {
    "abstract": "Remote sensing has become a powerful tool to derive biophysical properties of plants. One of the most popular methods for extracting vegetation information from remote sensing data is through vegetation indices. Models to predict soil erosion like the \"Revised Universal Soil Loss Equation\" (RUSLE) can use vegetation indices as input to measure the effects of soil cover. Several studies correlate vegetation indices with RUSLE's cover factor to get a linear mapping that describes a broad area. The results are considered as incomplete because most indices only detect healthy vegetation. The aim of this study is to devise a genetic programming approach to synthetically create vegetation indices that detect healthy, dry, and dead vegetation. In this work, the problem is posed as a search problem where the objective is to find the best indices that maximize the correlation of field data with Landsat5-TM imagery. Thus, the algorithm builds new indices by iteratively recombining primitive-operators until the best indices are found. This article outlines a GP methodology that was able to design new vegetation indices that are better correlated than traditional man-made indices. Experimental results demonstrate through a real world example using a survey at \"Todos Santos\" Watershed, that it is viable to design novel indices that achieve a much better performance than common indices such as NDVI, EVI, and SAVI.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "Multi-population competitive co-evolution is ex- plored as a way of developing controllers for a simple (but definitely not trivial) car racing game. The three main uses we see for this method are to evolve more complex general intelligence than would be possible with other methods, to compare different evolvable architectures for controllers, and to develop behaviourally diverse populations of agents for com- puter games. Nine-population co-evolution is compared with single-population co-evolution and standard evolution strate- gies, steady-state and generational versions of the algorithm are compared, and a number of different controller architectures are compared with each other. I. I NTRODUCTION",
    "actual_venue": "Ieee Congress On Evolutionary Computation"
  },
  {
    "abstract": "Use the \"Report an Issue\" link to request a name change.",
    "actual_venue": "Nips"
  },
  {
    "abstract": "This article proposes a framework of quality indicators for learning analytics that aims to standardise the evaluation of learning analytics tools and to provide a mean to capture evidence for the impact of learning analytics on educational practices in a standardised manner. The criteria of the framework and its quality indicators are based on the results of a Group Concept Mapping study conducted with experts from the field of learning analytics. The outcomes of this study are further extended with findings from a focused literature review.",
    "actual_venue": "Educational Technology And Society"
  },
  {
    "abstract": "A common approach to solve multi-label classification problems is the transformation method, in which a multi-label problem is converted into multiple single-label representations. With an efficient implementation of single-label algorithms, and considering dependency between labels and the fact that similar samples often share the same labels, we can expect a highly effective classification in multi-label datasets. In this paper, to tackle multi-label classification problem, first using an improved twin support vector machine classifier, the hyperplanes containing structural information of samples and local information of each class label, are found. Then the prior probability of each hyperplane and the sample points that are located in the margins of the hyperplanes are extracted.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Motivation: Next-generation DNA sequencing machines are generating an enormous amount of sequence data, placing unprecedented demands on traditional single-processor read-mapping algorithms. CloudBurst is a new parallel read-mapping algorithm optimized for mapping next-generation sequence data to the human genome and other reference genomes, for use in a variety of biological analyses including SNP discovery, genotyping and personal genomics. It is modeled after the short read-mapping program RMAP, and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences. This level of sensitivity could be prohibitively time consuming, but CloudBurst uses the open-source Hadoop implementation of MapReduce to parallelize execution using multiple compute nodes. Results: CloudBurst's running time scales linearly with the number of reads mapped, and with near linear speedup as the number of processors increases. In a 24-processor core configuration, CloudBurst is up to 30 times faster than RMAP executing on a single core, while computing an identical set of alignments. Using a larger remote compute cloud with 96 cores, CloudBurst improved performance by > 100-fold, reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "For electric car propulsion systems, the wheel motor is an application that requires electrical machine shape flexibility, compactness, robustness, high efficiency and high torque. The axial flux machine is an interesting solution, where the motor is directly coupled or inside the driven wheel. In this paper, axial flux induction and synchronous machines as wheel motor application are presented, and some considerations for each motor type are drawn by the authors. The structure with two rotors seems to be a very promising solution for both induction and synchronous machines. The twin rotor structures allow saving of the motor active material: stator winding copper or stator core iron. Thus the machines can have higher power density and efficiency in comparison with two individual motors. In addition, for the induction case, the two rotors can rotate at different speeds, and thus the motor can act as a mechanical differential, even if supplied by a single invertor. The axial flux PM motor with two rotors is very compact and can be integrated inside the wheel. Since the identification of a unique drive solution for any type of electric vehicle, the authors focus the attention on the small electric city cars to be used in European towns where the environment pollution problems are quite heavy",
    "actual_venue": "Industrial Electronics, Control, And Instrumentation, , Proceedings Of The Ieee Iecon International Conference"
  },
  {
    "abstract": "A new method of motion deadlock resolving by using fuzzy decision in robot soccer games is proposed in this paper. For the reasons of complex competition tasks and limited intelligence, soccer robots fall into motion deadlocks in many conditions, which is very difficult for robots to decide whether it is needed to retreat for finding new opportunities. Based on the analysis of human decision for dealing with these kinds of motion deadlocks, the fuzzy decision method is introduced in this paper. Then, fuzzy rules based deadlock resolving system is designed according to relative positions and orientations among robots and the ball in local regions. Lots of experiments by human experts and the fuzzy controller are implemented for comparison. Experimental results show that the method proposed is reasonable and efficient for motion deadlock resolving in most conditions for real soccer robot games. Â© Springer-Verlag Berlin Heidelberg 2007.",
    "actual_venue": "Lecture Notes In Computer Science (Including Subseries Lecture Notes In Artificial Intelligence And Lecture Notes In Bioinformatics"
  },
  {
    "abstract": "Despite the widespread use of high throughput expression platforms and the availability of a desktop implementation of Gene Set Enrichment Analysis (GSEA) that enables non-experts to perform gene set based analyses, the availability of the necessary precompiled gene sets is rare for species other than human.A software tool (GO2MSIG) was implemented that combines data from various publicly available sources and uses the Gene Ontology (GO) project term relationships to produce GSEA compatible hierarchical GO based gene sets for all species for which association data is available. Annotation sources include the GO association database (which contains data for over 200000 species), the Entrez gene2go table, and various manufacturers' array annotation files. This enables the creation of gene sets from the most up-to-date annotation data available. Additional features include the ability to restrict by evidence code, to remap gene descriptors, to filter by set size and to speed up repeat queries by caching the GO term hierarchy. Synonymous GO terms are remapped to the version preferred by the GO ontology supplied. The tool can be used in standalone form, or via a web interface. Prebuilt gene set collections constructed from the September 2013 GO release are also available for common species including human. In contrast human GO based sets available from the Broad Institute itself date from 2008.GO2MSIG enables the bioinformatician and non-bioinformatician alike to generate gene sets required for GSEA analysis for almost any organism for which GO term association data exists. The output gene sets may be used directly within GSEA and do not require knowledge of programming languages such as Perl, R or Python. The output sets can also be used with other analysis software such as ErmineJ that accept gene sets in the same format. Source code can be downloaded and installed locally from http://www.bioinformatics.org/go2msig/releases/ or used via the web interface at http://www.go2msig.org/cgi-bin/go2msig.cgi.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "In this paper, an extension of the DDD-FFT approach presented in [1] is developed for heterogeneous elasticity. For such a purpose, an iterative spectral formulation in which convolutions are calculated in the Fourier space is developed to solve for the mechanical state associated with the discrete eigenstrain-based microstructural representation. With this, the heterogeneous DDD-FFT approach is capable of treating anisotropic and heterogeneous elasticity in a computationally efficient manner. In addition, a GPU implementation is presented to allow for further acceleration. As a first example, the approach is used to investigate the interaction between dislocations and second-phase particles, thereby demonstrating its ability to inherently incorporate image forces arising from elastic inhomogeneities.",
    "actual_venue": "Journal Of Computational Physics"
  },
  {
    "abstract": "RÃSUMÃ. Il Ã©merge des entreprises une forte demande d'Ã©volution de leurs applications informatiques vers plus de coopÃ©ration. Le problÃ¨me est de rÃ©aliser cette migration tout en gardant la qualitÃ© de l'existant rompu aux difficultÃ©s rencontrÃ©es. Ceci suppose une rÃ©-ingÃ©nierie de ces applications. Dans certains cas, des solutions provisoires et non satisfaisantes ont Ã©tÃ© apportÃ©es sous forme de coopÃ©ration externe. Celle-ci consiste par des moyens humains, logiciels et matÃ©riels, Ã  mettre en relation certaines entitÃ©s produisant de l'information Ã  celles qui l'utilisent. Cette coopÃ©ration externe prÃ©sente divers inconvÃ©nients : souvent lente, peu efficace, peu souple, elle demande des moyens parfois disproportionnÃ©s en rapport aux gains obtenus. En automatisant la coopÃ©ration nous pensons pouvoir Ã©liminer l'ensemble des problÃ¨mes Ã©numÃ©rÃ©s ci-dessus tout en amÃ©liorant tant du point de vue qualitatif que quantitatif la part prise par la coopÃ©ration. Nous allons pour cela proposer une mÃ©thode basÃ©e sur une organisation des modules opÃ©ratoires constituant l'application en groupes de travail dynamiques. Cette mÃ©thode permettra Ã©galement de mettre en Ã©vidence les informations, de les mettre en relation et de les composer Ã©ventuellement afin d'en crÃ©er de nouvelles plus riches sÃ©mantiquement et donc plus Ã  mÃªme d'Ãªtre utilisÃ©es dans un environnement coopÃ©ratif. Notre approche globale permet la mise en Åuvre de la coopÃ©ration par rÃ©-ingÃ©nierie de l'application sans modifier les modules opÃ©ratoires existants. Nous proposons une mÃ©thode intÃ©gralement dÃ©rivable (aprÃ¨s vÃ©rification formelle) en rÃ¨gles de type ÃCA, elles mÃªme intÃ©grÃ©es Ã  une plate-forme qui permet la mise en coopÃ©ration. MOTS CLES. Architecture CoopÃ©rative, CoopÃ©ration, ÃvÃ©nements, Groupes de Travail, Messages, MÃ©thode, RÃ©- ingÃ©nierie, RÃ¨gles. ABSTRACT. There an increasing demand from firms to evolve their applications to be more co-operative. The problem is to realise this migration and to maintain the quality of existing applications which are reliable due to previously encountered difficulties. This assumes the re-engineering of these applications. In some cases, temporary and non-satisfying applications have been added as external cooperation. It consists of human, software, hardware to link with some entities producing information for those who need it. This external cooperation presents several inconveniences : often slow, not very efficient and flexible. It also needs a lot of work, often disproportionate regarding to the gain. Automating cooperation, we hope to eliminate problems enumerated above, and we think that we can improve qualitatively and quantitatively the amount of co-operation. We will propose a method based on an organisation of dynamic workgroups composed of operative modules. This method will also allow to bring to the fore co-operative elements, and to link them with an eventual composition to create new ones semantically richer and more suitable to a co-operative environment/context. In order to propose an operational method, the last stage will propose to obtain rules derived from a specification language associated to the method. These rules allow the realisation of the implementation of the concrete co-operation. Next, they will be executed into the distributed run- time we developed. They will allow the dynamic workgroup management and the circulating of co-operative elements. Our approach allows the provision of co-operation thanks to the application re-engineering without modifying any existing operative module. We propose a method entirely derivable (after a formal verification) into rules integrated into a co-operative architecture allowing the co-operation.",
    "actual_venue": "Inforsid"
  },
  {
    "abstract": "This paper investigates a claw-pole double-rotor machine (DRM) for power-split hybrid electric vehicles (HEVs). Based on the mathematical analysis of the machine, the boundary speed-torque characteristic required by the hybrid electric system is studied. To achieve high power density with acceptable torque ripple for automotive applications, the back electromotive force (EMF) and torque performance of the DRM are investigated with respect to the configurations of permanent-magnet rotor, claw-pole dimensions, and air-gap length. Based on the optimized model, characteristics of the claw-pole DRM, such as flux density, inductance, torque, core losses, and efficiency, are investigated by finite-element method. A downsized prototype machine is manufactured and tested. The experimental EMF, inductance, and torque performance agree well with simulation data. A drive cycle containing various working modes of the DRM is carried out, and the feasibility of using the machine as a power-split device for HEVs is validated.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Knowledge of the Global Positioning System (GPS) signal-in-space (SIS) anomalies in history has a great importance for not only assessing the general performance of GPS SIS integrity but also validating the fundamental assumption of receiver autonomous integrity monitoring (RAIM): at most one satellite fault at a time. The main purpose of this paper is to screen out all potential SIS anomalies in the last decade by comparing broadcast ephemerides and clocks with precise ones. Validated broadcast navigation messages are generated from 397,044,414 navigation messages logged by on average 410 International GNSS Service (IGS) stations during the period 6/1/2000-8/31/2010. Both IGS and National Geospatial-Intelligence Agency (NGA) precise ephemerides/clocks are used as truth references. Finally, 1256 potential SIS anomalies are screened out. These anomalies show an improving SIS integrity performance in the last decade, from tens or hundreds of anomalies per year before 2003 to on average two anomalies per year after 2008. Moreover, the fundamental assumption of RAIM is valid because never have two SIS anomalies or more occurred simultaneously since 2004.",
    "actual_venue": "Ieee Trans Aerospace And Electronic Systems"
  },
  {
    "abstract": "The amount of data moved over dedicated and non-dedicated network links increases much faster than the increase in the network capacity, but the current solutions fail to guarantee even the promised achievable transfer throughputs. In this paper, we propose a novel dynamic throughput optimization model based on mathematical modeling with offline knowledge discovery/analysis and adaptive online decision making. In offline analysis, we mine historical transfer logs to perform knowledge discovery about the transfer characteristics. Online phase uses the discovered knowledge from the offline analysis along with real-time investigation of the network condition to optimize the protocol parameters. As real-time investigation is expensive and provides partial knowledge about the current network status, our model uses historical knowledge about the network and data to reduce the real-time investigation overhead while ensuring near optimal throughput for each transfer. Our network and data agnostic solution is tested over different networks and achieved up to 93% accuracy compared with the optimal achievable throughput possible on those networks.",
    "actual_venue": "Arxiv: Distributed, Parallel, And Cluster Computing"
  },
  {
    "abstract": "We present a new probability density function (PDF) based on the generalized Laguerre expansion that, for the first time, completely characterize the widest range of small scale fading distributions in wireless communications. This unified PDF formula characterizes the most general case of small scale fading consisting of an arbitrary number of specular waves plus diffused power. The same formula covers all existing distributions for small scale fading, which include the Nakagami, Rician, and Rayleigh fadings, as special cases.",
    "actual_venue": "Pimrc"
  },
  {
    "abstract": "In this paper, we examine a status updating system where updates generated by the source are sent to the monitor through an erasure channel. We assume each update consists of k symbols and the symbol erasure in each time slot follows an independent and identically distributed (i.i.d.) Bernoulli process. We assume rateless coding scheme is adopted at the transmitter and an update can be successfully decoded if k coded symbols are received successfully. We assume perfect feedback available at the source, so that it knows whether a transmitted symbol has been erased instantly. Then, at the beginning of each time slot, the source has the choice to start transmitting a new update, or continue with the transmission of the previous update if it is not delivered yet. We adopt the metric \"Age of Information\" (AoI) to measure the freshness of information at the destination, where the AoI is defined as the age of the latest decoded update at the destination. Our objective is to design an optimal online transmission scheme to minimize the time-average AoI. The transmission decision is based on the instantaneous AoI, the age of the update being transmitted, as well as the number of successfully delivered symbols of the update. We formulate the problem as a Markov Decision Process (MDP) and identify the monotonic threshold structure of the optimal policy. Numerical results corroborate the structural properties of the optimal solution.",
    "actual_venue": "Ieee International Conference On Communications"
  },
  {
    "abstract": "As robots are increasingly developed to assist humans socially with everyday tasks in home and healthcare settings, questions regarding the robot's safety and trustworthiness need to be addressed. The present work investigates the practical and ethical challenges in designing and evaluating social robots that aim to be perceived as safe and can win their human users' trust. With particular focus on collaborative scenarios in which humans are required to accept information provided by the robot and follow its suggestions, trust plays a crucial role and is strongly linked to persuasiveness. Accordingly, human-robot trust can directly affect people's willingness to cooperate with the robot, while under-or overreliance may have severe or even dangerous consequences. Problematically, investigating trust and human perceptions of safety in HRI experiments proves challenging in light of numerous ethical concerns and risks, which this paper aims to highlight and discuss based on experiences from HRI practice.",
    "actual_venue": "Lecture Notes In Artificial Intelligence"
  },
  {
    "abstract": "This paper presents a speaker identification system using empirical mode decomposition (EMD) feature extraction method and artificial neural network in speaker identification. The EMD is an adaptive multi-resolution decomposition technique that appears to be suitable for non-linear, non-stationary data analysis. The EMD sifts the complex signal of time series without losing its original properties and then obtains some useful intrinsic mode function (IMF) components. Calculating the energy of each component can reduce the computation dimensions and enhance the performance of classification. The features were used as inputs to neural network classifiers for speaker identification. In the speaker identification, the back-propagation neural network (BPNN) and generalized regression neural network (GRNN) were applied to verify the performances and the training time in the proposed system. The experimental results indicated the GRNN can achieve better recognition rate performance with feature extraction using the EMD method than BPNN.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "â¢MAGARFFCM-GRN is proposed to infer large-scale GRNs with high accuracy and efficiency.â¢RF is used to reduce the search space of MAGA and three genetic operators of MAGA are improved.â¢The comparison with six state-of-the-art algorithms demonstrates the effectiveness of our proposal.",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "A new technical framework for remote sensing image matching by integrating affine invariant feature extraction and RANSAC is presented. The novelty of this framework is an automatic optimization strategy for affine invariant feature matching based on RANSAC. An automatic way to determine the distance threshold of RANSAC is proposed, which is a key problem to implement this RANSAC-based automatic optimization. Since affine invariant feature matching technology has been successfully applied to remote sensing image matching, we design an experiment to compare the proposed framework (with optimization) with the standard affine invariant feature matching (without optimization). By using three pairs with different types of imagery, the experimental results indicate that the proposed framework can always get higher correctness of image matching in automatic way, compared to the standard affine invariant feature matching technology.",
    "actual_venue": "Computers And Electrical Engineering"
  },
  {
    "abstract": "Speaker recognition remains a challenging task under noisy conditions. Inspired by auditory perception, computational auditory scene analysis (CASA) typically segregates speech by producing a binary time-frequency mask. We first show that a recently introduced speaker feature, Gammatone Frequency Cepstral Coefficient, performs substantially better than conventional speaker features under noisy conditions. To deal with noisy speech, we apply CASA separation and then either reconstruct or marginalize corrupted components indicated by the CASA mask. Both methods are effective. We further combine them into a single system depending on the detected signal to noise ratio (SNR). This system achieves significant performance improvements over related systems under a wide range of SNR conditions.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "The business challenges that some companies face require enterprise-wide solutions that call for an integrated approach and an effective management of organisational resources in order to achieve business objectives with an acceptable level of risk. A maturity model is an improvement approach which provides organisations with the essential elements for effective change. The maturity models process helps to integrate traditionally separate organisational functions, enhances goals and priorities, supplies guidance for quality processes and shares benchmarks for appraising current outcomes. The benefits management approach emerges as a complement to traditional management practices and proposes a continuous mapping of business benefits and the implementation and monitoring of intermediate results. Benefits management reinforces the distinction between project results and business benefits. Based on a case study, the authors show how a set of business objectives derived from a maturity level upgrade can be obtained from identifying, structuring and monitoring objectives and benefits. This was supported by information technology enablers and organisational changes which were all framed in an organisational maturity level that had been previously measured.",
    "actual_venue": "Ijitpm"
  },
  {
    "abstract": "Deals with a tactile sensor system for revolute joints of manipulators in order to detect contacts with obstacles. It has the ability to detect, in a sampling period, more than one location of contact switches which are turned ON even on a signal line. In the paper, the structure and the principle of the tactile sensor are described, followed by the analysis of the circuit and experimental results of the sensor system",
    "actual_venue": "Iros"
  },
  {
    "abstract": "This paper addresses the issue of parallelizing imperfectly nested loops. Current parallelizing compilers or transformations\n would either only parallelize the inner-most loop (which is more like vectorization than parallelization), or not parallelize\n the loops at all. We present an approach that transforms an imperfectly nested loop into at most three fully parallel perfectly\n nested loops. The transformed loops can be parallelized by any parallelizing compiler. The advantage of our technique is the\n simplicity of the transformed loops and low synchronization overhead. The feasibility of this approach was tested using several\n types of loops including those from the Eispack math library and from Linpack benchmark on different multi-processor platforms\n and performance was compared with Sunâs MP C and Crayâs autotasking. The results show that our method is very effective.",
    "actual_venue": "Hipc"
  },
  {
    "abstract": "Artificial Bee Colony (ABC) is a recent swarm intelligence based approach to solve nonlinear and complex optimization problems. Exploration and exploitation are the two important characteristics of the swarm based optimization algorithms. Exploration capability of an algorithm is the capability of exploring the solution space to find the possible solution while exploitation capability of an algorithm is the capability of exploiting a particular region of the search space for a better solution. Usually, exploration and exploitation capabilities are contradictory in nature, i.e., a better exploration capability results a worse exploitation capability and vice versa. An economic and efficient algorithm can explore the complete solution space and shows a convergent behavior after a finite number of trials. Exploration and exploitation capabilities, are quantified using various diversity measures. In this paper, an analytical study has been carried out for various diversity measures for ABC process.",
    "actual_venue": "Proceedings Of Seventh International Conference On Bio-Inspired Computing: Theories And Applications"
  },
  {
    "abstract": "Organizational forms involving more detailed contracts than are found in traditional spot market exchanges appear to be increasingly prevalent. There has been relatively little analysis, however, of the extent to which firms learn how to use contracts to manage their interfirm relationships over time. In this paper, we conduct a detailed case study of a time series of 11 contracts concluded during 1989-1997 between the same two partners, both of whom participate in the personal computer industry, to explore whether and how firms learn to contract. We find many changes to the structure of the contracts that cannot be fully explained by changes in the assets at risk in the relationship, and evidence that these changes are largely the result of processes in which the firms were learning how to work together, including learning how to contract with each other. The nature of this learning appears to have been quite incremental and local, that is, not very far sighted. We suggest how and when contracts might serve as repositories for knowledge about how to govern collaborations, and suggest some boundary conditions for this phenomenon. Our findings also provide implications for the debate about whether contracts have a positive or negative effect on interorganizational trust. We conclude with suggestions for future research.",
    "actual_venue": "Organization Science"
  },
  {
    "abstract": "Compared to collocated interaction, videoconferencing disrupts the ability to use gaze and gestures to mediate interaction, direct reactions to specific people, and provide a sense of presence for the satellite (i.e., remote) participant. We developed a kinetic videoconferencing proxy with a swiveling display screen to indicate which direction that the satellite participant was looking. Our goal was to compare two alternative motion control conditions, in which the satellite participant directed the display screen's motion either explicitly (aiming the direction of the display with a mouse) or implicitly (with the screen following the satellite participant's head turns). We then explored the effectiveness of this prototype compared to a typical stationary video display in a lab study. We found that both motion conditions resulted in communication patterns that indicate higher engagement in conversation, more accurate responses to the satellite participant's deictic questions (i.e., \"What do you think?\"), and higher user rankings. We also discovered tradeoffs in attention and clarity between explicit versus implicit control, a tension in how motion toward one person can exclude other people, and ways that swiveling motion provides attention awareness, even without direct eye contact.",
    "actual_venue": "Interact"
  },
  {
    "abstract": "Despite the great demand on and attempts at package express shipping services such as the same-day delivery feasible for online firms, turning a profit is still difficult. To develop more economical or even cost-free transportation of packages, in this paper, we propose to make use of the existing taxis on the street that are delivering passengers, in a crowd-sourced manner. To the best of our knowledge, this is the first work that exploits taxis occupied by passengers to help deliver package collectively, without hurting the quality of taxi services. Specifically, we propose a two-phase framework for the package express shipping. In the first phase, we rank the road segments according to their influential factor values, which is similar to the idea of identifying key people in social networks. Hubs are then identified based on the ranking and the geographical locations of the road segments. In the second phase, we develop two inter-hub routing algorithms, namely, First-Come-First-Service (FCFS) and Destination-Closer (Des Closer), to ship a package to its destination. We evaluate the two-phase framework on a large-scale real-world taxi data set, generated by 7,600 taxis in a month. Results show that, on average, the package delivery time based on Des Closer is 5.3 hours, which is 2.6x shorter than that of FCFS, the number of participating taxis per package based on Des Closer is 3.10, which is 10.6x fewer than that of FCFS.",
    "actual_venue": "Uic/Atc/Scalcom"
  },
  {
    "abstract": "Grid database is a new research area combined with the database techniques and Grid techniques, while traditional database query and optimization techniques couldnât fully satisfy the need of Grid database because of the dynamic properties of Grid Nodes. The paper brings forward cost model in dynamic grid database environment , and also gives the dynamic query optimization algorithm used for the query plan to make adaptive evolvement along with the fluctuation of gird enviroment. Finally the paper tests the models and algorithms through experiment.",
    "actual_venue": "Snpd"
  },
  {
    "abstract": "Software Product Lines (SPLs) are families of products whose commonalities and variability can be captured by Feature Models (FMs). T-wise testing aims at finding errors triggered by all interactions amongst t features, thus reducing drastically the number of products to test. T-wise testing approaches for SPLs are limited to small values of t -- which miss faulty interactions -- or limited by the size of the FM. Furthermore, they neither prioritize the products to test nor provide means to finely control the generation process. This paper offers (a) a search-based approach capable of generating products for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of products. Experiments conducted on 124 FMs (including large FMs such as the Linux kernel) demonstrate the feasibility and the practicality of our approach.",
    "actual_venue": "Arxiv: Software Engineering"
  },
  {
    "abstract": "Prioritization or eliciting final weights from the pairwise comparison matrices might be a major part of some Multiple Criteria Decision Analysis (MCDA) methods which are based upon these matrices. However, in a fuzzy environment, two other issues, i.e., consistency and reciprocity, are also arisen in such matrices with at least one fuzzy component. In this paper, a single-decision-making optimization model along with two group-decision-making optimization models are developed towards prioritization in fuzzy pairwise comparison matrices. A comprehensive numerical analysis is performed to validate the proposed models. Finally, some conclusions are reported.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "A study of a general central pattern generator (CPG) is carried out by means of a measure of the gain of information between the number of available topology configurations and the output rhythmic activity. The neurons of the CPG are chaotic Hindmarsh-Rose models that cooperate dynamically to generate either chaotic or regular spatiotemporal patterns. These model neurons are implemented by computer simulations and electronic circuits. Out of a random pool of input configurations, a small subset of them maximizes the gain of information. Two important characteristics of this subset are emphasized: (1) the most regular output activities are chosen, and (2) none of the selected input configurations are networks with open topology. These two principles are observed in living CPGs as well as in model CPGs that are the most efficient in controlling mechanical tasks, and they are evidence that the information-theoretical analysis can be an invaluable tool in searching for general properties of CPGs.",
    "actual_venue": "Neural Computation"
  },
  {
    "abstract": "This paper proposes an extension of the functionality of a SIP proxy server for providing QoS to SIP-signalled voice over IP calls in a scalable and efficient way. The basic concept lies in aggregating SIP calls into bandwidth-provisioned trunks and on admitting calls on trunks only within the limit of their bandwidth. The SIP proxy server derives bandwidth requirements of SIP calls directly from parsed SIP messages. The bandwidth of the logical trunks can be dynamically changed. With a moderate trunk over-sizing, the trunk resizing rate can be kept much lower than the call arrival process. The approach scales well in a scenario where a centralized entity (bandwidth broker) is in charge of managing all the trunks of a given domain. We present simulation results that confirm this good scalability. The functionality and scalability of the proposed approach is compared with other ones currently discussed in the IETF or in the literature. Copyright (C) 2006 John Wiley & Sons, Ltd.",
    "actual_venue": "International Journal Of Communication Systems"
  },
  {
    "abstract": "Background frame differencing is one of the most efficient, image segmentation techniques in case that a static background frame is available. The technique, however, involves two major problems: one is that decision with a single global threshold often produces poor segmentation results and the other is that an estimation for the optimal threshold is actually difficult. To overcome these problems, this paper proposes a very simple yet effective algorithm that refers to the status of previously judged neighboring pixels: more specifically, the proposed algorithm varies the threshold value in accordance with the status of the reference pixels. The experimental results illustrate that the proposed technique gives better segmentation results than the simple background differencing one with the optimal global threshold.",
    "actual_venue": "Image Processing, Icip Ieee International Conference"
  },
  {
    "abstract": "It is known from biological data that the response patterns of interneurons in the olfactory macroglomerulus (MGC) of insects are of central importance for the coding of the olfactory signal. We propose an analytically tractable model of the MGC which allows us to relate the distribution of response patterns to the architecture of the network.",
    "actual_venue": "Nips"
  },
  {
    "abstract": "Psi-calculi is a parametric framework for extensions of the pi-calculus with data terms and arbitrary logics. In this framework there is no direct way to represent action priorities, where an action can execute only if all other enabled actions have lower priority. We here demonstrate that the psi-calculi parameters can be chosen such that the effect of action priorities can be encoded.To accomplish this we define an extension of psi-calculi with action priorities, and show that for every calculus in the extended framework there is a corresponding ordinary psi-calculus, without priorities, and a translation between them that satisfies strong operational correspondence. This is a significantly stronger result than for most encodings between process calculi in the literature.We also formally prove in Nominal Isabelle that the standard congruence and structural laws about strong bisimulation hold in psi-calculi extended with priorities.",
    "actual_venue": "Electronic Proceedings In Theoretical Computer Science"
  },
  {
    "abstract": "A main open question related to character-based tree reconstruction is designing generalizations of the Perfect Phylogeny approach that couple efficient algorithmic solutions to the capability of explaining the input binary data, by allowing back mutations of some characters. Following this goal, the Persistent Phylogeny model and the related tree reconstruction problem (the PPP problem) have been recently introduced: this model allows only one back mutation for each character. The investigation of the combinatorial properties and the complexity of the model is still open: the most important such question is whether the PPP problem is NP-hard. Here we propose a graph-based approach to the PPP problem by showing that instances can be represented by colored graphs, while the solutions are obtained by operations on such graphs. Indeed, we give a graph-based characterization of the solutions to the PPP problem by showing the relationship between certain sequences of graph operations on the instance graphs and traversals of a persistent phylogeny solving these instances. Based on this result and on some combinatorial properties of the instance graphs we are able to give a polynomial time algorithm for a restricted version of the PPP problem.",
    "actual_venue": "Theor Comput Sci"
  },
  {
    "abstract": "Framing or priming a situation can subtly influence how a person reacts to or thinks about the situation. In this paper, we describe a recent study and some preliminary results in which the framing of a robot is manipulated such that it is presented as a social agent or as a machine-like entity. We ask whether framing the robot in these ways influences young children's social behavior during an interaction with the robot, independent of any changes in the robot itself. Following the framing manipulation, children play a fifteen-minute game with the robot. Their behavior, such as the amount of conversation, mimicry of the robot, and various courteous, prosocial actions will be coded and compared across conditions.",
    "actual_venue": "HRI"
  },
  {
    "abstract": "This memo specifies a new Type (the Key ID Information Type) for the General Extension Payload in the Multimedia Internet KEYing (MIKEY) Protocol. This is used in, for example, the Multimedia Broadcast/Multicast Service specified in the Third Generation Partnership Project.",
    "actual_venue": "RFC"
  },
  {
    "abstract": "A wideband radio-frequency (RF) power detection system is presented. The detection technique uses NMOS devices operating in the triode regime to generate an average current proportional to RF input power; this current is converted to voltage and amplified using a piecewise linear logarithmic approximation. Optimization of the NMOS devices is discussed, and a method of gain control is proposed for compensation of temperature and process variation. The power detector occupies an active area of 0.36 mm(2) in a 0.18 mu m CMOS process and consumes 10.8 mW from the power supply. Error between the output and a linear-in-dB best-fit curve is +/- 2.4 dB for a 20 dB input range, when measured at discrete frequencies. The output response is frequency independent, varying by less than 1.8 dB for a fixed input power as frequency is swept across the UWB spectrum.",
    "actual_venue": "Ieee Journal Of Solid-State Circuits"
  },
  {
    "abstract": "Queuing analysis is important in providing guid- ing principles for packet network analysis. Stochastic fluid queueing models have been widely used as burst scale models for high speed communication networks. In this paper, we propose a novel two-level Markov On-Off source model to model the burstiness of a packet stream at different time scales. Analytical results are obtained to reveal the impact of traffic burstiness at two levels on the queue lengths in a tandem queue system. Our method combines the modeling power of the Poisson processes with that of stochastic differential equations to handle the complex interactions between the packet arrivals and the queue content. Our results for the tandem queuing network could be used to further justify the packet spacing scheme in helping deploying small buffer routers.",
    "actual_venue": "CDC"
  },
  {
    "abstract": "Fan (2014) presented an accelerated modified Levenberg-Marquardt method for nonlinear equations. At every iteration, the accelerated modified LM method computed not only a LM trial step, but also an additional approximate LM step which employed a line search. In this paper, based on the accelerated modified LM method, we compute the approximate LM step one more time at every iteration, and obtain a high-order accelerating modified Levenberg-Marquardt method. Under the local error bound condition which is weaker than nonsingularity, the convergence order of this new method is shown to be fourth. A globally convergence is also given by the trust region technique. Numerical results show that the new method is efficient and could save many calculations of the Jacobian.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "In distributional semantics words are represented by aggregated context features. The similarity of words can be computed by comparing their feature vectors. Thus, we can predict whether two words are synonymous or similar with respect to some other semantic relation. We will show on six different datasets of pairs of similar and non-similar words that a supervised learning algorithm on feature vectors representing pairs of words outperforms cosine similarity between vectors representing single words. We compared different methods to construct a feature vector representing a pair of words. We show that simple methods like pairwise addition or multiplication give better results than a recently proposed method that combines different types of features. The semantic relation we consider is relatedness of terms in thesauri for intellectual document classification. Thus our findings can directly be applied for the maintenance and extension of such thesauri. To the best of our knowledge this relation was not considered before in the field of distributional semantics.",
    "actual_venue": "Lrec - Seventh International Conference On Language Resources And Evaluation"
  },
  {
    "abstract": "A shuffle ideal is a language which is a finite union of languages of the form A*a(1)A* ... A*a(k)A* where A is a finite alphabet and the a(i)'s are letters. We show how to represent shuffle ideals by special automata and how to compute these representations. We also give a temporal logic characterization of shuffle ideals and we study its expressive power over infinite words. We characterize the complexity of deciding whether a language is a shuffle ideal and we give a new quadratic algorithm for this problem. Finally we also present a characterization by subwords of the minimal automaton of a shuffle ideal and study the complexity of basic operations on shuffle ideals.",
    "actual_venue": "Theoretical Informatics And Applications"
  },
  {
    "abstract": "This paper presents a coded modulation scheme based on M-ary orthogonal modulation by means of Walsh-Hadamard (WH) sequences, suitable for low-earth-orbit (LEO) direct sequence/code division multiple access (DS/CDMA) satellite communication systems. Based on the IS-95 scheme, we consider Reed-Solomon (RS)-coded M-ary orthogonal modulation with error or erasures decoding, which presents good performance enhancement with low complexity. LEO satellite links are characterized by large Doppler frequency shifts caused by the difference in velocity between the satellite and the earth mobile terminal, which make conventional non-coherent detection ineffective. In order to overcome the phase shift variations during the symbol period, which result in orthogonality loss of the WH sequences, we applied a differential encoding process to the spreading sequences or the WH chips prior to transmission. A special diversity process suitable for the environment under consideration is also applied. Simulation results show that the proposed diversity/coding/modulation scheme attains very good performance at low transmitter/receiver complexity. (C) 1998 John Wiley & Sons, Ltd.",
    "actual_venue": "International Journal Of Satellite Communications"
  },
  {
    "abstract": "Noniterative data-driven techniques are design methods that allow optimal feedback control laws to be derived from inputâoutput (I/O) data only, without the need of a model of the process. A drawback of these methods is that, in their standard formulation, they are not statistically efficient. In this paper, it is shown that they can be reformulated as L2-regularized optimization problems, by keeping the same assumptions and features, such that their statistical performance can be enhanced using the same identification dataset. A convex optimization method is also introduced to find the regularization matrix. The proposed strategy is finally tested on a benchmark example in the digital control system design.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "This study investigates the argument patterns in Yahoo! Answers, a major question and answer (Q&A) site. Mainly drawing on the ideas of Toulmin (), argument pattern is conceptualized as a set of 5 major elements: claim, counterclaim, rebuttal, support, and grounds. The combinations of these elements result in diverse argument patterns. Failed opening consists of an initial claim only, whereas nonoppositional argument pattern also includes indications of support. Oppositional argument pattern contains the elements of counterclaim and rebuttal. Mixed argument pattern entails all 5 elements. The empirical data were gathered by downloading from Yahoo! Answers 100 discussion threads discussing global warmingâa controversial topic providing a fertile ground for arguments for and against. Of the argument patterns, failed openings were most frequent, followed by oppositional, nonoppositional, and mixed patterns. In most cases, the participants grounded their arguments by drawing on personal beliefs and facts. The findings suggest that oppositional and mixed argument patterns provide more opportunities for the assessment of the quality and credibility of answers, as compared to failed openings and nonoppositional argument patterns. Â© 2012 Wiley Periodicals, Inc.",
    "actual_venue": "Jasist"
  },
  {
    "abstract": "Service recommendation facilitates developers to select services to create new mashups with large-granularity and added value. Currently, most studies concentrate on mining and recommending common composition patterns in mashups. However, latent negative patterns in mashups, which indicate the inappropriate combinations of services, remain largely ignored. By combining additional negative patterns between services with the already-exploited common mashup patterns, we present a more comprehensive and accurate model for service recommendation. Both positive association rules and negative ones are mined from services' annotated tags to predict future mashups. The extensive experiment conducted on real-world data sets shows a 33% enhancement in terms of F1-Score compared to classic association mining approach.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The current abstract view of the detailed architecture of a software program simply focuses on a set of modules that interact with each other. The pictorial view of the architecture of modules abstracts to a directed, connected, acyclic graph whose nodes are modules and whose arcs reveal the reference dependency relationship. No distinction exists among the modules. This paper provides a decided distinction. This paper introduces a new, formal, objective classification scheme of ten classes for modules. The process of fashioning the classification scheme applies the software principle of information hiding to three internal structural components of a module. These components are the variable, the data structure, and the subprogram. Two conceptual constructs enable information hiding. One is the encapsulation of a data structure (object). The other is the inaccessible section of a module which is the section that no external module can reference. Each module belongs to exactly one class. The assignment of a module to a class is objective, not subjective. The scheme opens up the opportunity for a deeper study into each class. The classes of the scheme are subjectively ranked into four categories, ranging from best to worst. The paper lists five benefits of this classification scheme, and relates these benefits to three application areas.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "This paper introduces alternating-direction implicit (ADI) solvers of higher order of time-accuracy (orders two to six) for the compressible Navier-Stokes equations in two- and three-dimensional curvilinear domains. The higher-order accuracy in time results from 1) An application of the backward differentiation formulae time-stepping algorithm (BDF) in conjunction with 2) A BDF-like extrapolation technique for certain components of the nonlinear terms (which makes use of nonlinear solves unnecessary), as well as 3) A novel application of the Douglas-Gunn splitting (which greatly facilitates handling of boundary conditions while preserving higher-order accuracy in time). As suggested by our theoretical analysis of the algorithms for a variety of special cases, an extensive set of numerical experiments clearly indicate that all of the BDF-based ADI algorithms proposed in this paper are \\\"quasi-unconditionally stable\\\" in the following sense: each algorithm is stable for all couples ( h , Î t ) of spatial and temporal mesh sizes in a problem-dependent rectangular neighborhood of the form ( 0 , M h ) Ã ( 0 , M t ) . In other words, for each fixed value of Ît below a certain threshold, the Navier-Stokes solvers presented in this paper are stable for arbitrarily small spatial mesh-sizes. The second-order formulation has further been rigorously shown to be unconditionally stable for linear hyperbolic and parabolic equations in two-dimensional space. Although implicit ADI solvers for the Navier-Stokes equations with nominal second-order of temporal accuracy have been proposed in the past, the algorithms presented in this paper are the first ADI-based Navier-Stokes solvers for which second-order or better accuracy has been verified in practice under non-trivial (non-periodic) boundary conditions.",
    "actual_venue": "Journal Of Computational Physics"
  },
  {
    "abstract": "Coping with uncertainties or ignorance in decision problems may lead to the idea that several scenarios can occur or that several sets of data can constitute a good representation of the reality. In consequence, numerous authors have focused on the robust aspect of these problems. In such a context, one can consider that a scenario (or a particular instance of the data) permits to partially qualify the solutions, just as a criterion does. In other words, the evaluation of a specific solution for a given scenario could be perceived as the evaluation of this solution according to one particular criterion. With this in mind, the applicability of classic multicriteria concepts to the robustness framework, in the context of optimization problems, is explored. We achieve this by studying their similarities and differences. The distinguishing characteristics bring us to introduce a new problematic: the multicriteria evaluation of robustness.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "We analyze the error performance of a cooperative diversity system having two cooperating mobile units or users (designated as source and relay) and a destination. The relay cooperates with the source through the decode and forward protocol to enable the transmission of a data symbol from the source to the destination in a flat Rayleigh fading environment. A selection combining scheme is used to obtain an easily implementable receiver structure at the destination. An analytical expression for the end-to-end symbol error probability (SEP) of the system for M-ary phase-shift keying is derived using a new approach, which we call a paired error approach. As a special case, we obtain, in closed form, the end-to-end SEP for binary phase-shift keying. We also present a modified selection combining scheme to incorporate the effect of the interuser link, that is, the source to relay link, in the evaluation of the error performance. Results show that cooperative diversity becomes effective for a larger range of average signal-to-noise ratio (SNR) of the user to destination link as the average SNR of the interuser link increases. We also find that the modified selection combining scheme offers a substantial SNR gain over selection combining based cooperation.",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "The increasing body of distributed and heterogeneous information and the autonomous, heterogeneous and dynamic nature of information resources are important issues hindering effective and efficient data access, retrieval and knowledge sharing. The importance ...",
    "actual_venue": "Cbms"
  },
  {
    "abstract": "For good security and large payload in information hiding, matrix embedding is a popular method for increasing the embedding efficiency. This paper analyzes the security of matrix embedding against cryptanalytic attacks. The secrecy security of matrix embedding using information theory under the conditions of known-cover attack and chosen-stego attack is studied. After that, the unicity distance of the key, message equivocation and the relationship among wet ratio, embedding rate and key equivocation for the wet paper channel are given through analyzing the key model under the known-cover attack condition. Furthermore, an effective differential attack to matrix embedding under chosen-stego attack condition is proposed. The results of analysis show that the matrix embedding is not secure enough with respect to cryptographic secrecy against the stronger adversaries.",
    "actual_venue": "International Journal Of Computational Intelligence Systems"
  },
  {
    "abstract": "We assume a database of items in which each item is described by a set of attributes, some of which could be multi-valued. We refer to each of the distinct attribute values as a feature. We also assume that we have information about the interactions (such as visits or likes) between a set of users and those items. In our paper, we would like to rank the features of an item using user-item interactions. For instance, if the items are movies, features could be actors, directors or genres, and user-item interaction could be user liking the movie. These information could be used to identify the most important actors for each movie. While users are drawn to an item due to a subset of its features, a user-item interaction only provides an expression of user preference over the entire item, and not its component features. We design algorithms to rank the features of an item depending on whether interaction information is available at aggregated or individual level granularity and extend them to rank composite features (set of features). Our algorithms are based on constrained least squares, network flow and non-trivial adaptations to non-negative matrix factorization. We evaluate our algorithms using both real-world and synthetic datasets.",
    "actual_venue": "Icde"
  },
  {
    "abstract": "This study focuses on an extended model of standard cellular automaton (CA), which includes an extra index, comprising a radius that defines a perception area for each cell in addition to the radius defined by the CA rule. Such an extension can be realized by introducing a recursive algorithm called the âRecursive Estimation of Neighbors.â The extended CA rules form a sequence ordered by this index, which includes the CA rule as its first term. This extension aims to construct a model that can be used within the CA framework to study the relation between information processing and pattern formation in collective systems. Even though the extension presented here is merely an extrapolation to a CA having a larger rule neighborhood identical to the perception area, the extra radius can be interpreted as an individual attribute of each cell. The novel perspective to CA provided here makes it possible to build heterogeneous CAs, which contain cells having different extra radii. Several pattern formations in the extension of one-dimensional elementary CAs and two-dimensional Life-like CAs are presented. It is expected that the extended model can be applied to various simulations of complex systems and in other fields.",
    "actual_venue": "Artificial Life And Robotics"
  },
  {
    "abstract": "Middleware architectures play a crucial role in determining the overall quality of many distributed applications. Systematic evaluation methods for middleware architectures are therefore important to thoroughly assess the impact of design decisions on quality goals. This paper presents MEMS, a scenario-based evaluation approach. MEMS provides a principled way of evaluating middleware architectures by leveraging generic qualitative and quantitative evaluation techniques such as prototyping, testing, rating, and analysis. It measures middleware architectures by rating multiple quality attributes, and the outputs aid the determination of the suitability of alternative middleware architectures to meet an applicationâs quality goals. MEMS also benefits middleware development by uncovering potential problems at early stage, making it cheaper and quicker to fix design problems. The paper describes a case study to evaluate the security architecture of grid middleware architectures for managing secure conversations and access control. The results demonstrate the practical utility of MEMS for evaluating middleware architectures for multiple quality attributes.",
    "actual_venue": "Qosa"
  },
  {
    "abstract": "A system that automatically plans grasp operations is described. This work has been undertaken in the framework of a project for automatic planning of pick-and-place tasks. The originality of the system comes from the consideration of the strong interdependencies existing between the different planning modules that are involved. It deals explicitly with the interactions between transfer motions, approach motions, the propagation of position uncertainty constraints as well as constraints induced by the local environment of the next step in a pick-and-place context. The approach motion step is based on a motion planner that builds an exact representation of a three-dimensional (x, y, Î¸) free configuration space",
    "actual_venue": "Cincinnati, Oh"
  },
  {
    "abstract": "Timestamps are often found to be dirty in various scenarios, e.g., in distributed systems with clock synchronization problems or unreliable RFID readers. Without cleaning the imprecise timestamps, temporal-related applications such as provenance analysis or pattern queries are not reliable. To evaluate the correctness of timestamps, temporal constraints could be employed, which declare the distance restrictions between timestamps. Guided by such constraints on timestamps, in this paper, we study a novel problem of repairing inconsistent timestamps that do not conform to the required temporal constraints. Following the same line of data repairing, the timestamp repairing problem is to minimally modify the timestamps towards satisfaction of temporal constraints. This problem is practically challenging, given the huge space of possible timestamps. We tackle the problem by identifying a concise set of promising candidates, where an optimal repair solution can always be found. Repair algorithms with efficient pruning are then devised over the identified candidates. Experiments on real datasets demonstrate the superiority of our proposal compared to the state-of-the-art approaches.",
    "actual_venue": "Pvldb"
  },
  {
    "abstract": "In this paper, we present a preliminary design of a framework for coordinating and enforcing usage control policies across different collaborating organizations. We named our framework xDUCON. The main goal of xDUCON is the specification of usage control policies that concisely capture conditions, authorizations, and obligations on both providers and consumers of resources. The novelty of xDUCON is its enforcement design that is based on the Shared Data Space (SDS) abstraction. The SDS allows the coordination of the decision and enforcement points abstracting from the details of the actual deployment of the framework. Moreover, the SDS abstraction caters for the necessary synchronization facilities necessary to realize a concrete implementation of control usage framework such as support for entity mutability and control over long-lived sessions to evaluate the access rights of a subject while the access is being executed.",
    "actual_venue": "Policy"
  },
  {
    "abstract": "A two-jointed arm is used to discuss the conditions under which a neural controller can acquire a precise internal model of a plant to be controlled without the help of an external superviser. The problem can be solved by a 'modified Hebbian rule' ensuring convergence of the synaptic strengths, a feedforward network called 'power network', and a learning algorithm called 'autoimitation'. The modified Hebbian rule describes a neuron, that - in addition to a number of inputs with plastic weights - has also a teaching input established with a fixed synaptic weight. The power network can adopt accurate models even of non-linear plants like the two-jointed arm when established with modified Hebbian synapses. The auto-imitation algorithm provides the power network with the values to be achieved by the network after learning. The training must be able to generate arbitrary movements, first of low velocity, then of higher velocity.",
    "actual_venue": "Artificial Neural Networks In Medicine And Biology"
  },
  {
    "abstract": "By exploiting a causality property of the nonlinear Fourier transform, a novel decision-feedback detection strategy for nonlinear frequency-division multiplexing (NFDM) systems is introduced. The performance of the proposed strategy is investigated both by simulations and by theoretical bounds and approximations, showing that it achieves a considerable performance improvement compared to previously adopted techniques in terms of Q-factor. The obtained improvement demonstrates that, by tailoring the detection strategy to the peculiar properties of the nonlinear Fourier transform, it is possible to boost the performance of NFDM systems and overcome current limitations imposed by the use of more conventional detection techniques suitable for the linear regime. (C) 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement",
    "actual_venue": "Optics Express"
  },
  {
    "abstract": "Successfully navigating the norms of a society is a complex task that involves recognizing diverse kinds of rules as well as the relative weight attached to them. In the United States (U.S.), different kinds of rulesâfederal statutes and regulations, scientific norms, and professional idealsâguide the work of researchers. Penalties for violating these different kinds of rules and norms can range from the displeasure of peers to criminal sanctions. We proposed that it would be more difficult for researchers working in the U.S. who were born in other nations to distinguish the seriousness of violating rules across diverse domains. We administered a new measure, the evaluating rules in science task (ERST), to National Institutes of Health-funded investigators (101 born in the U.S. and 102 born outside of the U.S.). The ERST assessed perceptions of the seriousness of violating research regulations, norms, and ideals, and allowed us to calculate the degree to which researchers distinguished between the seriousness of each rule category. The ERST also assessed researchersâ predictions of the seriousness that research integrity officers (RIOs) would assign to the rules. We compared researchersâ predictions to the seriousness ratings of 112 RIOs working at U.S. research-intensive universities. U.S.-born researchers were significantly better at distinguishing between the seriousness of violating federal research regulations and violating ideals of science, and they were more accurate in their predictions of the views of RIOs. Acculturation to the U.S. moderated the effects of nationality on accuracy. We discuss the implications of these findings in terms of future research and education.",
    "actual_venue": "Science And Engineering Ethics"
  },
  {
    "abstract": "Bit error rate (BER) and channel capacity are two important metrics to assess the performance of free-space optical communication (FSOC) systems. Due to the fading of the optical signal owing to the atmospheric effects, these two quantities behave as random variables. Most of the studies in this direction have focused on the calculation of only the average of these quantities. However, since the complete information about a random variable is encoded in its distribution, it is more informative to examine the latter itself. In this work, the authors derive exact probability density function (PDF) expressions for the BER and the channel capacity for an arbitrary irradiance model. In particular, they investigate these exact results for log-normal, gamma-gamma, and K distributions. For the BER analysis, they focus on the binary phase shift keying and quadrature phase shift keying modulation schemes. The authors' analytical reults are found to be in conformity with Monte Carlo simulations. The exact PDFs of the BER and the channel capacity reveal that there are several instances when the average is unable to capture the actual behaviour of these quantities, and therefore one must be careful in drawing conclusions based on the first moment only.",
    "actual_venue": "Iet Communications"
  },
  {
    "abstract": "Receding horizon trajectory optimization for optimal information gathering in opportunistic navigation environments is considered. A receiver is assumed to be dropped in an environment consisting of multiple signals of opportunity (SOPs) transmitters. The receiver has minimal a priori knowledge about its own states and the SOPs??? states. The receiver draws pseudorange observations from the SOPs. The receiver???s objective is to build a high-fidelity signal landscape map while simultaneously localizing itself within this map in space and time. Assuming that the receiver can control its maneuvers, the following two problems are considered. First, the minimal conditions under which the environment is completely observable are established. It is shown that receiver-controlled maneuvers reduce the minimal a priori information about the environment required for complete observability. Second, the trajectories that the receiver should traverse are prescribed. To this end, a one-step look-ahead (greedy) strategy is compared with a multistep look-ahead (receding horizon) strategy. The limitations and achieved improvements in the map quality and space-time localization accuracy due to the receding horizon strategy are quantified. The computational burden associated with the receding horizon strategy is also discussed.",
    "actual_venue": "Aerospace And Electronic Systems, Ieee Transactions"
  },
  {
    "abstract": "In this paper we present a clustering-based method for representing semantic concepts on multimodal low-level feature spaces and study the evaluation of the goodness of such models with entropy-based methods. As different semantic concepts in video are most accu- rately represented with different features and modalities, we utilize the relative model-wise confidence values of the feature extraction techniques in weighting them automatically. The method also pro- vides a natural way of measuring the similarity of different concepts in a multimedia lexicon. The experiments of the paper are conducted using the development set of the TRECVID 2005 corpus together with a common annotation for 39 semantic concepts.",
    "actual_venue": "Toronto, Ont"
  },
  {
    "abstract": "In this paper we develop an existence theory for small amplitude, steady, two-dimensional water waves in the presence of wind in the air above. The presence of the wind is modeled by a Kelvin-Helmholtz type discontinuity across the air-water interface, and a corresponding jump in the circulation of the fluids there. We consider both fluids to be inviscid, with the water region being irrotational and of finite depth. The air region is considered with constant vorticity in the case of infinite depth and with a general vorticity profile in the case of a finite, lidded atmosphere.",
    "actual_venue": "Siam Journal On Mathematical Analysis"
  },
  {
    "abstract": "A data automaton is a finite automaton equipped with variables counters or registers ranging over infinite data domains. A trace of a data automaton is an alternating sequence of alphabet symbols and values taken by the counters during an execution of the automaton. The problem addressed in this paper is the inclusion between the sets of traces data languages recognized by such automata. Since the problem is undecidable in general, we give a semi-algorithm based on abstraction refinement, which is proved to be sound and complete modulo termination. Due to the undecidability of the trace inclusion problem, our procedure is not guaranteed to terminate. We have implemented our technique in aÃ¯Â¾Â¿prototype tool and show promising results on several non-trivial examples.",
    "actual_venue": "Tacas"
  },
  {
    "abstract": "A methodology has been implemented for analyzing microarray and NMR spectral data obtained from the same set of toxic-exposure dose-response experiments. The NMR spectra additionally track the time course of exposure. Analyses consist of screening the data to eliminate variates with insignificant signal, normalization appropriate to the experimental design, Principal Components Analysis, and nonlinear classification using a Support Vector Machine. It is found that exposure at subtoxic levels can be detected.",
    "actual_venue": "Csb Workshops"
  },
  {
    "abstract": "A hybrid fuzzy knowledge based system and genetic algorithm has been proposed to solve the maintenance-scheduling problem for thermal generating units. In contrast to the existing maintenance scheduling methods, the proposed approach implements fuzzy knowledge based system to emulate the power plant personnel's experiences considering the uncertainties in the constraints, such as, the operating hours of the generators, specific fuel consumption, crew and resources availability. The genetic algorithm optimises the total generating cost and the maintenance cost as the objective functions, Simulations were carried out on a practical thermal power plant consisting of 19 generating units, over a six month planning horizon. The performance of this hybrid genetic algorithm-fuzzy knowledge based system is compared to the classical method currently in use at the thermal power system. The results obtained show that this hybrid approach is an effective and practical way in solving the maintenance scheduling problem. Moreover, the method proposed is easily revisable and able to handle larger systems and longer planning horizons in a considerable computational time.",
    "actual_venue": "Hybrid Information Systems"
  },
  {
    "abstract": "The way in which organizations manage the knowledge that underlies their capabilities can be a powerful tool in explaining their behavior and competitiveness. While much research in knowledge management has illuminated the understanding of knowledge management, little is understood about the attributes of implementing a system that helps to create, mobilize and diffuse knowledge. Aiming at narrowing this gap, our research examines the experiences of a major oilfield services company in transforming itself to overcome a hierarchical knowledgeflow to support its operations in the oil fields by putting into operation a system that facilitates the creation, mobilization and diffusion of knowledge in the technical service delivery process. By adopting the Abductive Strategy, and utilizing the means-end model with its laddering technique, the study reveals the attributes in operationalizing\"InTouch\", the system that supports the process.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "Model based predictive control (MBPC) has been extensively investigated and is widely used in industry. Besides this, interest in non-linear systems has motivated the development of MBPC formulations for non-linear systems. Moreover, the importance of security and reliability in industrial processes is in the origin of the fault tolerant strategies developed in the last two decades. In this paper a MBPC based on support vector machines (SVM) able to cope with faults in the plant itself is presented. The fault tolerant capability is achieved by means of the accurate on-line support vector regression (AOSVR) which is capable of training an SVM in an incremental way. Thanks to AOSVR is possible to train a plant model when a fault is detected and to change the nominal model by the new one, that models the faulty plant. Results obtained under simulation are presented.",
    "actual_venue": "Eng Appl Of Ai"
  },
  {
    "abstract": "In todayÃ½s deep-submicron designs, the interconnect delays contribute an increasing part to the overall performance of an implementation. Particularly when targeting field programmable gate arrays (FPGAs), interconnect delays are crucial, since they can easily vary by orders of magnitude. Many existing performance-directed retiming methods use simple delay models which either neglect routing delays or use inaccurate delay estimations. In this paper, we propose a retiming approach which overcomes the problem of inaccurate delay models. Our retiming technique uses delay information extracted from a fully placed and routed design and takes account of register timing requirements. By applying physical constraints, we ensure that the delay information remains valid during retiming. In our experiments, we achieved up to 27% performance improvement.",
    "actual_venue": "Date"
  },
  {
    "abstract": "We consider switching Hâ controllers for a class of linear parameter varying (LPV) systems scheduled along a measurable parameter trajectory. The candidate controllers are selected from a given controller set according to the switching rules based on the scheduling variable. We provide sufficient conditions to guarantee the stability of the switching LPV systems in terms of the dwell time and the average dwell time. Our results are illustrated with an example, where switching between two robust controllers is performed for an LPV system.",
    "actual_venue": "Systems And Control Letters"
  },
  {
    "abstract": "In this paper we intend to optimize a wavelet-based dictionary for transient modeling with application to parametric audio coding. Transient modeling is performed by matching pursuit with an overcomplete dictionary composed of orthonormal wavelet functions that implement a wavelet-packet filter bank. We try to find the prototype filter length, the decomposition depth and the orthogonal wavelet family that lead to the best balance between mean squared error and computational cost. We are also interested in the structure of the wavelet decomposition tree. In such sense, comparison between the wavelet transform and the full wavelet-packet transform is performed. Finally, comparative analysis between wavelets and exponentially damped sinusoids is shown in experimental results. The proposed transient modeling method is suitable to be integrated into a parametric audio coder based on the three-part model of sines, transients and noise (STN model).",
    "actual_venue": "Digital Signal Processing"
  },
  {
    "abstract": "This paper describes the application of a master/slave configuration of processors to study a comparison of alternative material handling configurations for automated manufacturing. Such a study usually requires a large number of simulation replications, and carrying out those replications on a multi-processor platform yields significant savings in elapsed time. In the present application, a master processor carries out the statistical computations for a 2k factorial design on up to eight slave processors. This paper will compare the results from using two, four and eight processors.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "Several mathematical models of different physiological systems are spread through literature. They serve as tools which improve the understanding of (patho-) physiological processes, may help to meet clinical decisions and can even enhance medical therapies. These models are typically implemented in a signal-flow-oriented simulation environment and focus on the behavior of one specific subsystem. Neglecting other physiological subsystems and using a technical description of the physiology hinders the exchange with and acceptance of clinicians. By contrast, this paper presents a new model implemented in a physical, object-oriented modeling environment which includes the cardiovascular, respiratory and thermoregulatory system. Simulation results for a healthy subject at rest and at the onset of exercise are given, showing the validity of the model. Finally, simulation results showing the interaction of the cardiovascular system with a ventricular assist device in case of heart failure are presented showing the flexibility and mightiness of the model and the simulation environment. Thus, we present a new model including three important physiological systems and one medical device implemented in an innovative simulation environment.",
    "actual_venue": "Medical And Biological Engineering And Computing"
  },
  {
    "abstract": "Gossip, or epidemic, protocols have emerged as a powerful strategy to implement highly scalable and resilient reliable broadcast primitives. Due to scalability reasons, each participant in a gossip protocol maintains a partial view of the system. The reliability of the gossip protocol depends upon some critical properties of these views, such as degree distribution and clustering coefficient. Several algorithms have been proposed to maintain partial views for gossip protocols. In this paper, we show that under a high number of faults, these algorithms take a long time to restore the desirable view properties. To address this problem, we present HyParView, a new membership protocol to support gossip-based broadcast that ensures high levels of reliability even in the presence of high rates of node failure. The HyParView protocol is based on a novel approach that relies in the use of two distinct partial views, which are maintained with different goals by different strategies.",
    "actual_venue": "DSN"
  },
  {
    "abstract": "With orchestrations, one service may be realized through the cooperation of several services. This cooperation has to be formally described. In this paper, we propose to describe service orchestrations according to UML2 meta- model through three UML2 diagrams. Component dia- grams describe each service external interfaces. Collabo- ration diagrams describe the structural composition of ser- vices. And activity diagrams describe the orchestration of services. The main contribution of this article is to mix the orches- tration and composition meta-model with a context meta- model. Thus, we propose to include the descriptions of context awareness into the orchestration and composition meta-model. This approach allows application designers to describe flexible orchestration of services. Furthermore, describing compositions and their context-awareness with a model (conform to a meta-model) allows middleware with model-transformation capabilities to produce ad-hoc com- positions in term of adaptation to current context execution and in term of target execution platforms. We present in this article two kinds of adaptation of con- text aware orchestrations: deployment time and run time adaptations.",
    "actual_venue": "Wetice"
  },
  {
    "abstract": "This paper deals with the velocity control of motor drives when saturated inputs are considered. Two classes of velocity controllers guaranteeing exponential stability are introduced. The first controller allows nonlinear proportional an integral action, while the other one includes an antiwindup loop. A detailed experimental study is provided, which confirms the theoretical results.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "The fast vector space and probabilistic methods use the term counts and the slower proximity methods use term positions. We present the spectral-based information retrieval method which is able to use both term count and position information to obtain high precision document rankings. We are able to perform this, in a time comparable to the vector space method, by examining the query term spectra rather than query term positions. This method is a generalisation of the vector space method (VSM). Therefore, our spectral method can use the weighting schemes and enhancements used in the VSM.",
    "actual_venue": "Asian"
  },
  {
    "abstract": "In this paper, we propose a hierarchical model predictive control (MPC) approach for signal split optimization in large-scale urban road networks. To reduce the computational complexity, a large-scale urban road network is first divided into several subnetworks using a network decomposition method. Second, the MPC optimization problem of the large-scale urban road network is presented, in which th...",
    "actual_venue": "Ieee Transactions Intelligent Transportation Systems"
  },
  {
    "abstract": "This paper describes a novel theoretical characterization of the performance of nonlocal means (NLM) for noise removal. NLM has proved effective in a variety of empirical studies, but little is understood fundamentally about how it performs relative to classical methods based on wavelets or how its parameters should be chosen. For cartoon images and images which may contain thin features and regular textures, the error decay rates of NLM are derived and compared with those of linear filtering, oracle estimators, Yaroslavsky's filter, and wavelet thresholding estimators. The trade-off between global and local search for matching patches is examined, and the bias reduction associated with the local polynomial regression version of NLM is analyzed. The theoretical results are validated via simulations for two-dimensional images corrupted by additive white Gaussian noise.",
    "actual_venue": "Siam Journal On Imaging Sciences"
  },
  {
    "abstract": "A personalized learning system needs a large pool of items for learners to solve. When working with a large pool of items, it is useful to measure the similarity of items. We outline a general approach to measuring the similarity of items and discuss specific measures for items used in introductory programming. Evaluation of quality of similarity measures is difficult. To this end, we propose an evaluation approach utilizing three levels of abstraction. We illustrate our approach to measuring similarity and provide evaluation using items from three diverse programming environments.",
    "actual_venue": "Arxiv: Computers And Society"
  },
  {
    "abstract": "An encyrption algorithm based on sequences of random permutations and the structure of Hungarian rings is given.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "The median set of a graph G with weighted vertices comprises the vertices minimizing the average weighted distance to the vertices of G. We characterize the graphs in which, with respect to any nonnegative vertex weights, median sets always induce connected subgraphs. The characteristic conditions can be tested in polynomial time (by employing linear programming) and are immediately verified for a number of specific graph classes.",
    "actual_venue": "Siam J Discrete Math"
  },
  {
    "abstract": "The vast majority of the existing work in gain-scheduling (GS) control literature assumes perfect knowledge of scheduling parameters. Generally, this assumption is not realistic since for practical control applications measurement noises are unavoidable. In this paper, novel synthesis conditions are derived to synthesise robust GS controllers with mixed H-2/H-infinity performance subject to uncertain scheduling parameters. The conditions are formulated in terms of parameterised bilinear matrix inequalities (PBMIs) that depend on varying parameters inside multi-simplex domain. The conditions provide practical GS controllers independent of the derivatives of scheduling parameters. That is, the designed controllers are feasible for implementation. Since bilinear matrix inequality problems are intractable, an iterative PBMI algorithm is developed to solve the developed synthesis conditions. By the virtue of this algorithm, conservativeness reduction is achieved with few iterations. Examples are presented to illustrate the effectiveness of the developed conditions. Compared to other design methods from literature, the developed conditions achieve better performance.",
    "actual_venue": "International Journal Of Control"
  },
  {
    "abstract": "Due to language deficiencies, individual non-native speakers (NNS) face many difficulties while writing. In this paper, we propose to build a collaborative editing system that aims to facilitate the sharing of language knowledge among non-native co-authors, with the ultimate goal of improving writing quality. We describe CEPT, which allows individual co-authors to generate their own revisions as well as incorporating edits from others to achieve mutual inspiration. The main technical challenge is how to aggregate edits of multiple co-authors and present them in an easy-to-understand way. After iterative design, CEPT highlights three novel features: 1) cross-version sentence mapping for edit tracking, 2) summarization of edits from multiple co-authors, and 3) a collaborative editing interface that enables co-authors to examine, comment on, and borrow edits of others. A preliminary lab study showed that CEPT could significantly improve both the language quality and collaboration experience of NNS writers, due to its efficacy for sharing language knowledge.",
    "actual_venue": "Cscw"
  },
  {
    "abstract": "In this article an overview of the state of the art of QFD in software development or also called Software QFD is given. The differences between classic QFD in manufacturing industries and Software QFD are described. Following certain software specific QFD models (Zultner, Shindo, Ohmori, Herzwurm and Schockert), which can be considered as the most appreciated ones in theory as well as in practice, are introduced. Experiences in practice with these Software QFD models are presented as well. Finally, through explaining the main principles of a special QFD variant for e-commerce, called Continuous QFD (CQFD), the article will show that QFD is suitable for planning electronic business applications as well.",
    "actual_venue": "RE"
  },
  {
    "abstract": "Many toolkits for extending the physical metaphors for learning have been proposed over the years. This paper presents the design and implementation of a toolkit for building tangible learning applications based on the IEEE 802.15.14 based sensor wireless networks. This toolkit provides a connection-less and a generic object-oriented programmable environment. A sample application showing how young children can browse the Internet using tangible components is also presented.",
    "actual_venue": "Niigata"
  },
  {
    "abstract": "This paper investigates spectrum sharing (in the form of code sharing) between two Universal Mobile Telecommunication System (UMTS) operators in the UMTS extension band (2500-2690MHz) with equal and unequal number of proprietary carriers, respectively. The paper proposes a Dynamic Spectrum Allocation (DSA) algorithm to address the problem of spectrum sharing between two operators on a non-pool basis. It also investigates the impact of Adjacent Channel Interference (ACI) on the spectrum sharing gain. Additionally, an architecture that enables spectrum sharing to take place between two or more UMTS operators is presented. The simulated performance of the proposed DSA algorithm shows that under peak-hour loading, up to 32% increase in capacity can be obtained when compared to currently used Fixed Spectrum Allocation (FSA).",
    "actual_venue": "Ieee International Symposium On Personal, Indoor And Mobile Radio Communications"
  },
  {
    "abstract": "The OWLS-MX matchmaker selects OWL-S 1.1 services that are relevant to a given service request by means of combined logic based and approximative matching filters (5). In this paper, we build upon this work and analyse its retrieval performance in terms of false positive and false negatives to reveal the benefits and pitfalls of both logic based and hybrid matching with OWLS-MX. The analysis results have been exploited in an improved version OWLS-MX+.",
    "actual_venue": "Smrr"
  },
  {
    "abstract": "In conventional printed circuit board manufacturing, pressure and heat are used to attach the copper layer to the laminate. To achieve narrow lines the additive method is used instead. To produce the seed metal, a palladium bath is quite often used. This method is used in conventional PCB production. In our paper we suggest a new method of arriving at a very thin and even surface by sputtering. The sputtering process is done in a simple chamber with an argon atmosphere. The process is rapid (only few tens of seconds) and inexpensive, since the produced layer is very thin. This sputtered metal acts as an adhesion layer, and copper can be grown on this layer either through electrolysis or by electroless means. In the adhesion test, the sputtered and copper-coated specimens were subjected to a peel strength test. It has been noted that the adhesion of sputtered metal to the epoxies differs greatly depending on the metal used. The effects of heating after sputtering have also been identified. It has also been noted that the treatment of the surface has a significant effect on adhesion. In this paper we describe the sputtering method, the results of the peel strength tests and the treatments, which act on the adhesion of the metal to the base laminates.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "In recent years, due to the growth of information on the internet, the number of available Web services has increased. Clustering Web services based on their functional features to different domains have started to play a major role in several service management tasks such as efficient Web service discovery and recommendations. In this paper, we propose a novel ontology-based approach for Web service clustering. Instead of using traditional methods, we focus on the similarity and specificity of terms for ontology generation. The amount of domain-specific information included in a term is used to define the specificity of that term. Specific terms are more powerful than general terms for describing a large amount of domain information. Taking advantage of this, we generate a new ontology, which is then used to calculate similarity by defining new logic-based filters. When the similarity calculation fails, we apply information retrieval-based methods. Based on a comprehensive evaluation that we conducted to measure the performance of our method, our novel clustering approach was shown to be more effective in terms of precision, recall, F-measure, purity and entropy than other existing clustering approaches.",
    "actual_venue": "Ieee International Conference On Web Services"
  },
  {
    "abstract": "This paper presents the architecture of a multiagent society (MAS) designed to study the dynamics of belief change in natural and artificial societies. It also presents a multiagent domain called Multiagent Wumpus World (MWW) designed to test the capabilities of the proposed MAS. It also reports on a set of experiments designed to study the formation of false social beliefs. Our results indicate that more false beliefs are likely to be generated about objects/events whose presence is harder to confirm or disconfirm.",
    "actual_venue": "Honolulu, Hi"
  },
  {
    "abstract": "We present Heart-to-Heart (H2H), a system to authenticate external medical device controllers and programmers to Implantable Medical Devices (IMDs). IMDs, which include pacemakers and cardiac defibrillators, are therapeutic medical devices partially or wholly embedded in the human body. They often have built-in radio communication to facilitate non-invasive reprogramming and data readout. Many IMDs, though, lack well designed authentication protocols, exposing patients to over-the-air attack and physical harm. H2H makes use of ECG (heartbeat data) as an authentication mechanism, ensuring access only by a medical instrument in physical contact with an IMD-bearing patient. Based on statistical analysis of real-world data, we propose and analyze new techniques for extracting time-varying randomness from ECG signals for use in H2H. We introduce a novel cryptographic device pairing protocol that uses this randomness to protect against attacks by active adversaries, while meeting the practical challenges of lightweight implementation and noise tolerance in ECG readings. Finally, we describe an end-to-end implementation in an ARM-Cortex M-3 microcontroller that demonstrates the practicality of H2H in current IMD hardware. Previous schemes have had goals much like those of H2H, but with serious limitations making them unfit for deployment---such as naively designed cryptographic pairing protocols (some of them recently broken). In addition to its novel analysis and use of ECG entropy, H2H is the first physiologically-based IMD device pairing protocol with a rigorous adversarial model and protocol analysis.",
    "actual_venue": "Acm Conference On Computer And Communications Security"
  },
  {
    "abstract": "This paper deals with a functional equation and inequality arising in dynamic programming of multistage decision processes. Using several fixed-point theorems due to Krasnoselskii, Boyd-Wong and Liu, we prove the existence and/or uniqueness and iterative approximations of solutions, bounded solutions and bounded continuous solutions for the functional equation in two Banach spaces and a complete metric space, respectively. Utilizing the monotone iterative method, we establish the existence and iterative approximations of solutions and nonpositive solutions for the functional inequality in a complete metric space. Six examples which dwell upon the importance of our results are also included. Â© 2012 Springer Science+Business Media New York.",
    "actual_venue": "J Optimization Theory And Applications"
  },
  {
    "abstract": "This paper evaluates the potential impact of explicit phrases on retrieval quality through a case study with the TREC Terabyte benchmark. It compares the performance of user- and system-identified phrases with a standard score and a proximity-aware score, and shows that an optimal choice of phrases, including term permutations, can significantly improve query performance.",
    "actual_venue": "Ecir"
  },
  {
    "abstract": "3D shape retrieval is always a challenging research topic because of complex geometric structural variations involved. Although many feature extraction and retrieval algorithms have been proposed, they generally only use single 3D model descriptor hence cannot obtain better retrieval performance. In this paper, we propose a new 3D shape retrieval framework based on compressive sensing (CS) and multi-feature fusion (MFF). Firstly, we extract three new features including the CS Chebyshev ray (CSCR) feature, the CS spatial hierarchical (CSSH) feature and the Extended Gaussian sphere (EGS) feature. Actually, CSCR, CSSH and EGS respectively represent the volume tensor, the layered detail and the statistical space distribution on the model surface of a 3D model. To make the best use of these features, a supervised learning is used to determine the weighting coefficients for these features. Finally, the features and their corresponding weighting coefficients are used to determine the similarity of 3D models in the multi-feature fusion (MFF) framework for 3D shape retrieval. For performance assessment, two publicly available datasets that contain 3D models with large geometric variations are used, including ModelNet-10 and PSB datasets. Comprehensive experimental results have demonstrated the efficacy of the proposed method for 3D shape retrieval.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.",
    "actual_venue": "Asplos"
  },
  {
    "abstract": "In this paper, a new design concept for accelerating parallel Jacobi method by using Network-on-Chip (NoC) is presented. The implementation of the Brent-Luk-EVD array is used as an example. In order to further study the tradeoff between the performance/complexity of EVD processors and the load/throughput of interconnects, a mesh structure NoC design based Jacobi EVD array with the simplified Î¼-CORDIC processor PE has be implemented on FPGA. The hardware experimental results show that using a NoC architecture makes it able to deal with large-scale size EVD problem and reduce the computation time.",
    "actual_venue": "Ispacs"
  },
  {
    "abstract": "The integral equations studied here play very important role in the theory of parabolic initial-boundary value problems (heat conduction problems) and in various physical, technological and biological problems (epidemiology problems). This paper is concerned with the iterative-collocation method for solving these equations. We propose an iterative method with corrections based on the interpolation polynomial of spatial variable of the Lagrange type with given collocation points. The coefficients of these corrections can be determined by a system of Volterra integral equations. The convergence of the presented algorithm is proved and an error estimate is established. The presented theory is illustrated by numerical examples and a comparison is made with other methods.",
    "actual_venue": "Numerical Methods And Application"
  },
  {
    "abstract": "The Department of Defense has identified that integrating the human element into large scale System of Systems (SoS) models is a significant challenge that remains unaddressed. Failure in doing so leads to significant limitations in our SoS analytical capabilities as human performance is a large contributor to the performance of a SoS. The primary challenge is that, in most SoS domains, the problems being analyzed are large in scale. Conversely, most Human Performance Modeling (HPM) initiatives look at integrating detailed cognitive models that capture fine grained details of human perception, decision making, and response with detailed systems models and simulations (e.g., Lebiere et al., 2003). It is not feasible to integrate such fine grained cognitive models with systems models and perform SoS scale analysis. This paper documents a capability that integrates HPM into a large scale SoS simulation toolset and demonstrates the utility of the toolset.",
    "actual_venue": "System Of Systems Engineering"
  },
  {
    "abstract": "Cloud computing is leading to transformational changes with stringent requirements on usability, performance and security over very heterogeneous workloads. Their run-time management requires realistic algorithms and techniques for sampling, measurement and characterization for load prediction. Due to the expectation of elasticity, large swings in their demand are common, which cannot be modeled accurately based on raw measures such as the number of session requests, which show very large variability and poor auto-correlation. We demonstrate the use of load prediction algorithms for cloud platforms, using a two-step approach of load trend tracking followed by load prediction, using cubic spline Interpolation, and hotspot detection algorithm for sudden spikes. Such algorithms integrated into the autonomic management framework of a cloud platform can be used to ensure that the SaaS sessions, virtual desktops or VM pools are autonomically provisioned on demand, in an elastic manner. Results indicate that the algorithms are able to match representative SaaS load trends accurately. This approach is suitable to support different load decision systems on cloud platforms with highly variable trends in demand, and is characterized by a moderate computational complexity compatible to run-time decisions.",
    "actual_venue": "UCC"
  },
  {
    "abstract": "This paper is the second of two Learned Publishing articles in which we report the results of a series of interviews, with senior publishers and editors exploring open access megajournals (OAMJs). Megajournals (of which PLoS One is the best known example) represent a relatively new approach to scholarly communication and can be characterized as large, broad-scope, open access journals, which take an innovative approach to peer review, basing acceptance decisions solely on the technical or scientific soundness of the article. Based on interviews with 31 publishers and editors, this paper reports the perceived cultural, operational, and technical challenges associated with launching, growing, and maintaining a megajournal. We find that overcoming these challenges while delivering the societal benefits associated with OAMJs is seen to require significant investment in people and systems, as well as an ongoing commitment to the model.",
    "actual_venue": "Learned Publishing"
  },
  {
    "abstract": "Asymmetry is a common phenomenon in higher organisms. In humans, the cortical representation of language exhibits a high degree of asymmetry with a prevalence of about 90% of left hemispheric dominance, the underlying mechanisms of which are largely unknown. Another sign that exhibits a form of lateralization is the scalp hair-whorl direction, which is either clockwise or anti-clockwise. The scalp hair-whorl develops from the same germ layer as the nervous system, the ectoderm, between the 10th and 16th week in utero and has been shown to be associated with various neurodevelopmental disorders. Here, we use an established fMRI paradigm to examine the association of a solely biological marker of asymmetry, hair-whorl direction and language lateralization. We show that the mechanism that influences hair-whorl direction and handedness [Klar, A.J.S., 2003. Human handedness and scalp hair-whorl direction develop from a common genetic mechanism. Genetics 1651, 269â276.] also affects cerebral language dominance.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Subliminal priming has the potential to influence people's attitudes and behaviour, making them prefer certain choices over others. Yet little research has explored its feasibility on smartphones, even though the global popularity and increasing use of smartphones has spurred interest in mobile behaviour change interventions. This paper addresses technical, ethical and design issues in delivering mobile subliminal priming. We present three explorations of the technique: a technical feasibility study, and two participant studies. A pilot study (n=34) explored subliminal goal priming in-the-wild over 1 week, while a semi-controlled study (n=101) explored the immediate effect of subliminal priming on 3 different types of stimuli. We found that although subliminal priming is technically possible on smartphones, there is limited evidence of impact on changes in how much stimuli are preferred by users, with inconsistent effects across stimuli types. We discuss the implications of our results and directions for future research.",
    "actual_venue": "Proceedings Of The International Conference On Human-Computer Interaction With Mobile Devices And Services"
  },
  {
    "abstract": "Radio Frequency Identification (RFID) systems with dense tag populations face the problem of tag collisions, i.e. simultaneous replies of multiple transponders in a single slot. State-of-the-art RFID readers can use two methods for resolving this collision problem. The first method bases on Dynamic Frame Slotted ALOHA (DFSA) for optimizing the frame length, which results in a reduced number of collisions. The second and more efficient method resolves collisions on the physical layer, e.g. by means of multi-antenna algorithms. However, the well-known EPCglobal class 1 gen 2 standards only allows for a single tag acknowledgment, even if the physical layer is able to identify multiple collided tags. This results in an overall reduced performance. For overcoming this drawback, we propose a system that has the capability to acknowledge multiple tags within a single slot, resulting in a significantly increased performance. Our proposal offers the benefit that it is backwards compatible with existing EPCglobal Class 1 gen 2 tags and readers. Hence, our improved tags can be read by conventional readers without affecting the performance. Furthermore, existing tags can be read simultaneously with our improved tags by optimized readers. Using our approach, we are able to increase the maximum reading efficiency compared to state-of-the-art systems by a factor of approx. 3. This can be translated into a bulk reading time reduction of 50%, which is a significant improvement w.r.t. state-of-the-art systems.",
    "actual_venue": "Ieee International Conference On Rfid Technology And Applications"
  },
  {
    "abstract": "It is well known that data centers are consuming a large amount of energy that incurs significant financial and environmental costs. Recently, there has been an increasing interest in utilizing green energy for data centers, where green energy sources include solar and wind. This paper studies the crucial problem of maximizing the utilization of green energy through scheduling complex jobs in data centers in order to reduce the use of traditional brown energy. However, it is highly challenging for data centers to make use of green energy. First, the availability of typical green energy is variable to dynamic changes of natural environments, for example, weather. Second, although predictions can be made for the future availability of green energy, it is inevitable that such predictions have errors. Third, jobs are associated with strict deadlines, and it is required that jobs are completed before their deadlines. Finally, because the reliability in a data center relies upon temperature, the awareness of temperature should be taken into account while maximizing the green energy. In this paper, we consider online scheduling of jobs whose arrivals to the data center system dynamically. In addition, we explicitly take the power consumption of switches into account when scheduling jobs onto computing nodes. Two solar energy-aware algorithms called SEEDMin and SEEDMax have been proposed. Then, we extend SEED to RSEED with the awareness of reliability. To evaluate the effectiveness of the proposed algorithms, comprehensive simulations have been conducted, and the proposed algorithms are compared with other state-of-art algorithms. Experimental results demonstrate that both SEEDMin and SEEDMax can significantly increase the utilization of solar energy without violating job deadlines and overall energy budget. The amount of solar energy utilized by SEEDMin and SEEDMax is 33.4%and35.3% larger than that of two traditional scheduling algorithms, MinMin and MinMax, respectively. Also, it can be seen that RSEED greatly improves the reliability by decreasing the temperature. Copyright Â© 2013 John Wiley & Sons, Ltd.",
    "actual_venue": "Concurrency And Computation: Practice And Experience"
  },
  {
    "abstract": "OFDM systems have gained outstanding popularity for high data rate wireless communications. In practice, however, the performance of OFDM systems is often limited due to hardware impairments that result from stringent cost requirements. Receiver I/Q imbalance is known to be one of the most serious hardware impairments degrading the performance of OFDM systems. Previous work has shown that receiver I/Q imbalance may severely increase the symbol error rate depending on the characteristics of the communications channel.In this paper we investigate the maximum data rate, i.e. the capacity of OFDM systems that are impaired by receiver I/Q imbalance. We derive the system capacity analytically and consider the upper bound for a fixed channel realization as well as for Rayleigh fading channels.",
    "actual_venue": "Ieee International Conference On Communications, Proceedings, Vols"
  },
  {
    "abstract": "In this paper, we present a new approach for the visualization of time-series data based on spirals. Different to classical bar charts and line graphs, the spiral is suited to visualize large data sets and supports much better the identification of periodic ...",
    "actual_venue": "Infovis"
  },
  {
    "abstract": "It is well established that making decisions from defined data according to various criteria requires the use of MultiCriteria Decision Aiding or Analysis (MCDA) methods. However the necessary input data for these approaches are often ill-known especially when the data are a priori estimated. The common MCDA approaches consider these data as singular/scalar values. This paper deals with the consideration of more realistic, values by studying the impact of imprecision on a classical âpreciseâ ranking established with ACUTA, a method based on additive utilities. We propose a generic approach to establish the concordance of pairwise relations of preference despite interval-based imprecision by complementing ACUTA with a computation of Kendall's index of concordance and of a threshold for maintaining this concordance. The methodology is applied to an industrial case subjected to Sustainable Development problems.",
    "actual_venue": "Computational Intelligence In Multicriteria Decision-Making"
  },
  {
    "abstract": "In this paper, a Multi Layer Perceptron (MLP) based Artificial Immune System (AIS) is presented for breast cancer classification. The proposed algorithm integrates clonal selection principle of AIS in MLP learning to reduce its computational costs and accelerate its convergence to a Mean Squared Error Threshold (MSEth) set by the user. Applied on the Wisconsin Diagnosis Breast Cancer database (WDBC), the results show that combining Artificial Immune Systems and Neural Networks is effective. Indeed, a significant reduction of computation time has been obtained with a slight improvement of classification accuracy.",
    "actual_venue": "Sixth International Conference On Image Processing Theory, Tools And Applications"
  },
  {
    "abstract": "The Chaos router is compared with several contemporary adaptive router designs as well as with the state-of-the-art oblivious router used in contemporary parallel machines. The Chaos router is argued to approximate the oblivious router for raw performance and to be superior (as indicated in simulation studies) at high traffic loads and nonuniform loads.",
    "actual_venue": "Heinz Nixdorf Symposium"
  },
  {
    "abstract": "The use of intravenous thrombolysis for stroke is limited by contraindications that may be difficult to identify promptly and accurately. Evidence supports the use of information technology-based clinical decision support (CDS) tools to achieve improvements in care delivery. The objective of this pilot study was to investigate the efficacy of a CDS tool to screen health records for contraindications to intravenous stroke thrombolysis.A CDS tool was developed to rapidly screen health information in seven affiliated hospitals for contraindications to stroke thrombolysis. A fixed-sequence, 2-period crossover study was conducted to test the efficacy of the CDS tool. Four mock patient records derived from the stroke registry that contained a total of nine contraindication items in two or more of the hospitals were used for testing purposes. The test patients were preset and balanced between groups with and without the CDS tool appearing six times in each group before recruiting the participating physicians. Physicians who were responsible for thrombolytic therapy and willing to sign informed consent were recruited. The participating physicians were asked to check a list of contraindications for two of the patients by using a shared electronic medical record system among the seven hospitals with and without the CDS tool. The test time and missed contraindications were recorded and analyzed statistically.A total of 14 physicians who were responsible for stroke thrombolysis were approached, and 12 signed informed consent and took the test. By using the CDS tool, the test time was reduced significantly from 14.6âÂ±â7.4 to 7.3âÂ±â5.2 min (Pâ=â0.010). In a total of 54 contraindications, the number of missed contraindications was reduced significantly from 23 (42.6 %) to seven (13.0 %) (Pâ=â0.001). The difference of missed contraindication number between the two groups was statistically significant either per physician or per contraindication item.By screening health records for relevant contraindications, the use of a CDS tool may reduce the time needed to review medical records and reduce the number of missed contraindications for stroke thrombolysis.",
    "actual_venue": "Bmc Med Inf And Decision Making"
  },
  {
    "abstract": "Continuous-time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller 2002; 2003), are an elegant modeling language for structured stochastic processes that evolve over continuous time. The CTBN framework is based on homogeneous Markov processes, and defines two distributions with respect to each local variable in the system, given its parents: an exponential distribution over when the variable transitions, and a multinomial over what is the next value. In this paper, we present two extensions to the framework that make it more useful in modeling practical applications. The first extension models arbitrary transition time distributions using Erlang-Coxian approximations, while maintaining tractable learning. We show how the censored data problem arises in learning the distribution, and present a solution based on expectation-maximization initialized by the Kaplan-Meier estimate. The second extension is a general method for reasoning about negative evidence, by introducing updates that assert no observable events occur over an interval of time. Such updates were not defined in the original CTBN framework, and we show that their inclusion can significantly improve the accuracy of filtering and prediction. We illustrate and evaluate these extensions in two real-world domains, email use and GPS traces of a person traveling about a city.",
    "actual_venue": "Aaai"
  },
  {
    "abstract": "We present a strategy for optimizing parallel algorithms introducing redundant computations. In order to calculate the optimal amount of redundancy, we generalize the LogP model to capture messages of varying sizes using functions instead of constants for the machine parameters. We validate our method for a wave simulation algorithm on a Parsytec PowerXplorer with eight processors and a workstation cluster with fourworkstations.",
    "actual_venue": "Apdc"
  },
  {
    "abstract": "Cultural approaches to technology enablement must include type of content and type of medium with type of culture. Refining\n beyond shared interpretations of content by culture we need to examine cognitive mechanisms that may underpin the cultural\n modes of assimilation of e-learning and enlivening knowledge management. The paper compares western and Asian ( Malaysian)\n contexts for learning. Design implications are alluded to.",
    "actual_venue": "Etrain"
  },
  {
    "abstract": "Let (mathcal {A}) be a von Neumann algebra acting on the complex Hilbert space (mathcal {H}) and (Phi {:},mathcal {A} longrightarrow mathcal {A}) be a surjective map that satisfies the condition $$begin{aligned} Phi (T)Phi (P)+Phi (P)Phi (T)^*=TP+PT^* end{aligned}$$for all T and all projections P in (mathcal {A}). We characterize the concrete form of (Phi ) on selfadjoint elements of (mathcal {A}). Also when (mathcal {A}) is a factor von Neumann algebra, it is shown that (Phi ) is either of the form (Phi (T)=T+itau (T) I) or of the form (Phi (T)=-T+itau (T)I), where (tau {:},mathcal {A}longrightarrow mathbb {R}) is a real map.",
    "actual_venue": "Periodica Mathematica Hungarica"
  },
  {
    "abstract": "Various types of scalable and high performance computing systems are developed to realize distributed applications. In order to realize not only scalable but also high performance information systems, serverclusterc systems are widely used. In server cluster systems, the larger electric energy is consumed since application processes are performed on the larger number of servers. The improved delay time-based (IDTB)algorithm is discussed to select a server for each request process in a homogeneous server cluster so that the total energy consumption of the homogeneous server cluster to perform application processes can be reduced. In a homogeneous cluster, every server follows the same computation model and power consumption model. In heterogeneous cluster, servers follow different types of computation models and power consumption models. In this paper, we propose an extended IDTB (EIDTB) algorithm to reduce the total energy consumption and response time of each process in heterogeneous server cluster. We evaluate the EIDTB algorithm compared with the IDTB algorithm in a heterogeneous server cluster. In the evaluation, we show the average total energy consumption of a heterogeneous server cluster in the EIDTB algorithm can be reduced maximum 57% of the IDTB algorithm.",
    "actual_venue": "Aina"
  },
  {
    "abstract": "A ( q , r ) -tree-coloring of a graph G is a q-coloring of vertices of G such that the subgraph induced by each color class is a forest of maximum degree at most r. An equitable ( q , r ) -tree-coloring of a graph G is a ( q , r ) -tree-coloring such that the sizes of any two color classes differ by at most one. Let the strong equitable vertex r-arboricity of G, denoted by v a r Åº ( G ) , be the minimum p such that G has an equitable ( q , r ) -tree-coloring for every q Åº p .Z. Guo, H. Zhao, Y. Mao 4 found the exact values of v a 2 Åº ( K n , n , n ) for each n except when n is divisible by 20. In this paper, we find the exact value for each v a 2 Åº ( K m , n ) and v a 2 Åº ( K l , m , n ) . Equitable vertex arboricity has many applications such as scheduling and resource allocation.Z. Guo, H. Zhao, Y. Mao 4 found the exact values of equitable vertex 2-arboricity of K n , n , n except when n is divisible by 20.We find a linear-time algorithm to find the exact values of equitable vertex 2-arboricity for all K m , n and K l , m , n .",
    "actual_venue": "Inf Process Lett"
  },
  {
    "abstract": "Many basic locomotor patterns of living bodies are rhythmic, and oscillatory components of physical systems effectively contribute to the generation of the movement. The control signals for the basic locomotor patterns are generated by the central pattern generator (CPG), which is composed of collective neural oscillators, and the activity of the CPG is tightly synchronized with the movement of the physical systems. That is, appropriate locomotor patterns are realized by mutual synchronization between the physical system and the neural system. In this article a simple learning model is proposed to acquire an appropriate parameter set, the intrinsic frequency of the CPG, and the interaction between the CPG and the physical system, in order to obtain a desired locomotor pattern. The performance of the proposed learning model is confirmed by computer simulations and an adaptive control experiment of a one-dimensional hopping robot.",
    "actual_venue": "Adaptive Behaviour"
  },
  {
    "abstract": "MapReduce is an emerging programming model for data-intense application proposed by Google, which has attracted a lot of attention recently. MapReduce borrows from functional programming, where programmer defines Map and Reduce tasks executed on large set of distributed data. In this paper we propose an implementation of the MapReduce programming model. We present the architecture of the prototype based on Bit Dew, a middleware for large scale data management on Desktop Grid. We describe the set of features which makes our approach suitable for large scale and loosely connected Internet Desktop Grid: massive fault tolerance, replica management, barriers-free execution, latency-hiding optimisation as well as distributed result checking. We also present performance evaluation of the prototype both against micro-benchmarks and real MapReduce application. The scalability test shows that we achieve linear speedup on the classical Word Count benchmark. Several scenarios involving lagger hosts and host crashes demonstrate that the prototype is able to cope with an experimental context similar to real-world Internet.",
    "actual_venue": "Pgcic"
  },
  {
    "abstract": "We have investigated an active size controlled droplet generation system by using magnetically driven microtool (MMT). With a lateral motion of the MMT in microchannels, the continuous phase can be pinched off by the movement of MMT to obtain size-controlled droplets actively. With this method, particle-enclosed droplet can be produced on demand to fit the size of each enclosed particle, and which is difficult to carry out only by the fluid dynamic force. For the current study, the system has been evaluated in terms of the frequency of the actuation of MMT and the size of the produced droplets, and found out the response time to change the droplet size by MMT actuation is one third of that only by the fluid dynamic force. This system contributes to the effective transportation of cells in microchannel.",
    "actual_venue": "Iros"
  },
  {
    "abstract": "Previous work on iris recognition focused on either visible light (VL), near-infrared (NIR) imaging, or their fusion. However, limited numbers of works have investigated cross-spectral matching or compared the iris biometric performance under both VL and NIR spectrum using unregistered iris images taken from the same subject. To the best of our knowledge, this is the first work that proposes a framework for cross-spectral iris matching using unregistered iris images. To this end, three descriptors are proposed namely, Gabor-difference of Gaussian (G-DoG), Gabor-binarized statistical image feature (G-BSIF), and Gabor-multi-scale Weberface (G-MSW) to achieve robust cross-spectral iris matching. In addition, we explore the differences in iris recognition performance across the VL and NIR spectra. The experiments are carried out on the UTIRIS database which contains iris images acquired with both VL and NIR spectra for the same subject. Experimental and comparison results demonstrate that the proposed framework achieves state-of-the-art cross-spectral matching. In addition, the results indicate that the VL and NIR images provide complementary features for the iris pattern and their fusion improves notably the recognition performance.",
    "actual_venue": "Ipsj Trans Computer Vision And Applications"
  },
  {
    "abstract": ". We present regular model checking, a framework for algorithmicverification of infinite-state systems with, e.g., queues, stacks,integers, or a parameterized linear topology. States are represented bystrings over a finite alphabet and the transition relation by a regularlength-preserving relation on strings. Major problems in the verificationof parameterized and infinite-state systems are to compute the set ofstates that are reachable from some set of initial states, and to...",
    "actual_venue": "CAV"
  },
  {
    "abstract": "Currently, most robust classifiers with parameters focus on the determination of the optimal or suboptimal parameters. There are no research studies or even discussions about robust classifiers on all of the possible parameters. This paper considers the robust rough classifier and finds that the robust rough classifier satisfies a nested topological structure; then, the nested classifier, which re...",
    "actual_venue": "Ieee Transactions On Fuzzy Systems"
  },
  {
    "abstract": "Speaker variability strongly impacts human perception and technology performance, yet large-scale, systematic study of the acoustic characteristics involved is rarely undertaken. This study provides statistics on selected segmental and suprasegmental acoustic parameters from measures made on spontaneous conversational telephone speech from 160 speakers in the Switchboard Corpus.([1]) Since spontaneous conversational speech is more dynamically variable than read speech and is representative of actual human communication, it was preferred for our applied research purposes.",
    "actual_venue": "Icslp - Fourth International Conference On Spoken Language Processing, Proceedings, Vols"
  },
  {
    "abstract": "With scientific data available at geocoded locations, investigators are increasingly turning to spatial process models for carrying out statistical inference. However, fitting spatial models often involves expensive matrix decompositions, whose computational complexity increases in cubic order with the number of spatial locations. This situation is aggravated in Bayesian settings where such computations are required once at every iteration of the Markov chain Monte Carlo (MCMC) algorithms. In this paper, we describe the use of Variational Bayesian (VB) methods as an alternative to MCMC to approximate the posterior distributions of complex spatial models. Variational methods, which have been used extensively in Bayesian machine learning for several years, provide a lower bound on the marginal likelihood, which can be computed efficiently. We provide results for the variational updates in several models especially emphasizing their use in multivariate spatial analysis. We demonstrate estimation and model comparisons from VB methods by using simulated data as well as environmental data sets and compare them with inference from MCMC.",
    "actual_venue": "Computational Statistics And Data Analysis"
  },
  {
    "abstract": ". We demonstrate a system capable of tracking, in real worldimage sequences, landmarks such as eyes, mouth, or chin on a face. In afirst version knowledge previously collected about faces is used for findingthe landmarks in the first frame. In a second version the system is ableto track the face without any prior knowledge about faces and is thusapplicable to other object classes.1 IntroductionIn the last decade there has been much development in the area of face recognition:...",
    "actual_venue": "Icann"
  },
  {
    "abstract": "In wireless ad hoc and sensor networks, energy is a scarce resource and a considerable amount of energy is dissipated due to interference. Therefore, interference is one of the major challenges in wireless ad hoc and sensor networks. It alters or disrupts a message as it is being transmitted along a channel between source and destination. Since the messages are disrupted when the interference occurs, they have to be detected and the interfered messages have to be retransmitted. In this paper, we propose central and distributed heuristic algorithms for reducing average interference in a receiver-centric interference model. In the literature, the minimum spanning tree (MST) algorithm is generally used through the interference coverage graph directly or indirectly in order to generate minimum average interference topology. Our algorithm, dynamic average interference (DAI), however, generates lower average interference as well as more sparse topology than MST. We realized that if the transmission ranges of nodes are taken into consideration at each stage of the topology control algorithm, the interference of links are changed dynamically. This interference changing enables up to 22% more energy saving than the MST algorithm. Thus, DAI provides energy saving by reducing the interference as far as possible in generated topology.",
    "actual_venue": "Comput J"
  },
  {
    "abstract": "Matroids generalize the familiar notion of linear dependence from linear algebra. Following a brief discussion of founding work in computability and matroids, we use the techniques of reverse mathematics to determine the logical strength of some basis theorems for matroids and enumerated matroids. Next, using Weihrauch reducibility, we relate the basis results to combinatorial choice principles and statements about vector spaces. Finally, we formalize some of the Weihrauch reductions to extract related reverse mathematics results. In particular, we show that the existence of bases for vector spaces of bounded dimension is equivalent to the induction scheme for Sigma(0)(2) formulas.",
    "actual_venue": "Computability And Complexity: Essays Dedicated To Rodney G Downey On The Occasion Of His Birthday"
  },
  {
    "abstract": "A prototype electrical impedance tomography system was evaluated prior to its use for the detection of intraperitoneal bleeding, with the assistance of patients undergoing continuous ambulatory peritoneal dialysis (CAPD), The system was sensitive enough to detect small amounts of dialysis fluid appearing in subtractive images over short time periods. Uniform sensitivity to blood appearing anywhere within the abdominal cavity was produced using a post-reconstructive filter that corrected for changes in apparent resistivity of anomalies with their radial position, The image parameter used as an indication of fluid quantity, the resistivity index, varied approximately linearly with the quantity of fluid added. A test of the system's response to the introduction of conductive fluid out of the electrode plane (when a blood-equivalent fluid was added to the stomach) found that the sensitivity of the system was about half that observed in the electrode plane. Breathing artifacts were found to upset quantitative monitoring of intraperitoneal bleeding, but only on time scales short compared with the fluid administration rate. Longer term breathing changes, such as those due to variations in the functional residual capacity of the lungs, should ultimately limit the sensitivity over long time periods.",
    "actual_venue": "Ieee Trans Biomed Engineering"
  },
  {
    "abstract": "This work develops and evaluates new algorithms based on GARCH models, neural networks and boosting techniques, designed to model and predict heteroskedastic time series. The main novel elements of these new algorithms are as follows: (a) in regard to neural networks, the simultaneous estimation of the conditional mean and volatility through the maximization of likelihood; (b) in regard to boosting, its simultaneous application to mean and variance components of the likelihood, and the use of likelihood-based models (e.g., GARCH) as the base hypothesis rather than gradient fitting techniques using least squares. The behavior of the proposed algorithms is evaluated over simulated data and over the Standard & Poor's 500 Index returns series, resulting in frequent and significant improvements in relation to the ARMA-GARCH models.",
    "actual_venue": "Mathematical And Computer Modelling"
  },
  {
    "abstract": "Fast scalable collective operations are very important for achieving good application performance for highly parallel systems. This paper discusses scalability and absolute performance of various algorithms for collective communication. Performance is measured on the world largest SCI cluster, a machine with 96 dual Pentium II nodes interconnected with SCI, showing good scalability and performance.",
    "actual_venue": "Pvm/Mpi"
  },
  {
    "abstract": "The vertical industry software market (for hotels, hospitals, convenience store chains, ...) is highly competitive. To better compete in this environment, TecnologÃ­a Computacional Aplicada (TCA) ---a mid size vertical market IS development company--- decided four years ago to explore agent technologies as a key component for a new business strategy. As a result of this exercise TCA converted its legacy INNSIST hotel information system ---with 1.2 million lines of code developed over 22 years and a solid customer base--- into the fully deployed agent-intensive system whose main features we present here. A previous stage of this project was presented in [2].",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "Node searching in delay tolerant networks is of great importance for different applications, in which a locator node finds a target node in person. In the previous distributed node searching method, a locator traces the target along its movement path from its most frequently visited location. For this purpose, nodes leave traces during their movements and also store their long-term movement patterns in their frequently visited locations i.e., preferred locations. However, such tracing leads to a long delay and high overhead on the locator by long-distance moving. Our trace data study confirms these problems and provides the foundation of our design of a new node searching method, called target-oriented method TSearch. By leveraging social network properties, TSearch aims to enable a locator to directly move toward the target. Nodes create encounter records ERs indicating the locations and times of their encounters and make the ERs easily accessible by locators through message exchanges or a hierarchical structure. In node searching, a locator follows the targetâs latest ER, the latest ERs of its friends i.e., frequently meeting nodes, its preferred locations, and the targetâs possible locations deduced from additional information for node searching. Extensive trace-driven and real-world experiments show that TSearch achieves significantly higher success rate and lower delay in node searching compared with previous methods.",
    "actual_venue": "Ieee/Acm Trans Netw"
  },
  {
    "abstract": "An important concern in the design of a publish/subscribe system is its expressiveness, which is the ability to represent various types of information in publications and to precisely select information of interest through subscriptions. We present an enhancement to existing content-based publish/subscribe systems with support for a 2D spatial data type and eight associated relational operators, including those to reveal overlap, containment, touching, and disjointedness between regions of irregular shape. We describe an algorithm for evaluating spatial relations that is founded on a new dynamic discretization method and region-intersection model. In order to make the data type practical for large-scale applications, we provide an indexing structure for accessing spatial constraints and develop a simplification method for eliminating redundant constraints. Finally, we present the results of experiments evaluating the effectiveness and scalability of our approach.",
    "actual_venue": "Middleware"
  },
  {
    "abstract": "Motivation and problem-domain preferences of scientists can affect aggregate level emergence and growth of problem domains in science. In this study, we introduce an agent-based model that is based on information foraging and expectancy theory to examine the impact of rationality and openness on the growth and evolution of scientific domains. We simulate a virtual socio-technical system, in which scientists with different preferences search for problem domains to contribute knowledge, while considering their motivational gains. Problem domains become mature and knowledge spills occur over time to facilitate creation of new problem domains. We conduct experiments to examine emergence and growth of clusters of domains based on local interactions and preferences of scientists and present preliminary qualitative observations.",
    "actual_venue": "Agent Directed Simulation"
  },
  {
    "abstract": "This paper develops two themes. The first is to show that the task of ârecognitionâ can be realized by an artificial neural net, and that the structures thus generated can be in close analogy to those observed in nature. The second is to note that absolute signal levels are almost meaningless in biological contexts; for these, the current state of a system is revealed by dynamic rather than static behaviour. This was Walter Freeman's observation, which led him to the oscillatory models which he has developed so notably; see the reference list. Coupling of the two sets of ideas leads to some striking conclusions, developed in Whittle (Neural Nets and Chaotic Carriers (1998) and Neural Nets and Chaotic Carriers, 2nd edn. (2010). Imperial College Press, London).",
    "actual_venue": "Comput J"
  },
  {
    "abstract": "Parameterization is an eective technique for building flex- ible, reusable software. When dealing with parameterized components, an important concern is the time at which pa- rameters are bound. Many languages provide syntactic sup- port for parameterized components; this mode of parameter- ization can be called static parameterization. In order to be able to support dynamic reconfiguration, the Service Facility pattern has been proposed as an enabling technology for dy- namic parameterization. However, static parameterization has the advantage of strong type-checking that dynamic pa- rameterization does not. In this paper, we present DynIn- staCheck â a tool that automatically instruments dynami- cally bound parameterized components with run-time check- ing code that ensures type-safe parameter binding. The source instrumentation is done in a non-intrusive way, using aspect-oriented programming.",
    "actual_venue": "Acm Symposium On Applied Computing"
  },
  {
    "abstract": "The use of a service-oriented architecture (SOA) has been identified as a promising approach for improving health care by facilitating reliable clinical decision support (CDS). A review of the literature through October 2013 identified 44 articles on this topic. The review suggests that SOA related technologies such as Business Process Model and Notation (BPMN) and Service Component Architecture (SCA) have not been generally adopted to impact health IT systems' performance for better care solutions. Additionally, technologies such as Enterprise Service Bus (ESB) and architectural approaches like Service Choreography have not been generally exploited among researchers and developers. Based on the experience of other industries and our observation of the evolution of SOA, we found that the greater use of these approaches have the potential to significantly impact SOA implementations for CDS",
    "actual_venue": "Journal Of Medical Systems"
  },
  {
    "abstract": "This paper describes the microarchitecture of the RS64 IV, a multithreaded PowerPCÂ® processor, and its memory system. Because this processor is used only in IBM iSeriesâ¢ and pSeriesâ¢ commercial servers, it is optimized solely for commercial server workloads. Increasing miss rates because of trends in commercial server applications and increasing latency of cache misses because of rapidly increasing clock frequency are having a compounding effect on the portion of execution time that is wasted on cache misses. As a result, several optimizations are included in the processor design to address this problem. The most significant of these is the use of coarse-grained multithreading to enable the processor to perform useful instructions during cache misses. This provides a significant throughput increase while adding less than 5% to the chip area and having very little impact on cycle time. When compared with other performance-improvement techniques, multithreading yields an excellent ratio of performance gain to implementation cost. Second, the miss rate of the L2 cache is reduced by making it four-way associative. Third, the latency of cache-to-cache movement of data is minimized. Fourth, the size of the L1 caches is relatively large. In addition to addressing cache misses, pipeline \"holes\" caused by branches are minimized with large instruction buffers, large L1 I-cache fetch bandwidth, and optimized resolution of the branch direction. In part, the branches are resolved quickly because of the short but efficient pipeline. To minimize pipeline holes due to data dependencies, the L1 D-cache access is optimized to yield a one-cycle load-to-use penalty.",
    "actual_venue": "Ibm Journal Of Research And Development"
  },
  {
    "abstract": "In the current work, we propose a two-step optimisation method to determine the coefficients of the Prony series expansion for a viscoelastic constitutive model by combining the benefits of particle swarm optimisation and linear least square solver. The entire fitting progress is decoupled and divided into two portions. Each optimisation approach is used for obtaining one set of parameters. This method overcomes the usual difficulties of original function determinations and could find the global optimal solution with a high precision. The quality of the developed method is verified by three examples on both time-domain and frequency-domain experimental data. Simulation results are consistent with corresponding experimental data, showing that the new technique is valid and applicable for estimating structural properties of viscoelastic materials.",
    "actual_venue": "International Journal Of Computing Science And Mathematics"
  },
  {
    "abstract": "In this paper, we first introduce the swing words. Based on intrinsic properties of these words, we present a new approach to solve the Circle Formation Problem in the semi-synchronous model (SSM)-- no two robots are supposed to be at the same position in the initial configuration. The proposed protocol is quiescent-- all the robots are eventually motionless in the desired configuration, which remains true thereafter. In SSM, the improvement of the latest recent work for this problem is twofold: (1) the protocol works for any number n of weak robots, except if n = 4, and (2) no robot is required to reach its computed destination in one step. Finally, starting from a biangular configuration, our protocol also solves CFP in the fully asynchronous model (CORDA). To our best knowledge, it is the first CFP protocol for SSM which is compatible with CORDA.",
    "actual_venue": "Sirocco"
  },
  {
    "abstract": "In spite of the importance of well-understo od semanÂ­ tics for knowledge representation systems, proponents of default logic have tended to ignore the lack of a general model-theoretic semantics for the formalism. This shortcoming is addressed by the presentation of such a model-theory. This characterization differs in some ways from traditional semantics. These differences are explained and motivated, and some applications of the semantics are discussed.",
    "actual_venue": "Ijcai"
  },
  {
    "abstract": "The per-method access control lists of standard internet technologies allow only simple forms of access control to be expressed and enforced. They also fail to enforce a strict need-to-know view of persistent data. Real applications require more flexible security constraints including parameter restrictions, logging of accesses and state-dependent access constraints. In particular, the concept of parameterised roles, central to a fine-grained specification of access rules and compliance with privacy laws, should be supported in a natural way. In this paper we demonstrate how an object-based approach using the mechanism of bracket capabilities can be used to enforce various kinds of access constraints including discretionary, mandatory and parameterised role-based access control. We give examples from a health information system incorporating secure patient access and secure access by appropriate medical and administrative personnel.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "Software Transactional Memory (STM) Systems have been proposed in order to make parallel programs easier to develop and verify compared to conventional lock-based programming techniques. However, conventional STMs do not scale in performance to a large number of concurrent threads for several classes of applications. While the atomicity semantics of traditional STMs greatly simplify the correct sharing of data between threads, these same atomicity semantics incur a large penalty in program execution time.",
    "actual_venue": "Pact"
  },
  {
    "abstract": "A good radar warning receiver should observe a radar very soon after it begins transmitting, so in designing our radar warning receiver we would like to ensure that the intercept time is low or the probability of intercept after a specified time is high. We consider a number of problems concerning the overlaps or coincidences of two periodic pulse trains. We show that the first intercept time of two pulse trains started in phase is a homogeneous Diophantine approximation problem which can be solved using the convergents of the simple continued fraction (s.c.f.) expansion of the ratio of their pulse repetition intervals (PRIs). We find that the intercept time for arbitrary starting phases is an inhomogeneous Diophantine approximation problem which can be solved in a similar manner. We give a recurrence equation to determine the times at which subsequent coincidences occur. We then demonstrate how the convergents of the s.c.f. expansion can be used to determine the probability of intercept of the two pulse trains after a specified time when one or both of the initial phases are random. Finally, we discuss how the probability of intercept varies as a function of the PRIs and its dependence on the Farey points",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "In this position paper we support that there is still great potential for advancements in the research area of software clone refactoring, and argue about some possible research objectives and directions through illustrative examples.",
    "actual_venue": "Software Clones"
  },
  {
    "abstract": "Activity within the brain causes electrical potentials to exist on the scalp. The study of these potentials is generally referred to as electroencephalography (EEG). EEG is widely used in medical diagnosis and in biofeedback studies in which a person can learn to control some element of their EEG spectrum, usually in response to some visual stimulus. This paper reports the control of the alpha component of the EEG spectrum which does not require the learning that biofeedback demands. We show that participants can achieve rapid and reliable remote control of electrical devices using increase in alpha wave activity associated with reduced visual input. The physiological basis, technical achievements and challenges, and applications will be discussed.",
    "actual_venue": "Interact"
  },
  {
    "abstract": "This paper focuses on policy languages for (role-based) access control [14, 32], especially in their modern incarnations in the form of trust-management systems [9] and usage control [30, 31]. Any (declarative) approach to access control and trust management has to address the following issues: Explicit denial, inheritance, and overriding, and History-sensitive access control.Our main contribution is a policy algebra, in the timed concurrent constraint programming paradigm, that uses a form of default constraint programming to address the first issue, and reactive computing to address the second issue.The policy algebra is declarative --- programs can be viewed as imposing temporal constraints on the evolution of the system --- and supports equational reasoning. The validity of equations is established by coinductive proofs based on an operational semantics.The design of the policy algebra supports reasoning about policies by a systematic combination of constraint reasoning and model checking techniques based on linear time temporal-logic. Our framework permits us to perform security analysis with dynamic state-dependent restrictions.",
    "actual_venue": "Ppdp"
  },
  {
    "abstract": "Autonomous indoor navigation requires a robot to have abilities to recognize landmarks and to avoid obstacles. Our goal is to provide these abilities in a generic way so that the robot need not have an accurate and complete geometric model of all the objects in its environment. Rather, we wish to provide abilities to recognize common objects such as desks and doors which can be used as landmarks. We use a functionality-based representation for objects. Objects consist of functional parts, which are characterized by their significant surfaces, and by the accompanying functional evidence for them. Our system works with planar surfaces only and assumes that the objects are in a âstandardâ pose. The localization and orientation of an object are represented with the most significant surface in an âs-mapâ. Our system has been tested on a number of real scenes where it performs robustly and efficiently; some results are shown in the paper.",
    "actual_venue": "Image And Vision Computing"
  },
  {
    "abstract": "Spatial records of species are commonly misidentified, which can change the predicted distribution of a species obtained from a species distribution model (SDM). Experiments were undertaken to predict the distribution of real and simulated species using MaxEnt and presence-only data contaminated with varying rates of misidentification error. Additionally, the difference between the niche of the target and contaminating species was varied. The results show that species misidentification errors may act to contract or expand the predicted distribution of a species while shifting the predicted distribution towards that of the contaminating species. Furthermore the magnitude of the effects was positively related to the ecological distance between the species' niches and the size of the error rates. Critically, the magnitude of the effects was substantial even when using small error rates, smaller than common average rates reported in the literature, which may go unnoticed while using a standard evaluation method, such as the area under the receiver operating characteristic curve. Finally, the effects outlined were shown to impact negatively on practical applications that use SDMs to identify priority areas, commonly selected for various purposes such as management. The results highlight that species misidentification should not be neglected in species distribution modeling.",
    "actual_venue": "Isprs International Journal Of Geo-Information"
  },
  {
    "abstract": "In this paper we will compare the integral model suggested by Gautier and Khalil for identification of robot dynamics with the differential equations of the robot. The advantage of identification by the integral model is, that no measurement of acceleration is required. Although appealing at a first glance, this approach yields problems in conjunction with the necessary least squares algorithm for parameter estimation. We even succeeded in getting better results by employing the differential equ ations, without measurement of acceleration . It will be shown, how to achieve reasonable good results with low noise by just derivating the position s ignals. Finally, our results will be illustrated by experiments wit h a direct driven two link robot.",
    "actual_venue": "San Diego, Ca"
  },
  {
    "abstract": "Distributed cache systems are used to store and retrieve frequently used data for faster access by exploiting the memory of more than one machine, but they appear as one logical big cache. In this paper, we studied the performance of two popular open source distributed cache systems (Hazelcast and Infinispan) indifferently. The conducted performance analysis shows that Infinispan outperforms Hazelcast in the simple data retrieval scenarios as well as most of SQL-like queries scenarios, whereas Hazelcast outperforms Infinispan in SQL-like queries for small data sizes.",
    "actual_venue": "Icpe"
  },
  {
    "abstract": "Polarimetric synthetic aperture radar is expected to distinguish wet snow from bare ground. However, since both of them show surface scattering, which is sensitive to incidence angle, it often fails in the distinction in mountainous areas. In this letter, we propose an adaptive distinction method using quaternion neural networks. In the ALOS-2 data, we find a monotonic and nonlinear dependence of ...",
    "actual_venue": "Ieee Geoscience And Remote Sensing Letters"
  },
  {
    "abstract": "This paper studies large population dynamic games involving nonlinear stochastic dynamical systems with agents of the following mixed types: (i) a major agent and (ii) a population of N minor agents where N is very large. The major and minor agents are coupled via both (i) their individual nonlinear stochastic dynamics and (ii) their individual finite time horizon nonlinear cost functions. This problem is analyzed by the so-called epsilon-Nash mean field game theory. A distinct feature of the mixed agent mean field game problem is that even asymptotically (as the population size N approaches infinity) the noise process of the major agent causes random fluctuation of the mean field behavior of the minor agents. To deal with this, the overall asymptotic (N -> infinity) mean field game problem is decomposed into (i) two nonstandard stochastic optimal control problems with random coefficient processes which yield forward adapted stochastic best response control processes determined from the solution of (backward in time) stochastic Hamilton-Jacobi-Bellman (SHJB) equations and (ii) two stochastic coefficient McKean-Vlasov (SMV) equations which characterize the state of the major agent and the measure determining the mean field behavior of the minor agents. This yields a stochastic mean field game (SMFG) system which is in contrast to the deterministic mean field game systems of standard MFG problems with only minor agents. Existence and uniqueness of the solutions to SMFG systems (SHJB and SMV equations) is established by a fixed point argument in the Wasserstein space of random probability measures. In the case where minor agents are coupled to the major agent only through their cost functions, the epsilon(N)-Nash equilibrium property of the SMFG best responses is shown for a finite N population system where epsilon(N) = O(1/root N).",
    "actual_venue": "Siam Journal On Control And Optimization"
  },
  {
    "abstract": "In this paper, we study hybrid contention-free/contention-based traffic management schemes in presence of delay-sensitive and delay-insensitive data in multihop CDMA wireless mesh networks. We suggest a greedy incremental contention-based ordering algorithm for contention-free schedules and also propose a time-scale-based framework for integration of contention and contention-free traffic management schemes. Further, for the contention-free phase, we propose a power control algorithm that gives an end-to-end throughput guarantee. With the aid of simulation, we observe the additional end-to-end throughput that can be achieved when scheduling and tight power control are applied.",
    "actual_venue": "Computer Communications"
  },
  {
    "abstract": "The accurate and reliable estimation of fiber orientation distributions, based on diffusion-sensitized magnetic resonance images is a major prerequisite for tractography algorithms or any other derived statistical analysis. In this work, we formulate the principle of fiber continuity (FC), which is based on the simple observation that the imaging of fibrous tissue implies certain expectations for ...",
    "actual_venue": "Ieee Transactions On Medical Imaging"
  },
  {
    "abstract": "Within the research on Micro Aerial Vehicles (MAVs), the field on flight control and autonomous mission execution is one of the most active. A crucial point is the localization of the vehicle, which is especially difficult in unknown, GPS-denied environments. This paper presents a novel vision based approach, where the vehicle is localized using a downward looking monocular camera. A state-of-the-art visual SLAM algorithm tracks the pose of the camera, while, simultaneously, building an incremental map of the surrounding region. Based on this pose estimation a LQG/LTR based controller stabilizes the vehicle at a desired setpoint, making simple maneuvers possible like take-off, hovering, setpoint following or landing. Experimental data show that this approach efficiently controls a helicopter while navigating through an unknown and unstructured environment. To the best of our knowledge, this is the first work describing a micro aerial vehicle able to navigate through an unexplored environment (independently of any external aid like GPS or artificial beacons), which uses a single camera as only exteroceptive sensor.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "Since the late 90's, 3DCG technology in both hardware and software has progressed and advanced very quickly, 3D animation becoming more and more familiar in our lives. The development of this technology has increased the amount of information that can be included in game design software in terms of sound and vision, thus enabling more complex, lifelike characters than before. Specifically looking at character animation, this aspect of design now commonly shows intricate facial expressions and hand finger movements, to name but two features. As a result of this, more sophisticated skills are required from the animators to achieve company and public expectation of game character believability. With this new expectation, animators need new ways to improve their skills in understanding movement, facial register, anatomy etc. To this end I have started to run workshops in collaboration with animation and game studios. This current workshop has been held for major game developers and schools since 1997. The workshop has proved a very effective approach to a deeper understanding of how in game characters should act. By partaking in, and learning physical theatre, animators, game developers and other 3D specialists increase their knowledge for that next all-important character.",
    "actual_venue": "Siggraph Asia Educators Program"
  },
  {
    "abstract": "In k-anycasting, a sensor wants to report event information to any k sinks in the network. This is important to gain in reliability and efficiency in wireless sensor and actor networks. In this paper, we describe KanGuRou, the first position-based energy efficient k-anycast routing which guarantees the packet delivery to k sinks as long as the connected component that contains s also contains sufficient number of sinks. A node s running KanGuRou first computes a tree including k sinks among the M available ones, with weight as low as possible. If this tree has mâ¥1 edges originated at node s, s duplicates the message m times and runs m times KanGuRou over a subset of defined sinks. Simulation results show that KanGuRou allows up to 62% of energy saving compared to plain anycasting.",
    "actual_venue": "Adhoc-Now"
  },
  {
    "abstract": "In this paper we develop a distributed rate control algorithm for multiple-unicast-sessions when network coding is allowed. Building on our recent flow-based characterization of network coding, we formulate the problem as a convex optimization problem. The formulation exploits pairwise coding possibilities between any pair of sessions, where the objective function is the sum of the utilities based on the rates supported by each session. With some manipulation on the Lagrangian of the formulated problem, a distributed algorithm is developed with no interaction between intermediate nodes, and each source having the freedom to choose its own utility function. The only information required by the source is the weighted sum of the queue length updates of each link, which can be piggy-backed on the acknowledgment messages. In addition to the optimal rate control algorithm, we propose a decentralized pairwise random coding scheme (PRC) that is optimal when a sufficiently large finite field is used for network coding. The convergence of the rate control algorithm is proved analytically and verified by extensive simulations. Simulations also demonstrate the advantage of our algorithm over the state-of-the-art in terms of throughput and fairness.",
    "actual_venue": "Phoenix, Az"
  },
  {
    "abstract": "Computational prediction of protein subcellular localization is a challenging problem. Several approaches have been presented during the past few years; some attempt to cover a wide variety of localizations, while others focus on a small number of localizations and on specific organisms. We present a comprehensive system, integrating protein sequence-derived data and text-based information. Itis tested on three large data sets, previously used by leading prediction methods. The results demonstrate that our system performs significantly better than previously reported results, for a wide range of eukaryotic subcellular localizations.",
    "actual_venue": "Pacific Symposium On Biocomputing"
  },
  {
    "abstract": "A joint source-channel coding (JSCC) scheme based on hybrid digital/analog coding is proposed for the transmission of correlated sources over discrete-memoryless two-way channels (DM-TWCs). The scheme utilizes the correlation between the sources in generating channel inputs, thus enabling the users to coordinate their transmission to combat channel noise. The hybrid scheme also subsumes prior coding methods such as rate-one separate source-channel coding and uncoded schemes for two-way lossy transmission, as well as the correlation-preserving coding scheme for (almost) lossless transmission. Moreover, we derive a distortion outer bound for the source-channel system using a genie-aided argument. A complete JSSC theorem for a class of correlated sources and DM-TWCs whose capacity region cannot be enlarged via interactive adaptive coding is also established. Examples that illustrate the theorem are given.",
    "actual_venue": "Ieee International Symposium On Information Theory"
  },
  {
    "abstract": "In this work are described the possibilities for improving efficiency of wood pellets production. The introductory part is devoted to analyzing the properties of wood pellets, combustion and production of pollutants when combusted it. The production of wood pellets and used pelleting machines was analyzed. The main task of this work is to introduce efficiency pelleting lines and cost reduction of the wood pellets production as fuel with a focus on the effects of adding additives. The results of experimental measurements and properties of wood pellets with different additives are presented in the final part.",
    "actual_venue": "International Journal Of Energy Optimization And Engineering"
  },
  {
    "abstract": "In continuous-time Kalman filtering for jump Markov systems, it is required that the measurement noise covariance be nonsingular. In this work, the case of noise-free measurements is considered and it is proposed that a reduced-order filter be used to overcome this singularity problem. This filter is optimal in the minimum variance sense and is of dimension (nâp) where n and p are the state and measurement vector dimensions, respectively. After the optimal filter equations are derived for the finite-time case, we focus on the infinite-time case and characterize the set of all assignable estimation error covariances and parametrize the set of all estimator gains. The conditions for the existence of the optimal steady-state filter are obtained in terms of the system theoretic properties of the original signal model.",
    "actual_venue": "Journal Of The Franklin Institute"
  },
  {
    "abstract": "The BaBar experiment at the Stanford Linear Accelerator Center (SLAC) is designed to perform a high precision investigation of the decays of B-meson produced from electron-positron interactions. The experiment, started in May 1999, will generate approximately 300 TB/year of data for 10 years. All of the data will reside in objectivity databases (object oriented databases), accessible via the Advanced Multi-threaded Server (AMS). To date, over 70 TB of data have been placed in Objectivity/DB, making it one of the largest databases in the world. Providing access to such a large quantity of data through a database server is a daunting task. A full-scale testbed environment had to be developed to tune various software parameters and a fundamental change had to occur in the AMS architecture to allow it to scale past several hundred terabytes of data. Additionally, several protocol extensions had to be implemented to provide practical access to large quantities of data. The paper describes the design of the database and the changes that we needed to make in the AMS for scalability reasons and how the lessons we learned would be applicable to virtually any kind of database server seeking to operate in the Petabyte region",
    "actual_venue": "Pittsburgh, Pa"
  },
  {
    "abstract": "This review presents results obtained from our group's approach to model quantum mechanics with the aid of nonequilibrium thermodynamics. As has been shown, the exact Schrodinger equation can be derived by assuming that a particle of energy. is actually a dissipative system maintained in a nonequilibrium steady state by a constant throughput of energy (heat flow). Here, also other typical quantum mechanical features are discussed and shown to be completely understandable within our approach, i.e., on the basis of the assumed sub-quantum thermodynamics. In particular, Planck's relation for the energy of a particle, the Heisenberg uncertainty relations, the quantum mechanical superposition principle and Born's rule, or the. dispersion of the Gaussian wave packet., a.o., are all explained on the basis of purely classical physics.",
    "actual_venue": "Entropy"
  },
  {
    "abstract": "Order-preserving encryption (OPE) schemes, whose ciphertexts preserve the natural ordering of the plaintexts, allow efficient range query processing over outsourced encrypted databases without giving the server access to the decryption key. Such schemes have recently received increased interest in both the database and the cryptographic communities. In particular, modular order-preserving encryption (MOPE), due to Boldyreva et al., is a promising extension that increases the security of the basic OPE by introducing a secret modular offset to each data value prior to encrypting it. However, executing range queries via MOPE in a naive way allows the adversary to learn this offset, negating any potential security gains of this approach. In this paper, we systematically address this vulnerability and show that MOPE can be used to build a practical system for executing range queries on encrypted data while providing a significant security improvement over the basic OPE. We introduce two new query execution algorithms for MOPE: our first algorithm is efficient if the user's query distribution is well-spread, while the second scheme is efficient even for skewed query distributions. Interestingly, our second algorithm achieves this efficiency by leaking the least-important bits of the data, whereas OPE is known to leak the most-important bits of the data. We also show that our algorithms can be extended to the case where the query distribution is adaptively learned online. We present new, appropriate security models for MOPE and use them to rigorously analyze the security of our proposed schemes. Finally, we design a system prototype that integrates our schemes on top of an existing database system and apply query optimization methods to execute SQL queries with range predicates efficiently. We provide a performance evaluation of our prototype under a number of different database and query distributions, using both synthetic and real datasets",
    "actual_venue": "Acm Sigmod Conference"
  },
  {
    "abstract": "In this paper, a new digital image stabilization involving multiple objects motion detection based on block-based Hough transform (HT) is presented. The line edges in a pair of edge blocks, where one is from current frame and the other is from previous frame are aligned to generate potential motion vectors to have votes on parameter voting matrix H using modified HT. The parameter voting matrix H is used for identifying feasible motion vectors, which generate the representative motion vector for compensation. The unexpected camera vibration is removed regardless of object's movement using the proposed image stabilization algorithm. The experimental results show that the proposed method outperforms traditional block-matching based digital image stabilization methods.",
    "actual_venue": "Iih-Msp"
  },
  {
    "abstract": "Recently the RFID has been received great attentions and gotten wide applications in many different areas including some administration systems for automatically managing students in a campus. However a RFID tag may infringe on its owner's location privacy because of its traceability. Therefore, location privacy problems in RFID-based student administration systems become a critical issue in applying such systems to the practical. Three representative schemes to solve these problems are explained and analyzed Moreover, a safe, fast and low cost scheme which offers better privacy protections is proposed In this scheme, a hash value is generated from a secret ID and a random number in a RFID tag by using a cheap hash circuit, and it is used for an identifier of a user. The secret ID is updated by using the hash circuit when a reader for updating is used. Finally, some comparisons between the proposed scheme and the other three typical schemes are presented in terms of safety, speed and cost.",
    "actual_venue": "Ninth International Workshop On Database And Expert Systems Applications, Proceedings"
  },
  {
    "abstract": "Standard compressive sensing results state that to exactly recover an s sparse signal in R^p, one requires O(s. log(p)) measurements. While this bound is extremely useful in practice, often real world signals are not only sparse, but also exhibit structure in the sparsity pattern. We focus on group-structured patterns in this paper. Under this model, groups of signal coefficients are active (or inactive) together. The groups are predefined, but the particular set of groups that are active (i.e., in the signal support) must be learned from measurements. We show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery, and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of groups under consideration, and not the particulars of the groups (e.g., compositions, sizes, extents, overlaps, etc.). Experiments show that our result holds for a variety of overlapping group configurations.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "One of the most powerful techniques for solving centralized constraint satisfaction problems (CSPs) consists of maintaining local consistency during backtrack search (e.g. [11]). Yet, no work has been reported on such a combination in asynchronous settings. The difficulty in this case is that, in the usual algorithms, the instantiation and consistency enforcement steps must alternate sequentially. When brought to a distributed setting, a similar approach forces the search algorithm to be synchronous in order to benefit from consistency maintenance. Asynchronism [24,14] is highly desirable since it increases flexibility and parallelism, and makes the solving process robust against timing variations. One of the most well-known asynchronous search algorithms is Asynchronous Backtracking (ABT). This paper shows how an algorithm for maintaining consistency during distributed asynchronous search can be designed upon ABT. The proposed algorithm is complete and has polynomial-space complexity. Since the consistency propagation is optional, this algorithms generalizes forward checking as well as chronological backtracking. An additional advance over existing centralized algorithms is that it can exploit available backtracking-nogoods for increasing the strength of the maintained consistency. The experimental evaluation shows that it can bring substantial gains in computational power compared with existing asynchronous algorithms.",
    "actual_venue": "CP"
  },
  {
    "abstract": "The focus of this paper is on cache-conscious data layout optimizations. Although these optimizations have already been adopted by industrial compilers, they were shown to be inefficient for multi-process applications on multi-core platforms. Such factors as asymmetric distribution of processes over hardware resources (cores, cpus or hardware threads), along with their temporal migrations, unpredictably influence optimization results. Herein we present a new methodology that extends classical data layout optimizations to support multi-core architectures. Based on data trace collection that reflects actual interleaving of data accesses, this method aims to improve spatial locality of the data, while mitigating potential false sharing events. Introduction of architectural characteristics into an analysis phase further increases the accuracy of data affinity estimation. Feasibility study of this method, applied to multi-process webserver lighttpd on Power5 machine, not only showed performance improvement, but also proved its suitability for incorporation into an industrial compiler.",
    "actual_venue": "Hipeac"
  },
  {
    "abstract": "The structure theorem of Joyal, Street and Verity says that every traced monoidal category arises as a monoidal full subcategory of the tortile monoidal category Int. In this paper we focus on a simple observation that a traced monoidal category is closed if and only if the canonical inclusion from into Int has a right adjoint. Thus, every traced monoidal closed category arises as a monoidal co-reflexive full subcategory of a tortile monoidal category. From this, we derive a series of facts for traced models of linear logic, and some for models of fixed-point computation. To make the paper more self-contained, we also include various background results for traced monoidal categories.",
    "actual_venue": "Mathematical Structures In Computer Science"
  },
  {
    "abstract": "As scientists continue to migrate their work to computational methods, it is important to track not only the steps involved in the computation but also the data consumed and produced. While this provenance information can be captured, in existing approaches, it often contains only weak references between data and provenance. When data files or provenance are moved or modified, it can be difficult to find the data associated with the provenance or to find the provenance associated with the data. We propose a persistent storage mechanism that manages input, intermediate, and output data files, strengthening the links between provenance and data. This mechanism provides better support for reproducibility because it ensures the data referenced in provenance information can be readily located. Another important benefit of such management is that it allows caching of intermediate data which can then be shared with other users. We present an implemented infrastructure for managing data in a provenance-aware manner and demonstrate its application in scientific projects.",
    "actual_venue": "Ssdbm"
  },
  {
    "abstract": "We introduce a fibrational semantics for many-valued logic programming, use it to define an SLD-resolution for annotation-free many valued logic programs as defined by Fitting, and prove a soundness and completeness result relating the two. We show that fibrational semantics corresponds with the traditional declarative (ground) semantics and deduce a soundness and completeness result for our SLD-resolution algorithm with respect to the ground semantics.",
    "actual_venue": "Jelia"
  },
  {
    "abstract": "Smartphones and other handheld devices have become popular and powerful Internet access devices, yet the Web is still largely optimized for the desktop. We describe a system that automatically transforms desktop-optimized pages to ones better suited to the target device. The system leverages existing platform-customized sites as examples of good design, identifies consistent components across these sites, and renders the desktop page into these components.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "We are concerned with dealing with performance requirements, such as achieve good time performance for retrieving tax appeals, during the development of information systems. We adapt a framework for non-functional requirements (global quality requirements) by treating (potentially conflicting or synergistic) performance requirements as goals. Our Performance Framework helps a developer to refine goals, select among competing implementation alternatives, justify implementation decisions, and evaluate the degree to which requirements are met. For manageability of development, we represent and organise knowledge about information systems and their design, implementation and performance. This paper further organises methods for dealing with performance goals, with some focus on implementation of long-term processes and integrity constraints. We illustrate the framework using some actual workload descriptions of a taxation appeals system, and describe a prototype development tool, currently under development.",
    "actual_venue": "Edbt"
  },
  {
    "abstract": "The set-partitioning problem (SPP) is widely known for both its application potential and its computational challenge. This NP-hard problem is known to pose considerable difficulty for classical solution methods such as those based on LP technologies. In recent years, the unconstrained binary quadratic program has proven to perform well as a unified modeling and solution framework for a variety of IP problems. In this paper we illustrate how this unified framework can be applied to SPP. Computational experience is presented, illustrating the attractiveness of the approach.",
    "actual_venue": "Computers And Or"
  },
  {
    "abstract": "The current study sought to determine the relative contributions of suprasegmental and segmental features to the perception of foreign accent and intelligibility in both first language (L1) and second language (L2) German and English speech. Suprasegmental and segmental features were manipulated independently by transferring (1) native intonation contours and/or syllable durations onto non-native segments and (2) non-native intonation contours and/or syllable durations onto native segments in both English and German. These resynthesized stimuli were then presented, in an intelligibility task, to native speakers of German and English who were proficient in both languages. Both of these groups of speakers and monolingual native speakers of English also rated the foreign accentedness of the manipulated stimuli. In general, tokens became more accented and less intelligible, the more they were manipulated. Tokens were also less accented and more intelligible when produced by speakers of (and in) the listeners' L1. Nonetheless, in certain L2 productions, there was both a reduction in perceived accentedness and decreased intelligibility for tokens in which native prosody was applied to non-native segments, indicating a disconnect between the perceptual processing of intelligibility and accent.",
    "actual_venue": "Speech Communication"
  },
  {
    "abstract": "Traditional induction variable (IV) analyses focus on computing the closed form expressions of variables. This paper presents a new IV analysis based on a property called distance interval. This property captures the value changes of a variable along a given control-flow path of a program. Based on distance intervals, an efficient algorithm detects dependences for array accesses that involve induction variables. This paper describes how to compute distance intervals and how to compute closed form expressions and test dependences based on distance intervals. This work is an extension of the previous induction variable analyses based on monotonic evolution [11]. With the same computational complexity, the new algorithm improves the monotonic evolution-based analysis in two aspects: more accurate dependence testing and the ability to compute closed form expressions. The experimental results demonstrate that when dealing with induction variables, dependence tests based on distance intervals are both efficient and effective compared to closed-form based dependence tests.",
    "actual_venue": "Lcpc"
  },
  {
    "abstract": "Managing process diversity becomes increasingly relevant in software development. Software organizations typically do not work on the greenfield and thus need to integrate external workflows with R&D internal workflow management and heterogeneous development and maintenance processes. To stay competitive with its software development, Alcatel has put in place an orchestrated improvement program of its processes and the underlying engineering tools environment. Why do we call this âe-R&Dâ? For two reasons. These improvement activities necessarily fit into the wider context of Alcatel's business process improvement and corporate e-business initiatives. The âe-R&Dâ also means enabling of interactive R&D processes and increasing collaborative work across the globe. At Alcatel we realized, during a substantial reengineering of our development and industrialization processes, that the approach to acquire an off-the-shelf process and tailor it to our needs was not applicable. Different processes need to be seamlessly integrated to avoid inconsistencies and inefficiency caused by replicated work. Specific focus is given within this article on how we manage process diversity in a product line where various components are embedded in individual architectures, asking for different but defined development and maintenance processes depending on pre-selected criteria.",
    "actual_venue": "Ann Software Eng"
  },
  {
    "abstract": "We propose a lock-free multiway search tree algorithm for concurrent applications with large working set sizes. Our algorithm is a variation of the randomized skip tree. We relax the ordering constraints among the nodes in the original skip tree definition. Optimal paths through the tree are temporarily violated by mutation operations, and eventually restored using online node compaction. Experimental evidence shows that our lock-free skip tree outperforms a highly tuned concurrent skip list under workloads of various proportions of operations and working set sizes. The max throughput of our algorithm is on average 41% higher than the throughput of the skip list, and 129% higher on the workload of the largest working set size and read-dominated operations.",
    "actual_venue": "Icpp"
  },
  {
    "abstract": "It is now common to record dozens to hundreds or more neurons simultaneously, and to ask how the network activity changes across experimental conditions. A natural framework for addressing questions of functional connectivity is to apply Gaussian graphical modeling to neural data, where each edge in the graph corresponds to a non-zero partial correlation between neurons. Because the number of possible edges is large, one strategy for estimating the graph has been to apply methods that aim to identify large sparse effects using an [Formula: see text] penalty. However, the partial correlations found in neural spike count data are neither large nor sparse, so techniques that perform well in sparse settings will typically perform poorly in the context of neural spike count data. Fortunately, the correlated firing for any pair of cortical neurons depends strongly on both their distance apart and the features for which they are tuned. We introduce a method that takes advantage of these known, strong effects by allowing the penalty to depend on them: thus, for example, the connection between pairs of neurons that are close together will be penalized less than pairs that are far apart. We show through simulations that this physiologically-motivated procedure performs substantially better than off-the-shelf generic tools, and we illustrate by applying the methodology to populations of neurons recorded with multielectrode arrays implanted in macaque visual cortex areas V1 and V4.",
    "actual_venue": "Journal Of Computational Neuroscience"
  },
  {
    "abstract": "Having a very large volume of unstructured text documents representing different opinions without knowing which document belongs to a certain category, clustering can help reveal the classes. The presented research dealt with almost two millions of opinions concerning customers' (dis)satisfaction with hotel services all over the world. The experiments investigated the automatic building of clusters representing positive and negative opinions. For the given high-dimensional sparse data, the aim was to find a clustering algorithm with a set of its best parameters, similarity and clustering-criterion function, word representation, and the role of stemming. As the given data had the information of belonging to the positive or negative class at its disposal, it was possible to verify the efficiency of various algorithms and parameters. From the entropy viewpoint, the best results were obtained with k-means using the binary representation with the cosine similarity, idf, and H2 criterion function, while stemming played no role.",
    "actual_venue": "Aimsa"
  },
  {
    "abstract": "Opponent-model (OM) search comes with two types of risk. The first type is caused by a playerâs imperfect knowledge of the opponent, the second type arises from low-quality evaluation functions. In this paper, we investigate the desirability of a precondition, called admissibility, that may prevent the second type of risk. We examine the results of two sets of experiments: the first set is taken from the game of LOA, and the second set from the KQKR chess endgame. The LOA experiments show that when admissibility happens to be absent, the OM results are not positive. The chess experiments demonstrate that when an admissible pair of evaluation functions is available, OM search performs better than minimax, provided that there is sufficient room to make errors. Furthermore, we conclude that the expectation âthe better the quality of the prediction of the opponentâs move, the more successful OM search isâ is only true if the quality of both evaluation functions is sufficiently high.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Cloud computing and virtualization techniques provide mobile devices with battery energy saving opportunities by allowing them to offload computation and execute code remotely. When the cloud infrastructure consists of heterogeneous servers, the mapping between mobile devices and servers plays an important role in determining the energy dissipation on both sides. From an environmental impact perspective, any energy dissipation related to computation should be counted. To achieve energy sustainability, it is important reducing the overall energy consumption of the mobile systems and the cloud infrastructure. Furthermore, reducing cloud energy consumption can potentially reduce the cost of mobile cloud users because the pricing model of cloud services is pay-by-usage. In this paper, we propose a game-theoretic approach to optimize the overall energy in a mobile cloud computing system. We formulate the energy minimization problem as a congestion game, where each mobile device is a player and his strategy is to select one of the servers to offload the computation while minimizing the overall energy consumption. We prove that the Nash equilibrium always exists in this game and propose an efficient algorithm that could achieve the Nash equilibrium in polynomial time. Experimental results show that our approach is able to reduce the total energy of mobile devices and servers compared to a random approach and an approach which only tries to reduce mobile devices alone.",
    "actual_venue": "Islped"
  },
  {
    "abstract": "A low-cost but accurate and backdrivable motor control system running on a DC motor and a harmonic drive gear is proposed. It compensates internal friction of the gear and the counter-electromotive torque by combining a model-based feed-forward method and a disturbance observer using a cheap torque sensor. A complementary use of those techniques lowers requirements to their performances i.e. precision, bandwidth, etc., while it is equipped with a flexible property against the external torque. A 2-DOF servo controller is also built upon the system in order to simultaneously achieve smooth responses and robust convergences to the reference.",
    "actual_venue": "International Conference On Robotics And Automation"
  },
  {
    "abstract": "Objective The ShARe/CLEF eHealth 2013 Evaluation Lab Task 1 was organized to evaluate the state of the art on the clinical text in (i) disorder mention identification/recognition based on Unified Medical Language System (UMLS) definition (Task 1a) and (ii) disorder mention normalization to an ontology (Task 1b). Such a community evaluation has not been previously executed. Task 1a included a total of 22 system submissions, and Task 1b included 17. Most of the systems employed a combination of rules and machine learners. Materials and methods We used a subset of the Shared Annotated Resources (ShARe) corpus of annotated clinical text-199 clinical notes for training and 99 for testing (roughly 180 K words in total). We provided the community with the annotated gold standard training documents to build systems to identify and normalize disorder mentions. The systems were tested on a held-out gold standard test set to measure their performance. Results For Task 1a, the best-performing system achieved an F-1 score of 0.75 (0.80 precision; 0.71 recall). For Task 1b, another system performed best with an accuracy of 0.59. Discussion Most of the participating systems used a hybrid approach by supplementing machine-learning algorithms with features generated by rules and gazetteers created from the training data and from external resources. Conclusions The task of disorder normalization is more challenging than that of identification. The ShARe corpus is available to the community as a reference standard for future studies.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "Visual camouflage and anticamouflage may be of widespread relevance throughout the animal kingdom. A question arises as to the possible mechanism underlying visual anticamouflage. A computational model of visual moving image filtering is proposed in which Reichardt's elementary motion detectors are employed for detecting motion information. Afterimages may play an important role in filtering motion object image. An electronic neural network setup was developed for real-time examination of the computational model. Thus, the separation of the moving object image from its background is realized in real-time, while the detected moving image is in high resolution with lower level noise. Â© 1999 IEEE.",
    "actual_venue": "Ieee Transactions On Systems, Man, And Cybernetics A:Systems And Humans"
  },
  {
    "abstract": "One of the most carefully engineered components of a digital integrated circuit is the clock distribution network. A clock is unarguably the most important signal and the network used for its distribution contributes to nearly half of the entire power dissipated by the IC. The design of a clock distribution network requires tremendous resources in terms of time and effort to achieve optimized results. This paper discusses the development of a new algorithm with smaller time complexity for automation of the design of clock distribution network that can greatly reduce the time and effort required, at the same time meeting the conditions set for delays and maximum allowable power dissipation.",
    "actual_venue": "Integration"
  },
  {
    "abstract": "A practical constraint that comes in the way of spectrum estimation of a continuous time stationary stochastic process is the minimum separation between successively observed samples of the process. When the underlying process is not band-limited, sampling at any uniform rate leads to aliasing, while certain stochastic sampling schemes, including Poisson process sampling, are rendered infeasible by the constraint of minimum separation. It is shown in this paper that, subject to this constraint, no point process sampling scheme is alias-free for the class of all spectra. It turns out that point process sampling under this constraint can be alias-free for band-limited spectra. However, the usual construction of a consistent spectrum estimator does not work in such a case. Simulations indicate that a commonly used estimator, which is consistent in the absence of this constraint, performs poorly when the constraint is present. These results should help practitioners in rationalizing their expectations from point process sampling as far as spectrum estimation is concerned, and motivate researchers to look for appropriate estimators of bandlimited spectra.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "Service-orientation is commonly recognized as an important enabler for improved efficiency and flexibility of transformation processes in business. The ability to flexibly bundle heterogeneous services in a homogeneous cross-organizational business process is a key requirement. Shared service models may be one approach to overcome the restrictions of existing approaches which are either business process centric or application centric. Nevertheless applying different design approaches increases the risk of the emergence of service-oriented-architectures (SOA) implying heterogeneous service maps. Especially the design of service maps on different layers leads to significant integration efforts in practice. This paper presents an approach how service maps that differ in scope, granularity and design may be integrated. A case of a Swiss bank has been chosen to demonstrate the applicability of the suggested model.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "This letter is concerned with the implementation of iterative decoding algorithms in analog integrated circuits. We study the convergence speed and the throughput of analog decoders for low-density parity-check codes, and show that they depend on the code, the decoding algorithm, the signal-to-noise ratio, and the average time constant of the analog circuit interconnections. However, they are not a function of the variance of the time constants. The analysis presented here can be used for selecting suitable codes and decoding algorithms for analog decoding. Furthermore, it can be used to estimate the throughput of an analog decoder, if the average time constant of the analog circuit is known",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "In a human robot collaboration scenario, where robot and human coordinate and cooperate to achieve a common task, the system could encounter with deviations. We propose an approach based on Interactive Reinforcement Learning that learns to handle deviations with the help of user interaction. The interactions with the user can be used to form the preferences of the user and help the robotic system to handle the deviations accordingly. Each user might have a different solution for the same deviation in the assembly process. The approach exploits the problem solving skills of each user and learns different solutions for deviations that could occur in an assembly process. The experimental evaluations show the ability of the robotic system to handle deviations in an assembly process, while taking different user preferences into consideration. In this way, the robotic system could both benefit from interaction with users by learning to handle deviations and operate in a fashion that is preferred by the user.",
    "actual_venue": "Intelligent Human Computer Interaction, Ihci"
  },
  {
    "abstract": "We develop a theory of `behavioral communitiesu0027 and the `atomic structureu0027 of networks. We define atoms to be groups of agents whose behaviors always match each other in a set of coordination games played on the network. This provides a microfoundation for a method of detecting communities in social and economic networks. We provide theoretical results characterizing such behavior-based communities and atomic structures and discussing their properties in large random networks. We also provide an algorithm for identifying behavioral communities. We discuss applications including: a method of estimating underlying preferences by observing behavioral conventions in data, and optimally seeding diffusion processes when there are peer interactions and homophily. We illustrate the techniques with applications to high school friendship networks and rural village networks.",
    "actual_venue": "Social Science Research Network"
  },
  {
    "abstract": "Dimension reduction algorithms have attracted a lot of attentions in face recognition and human gait recognition because they can select a subset of effective and efficient discriminative features. In this paper, we apply the discriminative geometry preserving projections (DGPP), a new subspace learning algorithm to address these problems. DGPP models both the intraclass geometry and interclass discrimination. Meanwhile, DGPP will not meet the undersampled problem. Thoroughly empirical studies on YALE face database, UMIST face database, FERET face database and USF human-ID gait database demonstrate that DGPP is superior the popular algorithms for dimension reduction, e.g., PCA, LDA, NPE and LPP.",
    "actual_venue": "Icip"
  },
  {
    "abstract": "Many real-world engineering problems require high computational power, especially regarding the processing time. Current parallel processing techniques play an important role in reducing the processing time. Recently, reconfigurable computation has gained large attention thanks to its ability to combine hardware performance and software flexibility. Also, the availability of high-density Field Programmable Gate Array devices and corresponding development systems allowed the popularization of reconfigurable computation, encouraging the development of very complex, compact, and powerful systems for custom applications. This work presents an architecture for parallel reconfigurable computation based on the dataflow concept. This architecture allows reconfigurability of the system for many problems and, particularly, for numerical computation. Several experiments were done analyzing the scalability of the architecture, as well as comparing its performance with other approaches. Overall results are relevant and promising. The developed architecture has performance and scalability suited for engineering problems that demand intensive numerical computation.",
    "actual_venue": "Journal Of Circuits, Systems, And Computers"
  },
  {
    "abstract": "The design and analysis of a new 0.06Î»Ã0.09Î» compact circular polarized square-shaped dual-resonant multiple split-ring patch antenna on a 1.905-mm-thick high-dielectric ceramic-polytetrafluoroethylene composite is presented. The proposed antenna was designed and analyzed by using a high-frequency electromagnetic simulator based on the finite element method and was fabricated on a printed circuit board. The measured -10dB return loss bandwidths were 44.44% 0.7-1.1GHz and 34% 2.25-3.1GHz at 0.9 and 2.5GHz center frequencies, respectively. The measured radiation patterns with 5.9 and 4.0dBi maximum gains were symmetric and steady, making the proposed antenna suitable for radio frequency identification, wireless local area network, wireless body area network, Low Rate-Wireless Personal Area Network LR-WPAN, and so on. The effects of linewidth, dielectric property of the substrate materials, and number of split rings on the return loss were investigated. The surface current distribution over the radiating patch and the characteristics of the Resistance, Inductance, Capacitance RLC equivalent circuit of the proposed antenna were also analyzed. Copyright Â© 2013 John Wiley & Sons, Ltd.",
    "actual_venue": "J Circuit Theory And Applications"
  },
  {
    "abstract": "This paper mainly focuses on the problem of camera calibration and 3D reconstruction from a single view of structured scene. It is well known that three constraints on the intrinsic parameters of a camera can be obtained from the vanishing points of three mutually orthogonal directions. However, there usually exist one or several pairs of line segments, which are mutually orthogonal and lie in the pencil of planes defined by two of the vanishing directions in the structured scenes. It is proved in this paper that a new independent constraint to the image of the absolute conic can be obtained if the pair of line segments is of equal length or with known length ratio in space. The constraint is further studied both in terms of the vanishing points and the images of circular points. Hence, four independent constraints on a camera are obtained from one image, and the camera can be calibrated under the widely accepted assumption of zero-skew. This paper also presents a simple method for the recovery of camera extrinsic parameters and projection matrix with respect to a given world coordinate system. Furthermore, several methods are presented to estimate the positions and poses of space planar surfaces from the recovered projection matrix and scene constraints. Thus, a scene structure can be reconstructed by combining the planar patches. Extensive experiments on simulated data and real images, as well as a comparative test with other methods in the literature, validate our proposed methods.",
    "actual_venue": "Image And Vision Computing"
  },
  {
    "abstract": "Formation flying is an emerging area in the Earth and space science and technology domains that utilize multiple inexpensive spacecraft by distributing the functionalities of a single platform spacecraft among miniature inexpensive platforms. Traditional spacecraft fault diagnosis and health monitoring practices involve around-the-clock monitoring, threshold checking, and trend analysis of a large amount of telemetry data by human experts that do not scale well for multiple space platforms. A novel hierarchical fault diagnosis framework and methodology is presented here that enables a systematic utilization of fuzzy rule-based reasoning to enhance the level of autonomy achievable in fault diagnosis at ground stations. Fuzzy rule-based fault diagnosis schemes for satellite formation flight are developed and investigated at different levels in the hierarchy for a leader-follower architecture. Our formation level fault diagnosis is found to be useful as a supervisory diagnosis scheme that can prompt the operators to have a closer look at the potential faulty components to determine the sources of a fault. Effectiveness of our proposed fault diagnosis methodology is demonstrated by utilizing synthetic formation flying data of five satellites that are configured in the leader-follower architecture, and are subjected to nonabrupt intermittent faults in the attitude control subsystem (ACS) and the electrical power subsystem (EPS) of the follower satellites.",
    "actual_venue": "Ieee Trans Aerospace And Electronic Systems"
  },
  {
    "abstract": "Content Centric Networking (CCN) is a new paradigm that addresses the gap between the content-centric needs of a user and the current widespread location-centric IP network architecture. In this paper, we propose a hybrid content centric architecture based on our pub/sub enhancement to CCN, Content-Oriented Publish/Subscribe System (COPSS). Our hybrid architecture (hybrid-COPSS) addresses both the need for incremental deployment of CCN and also elegantly combines the functionality of content centric networks and the efficiency of IP forwarding. Our architecture integrates IP multicast to achieve forwarding efficiency by taking advantage of shortest path routing. To overcome the lack of inter-domain IP multicast, hybrid-COPSS uses COPSS multicast with shortcuts as an overlay and IP multicast as the underlay to achieve inter-domain COPSS multicast. To demonstrate the benefits of our hybrid-COPSS architecture, we study its applicability for online gaming, which typically requires low latency. We use a gaming trace in our lab test-bed and microbenchmark the forwarding performance and queuing for a pure COPSS (representative of a pure CCN) based network versus hybrid-COPSS. Also, a large scale simulation (parameterized by the microbenchmark) on a representative ISP topology was used to evaluate the response latency and aggregate network load for the multi-player online gaming scenario. Our preliminary results show that hybrid-COPSS performs better in terms of response latency compare to pure COPSS in a single domain. In a multi-domain environment, hybrid-COPSS can significantly reduce inter-domain traffic while causing only a small increase in the average response latency.",
    "actual_venue": "ICN"
  },
  {
    "abstract": "Organizations adopt sophisticated management information systems, which provide top managers with an ample range of information to achieve multiple strategic performances. However, organizations differ in the extent to which they improve their performance. This paper analyzes the role of top management team in the relationship between management information systems and strategic performance. Using data collected from 92 top management teams, it analyses how different team compositions interact with a sophisticated management information system, and how this interaction affects strategic performances, which are focused on cost reduction and flexibility. The findings show how the effect of management information system on strategic performance (focused on flexibility) is moderated by top management team diversity.",
    "actual_venue": "Int J Information Management"
  },
  {
    "abstract": "Consider a seller with a single indivisible good facing a buyer whose willingness to pay depends on his privately-known taste and on product characteristics privately known by the seller. What selling procedure can arise as an equilibrium of the game in which the seller strategically chooses mechanisms conditional on his information? We characterize the set of equilibrium outcomes and establish that ex-ante revenue-maximizing mechanisms are in this set. There is generally a continuum of revenue-ranked equilibrium outcomes. Focusing on the revenue-maximizing equilibrium, we show that the seller, in general, benefits from private information and does not benefit from committing to a disclosure or a certification technology. We also provide conditions under which the privacy of the seller's information does not affect revenue.",
    "actual_venue": "Journal Of Economic Theory"
  },
  {
    "abstract": "A hierarchical network for visuo-motor coordination is proposed. The hierarchical approach allows learning geometric models of realistic robots with six or more axes. The network consists of several one-dimensional subnetworks, which learn the coordinate transform and rotation axis for each joint. In our simulation, the network reduces the end-effector error of a 7-axis anthropomorphic robot and 20-axis robot below the visual error.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "Speaker diarization may be difficult to achieve when applied to narrative films, where speakers usually talk in adverse acoustic conditions: background music, sound effects, wide variations in intonation may hide the inter-speaker variability and make audio-based speaker diarization approaches error prone. On the other hand, such fictional movies exhibit strong regularities at the image level, particularly within dialogue scenes. In this paper, we propose to perform speaker diarization within dialogue scenes of TV series by combining the audio and video modalities: speaker diarization is first performed by using each modality; the two resulting partitions of the instance set are then optimally matched, before the remaining instances, corresponding to cases of disagreement between both modalities, are finally processed. The results obtained by applying such a multi-modal approach to fictional films turn out to outperform those obtained by relying on a single modality.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "AbstractSoftware Process Improvement SPI activities aim at driving change in information technology development towards increased quality and productivity levels. The SPI Manifesto describes the key values and principles for a successful implementation of SPI. About two-thirds of its principles relate to human, social, and organizational aspects and one-third to technical aspects. This raises the question if these aspects have a relationship with 'social responsibility' principles. The new ISO 26000:2010 standard provides guidance about social responsibility SR, describing the core subjects and issues an organization has to consider when implementing SR. In this paper, we describe how the values and principles of the SPI Manifesto have a relationship with the SR issues described in the ISO 26000. In addition, this paper describes about the steps taken in an industry group to exchange best practices about how social strategies can be used to achieve higher acceptance and sustainability of SPI initiatives. Copyright Â© 2013 John Wiley & Sons, Ltd.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "Traditional association rule mining algorithms onlygenerate a large number of highly frequent rules, butthese rules do not provide useful answers for what thehigh utility rules are. In this work, we develop a novelidea of top-K objective-directed data mining, which focuseson mining the top-K high utility closed patterns thatdirectly support a given business objective. To associationmining, we add the concept of utility to capture highly desirablestatistical patterns and present a level-wise item-setmining algorithm. With both positive and negativeutilities, the anti-monotone pruning strategy in Apriorialgorithm no longer holds. In response, we develop a newpruning strategy based on utilities that allow pruning oflow utility itemsets to be done by means of a weaker butanti-monotonic condition. Our experimental results showthat our algorithm does not require a user specifiedminimum utility and hence is effective in practice.",
    "actual_venue": "Icdm"
  },
  {
    "abstract": "We propose a multi-layer space-time block coded orthogonal frequency division multiplexing (Multi-Layer STBC OFDM) scheme. It combines the spatial diversity and spatial multiplexing in order to exploit the benefits of both techniques. The transmit antennas are divided into several layers and each layer is encoded by STBC. The antennas transmit OFDM signals in order to deal with frequency-selective fading channels. In addition to Exhaustive Detection Scheme which applies the same detection at each subcarrier independently, we exploit the subcarrier correlation to develop two subcarrier-grouping based detection schemes. The subcarriers bound into the same group share the same detection parameters, e.g., layer detection order or nulling matrices, which are obtained at the center subcarrier of that group. The proposed schemes can significantly reduce the complexity with very small performance loss. The simulation results show that compared with Exhaustive Detection Scheme, the propose detection scheme is capable of reducing 51.79% complexity with only 0.5dB performance loss for 4 X 4 system, and reducing 69.96% complexity with only 1.0dB performance loss for 6 X 6 system.",
    "actual_venue": "Ieee Wireless Communications And Networking Conference"
  },
  {
    "abstract": "The quasi-ARX neurofuzzy (Q-ARX-NF) model has shown great approximation ability and usefulness in nonlinear system identification and control. It owns an ARX-like linear structure, and the coefficients are expressed by an incorporated neurofuzzy (InNF) network. However, the Q-ARX-NF model suffers from curse-of-dimensionality problem, because the number of fuzzy rules in the InNF network increases exponentially with input space dimension. It may result in high computational complexity and over-fitting. In this paper, the curse-of-dimensionality is solved in two ways. Firstly, a support vector regression (SVR) based approach is used to reduce computational complexity by a dual form of quadratic programming (QP) optimization, where the solution is independent of input dimensions. Secondly, genetic algorithm (GA) based input selection is applied with a novel fitness evaluation function, and a parsimonious model structure is generated with only important inputs for the InNF network. Mathematical and real system simulations are carried out to demonstrate the effectiveness of the proposed method.",
    "actual_venue": "Ieice Transactions On Fundamentals Of Electronics Communications And Computer Sciences"
  },
  {
    "abstract": "The design of both fast and numerically accurate programs is a real challenge. Thus, the CGPE tool was introduced to assist programmers in synthesizing fast and numerically certified codes in fixed-point arithmetic for the particular case of polynomial evaluation. For performance purposes, this tool produces programs using exclusively unsigned arithmetic and addition/subtraction or multiplication operations, thus requiring some constraints on the fixed-point operands. These choices are well-suited when dealing with the implementation of certain mathematical functions, however they prevent from tackling a broader class of polynomial evaluation problems. In this paper, we first expose a rigorous arithmetic model for CGPE that takes into account signed arithmetic. Then, in order to make the most out of advanced instructions, we enhance this tool with a multi-criteria instruction selection module. This allows us to optimize the generated codes according to different criteria, like operation count, evaluation latency, or accuracy. Finally, we illustrate this technique on operation count, and we show that it yields an average reduction of up to 22.3% of the number of operations in the synthesized codes of some functions. We also explicit practical examples to show the impact of using accuracy based rather than latency based instruction selection.",
    "actual_venue": "Symbolic And Numeric Algorithms For Scientific Computing"
  },
  {
    "abstract": "A number of instructional animation are discussed for teaching 3D computer graphics. They are effective in their assigned task because they focus on concepts and processes that are difficult or impossible to see without animation. They also use a minimalist approach making the animations as simple as possible, focussed on what is relevant, and interactive. One animation is a program that allows students to explore viewing systems, two are concerned with ray tracing transparent objects, and one visualises the formation of marble.",
    "actual_venue": "Acse"
  },
  {
    "abstract": "Mixed-initiative interaction (MII) plays an important role for the flexible dialogues in conversational agent. Since conventional research on MII process dialogues based on the predefined methodologies, they only provide simple and static dialogues rather than complicated and dynamic dialogues through context-aware themselves. In this paper, we proposed a spontaneous conversational agent that provides MII and can change topics of dialogue dynamically based on human cognitive architecture and memory structure. Based on the global workspace theory, one of the simple cognitive architecture models, the proposed agent is aware of the context of dialogue in conscious level and chooses the topic in unconscious level which is the most relevant to the current context as the next topic of dialogues. We represent the unconscious part of memory using semantic network which is a popular representation for storing knowledge in the field of cognitive science, and retrieve the semantic network according to the spreading activation theory which is proven to be efficient for inferring in semantic networks. It is verified that the proposed method spontaneously changes the topics of dialogues through some dialogue examples on the domain of schedule management.",
    "actual_venue": "Agents And Artificial Intelligence"
  },
  {
    "abstract": "In this paper a network-wide traffic signal control scheme in a model predictive control framework using mixed integer programming is presented. A concise model of traffic is proposed to describe a signalized road network considering conservation of traffic. In the model, the traffic of two sections that belong to a traffic signal group of a junction are represented by a single continuous variable. Therefore, the number of variables required to describe traffic in the network becomes half compared with the models that describe section wise traffic flows. The traffic signal at the junction is represented by a binary variable to express a signal state either green or red. The proposed model is transformed into a mixed logical dynamical system to describe the traffic flows in a finite horizon, and traffic signals are optimized using mixed integer linear programming (MILP) for a given performance index. The scheme simultaneously optimizes all traffic signals in a network in the context of model predictive control by successively extending or terminating a green or red signal of each junction. Consequently, traffic signal patterns with the optimal free parameters, i.e., the cycle times, the split times and the offsets, are realized. Use of the proposed concise traffic model significantly reduces the computation time of the scheme without compromising the performance as it is evaluated on a small road network and compared with a previously proposed scheme.",
    "actual_venue": "Journal Of Robotics And Mechatronics"
  },
  {
    "abstract": "Prosodic boundary prediction is very important and challenging in the speech synthesis task, the result of prosodic prediction directly determines the quality of speech synthesis. In this paper, we proposed a prosodic boundary prediction method based on \"encoding-decoding\" frame while using an effective position attention mechanism to further improve performance. Finally, we investigate the use of Random Forest and Gradient Boosting Decision Tree to explore the potential of combined multiple models. The experimental results show that compared with the current best method of prosodic structure (Bi-LSTM), the proposed method presented a good result with F1-Score in terms of prosodic words, prosodic phrases, intonation phrases; the subjective experiment also shows that the proposed method can improve the quality and naturalness of synthesized speech.",
    "actual_venue": "Intelligent Computing Theories And Application, Icic , Pt"
  },
  {
    "abstract": "Objective: We examine the construct of collective orientation, develop a measure to assess individual differences in collective orientation, and examine the extent to which the collective orientation of team members predicts performance on a variety of team tasks. Background: Scholars increasingly emphasize the importance of teamwork in collaborative work environments, and evidence indicates that a lack of teamwork is a prominent factor in many real-world accidents. Although it is clear that some persons are more team oriented than others are, there are few instruments available to assess individual differences in collective orientation in a team context. Method: We develop a scale to measure collective orientation in teams, gather evidence on reliability and construct validity, and examine the extent to which collective orientation predicts team performance. Results: Results indicate that the Collective Orientation Scale is reliable, correlates with cognate measures, and predicts performance on a variety of team tasks. Conclusion: We discuss the role of collective orientation in teams and the application of this scale to assess and diagnose teamwork deficiencies in work groups. Application: This research should contribute to a further understanding of factors that influence collaboration and coordination in teams.",
    "actual_venue": "Human Factors"
  },
  {
    "abstract": "This paper develops an optimal solution procedure for the multi-period online fulfillment assignment problem to determine how many and which of a retailer/e-tailer's capacitated regional warehouse locations should be set up to handle online sales over a finite planning horizon. To reduce the number of candidate solutions in each period, dominance rules from the facility location literature are extended to handle the nonlinear holding and backorder cost implications of our problem. Computational results indicate that multi-period considerations can play a major role in determining the optimal set of online fulfillment locations. In 92% of our test problems, the multi-period solution incorporated fewer openings and closings than myopic single period solutions. To illustrate the use of the model under changing demands, the multi-period solution yielded different supply chain configurations than the myopic single period solution in over 37% of the periods.",
    "actual_venue": "Mathematical And Computer Modelling"
  },
  {
    "abstract": "In the present paper, we use a generalization of the Euler-Maclaurin summation formula for integrals of the form @!\"a^bF\"0(x)g(x)dx where F\"0(x) (the weight) is a continuous and positive function and g(x) is twice continuously differentiable function in the interval [a,b]. Numerical examples are given to show the effectiveness of the method.",
    "actual_venue": "J Computational Applied Mathematics"
  },
  {
    "abstract": "Mobile electrocardiogram (ECG) monitoring is an emerging area that has received increasing attention in recent years, but still real-life validation for elderly residing in low and middle-income countries is scarce. We developed a wearable ECG monitor that is integrated with a self-designed wireless sensor for ECG signal acquisition. It is used with a native purposely designed smartphone application, based on machine learning techniques, for automated classification of captured ECG beats from aged people. When tested on 100 older adults, the monitoring system discriminated normal and abnormal ECG signals with a high degree of accuracy (97%), sensitivity (100%), and specificity (96.6%). With further verification, the system could be useful for detecting cardiac abnormalities in the home environment and contribute to prevention, early diagnosis, and effective treatment of cardiovascular diseases, while keeping costs down and increasing access to healthcare services for older persons.",
    "actual_venue": "Computational And Mathematical Methods In Medicine"
  },
  {
    "abstract": "In this work, a general form of Jordan's inequality: P\"2\"N(x)+a\"N\"+\"1(@p^2-4x^2)^N^+^1@?sinxx@?P\"2\"N(x)+1-@?n=0Na\"n@p^2^n@p^2^(^N^+^1^)(@p^2-4x^2)^N^+^1 is established, where x@?(0,@p/2],P\"2\"N(x)=@?\"n\"=\"0^Na\"n(@p^2-4x^2)^n,a\"0=2@p,a\"1=1@p^3,a\"n\"+\"1=2n+12(n+1)@p^2a\"n-116n(n+1)@p^2a\"n\"-\"1, and N=0 is a natural number. The applications of the above result give the general improvement of the Yang Le inequality and a new infinite series (sinx)/x=@?\"n\"=\"0^~a\"n(@p^2-4x^2)^n for 0",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "Memristor-based deep learning accelerators provide a promising solution to improve the energy efficiency of neuromorphic computing systems. However, the electrical properties and crossbar structure of memristors make these accelerators error-prone. To enable reliable memristor-based accelerators, a simulation platform is needed to precisely analyze the impact of non-ideal circuit and device properties on the inference accuracy. In this paper, we propose a flexible simulation framework, DL-RSIM, to tackle this challenge. DL-RSIM simulates the error rates of every sum-of-products computation in the memristor-based accelerator and injects the errors in the targeted TensorFlow-based neural network model. A rich set of reliability impact factors are explored by DL-RSIM, and it can be incorporated with any deep learning neural network implemented by TensorFlow. Using three representative convolutional neural networks as case studies, we show that DL-RSIM can guide chip designers to choose a reliability-friendly design option and develop reliability optimization techniques.",
    "actual_venue": "Ieee/Acm International Conference On Computer-Aided Design"
  },
  {
    "abstract": "Developers and managers need to be aware of the emotional climate of the projects they are involved to take corrective actions when necessary and to have a better understanding of the social factors affecting the project. With the growing trend of distributed teams and textual communication this type of awareness is more difficult to obtain and maintain. We propose to improve emotional climate awareness in software development projects by means of a visualization prototype which includes general and detailed views of the topics and emotions expressed in software project collaboration artifacts. We performed an initial case study in which the mailing list content of a software project was visualized. The study suggests that the length, frequency and emotion diversity of the exchanged content varies according to the project phase. However, a more extensive evaluation needs to be made.",
    "actual_venue": "Software Visualization"
  },
  {
    "abstract": "HÃ¤ggkvist conjectured in 1976 that every 2-connected k -regular bipartite graph G on at most 6 k vertices is hamiltonian. Chetwynd and HÃ¤ggkvist have shown that G is hamiltonian if G has at most 4.2 k vertices. The upper bound on | V ( G )| was subsequently improved to 5 k â 12 and then 5 k â 8 by Ash and Min Aung, respectively. We shall essentially verify HÃ¤ggkvistâ²s conjecture by showing that every 2-connected k -regular bipartite graph on at most 6 k â 38 vertices is hamiltonian.",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "Using an analytic method, we derive an alternative formula for the probability that a geometrically distributed word of length n possesses the restricted growth property. Equating our result with a previously known formula yields an algebraic identity involving alternating sums of binomial coefficients via a probabilistic argument. In addition, we consider refinements of our formula obtained by fixing the number of blocks, levels, rises, or descents.",
    "actual_venue": "Australasian Journal Of Combinatorics"
  },
  {
    "abstract": "In this paper we present a method to recognize shapes by analyzing a polygonal approximation of their boundaries. The method is independent of the used approximation method since its recognition strategy does not rely on the number of segments composing the shape. Length and turning angle information are extracted from the chain of segments. The comparison method is invariant to scale, translation and some occlusions of the extracted contour. A simple pre-processing method, also based on arc-length features, is presented to be used as a coarse fitting method to determine angle rotation and as a first filter to eliminate non pertinent candidates.",
    "actual_venue": "Ibpria"
  },
  {
    "abstract": "P2P network has been widely used because of advantages such as efficient use of network bandwidth, saving of computing resources, and quick information exchange. In particular, the infra that manages each nodes centrally in P2P network does not exist and each node is a structure performing the sender and receiver roles. The service applying P2P technique in MANET is increased because this structure is very similar to the structure of MANET. However, the reliability may be lower by providing an erroneous service from malicious nodes because the supervision of management for nodes participating in P2P does not perform. In this paper, we propose hybrid trust evaluation technique based on Trust Zone structure to improve the reliability between nodes. TZM node is elected for trust evaluation of member nodes internal each TrustZone. The certificate of member nodes is issued in the elected TZM and the information is stored in TZMT. The data transmission of malicious nodes is blocked by limiting the data transmission of nodes that do not issued the certificate. The reputation-based trust management technique was applied to perform the fair file transmission of nodes and block the behavior of selfish nodes. The excellent performance of the proposed technique in this paper was confirmed through experiments.",
    "actual_venue": "Peer-To-Peer Networking And Applications"
  },
  {
    "abstract": "Pedestrian detection plays an important role in the environmental perception and autonomous navigation for robotics, which provides critical information for the safe operation in complex environments. In this paper, a 3D pedestrian detector with calibrated LiDAR and RGB Camera is proposed, which takes full advantage of the precise range information of 3D LiDAR scanner and semantic information acquired from RGB image. The proposed approach integrates the segmented object clusters of point cloud and 2D bounding boxes generated by a visual object detector. The point cloud is segmented by a three-step segmentation approach, which can segment point cloud with both high precision and high efficiency. By fusion multi-sensor information in the image domain, the proposed approach provides 3D information of pedestrians in both LiDAR and camera coordinate systems. Experiments was conducted to evaluate the performance of the proposed approach.",
    "actual_venue": "Robio"
  },
  {
    "abstract": "This paper studies the optimization of the (S, T) inventory policy, where T is the replenishment interval and S is the order-up-to level. First, we demonstrate that the previously established joint convexity of the long-run average cost is false. Hence, the optimization is not straightforward. We then point out that the joint convexity concept depends on whether S and T are continuous or discrete variables, and in some situations it may not even be well defined. Nonetheless, we are able to identify several useful properties of the cost function, such as submodularity and coordinatewise convexity. Based on these properties, we develop efficient algorithms to compute the optimal policy for continuous and discrete demands.",
    "actual_venue": "Andsom-Manufacturing And Service Operations Management"
  },
  {
    "abstract": "This brief presents a differentially terminated CML transmitter with a self-calibration scheme based on time-domain reflectometry for preemphasis strength control. Without any handshaking or receiver mode control, the transmitter measures the time of flight by applying the same step input on the two transmission lines of the differential link. Since the receiver does not change its configuration, the proposed scheme greatly simplifies the preemphasis adaptation. To verify the calibration scheme, the proposed transmitter is fabricated in a 0.18 ??m- CMOS. For various lengths of the microstrip line on the printed circuit board up to 80 cm, the tested transmitter greatly improves signal integrity and shows clear eye diagrams at 5 Gb/s.",
    "actual_venue": "Ieee Trans On Circuits And Systems"
  },
  {
    "abstract": "We give a vectorial and parallel algorithm that performs the transitive closure of a directed graph in an average O(n^2 log n) operations using an improved breadth-first method. The vectorization problem solved here gives a good example of methods efficient for graph problems. Computation on CDC CYBER 205 has proved very efficient.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "This paper compares and contrasts the design philosophies and implementations of two computer system families: the IBM S/360 and its evolution to the current zSeries line, and the Tandem (now HP) NonStopÂ® Server. Both systems have a long history; the initial IBM S/360 machines were shipped in 1964, and the Tandem NonStop System was first shipped in 1976. They were aimed at similar markets, what would today be called enterprise-class applications. The requirement for the original S/360 line was for very high availability; the requirement for the NonStop platform was for single fault tolerance against unplanned outages. Since their initial shipments, availability expectations for both platforms have continued to rise and the system designers and developers have been challenged to keep up. There were and still are many similarities in the design philosophies of the two lines, including the use of redundant components and extensive error checking. The primary difference is that the S/360-zSeries focus has been on localized retry and restore to keep processors functioning as long as possible, while the NonStop developers have based systems on a loosely coupled multiprocessor design that supports a \"fail-fasté©´ philosophy implemented through a combination of hardware and software, with workload being actively taken over by another resource when one fails.",
    "actual_venue": "Ieee Trans Dependable Sec Comput"
  },
  {
    "abstract": "The characteristics of the concave-convex pi-pi interactions are evaluated in 32 buckybowl dimers formed by corannulene, sumanene, and two substituted sumanenes (with S and CO groups), using symmetry-adapted perturbation theory [SAPT(DFT)] and density functional theory (DFT). According to our results, the main stabilizing contribution is dispersion, followed by electrostatics. Regarding the ability of DFT methods to reproduce the results obtained with the most expensive and rigorous methods, TPSS-D seems to be the best option overall, although its results slightly tend to underestimate the interaction energies and to overestimate the equilibrium distances. The other two tested DFT-D methods, B97-D2 and B3LYP-D, supply rather reasonable results as well. M06-2X, although it is a good option from a geometrical point of view, leads to too weak interactions, with differences with respect to the reference values amounting to about 4 kcal/mol (25% of the total interaction energy). (C) 2017 Wiley Periodicals, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "In its ideal form, web-based simulation should allow simulation models as well as simulation results to be as readily distributable and composable as todayâs web documents. The rapid advances in web technology, most notably Java, are helping to make this a possibility. Support for executable web content, universal portability, component technology, and standard high-level packages for accessing databases and producing graphical user interfaces are important enablers of web-based simulation. Component-based software can be used to develop highly modular simulation environments supporting high reusability of software components. Because of the potentially large scope of web-based simulation, greater demands are placed on simulation environments. They should support rapid visual model development, access to local and remote databases, techniques for executing models or federations of models in a variety of ways, and embedding of simulation within larger systems. The use of component technology in the JSIM web-based simulation environment allows simulation models to be treated as components that can be dynamically assembled to build model federations. It also allows simulation inputs and outputs to be dynamically linked to database systems, making storage of simulation results easy and flexible.",
    "actual_venue": "Future Generation Comp Syst"
  },
  {
    "abstract": "With the advent of audio-visual IP clients, video telephony becomes a realistic option in many application scenarios. In order to guarantee an adequate quality to its users, providers of audio-visual telephony services need to know the impact of the audio and video transmission channel characteristics on perceived Quality of Experience (QoE) in a realistic interactive setting. For this aim, a conversational video telephony experiment was conducted where the audio and video channel settings were adjusted in a controlled way, and participants were asked about the perceived audio, video and overall quality after carrying out a conversation over the audio-visual channel. We analyze the results with respect to the impact the two modalities have, as well as with respect to the impact of the conversation scenario.",
    "actual_venue": "Mmsp"
  },
  {
    "abstract": "The FIPA Agent Communication Language includes a library of communicative acts supporting generic types of social interaction, such as information exchange and action performing. However, when developing specific applications, it is desirable to support agent interaction by means of more expressive constructs. In this paper, a structured approach to the design of communicative act libraries is put forward, which takes into account both the criteria of reusability and expressiveness. A linguistic approach is taken to ensure the expressiveness requirement. The proposal is illustrated by the design of an advisory sub-catalogue of reusable communicative acts.",
    "actual_venue": "Applied Artificial Intelligence"
  },
  {
    "abstract": "Current testing is useful for testing CMOS ICs because it can detect a large class of manufacturing defects, including defects that traditional stuck-at fault testing misses. The effectiveness of current testing can be enhanced if built-in current sensors are applied on-chip to monitor defect-related abnormal currents in the power supply buses. Such sensors have proved effective for built-in self-test. However, current testing requires the use of a special method to generate test vectors. The authors describe this method, which differs from that for traditional voltage-oriented testing, and postulate a test-generation algorithm for both on-chip and off-chip current testing. The algorithm uses realistic fault models extracted directly from the circuit layout.<>",
    "actual_venue": "Design & Test of Computers, IEEE Â "
  },
  {
    "abstract": "The constraint satisfaction problem (CSP) is a generic problem with many applications in different areas of artificial intelligence and operational research. When solving a CSP, the order in which the variables are selected to be instantiated has a tremendous impact in the cost of finding a solution. In this paper we explore a novel type of heuristic that combines different features that describe the current state of the instance to decide which variable to instantiate next. A generational genetic algorithm is used to automatically tune the parameters used by these new heuristics. This paper contributes to the development of new heuristics that can be either very specialized to one class of instances, or general enough to deal with different classes of instances with an acceptable performance.",
    "actual_venue": "Studies In Computational Intelligence"
  },
  {
    "abstract": "Older adults are normally characterized as consumers, rather than producers, of digital content. Current research concerning the design of technologies for older adults typically focuses on providing access to digital resources. Access is important, but is often insufficient, especially when establishing new social relationships. This paper investigates the nature and role of digital content that has been created by older adults, for the purpose of forging new relationships. We present a unique field study in which seven older adults (aged 71-92 years), who did not know each other, used a prototype iPad application (Enmesh) to create and share photographs and messages. The findings demonstrate that older adults, even those in the \\'1c\"oldest old\\'1d\" age group, embraced opportunities to express themselves creatively through digital content production. We show that self-expression and social engagement with peers can be realized when socio-technical systems are suitably designed to allow older adults to create and share their own digital content.",
    "actual_venue": "CHI"
  },
  {
    "abstract": "Matrix scaling is the problem of assigning values to the elements of a matrix that are proportional to a given input matrix. The assignment should fulfill a set of row- and column-sum requirements. We propose a new method that differs from divisor-type methods appeared until now in the literature. This method combines the largest remainder apportionment and bi-proportional rounding. Exhaustive application to the Greek parliamentary elections of 2007 justify our effort.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "A generalized minimum shift keying (GMSK) signal is defined, and its equivalence to a modified offset quadrature shift keying signal is shown. A simple formula for the spectrum of a GMSK signal is presented and the spectrum and out-of-band power are computed for two examples.",
    "actual_venue": "Information Theory, IEEE Transactions Â "
  },
  {
    "abstract": "In this paper an adaptive control scheme along with its simulation, and its implementation on a quadrotor are presented. Parametric and non- parametric uncertainties in the quadrotor model make it difficult to design a controller that works properly in various conditions during flight time. Decentralized adaptive controller, which is synthesized based on improved Lyapunov-based Model Reference Adaptive Control (MRAC) technique, is suggested to solve the problem. The proposed control scheme does not need knowing the value of any physical parameter for generating appropriate control signals, and retuning the controller is not required for different payloads. An accurate simulation that includes empirical dynamic model of battery, sensors, and actuators is performed to validate the stability of the closed loop system. The simulation study simplifies implementation of the controller on our real quadrotor. A practical algorithm is proposed to alleviate and accelerate the tuning of controller parameters. The controller is implemented on the quadrotor to stabilize its attitude and altitude. Simulation and experimental results demonstrate the efficiency and robustness of the proposed controller.",
    "actual_venue": "Journal Of Intelligent And Robotic Systems"
  },
  {
    "abstract": "Stochastic simulation requires a reliable source of randomness. Inversive methods are an interesting and very promising new approach to produce uniform pseudorandom numbers. In this paper, we present evidence that these methods are an important contribution to our toolbox. We survey the outstanding performance of inversive pseudorandom number generators in theoretical and empirical tests, in comparison to linear generators. In addition, this paper contains tables of parameters to implement inversive congruential generators.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "Ecosystems are typical complex adaptive systems. By a universal principle (MFP) on pattern formation of ecosystems, the underlying microscopic dynamic mechanisms that induce complex patterns in ecosystems can be revealed. The ecosystem patterns are further simulated by invoking theoretical model of artificial neural network of Self-Organization Feature Map (SOM), by which evolution processes, structural classifications and fractal growth of ecosystem patterns are elaborated. The results not only help us to analyze formation and dynamics of ecosystem patterns, but also provide implications for classifications, protection and optimization of ecosystems.",
    "actual_venue": "Icnc"
  },
  {
    "abstract": "Traditional multiple model process monitoring methods usually yield satisfactory results for multi-mode processes under the assumption that the processes are time invariant. However, for some petrochemical processes, such as ethylene cracking furnace, the process is time varying as the coking in the furnace tubes. To solve this problem, this study proposes a multiple model recursive monitoring method. A computational intelligence-based cluster algorithm is employed to separate different operating modes. Then, recursive kernel principal component analysis is used to reduce the dimension of the time-varying process data and extract the nonlinear principal components recursively. Furthermore, support vector data description is utilized to build models because the process data are non-Gaussian. Finally, the corresponding statistics are constructed to detect the process fault. The performance of this method is evaluated through a case study of ethylene cracking in a petrochemical plant.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "By using the invariant set of descending flow and variational method, we establish the existence of multiple solutions to a class of second-order discrete Neumann boundary value problems. The solutions include sign-changing solutions, positive solutions, and negative solutions. An example is given to illustrate our results.",
    "actual_venue": "Applied Mathematics Letters"
  },
  {
    "abstract": "Neuroimaging genomics is an emerging field that provides exciting opportunities to understand the genetic basis of brain structure and function. The unprecedented scale and complexity of the imaging and genomics data, however, have presented critical computational bottlenecks. In this work we present our initial efforts towards building an interactive visual exploratory system for mining big data in neuroimaging genomics. A GPU accelerated browsing tool for neuroimaging genomics is created that implements the ANOVA algorithm for single nucleotide polymorphism (SNP) based analysis and the VEGAS algorithm for gene-based analysis, and executes them at interactive rates. The ANOVA algorithm is 110 times faster than the 4-core OpenMP version, while the VEGAS algorithm is 375 times faster than its 4-core OpenMP counter part. This approach lays a solid foundation for researchers to address the challenges of mining large-scale imaging genomics datasets via interactive visual exploration.",
    "actual_venue": "Neuroinformatics"
  },
  {
    "abstract": "This paper proposes a robust and accurate method for measuring 3-D surfaces using a binocular system. To eliminate the effect caused by the distortion of projector lens, each structured light sheet is fitted to a conicoid. A curvilinear detector, which combines the zero-crossing detection algorithm with Steger's detector, is employed to detect the subpixel locations of the light stripes, thus preventing the Steger's curvilinear detector from failing to detect them in the endpoints of the stripes. The proposed coding method combines the information of the linked line with the gray code to avoid producing outliers caused by erroneous decoding and make the coding procedure more robust. Experiments showed that each structured light sheet fitted to a conicoid can effectively improve the measurement accuracy. The subpixel detection method can detect the exact subpixel locations of the stripes. Likewise, the encoding strategy results in the production of fewer outliers, while the reconstruction result becomes more perfect.",
    "actual_venue": "Ieee T Instrumentation And Measurement"
  },
  {
    "abstract": "The subspace channel estimation technique is investigated for MIMO-OFDM systems with space-time block codes (STBC). The noise subspace computed from the correlation matrix of received signals requires a large number of symbols to converge. Using the block cyclic property of the channel matrix, we propose the cyclic repetition method (CRM) to generate many times of equivalent symbols for each OFDM symbol. With these equivalent symbols, the subspace channel estimation can perform very well within a few OFDM symbols. Computer simulations demonstrate the proposed CRM-based channel estimation has better performance than the conventional ones.",
    "actual_venue": "Iswcs"
  },
  {
    "abstract": "âFifth generation computersâ are expected to capitalize on the dramatic progress of VLSI technology, in order to offer an improved performance/cost figure. An even more important requirement, however, is that they will support by architectural means the generation, execution, and maintenance of âquality software,â as a way out of the âsoftware crisis.â One approach towards the design and implementation of quality software is programming with abstract data types, in connection with elaborate type consistency checking. The objection raised against the abstract data type based programming style is poor run time efficiency when such programs are executed on a conventional machine. In this paper adata type architecture is described that offers efficient and convenient mechanism for constructing arbitrary data structures and encapsulating them into abstract data types, thus avoiding the inefficiency penalty mentioned above. Through a process of hierarchical decomposition, user-defined abstract data types are mapped on representations given in terms of a basicstructured machine data type. This approach combines high performance with generality and completeness. The hardware structure of the data type architecture can be classified as a strongly coupled, asymmetric multicomputer system with hierarchical function distribution among the computers. The system includes a pipeline for numerical and nonnumerical operations, performed on the vector-structured basic machine data type in the SIMD mode of operation. Software reliability and data security is enhanced through elaborate run time consistency checking. The computer, which was designed and built at the Technical University of Berlin, has recently become operational. This paper outlines the operational principle, the mechanisms, and the hardware and software structure of this innovative, fifth generation computer architecture.",
    "actual_venue": "International Journal Of Parallel Programming"
  },
  {
    "abstract": "AbstractBug resolution refers to the activity that developers perform to diagnose, fix, test, and document bugs during software development and maintenance. Given a bug report, we would like to recommend the set of bug resolvers that could potentially contribute their knowledge to fix it. We refer to this problem as developer recommendation for bug resolution. In this paper, we propose a new and accurate method named DevRec for the developer recommendation problem. DevRec is a composite method that performs two kinds of analysis: bug reports based analysis BR-Based analysis and developer based analysis D-Based analysis. We evaluate our solution on five large bug report datasets including GNU Compiler Collection, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 107,875 bug reports. We show that DevRec could achieve [email protected] and [email protected] scores of 0.4826-0.7989, and 0.6063-0.8924, respectively. The results show that DevRec on average improves [email protected] and [email protected] scores of Bugzie by 57.55% and 39.39%, outperforms DREX by 165.38% and 89.36%, and outperforms NonTraining by 212.39% and 168.01%, respectively. Moreover, we evaluate the stableness of DevRec with different parameters, and the results show that the performance of DevRec is stable for a wide range of parameters. Copyright Â© 2015 John Wiley & Sons, Ltd.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "A neural network model for muscle force control was constructed. The model contained a single motor-cortex output cell, the actual number of a motoneurons found in human muscles, Renshaw cells and muscle units. The size of the motor units (motoneurons and muscle units) was distributed as the human brachialis muscle, the extensor digitorum muscle and the first dorsal interosseous muscle. The relationship between the model's muscle force and the firing rate of a motoneurons was investigated. The relationship depended on the absolute refractory time of a motoneurons, RIPSP by Renshaw cells and the firing pattern of Renshaw cells. When these parameters were selected appropriately, the model showed a relationship similar to that observed in isometric contraction of human skeletal muscles. The size distribution of the motor units had a dominant effects on the relationship.",
    "actual_venue": "Artificial Neural Networks In Medicine And Biology"
  },
  {
    "abstract": "Let G be a graph with vertex set V(G) and edge set E(G). A function f:E(G)-{-1,1} is said to be a signed star dominating function of G if @?\"e\"@?\"E\"\"\"G\"(\"v\")f(e)=1 for every v@?V(G), where E\"G(v)={uv@?E(G)|u@?V(G)}. The minimum of the values of @?\"e\"@?\"E\"(\"G\")f(e), taken over all signed star dominating functions f on G, is called the signed star domination number of G and is denoted by @c\"S\"S(G). In this paper, a sharp upper bound of @c\"S\"S(GxH) is presented.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "Primitive routing protocols for ad-hoc networks are ''power hungry'' and can therefore consume considerable amount of the limited amount of battery power resident in the nodes. Thus, routing in ad-hoc networks is very much energy-constrained. Continuous drainage of energy degrades battery performance as well. If a battery is allowed to intermittently remain in an idle state, it recovers some of its lost charge due to the charge recovery effect, which, in turn, results in prolonged battery life. In this paper, we use the ideas of naturally occurring ants' foraging behavior (Dorigo and Stuetzle, 2004) [1] and based on those ideas, we design an energy-aware routing protocol, which not only incorporates the effect of power consumption in routing a packet, but also exploits the multi-path transmission properties of ant swarms and, hence, increases the battery life of a node. The efficiency of the protocol with respect to some of the existing ones has been established through simulations. It has been observed that the energy consumed in the network, the energy per packet in the case of EAAR are 60% less compared to MMBCR and the packets lost is only around 12% of what we have in AODV, in mobility scenarios.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "In this paper, we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds. Unlike conventional manifold regression algorithms that do not consider the class distinction of samples, our method introduces the class information to the regression process and tries to exploit the similar configurations shared by the label distribution of multi-class data. To utilize the correlations among data from different classes, we develop a cross-manifold label propagation process and employ labels from different classes to enhance the regression performance. The interclass relations are coded by a set of intermanifold graphs and a regularization item is introduced to impose inter-class smoothness on the possible solutions. In addition, the algorithm is further extended with the kernel trick for predicting labels of the out-of-sample data even without class information. Experiments on both synthesized data and real world problems validate the effectiveness of the proposed framework for semisupervised regression.",
    "actual_venue": "Icml"
  },
  {
    "abstract": "This work presents a new and simple formulation of a two-dimensional grid generation method that uses a special cubic spline algorithm developed to generate the analytical representation of arbitrary boundaries for two-dimensional domains. These cubic spline functions used for the fitting of points belonging to a boundary are special because they are generated based on a procedure that minimizes the length of the cubic spline functions defining the boundary. This avoids unwarranted inflection points common to cubic spline functions. Since the piecewise spline functions are cubic, they can easily accommodate contours of any shape, giving an analytical representation for the boundaries. The streamlines are constructed based on the boundaries generated by the spline model and the potential lines are built using the orthogonality principle.",
    "actual_venue": "Computers And Chemical Engineering"
  },
  {
    "abstract": "High-level synthesis has been an active research field since the early eighties. However, apart from a few exceptions the technology has so far failed to make a smooth transition into the industrial environment. The main reasons are related more to the lack of an integrated methodology, including design entry, simulation, and synthesis, than to the quality of the synthesis algorithms. Experiences in applying the HIS system (High-level IBM Synthesis System) to real designs led to the development of a system and methodology which integrate language modeling, simulation, high-level, and logic synthesis. This paper presents the main concepts behind this system, namely, its modeling, synthesis, and simulation capabilities. Moreover, it addresses some of the problems encountered when transferring high-level synthesis to a production environment.<>",
    "actual_venue": "Vlsi) Systems, Ieee Transactions"
  },
  {
    "abstract": "A novel dividend valuation model is put forward by using a Markov chain. The valuation procedure turns out to be very simple, since it requires the solution of a system of linear equations. The dividend valuation model is in accordance with the empirical evidence whereby dividend-price ratios can change as time proceeds. Moreover, it offers fresh insights into previous dividend valuation models.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "As a result of the emerging use of mesh-based multicomputers (and recently mesh-based multiprocessor systems-on-chip), issues related to processor management have attracted much attention. In a mesh-based multiprocessor, after repeated submesh allocations and de-allocations, the system network may be fragmented, i.e. there might be unallocated nodes in the network. As a result, in a system with contiguous processor allocation, no new tasks can start running due to the lack of enough free adjacent processors to form a suitable submesh. Although there might be enough free processors available, they remain idle until the allocator can find a set of adjacent free nodes forming a submesh to be used for the new task. This can lead to low system performance. Task migration was introduced as a solution to this problem through migration of tasks running on some submeshes to other free areas in order to reduce fragmentation by chaining the newly freed areas and disengaging nodes to form larger submeshes. In this paper, we propose a novel structured and formulated way to code task migration, which is helpful for congestion detection in different steps of task migration algorithms. Moreover, considering the fact that the 3D mesh-based multicomputers are now very popular, a new task migration algorithm in 3D meshes is proposed. We also address the special case of the 2D migration in a 3D mesh multicomputer.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "A new numerical optimization algorithm inspired by political competitions during parliamentary head elections is proposed in this paper. Competitive behaviors could be observed in many aspects of human social life. Our proposed algorithm is a stochastic, iterative and population-based global optimization technique like genetic algorithms and particle swarm optimizations. Particularly, our method tries to simulate the intra and inter group competitions in trying to take the control of the parliament. Performance of this method for function optimization over some benchmark multidimensional functions, of which global and local minimums are known, is compared with traditional genetic algorithms.",
    "actual_venue": "Micai"
  },
  {
    "abstract": "We study a Lindenmayer-like parallel rewriting system to model the growth of filaments (arrays of cells) in which developmental errors may occur. In essence this model is the fuzzy analogue of the derivation-controlled iteration grammar. Under minor assumptions on the family of control languages and on the family of fuzzy languages in the underlying iteration grammar, we show that (i) regular control does not provide additional generating power to the model, (ii) the number of fuzzy substitutions in the underlying iteration grammar can be reduced to two, and (iii) the resulting family of fuzzy languages possesses strong closure properties, viz. it is a full hyper-AFFL, i.e., a hyper-algebraically closed full Abstract Family of Fuzzy Languages.",
    "actual_venue": "New Trends In Formal Languages"
  },
  {
    "abstract": "This paper describes the optimization of a compliant motion task plan which guarantees the estimation of the inaccurately known position and orientation of the contacting objects to the required accuracy (\"active sensing\"). The task plan consists of (i) the desired sequence of contact formations and (ii) the compliant path to execute during each of these contact formations. The plan is optimized to minimize the expected execution time, an important criterion in industrial applications. The theory is implemented for contacts between polyhedral objects.",
    "actual_venue": "Ieee International Conference On Robotics And Automation, Vols -, Proceedings"
  },
  {
    "abstract": "Ontology learning is an important task in Artiflcial Intelli- gence, Semantic Web and Text Mining. This paper presents a novel framework for, and solutions to, three practical prob- lems in ontology learning. An incremental clustering ap- proach is used to solve the problem of unknown group names. Learned models at each level of an ontology address the problem of no control over concept abstractness. A metric learning module moves beyond the limitation of traditional use of features and incorporates heterogeneous semantic ev- idence into the learning process. The metric-based learn- ing framework integrates these separate components into a single, unifled solution. An extensive evaluation with Word- Net and Open Directory Project data demonstrates that the method is more efiective than a state-of-the-art baseline al- gorithm.",
    "actual_venue": "Acm International Conference On Information And Knowledge Management"
  },
  {
    "abstract": "Discover is a web resource supporting the visual arts and music curriculum in New Zealand schools. It contains 2500 multimedia items from the collections of the Alexander Turnbull Library, which holds the national cultural heritage collections, and 300 resources from other sources. The product uses a metadata scheme that combines simple (unqualified) DC and qualified DC, EAD and local extensions expressed in XML and uses the RDF framework proposed by DCMI for expressing qualified DC in RDF/XML. This metadata schema will continue to evolve to support interchange of the NLNZ's digital resources within the library, archival and education communities.",
    "actual_venue": "Dublin Core Conference"
  },
  {
    "abstract": "A novel local stereo matching algorithm is introduced to address the fundamental challenge of stereo algorithms, accuracy and computational complexity dilemma. The time consuming intensity dependent aggregation procedure of local methods is improved in terms of both speed and precision. Providing connected 2D support regions, the proposed approach exploits a new paradigm, namely separable successive weighted summation (SWS) among horizontal and vertical directions enabling constant operational complexity. The weights are determined by four-neighborhood intensity similarity of pixels and utilized to model the information transfer rate, permeability, towards the corresponding direction. The same procedure is also utilized to diffuse information through overlapped pixels during occlusion handling after detecting unreliable disparity assignments. Successive weighted summation adaptively cumulates the support data based on local characteristics, enabling disparity maps to preserve object boundaries and depth discontinuities. According to the experimental results on Middlebury stereo benchmark, the proposed method is one of the most effective local stereo algorithm providing high quality disparity models by unifying constant time filtering and weighted aggregation. Hence, the proposed algorithm provides a competitive alternative for various local methods in terms of achieving precise and consistent disparity maps from stereo video within fast execution time.",
    "actual_venue": "Sig Proc: Image Comm"
  },
  {
    "abstract": "Distributed data bases are currently an important field of research and development. Our paper concerns the design of general purpose distributed data base management systems. We present an architecture based upon logical relational machines which are the components of the distributed data base.",
    "actual_venue": "Vldb"
  },
  {
    "abstract": "The âembedded cluster reference interaction site modelâ (EC-RISM) approach combines statistical-mechanical integral equation\n theory and quantum-chemical calculations for predicting thermodynamic data for chemical reactions in solution. The electronic\n structure of the solute is determined self-consistently with the structure of the solvent that is described by 3D RISM integral\n equation theory. The continuous solvent-site distribution is mapped onto a set of discrete background charges (âembedded clusterâ)\n that represent an additional contribution to the molecular Hamiltonian. The EC-RISM analysis of the SAMPL2 challenge set of\n tautomers proceeds in three stages. Firstly, the group of compounds for which quantitative experimental free energy data was\n provided was taken to determine appropriate levels of quantum-chemical theory for geometry optimization and free energy prediction.\n Secondly, the resulting workflow was applied to the full set, allowing for chemical interpretations of the results. Thirdly,\n disclosure of experimental data for parts of the compounds facilitated a detailed analysis of methodical issues and suggestions\n for future improvements of the model. Without specifically adjusting parameters, the EC-RISM model yields the smallest value\n of the root mean square error for the first set (0.6Â kcalÂ molâ1) as well as for the full set of quantitative reaction data (2.0Â kcalÂ molâ1) among the SAMPL2 participants.",
    "actual_venue": "Journal Of Computer-Aided Molecular Design"
  },
  {
    "abstract": "In this study, a communication-avoiding generalized minimum residual method (CA-GMRES) is implemented on a hybrid CPUâGPU cluster, targeted for the performance acceleration of iterative linear system solver in the gyrokinetic toroidal five-dimensional Eulerian code (GT5D). In the GT5D, its sparse matrix-vector multiplication operation (SpMV) is performed as a 17-point stencil-based computation. The specialized part for the GT5D is only in the SpMV, and the other parts are usable also for other application program codes. In addition to the CA-GMRES, we implement and evaluate a modified variant of CA-GMRES (M-CA-GMRES) proposed in the previous study Idomura et al. (in: Proceedings of the 8th workshop on latest advances in scalable algorithms for large-scale systems (ScalA â17), 2017. https://doi.org/10.1145/3148226.3148234) to reduce the amount of floating-point calculations. This study demonstrates that beneficial features of the CA-GMRES are in its minimum number of collective communications and its highly efficient calculations based on dense matrixâmatrix operations. The performance evaluation is conducted on the Reedbush-L GPU cluster, which contains four NVIDIA Tesla P100 (Pascal GP100) GPUs per compute node. The evaluation results show that the M-CA-GMRES or CA-GMRES for the GT5D is advantageous over the GMRES or the generalized conjugate residual method (GCR) on GPU clusters, especially when the problem size (vector length) is large so that the cost of the SpMV is less dominant. The M-CA-GMRES is 1.09Â Ã, 1.22Â Ã and 1.50Â Ã faster than the CA-GMRES, GCR and GMRES, respectively, when 64 GPUs are used.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "A classical mechanistic model was developed to capture the existence of pre-sliding tangential deflection (PSTD) in contacting polysilicon and coated polysilicon surfaces. For the purposes of modeling asperity friction, experiments have shown, and been supported through detailed finite element analyses, that frictional forces developed through tangential sliding scale linearly through a material parameter known as the junction strength. A junction strength model coupled with a discrete quasi-static contact mechanics analysis, using contacting surface descriptions sampled by AFM from actual polysilicon surfaces, predicts inelastic tangential displacements that are qualitatively consistent with observed PSTD response. The simulations imply that the existence of PSTD depends not only on the spatial characteristics of contacting surfaces, but also on the local loading characteristics.",
    "actual_venue": "Icmens"
  },
  {
    "abstract": "Longitudinal sequences of infant brain MR images are increasingly applied in early brain development studies, while their registration are highly challenging as rapid brain development causes drastic image appearance changes. To this end, we propose a novel sparsity-learning-based strategy to tackle the longitudinal registration of infant subject. First, we prepare a set of intermediate sequences, whose longitudinal (voxel-to-voxel) correspondences are established in advance. For each time point of the subject, we then utilize sparsity learning to identify its correspondences in the intermediate images at the same age and thus of similar appearances. Next, the intermediate sequences are used to bridge the temporal \"gaps\" between different subject time points, while the sparsity-learning-based correspondence detection is jointly conducted for all subject images to impose the temporal consistency. Finally, the deformation field of each subject time point is reconstructed from the spatio-temporal correspondences. Experimental results show that our method is able to achieve the longitudinal registration of the infant subject despite its varying appearances along time.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Large multi-resolution terrain data sets are usually stored out-of-core. To visualize terrain data at interactive frame rates, the data needs to be organized on disk, loaded into main memory part by part, then rendered efficiently. Many main-memory algorithms have been proposed for efficient vertex selection and mesh construction. Organization of terrain data on disk is quite difficult because the error, the triangulation dependency and the spatial location of each vertex all need to be considered. Previous terrain clustering algorithms did not consider the per-vertex approximation error of individual terrain data sets. Therefore, the vertex sequences on disk are exactly the same for any terrain. In this paper, we propose a novel clustering algorithm which introduces the level-of-detail (LOD) information to terrain data organization to map multi-resolution terrain data to external memory. In our approach the LOD parameters of the terrain elevation points are reflected during clustering. The experiments show that dynamic loading and paging of terrain data at varying LOD is very efficient and minimizes page faults. Additionally, the preprocessing of this algorithm is very fast and works from out-of-core.",
    "actual_venue": "Visualization And Data Analysis"
  },
  {
    "abstract": "Where object-oriented languages deal with objects as described by classes, model-driven development uses models, as graphs of interconnected objects, described by metamodels. A number of new languages have been and continue to be developed for this model-based paradigm, both for model transformation and for general programming using models. Many of these use single-object approaches to typing, derived from solutions found in object-oriented systems, while others use metamodels as model types, but without a clear notion of polymorphism. Both of these approaches lead to brittle and overly restrictive reuse characteristics. In this paper we propose a simple extension to object-oriented typing to better cater for a model-oriented context, including a simple strategy for typing models as a collection of interconnected objects. Using a simple example we show how this extended approach permits more flexible reuse, while preserving type safety.",
    "actual_venue": "Models"
  },
  {
    "abstract": "In several fields of medicine, transportation and security the instantaneous gaze direction of a person under supervision is of crucial importance. We developed a contactless stereoscopic video-based eye tracker which works without any individual calibration. In real time it delivers information about the gaze direction frame by frame. The introduced algorithms are designed for computing the gaze direction within image acquisition time that is only limited by the hardware setup. The cameras are integrated into front-end modules by means of FPGA (programmable logic) circuits for image processing. Computation of the gaze direction is based on the spatial position of the pupil which is detected by a five-dimensional Hough transform. The system works under ambient light conditions whereas additional infrared illumination can be used to become independent of ambient light.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "The increasing use of more resource-intensive multimedia applications in communication has made it essential to ensure better utilization of available computing resources. At the same time, energy consumption has turned out to be one of the most important resource constraints in modern systems. Digital videos are an important part of multimedia, and a large number of video standards are currently available. In this paper, we work on the most commonly used video standard named H.264. We propose a method to reduce the energy consumption involved in video decoding by selective degradation of video quality. Experiments on the LIVE video database show that our proposed method is quite effective in practice.",
    "actual_venue": "Icdcit"
  },
  {
    "abstract": "This works presents an experience on the use of KDD (Knowledge Discovery in Databases) to identify and understand whether curriculum revisions can affect students in a Brazilian university. Presently, there is no framework to define the notion of impact caused by curriculum revisions, and the use of KDD can bring significant contributions, given the amount of data involved. The paper describes the analysis framework defined so far for measuring the impact of curriculum revisions, and reports the results obtained after the analysis of students records related to five distinct degrees. The results obtained so far indicate that individual revisions quite often do not affect students, being sometimes even beneficial to them. However. considering the set of revisions they face during their academic lifetime, it is possible to generalize that many students are lightly harmed. This harm influences the number of extra-classes they have to take to fulfill the requirements for obtaining a given degree, but the time required to graduate is not affected by revisions.",
    "actual_venue": "Data Mining And Knowledge Discovery: Theory, Tools And Technology"
  },
  {
    "abstract": "Cellular automata are a massively parallel computation model with discrete time and local rules. They are well adapted to biological or physical simulations. However, they are intrinsically anisotropic. The possibility of computing isotropic figures on cellular automata such as circles has already been proved.(4) Moreover, the previous construction enables to compute all the major discretizations known in the literature. We present in this article an extension of this work to the construction of spheres in three dimensions. A local characterization of a sphere is presented based upon the relationship between spheres and circles. This leads to the possibility of constructing a family of concentric discrete spheres in real time. Moreover, the approach can use many discretization schemes leading to the construction of various discrete spheres as done for circles.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "With the worldwide growth of the Internet, research on Cross-Language Information Retrieval (CLIR) is being paid much attention. Existing CLIR approaches based on query translation require parallel corpora or comparable corpora for the disambiguation of translated query terms. However, those natural language resources are not readily available. In this paper, we propose a disambiguation method for dictionary-based query translation that is independent of the availability of such scarce language resources, while achieving adequate retrieval effectiveness by utilizing Web documents as a corpus and using co-occurrence information between terms within that corpus. In the experiments, our method achieved 97% of manual translation case in terms of the average precision.",
    "actual_venue": "Iral"
  },
  {
    "abstract": "Ensuring that a particular and unsuspecting member of a group is the recipient of a salient-item hand-over is a complicated interaction. The robot must effectively, expediently and reliably communicate its intentions to advert any tendency within the group towards antinormative behaviour. In this paper, we study how a robot can establish the participant roles of such an interaction using imitated social and contextual cues. We designed two gaze cues, the first was designed to discourage antinormative behaviour through individualising a particular member of the group and the other to the contrary. We designed and conducted a field experiment (456 participants in 64 trials) in which small groups of people (between 3 and 20 people) assembled in front of the robot, which then attempted to pass a salient object to a particular group member by presenting a physical cue, followed by one of two variations of a gaze cue. Our results showed that presenting the individualising cue had a significant (z=3.733, p=0.0002 ) effect on the robot's ability to ensure that an arbitrary group member did not take the salient object and that the selected participant did.",
    "actual_venue": "HRI"
  },
  {
    "abstract": "Once acquainted with the modern information and communication tools made available with the advent of the Internet, five Brazilian rural communities participating in a pilot project to develop a self-sustaining telecenter model, engaged in citizen journalism using inexpensive digital video cameras. Community members used Web 2.0 collaborative tools to post short videos on the telecenter portal. The 95 video blogs published between September 2006 and May 2008 recorded various aspects of community life, including religious celebrations, oral history arts and crafts traditions, folklore, and environmental concerns. This study evaluates the impact of video blogging in these communities.",
    "actual_venue": "Andt"
  },
  {
    "abstract": "This paper studies the coverage probability of a device-to-device (D2D) link between a pair of nodes uniformly distributed in a circular region of arbitrary radius, thereby modeling the D2D network operation for a public safety scenario. The expression for the cumulative distribution function (CDF) of the signal-to-interference ratio (SIR) at the destination is derived, where the transmissions are affected by multipath fading, path loss and interference. The analysis involves finding the ratio distribution of two random variables, representing the desired signal and the intra-region interference, respectively. The expression for CDF helps in ascertaining the outage probability at the destination. Hence, given a desired outage probability, a limit on the number of simultaneously active D2D links in a circular region can be determined, thereby allowing interference avoidance. Numerical simulations are conducted to validate the theoretical model. The KolmogorovâSmirnov (KâS) test is applied to further characterize the matching between simulation and analytical model. Moreover, the results also help in identifying the minimum transmit power that ensures the desired quality-of-service (QoS), providing efficient transmission in energy constrained environments.",
    "actual_venue": "Physical Communication"
  },
  {
    "abstract": "For the recent two years, YouTube has already become one of the most successful VOD systems. Since it is established in early 2005, the number of the visits to YouTube not only maintain a high level, but also steadily increases even these days. It is widely believed that the online social networks contribute to this success. To study one new aspect of how much social networks contribute to the popularity of videos in YouTube, this paper provides an in-depth study into the effects of the external links in YouTube. Our study shows interesting characteristics of external links in YouTube. The existed studies reveal that `Music' and `Comedy' videos are the most popular ones. Compared with this, our study shows that the `Sports' and `Science & Technology' videos gains the most largest proportion of views from external links. Also, the percentage of views from external links reduces with power law. More specifically, we find that the top-5 external links contribute the popularity of the video most in the first four days and percentage can be up to 25%. For the video over than one years old, the top-5 external links are also able to contribute a stable part at about 4.5%. To end this, we hope our work can serve as a initial step for the study of the external environment.",
    "actual_venue": "Globecom"
  },
  {
    "abstract": "An active scope camera (ASC) is a special type of videoscope for search and rescue operations; the ASC has a self-propelled mechanism with a ciliary vibration drive. From an analysis of the problems encountered with the conventional ASC during applications to practical disaster environments, we propose a new tube-type ASC with enhanced mobility and practical functionality for rescue activities. We designed a smart structure to mount vibration motors on a long tubular cable without any rigid projections. A suitable tubular cable material was also selected experimentally to propagate the vibration efficiently. We integrated several practical functions such as a head bending structure, an auditory communication system, and a gravity indicator for the head orientation. We conducted several fundamental performance experiments. Finally, the enhanced performance of the tube-type ASC for practical use was demonstrated at a training site for first responders.",
    "actual_venue": "Intelligent Robots And Systems"
  },
  {
    "abstract": "Trustworthy data processing, which ensures the credibility and irrefutability of data, is crucial in many business applications. Recently, the Write-Once-Read-Many (WORM) devices have been used as trustworthy data storage. Nevertheless, how to efficiently retrieve data stored in WORM devices has not been addressed sufficiently and thus remains a grand challenge for large trustworthy databases. In this paper, we describe a trustworthy search tree framework (called TS-tree), which is a simple yet effective nonalterable search tree index for trustworthy databases. It can take the role of B-trees in trustworthy databases to answer various queries including range queries. It is efficient and scalable on larg",
    "actual_venue": "Aina"
  },
  {
    "abstract": "Basic properties of a new class of strictly positive real (SPR) functions are stated. Four problems are studied. The first deals with SPR preservation of transfer functions, obtained under the composition of polynomials with SPR0 functions. The second deals with Hurwitz stability preservation of the numerator of transfer functions, obtained under the composition of polynomials with SPR0 functions. The third deals with making a Hurwitz closed-loop plant, an SPR0 function by substituting s by SPR0 functions. The four deals with the synthesis of simultaneous SPR feedback plants. For the new class of SPR0 functions, a characterization is presented. For the first and second problems, sufficient conditions are presented using the new class of SPR0 functions. For the third and four problems, two examples are presented, the first being for simultaneous SPR closed-loop systems via constant controllers. The second is for simultaneous stabilization via universal feedback adaptive control.",
    "actual_venue": "Journal Of The Franklin Institute"
  },
  {
    "abstract": "A new via minimization approach is presented for two layer routing of printed circuit boards and VLSI chips. We have analyzed and characterized different aspects of the problem and have derived an equivalent graph model for the problem from the linear programming formulation. Based on the analysis of our unified formulation, we posed a practical heuristic algorithm. The algorithm can handle both grid-based and gridless routing. Also, an arbitrary number of wires is allowed to intersect at a via and we allow both Manhattan and knock-knee routings.",
    "actual_venue": "Anaheim, Ca, Usa"
  },
  {
    "abstract": "Real industry projects and teamwork can have a great impact on student learning and commitment, but provision of these features also produces significant challenges for academic assessment. This paper describes an approach to evaluate teams and individuals who develop unique industry projects. The paper documents several years of experimentation with different approaches, including both failures and refinements to successful approaches. The final approach adopted allows application of the same criteria to all students even though they are working on different projects. It structures individual accountability to ensure all members of a team contribute. Assessment data is collected from multiple assessors linking them to the learning outcomes and providing the students with an understanding of personal strengths and weaknesses.",
    "actual_venue": "ACE"
  },
  {
    "abstract": "This paper provides the derivation of speckle reducing anisotropic diffusion (SRAD), a diffusion method tailored to ultrasonic and radar imaging applications. SRAD is the edge-sensitive diffusion for speckled images, in the same way that conventional anisotropic diffusion is the edge-sensitive diffusion for images corrupted with additive noise. We first show that the Lee and Frost filters can be cast as partial differential equations, and then we derive SRAD by allowing edge-sensitive anisotropic diffusion within this context. Just as the Lee and Frost filters utilize the coefficient of variation in adaptive filtering, SRAD exploits the instantaneous coefficient of variation, which is shown to be a function of the local gradient magnitude and Laplacian operators. We validate the new algorithm using both synthetic and real linear scan ultrasonic imagery of the carotid artery. We also demonstrate the algorithm performance with real SAR data. The performance measures obtained by means of computer simulation of carotid artery images are compared with three existing speckle reduction schemes. In the presence of speckle noise, speckle reducing anisotropic diffusion excels over the traditional speckle removal filters and over the conventional anisotropic diffusion method in terms of mean preservation, variance reduction, and edge localization.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "Vision-based human face detection and recognition are widely used and have been shown to be effective in normal illumination conditions. Under severe illumination conditions, however, it is very challenging. In this paper, we address the effect of illumination on the face detection and the face recognition problem by introducing a novel illumination invariant method, called OptiFuzz. It is an optimized fuzzy-based illumination invariant method to solve the effect of illumination for photometric-based human face recognition. The rule of the Fuzzy Inference System is optimized by using a genetic algorithm. The Fuzzyâs output controls an illumination invariant model that is extended from Landâs reflectance model. We test our method by using Yale B Extended and CAS-PEAL face databases to represent the offline experiments, and several videos are recorded at our campus to represent the online indoor and outdoor experiments. ViolaâJones face detector and mutual subspace method are employed to handle the online face detection and face recognition experiments. Based on the experimental results, we can show that our algorithm outperforms the existing and the state-of-the-art methods in recognizing a specific person under variable lighting conditions with a significantly improved computation time. Other than that, using illumination invariant images is also effective in improving the face detection performance.",
    "actual_venue": "Mach Vis Appl"
  },
  {
    "abstract": "Motivated by a scientific application, where virtual organisations are dynamically created to achieve specific goals by sharing resources and information, we propose the synthesis of two lines of research: policy-based access control and distributed firewalls. Through this fusion we expect to deliver a scalable method of setting up security infrastructures for Grid computing infrastructures.",
    "actual_venue": "Policy"
  },
  {
    "abstract": "Inertial measurement units (IMUs) are successfully utilized to compensate localization errors in sensor fused inertial navigation systems. An IMU generally produces high-frequency signals ranging from 100 to 1000 Hz, and preintegration methods are applied to effectively process these high-frequency signals for inertial navigation systems. The main problem with an existing preintegration method is ...",
    "actual_venue": "Ieee Transactions On Automation Science And Engineering"
  },
  {
    "abstract": "We study the problem of how to stream lay- ered video (live and stored) over a lossy packet network in order to optimize the video quality that is rendered at the receiver. We present a unified framework that com- bines scheduling, FEC error protection, and decoder error concealment. In the context of the unified framework, we study both the case of a channel with perfect state infor- mation and the case of a channel with imperfect state in- formation (delayed or lost feedback). We adapt the theory of infinite-horizon, average-reward Markov decision pro- cesses (MDPs) with average-cost constraints to the prob- lem. Based on simulations with MPEG-4 FGS video, we show that (1) optimizing together scheduling, FEC error correction and error concealment improves performance significantly and (2) policies with static error protection give near-optimal performance. We also find that degradations in quality for a channel with imperfect state information are small; thus our MDP approach is suitable for networks with long end-to-end delays.",
    "actual_venue": "Infocom"
  },
  {
    "abstract": "With the increasing demand fur secure and high-quality communications in public access wireless IP networks, it is very important to have an in-depth understanding of the relationship between the security and quality of service (QoS). In wireless networks, authenti- cation can provide secure communications by preventing unauthorized usage and negotiating the credentials for data transmission. Nevertheless, it induces heavy overhead to data transmission, further deteriorating overall system performance. Thus, we analyze the impact of authen- tication an the security and QoS quantitatively in this paper. First, we introduce a system model based on a challeng4response authentication, which is widely used in many mobile environments. Then, a concept of security level is proposed to describe the protection of communica- tions according to the nature UP security, i.e., information secrecy, data integrity, and resource availability. By taking traffic and mobility patterns into account, our approach establishes a direst and quantitative connection between the security and QoS through the authentication. Finally, numerical results are provided to demonstrate the impact of security levels, mobikity and traffic patterns on overall system performance in terms of authentication delay and call dropping probability.",
    "actual_venue": "Infocom Seventeenth Annual Joint Conference Of The Ieee Computer And Communications Societies Proceedings Ieee"
  },
  {
    "abstract": "Hilbert's \"-calculus is based on an extension of the language of predicate logic by a term-forming operator \"x. Two fundamental results about the \"-calculus, the first and second epsilon theorem, play a role similar to that which the cut-elimination theorem plays in sequent calculus. In particular, Herbrand's Theorem is a consequence of the epsilon theorems. The paper investigates the epsilon theorems and the complexity of the elimination procedure underlying their proof, as well as the length of Herbrand disjunctions of existential theorems obtained by this elimination procedure.",
    "actual_venue": "Studia Logica"
  },
  {
    "abstract": "â¢Two modes of competition have developed in broadband markets: inter-platform and intra-platform competition.â¢We find that inter-platform competition has no significant effects over prices in Spanish market.â¢On the contrary, intra-platform competition is a key driver of the prices charged to consumers.â¢The impact of modes of competition on prices is affected by features of different national markets.",
    "actual_venue": "Information Economics And Policy"
  },
  {
    "abstract": "We present a local as well a semilocal convergence analysis for Newton's method in a Banach space setting. Using the same Lipschitz constants as in earlier studies, we extend the applicability of Newton's method as follows: local case: a larger radius is given as well as more precise error estimates on the distances involved. Semilocal case: the convergence domain is extended; the error estimates are tighter and the information on the location of the solution is at least as precise as before. Numerical examples further justify the theoretical results.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "In this study, the authors consider the parameter estimation problem of the response signal from a highly non-linear dynamical system. The step response experiment is taken for generating the measured data. Considering the stochastic disturbance in the industrial process and using the gradient search, a multi-innovation stochastic gradient algorithm is proposed through expanding the scalar innovat...",
    "actual_venue": "Iet Signal Processing"
  },
  {
    "abstract": "Despite the efforts to improve HCI issues, the users still find problems when interacting with applications. This naturally demands the need for help systems. But the construction of such systems depends on the availability of information (and the necessary reflections upon them) to compose its contents. This work investigates if the MoLIC language can contribute to the construction of help systems by checking the design reflections that this language leads a designer to exercise.",
    "actual_venue": "IHC"
  },
  {
    "abstract": "Search in general, and heuristic search in particular, is at the heart of many Artificial Intelligence algorithms and applications. There is now a growing and active community devoted to the empirical and theoretical study of heuristic search algorithms, thanks to the successful application of search-based algorithms to areas such as robotics, domain-independent planning, optimization, and computer games. In this extended abstract we highlight recent efforts in understanding suboptimal search algorithms, as well as ensembles of heuristics and algorithms. The result of these efforts are meta-reasoning methods which are applied to orchestrate the different components of modern search algorithms. Finally, we mention recent innovative applications of search that demonstrate the relevance of the field to general AI.",
    "actual_venue": "Thirty-First Aaai Conference On Artificial Intelligence"
  },
  {
    "abstract": "This paper presents a parallel join algorithm for the data-parallel execution model used in SIMD architectures. This algorithm is hash-based i.e., the tuples in a relation are divided into different buckets based on the hash value of the join attribute. In this algorithm the buckets are maintained in a distributed fashion, i.e., the tuples in a bucket are stored in an array of processors. The join operation is performed in parallel over all the buckets. The algorithm presented here has been implemented and evaluated on the Connection Machine (CM-2). We present here the results of the experimental evaluation of this algorithm for different values of design parameters and work-load. Using experimental evaluations of the CM communication primitives we develop analytical models for the performance evaluation of this algorithm and demonstrate the effectiveness of these models.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "This paper presents a keyword-based information visualization technique for unstructured text sequences. The text sequence data comes from nursing narratives records, which are mostly text fragments with incomplete and unreliable grammatical structures. Proper visualization of such text sequences can reveal patterns and trend information rooted in the text records, and has significant applications in many fields such as medical informatics and text mining. In this paper, an Iterative Visual Clustering (IVC) technique is developed to facilitate multi-scale visualization, and at the same time provide abstraction and knowledge discovery functionalities at the visualization level. Interactive visualization and user feedbacks are used to iteratively group keywords to form higher level concepts and keyword clusters, which are then feedback to the visualization process for evaluation and pattern discovery. Distribution curves of keywords and their clusters are visualized at various scales under Gaussian smoothing to search for meaningful patterns and concepts.",
    "actual_venue": "Visual"
  },
  {
    "abstract": "We solve the compressive sensing problem via convolutional factor analysis, where the convolutional dictionaries are learned in situ from the compressed measurements. An alternating direction method of multipliers (ADMM) paradigm for compressive sensing inversion based on convolutional factor analysis is developed. The proposed algorithm provides reconstructed images as well as features, which can be directly used for recognition (e.g., classification) tasks. We demonstrate that using similar to 30% (relative to pixel numbers) compressed measurements, the proposed model achieves the classification accuracy comparable to the original data on MNIST.",
    "actual_venue": "Ieee International Conference On Image Processing"
  },
  {
    "abstract": "Several works point out class imbalance as an obstacle on applying machine learning algorithms to real world domains. However, in some cases, learning algorithms perform well on several imbalanced domains. Thus, it does not seem fair to directly correlate class imbalance to the loss of performance of learning algorithms. In this work, we develop a systematic study aiming to question whether class imbalances are truly to blame for the loss of performance of learning systems or whether the class imbalances are not a problem by themselves. Our experiments suggest that the problem is not directly caused by class imbalances, but is also related to the degree of overlapping among the classes.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Wireless access points (APs) divide a plane into small areas where their coverage ranges overlap. A mobile device can be located within a particular small overlapped area based on the unique set of APs covering the device. We formally define an Optimal Loc-deployment problem for both coverage and area localization. Our objective is to deploy a minimum number of APs that provide full communication coverage while achieving the ability to locate a mobile device within a certain area no larger than a given accuracy parameter. We obtain a formula that precisely determines the optimal solution for more than half of the accuracy values. For the rest of the accuracy values, we propose an algorithm that will return an approximation whose difference to the optimal solution is less than Îµ for any Îµ >; 0. Finally, we conduct extensive numerical evaluation and real experiments to validate our proposed solutions.",
    "actual_venue": "Ieee T Vehicular Technology"
  },
  {
    "abstract": "Pakistanu0027s agricultural sector has been making gigantic contributions towards the nationu0027s economy, with agriculture accounting for 22% of the gross domestic product (GDP) while engaging approximately half of the countryu0027s labor force. A significant developmental challenge in this sector is inadequacy and inaccessibility of information regarding weather forecast. In this paper, we propose an Android-based solution for farmers that can facilitate the timely, localized, and customized dissemination of granular weather forecast that shields the whole agricultural ecosystem and supply chain from weather variability by appropriate decision-making. We describe our Android mobile application that sends a customized weather forecast that is configured according to the user preferences. Information is disseminated by the cloud server through encrypted SMS to the subscribing farmers containing weather information. This information is encoded through visuals and icons in a simple to understand user-interface that is accompanied by Urdu language text in a design tailored for low-literate farmers of Pakistan. The testing, feedback, and evaluation include design understanding, the effectiveness of icons and images, usability, adaptation to touch screen is in progress which will help us to reiterate the mobile app user interface (UI) to improve the preliminary design.",
    "actual_venue": "Ictd"
  },
  {
    "abstract": "This paper explores the capabilities of multi-objective genetic algorithms to cluster genomic data. We used multiple objective functions not only to further expand the clustering abilities of the algorithm, but also to give more biological significance to the results. Particularly, we grouped a large set of proteins described by a set collection of genomic attributes to infer functional interactions among them. We conducted various computational experiments that demonstrated the proficiency of the proposed method when compared to algorithms that rely on a single biological parameter.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "Many distributed interactive multimedia applications, such as live video conferencing and video sharing, require each participating client to transmit its captured video stream to other clients via relay servers. We consider connecting multiple clients through multiple relay servers and study the server selection problem from a dense pool of content delivery network edge locations and datacenters to reduce the end-to-end delays between clients. To achieve scalability in the presence of a large number of candidate servers, we formulate server selection as a geometric problem in a delay space instead of in a graph, which turns out to be an extension of the well-known Euclidean k-median problem. We propose practical approximation schemes when using only one or two servers with theoretical worst-case guarantees as well as fast heuristics when using k servers. We demonstrate the benefit of our optimized multiserver selection schemes through extensive evaluation based on real-world traces collected from the PlanetLab and Seattle platforms, containing personal mobile devices as well as real network experiments based on a prototype implementation.",
    "actual_venue": "Ieee Trans Multimedia"
  },
  {
    "abstract": "Stability of systems with a single delay and delay-dependent coefficients is studied along the line of the tau-decomposition approach. Criteria for determining crossing directions of imaginary characteristic roots with possibly multiplicity are presented, with which system stability for any given delay value can be determined in a systematic way. In contrast to the previous research on this type of systems, our analysis is based on a novel two-parameter framework. With the new geometric insight, stronger criteria concerning the crossing direction of imaginary characteristic roots with possibly multiplicity can be obtained using simplified and intuitive arguments. The stability analysis procedure is illustrated with an example inspired by biological applications.",
    "actual_venue": "American Control Conference"
  },
  {
    "abstract": "An efficient and practicable MIMO transceiver in which transmitter antenna selection is applied to QR detector and GMD precoding through limited feedback channel is implemented. For over 4 ÃÂ 5 antenna selection, the proposed antenna selection scheme can save more than 50% computational complexity compared with that of the exhausting method. From the simulation results, the proposed transceiver can achieve over 6 dB SNR improvement over the open-loop V-BLAST counterparts at BER=10-2 under i.i.d. channel. Finally, a MIMO joint transceiver hardware platform on a Xilinx FPGA is realized to verify the proposed algorithm and architecture.",
    "actual_venue": "Asp-Dac"
  },
  {
    "abstract": "We construct a model to study tradeoffs associated with aging in the adaptive immune system, focusing on cumulative effects of replacing naive cells with memory cells. Binding affinities are characterized by a stochastic shape space model. System loss arising from an individual infection is associated with disease severity, as measured by the total antigen population over the course of an infection. We monitor evolution of cell populations on the shape space over a string of infections, and find that the distribution of losses becomes increasingly heavy-tailed with time. Initially this lowers the average loss: the memory cell population becomes tuned to the history of past exposures, reducing the loss of the system when subjected to a second, similar infection. This is accompanied by a corresponding increase in vulnerability to novel infections, which ultimately causes the expected loss to increase due to overspecialization, leading to increasing fragility with age (i.e., immunosenescence). In our model, immunosenescence is not the result of a performance degradation of some specific lymphocyte, but rather a natural consequence of the built-in mechanisms for system adaptation. This \"robust, yet fragile\" behavior is a key signature of Highly Optimized Tolerance.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "In this paper we discuss strategies concerning the implementation of an agent-based simulation of complex phenomena. The model we consider accounts for population decomposition and interaction in industrial districts. The approach we follow is twofold: on one hand, we implement progressively more complex models using different approaches (vertical multiple implementations); on the other hand, we replicate the agent-based simulation with different implementations using jESOF, JAS and plain C++ (horizontal multiple implementations). By using both different implementation approaches and a multiple implementation strategy, we highlight the benefits that arise when the same model is implemented on radically different simulation environments, comparing the advantages of multiple modeling implementations. Our findings provide some important suggestions in terms of model validation, showing how models of complex systems tend to be extremely sensitive to implementation details. Finally we point out how statistical techniques may be necessary when comparing different platform implementations of a single model.",
    "actual_venue": "The Journal Of Artificial Societies And Social Simulation"
  },
  {
    "abstract": "In non-parametric image registration it is often not possible to work with the original resolution of the images due to high process- ing times and lack of memory. However, for some medical applications the information contained in the original resolution is crucial in certain regions of the image while being negligible in others. To adapt to this problem we will present an approach using tensor grids, which provide a sparser image representation and thereby allow the use of the high- est image resolution locally. Applying the presented scheme to a lung ventilation estimation shows that one may considerably save on time and memory while preserving the registration quality in the regions of interest.",
    "actual_venue": "Bildverarbeitung Fr Die Medizin"
  },
  {
    "abstract": "Fuzzy k-means and vector quantization are combined in this paper to complement each other in incremental mode because each has qualities which the other lacks. The threshold of vector quantization is given and the pattern of computing the distance between the new coming data point and the k centers is introduced in a new way. We firstly reduce redundant attributes and eliminate the difference of units of dimensions and make units of all attributes same. Then, we use k-center to produce initial k means and partition data points into no more than k clusters. Besides, we adopt vector quantization to classify incremental data points and then adjust means after the structure of clustering varying. Finally, it is applied to real datasets and results show its efficiency and precision. Â© 2010 ACADEMY PUBLISHER.",
    "actual_venue": "JCP"
  },
  {
    "abstract": "E-Commerce has been plagued with problems since its inception and this paper examines one of these problems: The lack of user trust in E-commerce created by the risk of phishing. Phishing has grown exponentially together with the expansion of the Internet. This growth and the advancement of technology has not only benefitted honest Internet users, but has enabled criminals to increase their effectiveness which has caused considerable damage to this budding area of commerce. Moreover, it has negatively impacted on both the user and online business, breaking down the trust relationship between them. In an attempt to explore this problem, the following was considered; firstly, e-commerce's vulnerability to phishing attacks. By referring to the Common Criteria Security Model, various critical security areas within e-commerce are identified, and with that, the areas of vulnerability and weakness. Secondly, the methods and techniques used in phishing such as phishing emails, phishing websites and addresses, distributed attacks and redirected attacks as well as the data that phishers seek to obtain, is examined. Furthermore, the way to reduce the risk of phishing and in turn increase the trust between users and websites is explored. Here the importance of Trust and the Uncertainty Reduction Theory plus the fine balance between trust and control is explored. Finally, the paper presents Critical Success Factors that aid in phishing prevention and control, these being: User Authentication, Website Authentication, Email Authentication, Data Cryptography, Communication, and Active Risk Mitigation.",
    "actual_venue": "Information Security South Africa"
  },
  {
    "abstract": "In this paper two parallel algorithms for solving dense linear equations are discussed. The algorithms are based on lSU-decomposition followed by forward and backward substitutions. The algorithms are numerically stable and have been tested on the Sequent Balance Machine with efficient utilization of all processors.",
    "actual_venue": "International Journal Of Computer Mathematics"
  },
  {
    "abstract": "Traffic grooming is the technique which combines low-bandwidth traffics into higher-bandwidth channels in order to satisfy certain constraints as to maximize the traffic throughput, minimize the connection-blocking probability, or optimize the wavelength bandwidth exploitation. A case considered in this paper is optimization of the multiplexing of customer services into bursts at the edge nodes of an optical network. This is formulated as an integer linear program and dealt with basing on the Hopfield network. The methods based on neural networks normally require a considerable delay which is hard to practice for optical data transport. This paper also proposes solutions to reduce this delay.",
    "actual_venue": "Icw/Ichsn/Icmcs/Senet"
  },
  {
    "abstract": "Advances in mobile computing and computer vision can support camera-based interactions with mobile devices, including systems that use image-matching to support getting information about objects identified by the camera. These interfaces, sometimes considered mobile augmented reality, can be applied in many domains. This paper reports on a field study of these interfaces in a tourism application, which begins to address questions about embodied interaction, existing photo-taking practices, and alternative interfaces.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "Summary form only given. The most challenging issues in nomadic computing environments arise from the combination of heterogeneity, dynamism, context-awareness, and mobility. Driven by these issues, this paper presents a new middleware infrastructure, named ESPERANTO, to support the integration of diverse nomadic computing domains. This middleware aims to glue the emerging heterogeneous nomadic computing technologies and service oriented architectures.",
    "actual_venue": "Aiccsa"
  },
  {
    "abstract": "In this paper, we consider a high-fidelity Convoy Movement Problem motivated by the coordination and routing of convoys within a road transportation network in an urban city. It encompasses two classical combinatorial optimization problems - vehicle routing and resource constrained scheduling. We present an effective hybrid algorithm to dynamically manage the movement of convoys, where we combine the standard Dijkstra's shortest-path algorithm with constraint programming techniques. The effectiveness of the algorithm is illustrated with testing on varying problem sizes and complexity.",
    "actual_venue": "Aaai"
  },
  {
    "abstract": "Next-generation Internet architectures require designs with inherent security guarantees.We present a network architecture that uses credentials to audit traffic in the data path, where defenses can be employed often more quickly and efficiently than on end-systems.",
    "actual_venue": "Ancs"
  },
  {
    "abstract": "Nous passons en revue dans cet article une sÃ©rie de mÃ©thodes utilisÃ©es pour l'Ã©valuation des systÃ¨mes de conversion texte-parole et commentons leurs avantages et dÃ©savantages respectifs. Ce panorama se limite aux mÃ©thodes subjectives, Ã  savoir celles utilisant des auditeurs humains. Nous ne traiterons pas ici des mÃ©thodes objectives qui tentent d'Ã©valuer la qualitÃ© par des techniques basÃ©es sur le traitement de signal. Dans la section 1, nous discutons de quatre facteurs influenÃ§ant les caractÃ©ristiques de l'Ã©valuation: les composants du systÃ¨me de conversion texte-parole, le niveau du texte, aspects de la parole et la fonction. Dans lessections 2 et 3, nous prÃ©sentons des mÃ©thodes en relation avec les modules linguistique et acoustico-phonÃ©tique. Enfin, dans la section 4, nous tirons quelques conclusions gÃ©nÃ©rales.",
    "actual_venue": "Speech Communication"
  },
  {
    "abstract": "This article finds that US queer youth of color prefer Tumblr to express intimate feelings and personal politics over other social media such as Facebook. It is based on 5years of cyberethnographic research in queer Tumblr circulations as well as multi-year rounds of qualitative interviewing with queer youth informants. Several informants experienced drastically negative consequences, to the point of being disowned by their families, because of what this article calls a design bias toward default publicness that shapes user experience on social media such as Facebook. This article identifies four design decisions that create default publicness on social media platforms, viewing these decisions through queer, feminist, and critical race theories that have argued that the public is never neutral terrain. It understands these design decisions as imperatives of platform capitalism, which extracts robust and verifiable user data for monetization, and structures these spaces accordingly.",
    "actual_venue": "New Media And Society"
  },
  {
    "abstract": "The problem of finding optimal locations of base stations, their pilot powers and channel assignments in UMTS mobile networks belongs to a class of NP-hard problems, and hence, metaheuristics optimization algorithms are widely used for this task. Invasive Weed Optimization (IWO) algorithm is relatively novel and succussed in several real-world applications. Our experiments demonstrate that the IWO algorithm outperforms the algorithms such as Evolutionary Strategies (ES) and Genetic Algorithms (GA) for optimizing the UMTS mobile network.",
    "actual_venue": "Icaisc"
  },
  {
    "abstract": "Guideline-based clinical decision support systems (CDSSs) can be effective in increasing physician compliance with recommendations. However, the ever growing pace at which medical knowledge is produced requires that clinical practice guidelines (CPGs) be updated regularly It is therefore mandatory that CDSSs be revised accordingly The French Association for Urology publishes CPGs on bladder cancer management every 2 years. We studied the impact of the 2004 revision of these guidelines, with respect to the 2002 version with a CDSS, UroDoc. We proposed a typology of knowledge base modifications resulting from the update of CPGs making the difference between practice, clinical conditions and recommendations refinement as opposed to new practice and new recommendations. The number of formalized recommendations increased from 577 in 2002 to 1,081 in 2004. We evaluated the two versions of UroDoc on a randomized sample of patient records. A single new practice that modifies a decision taken in 49% of all recorded decisions leads to a fall from 67% to 46% of the compliance rate of decisions.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "Recent developments in flexible endoscopy and other fields of medical technology have raised the need for compact slender shafts that can be made rigid and compliant at will. A novel compact mechanism, named FORGUIDE, with this functionality was developed. The FORGUIDE shaft rigidifies due to friction between a ring of cables situated between a spring and an inflated tube. A mathematical model for...",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "We have developed a new computer-aided diagnosis scheme for automated detection of lung nodules in digital chest radiographs based on a combination of morphological features and the wavelet snake. In our scheme, two processes were applied in parallel to reduce the false-positive detections after initial nodule candidates were selected. One process consisted of adaptive filtering for enhancement of nodules and suppression of normal lung structures, followed by extraction of conventional morphological features. The other process consisted of a novel approach for elimination of false positives called the edge-guided wavelet snake model. In the latter process, multiscale edges of the candidate nodules were extracted to yield parts of the nodule boundaries. A wavelet snake was then used for fitting of these multiscale edges for approximation of the true boundaries of nodules. A boundary feature called the weighted overlap between the snake and the multiscale edges was calculated and used for elimination of false positives. Finally, the weighted overlap and the morphological features were combined by use of an artificial neural network for efficient reduction of false positives. Our scheme was applied to a publicly available database of digital chest images for pulmonary nodules. Receiver operating characteristic analysis was employed for evaluation of the performance of each process in the scheme. The combined features yielded a large reduction of false positives, and thus achieved a high performance in discriminating between true and false positives. These results show that our new method, in particular the false-positive reduction method based on the wavelet snake, is effective in improving the performance of a computerized scheme for detection of pulmonary nodules in chest radiographs.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "In this paper we investigate automated methods for externalizing internal memory data structures.We consider a class of balanced trees that we call weight-balanced partitioning trees (or wp-trees) for indexing a set of points in Rd.Well-known examples of wp-trees include kd-trees, BBD-trees, pseudo-quad-trees, and BAR-trees. Given an efficient external wp-tree construction algorithm, we present a general framework for automatically obtaining a dynamic external data structure. Using this framework together with a new general construction (bulk loading) technique of independent interest, we obtain data structures with guaranteed good update performance in terms of I/O transfers. Our approach gives considerably improved construction and update I/O bounds for e.g. external kd-trees and BBD-trees.",
    "actual_venue": "Icalp"
  },
  {
    "abstract": "Motivation: Nuclear magnetic resonance (NMR) spectroscopy has been used to study mixtures of metabolites in biological samples. This technology produces a spectrum for each sample depicting the chemical shifts at which an unknown number of latent metabolites resonate. The interpretation of this data with common multivariate exploratory methods such as principal components analysis (PCA) is limited due to high-dimensionality, non-negativity of the underlying spectra and dependencies at adjacent chemical shifts. Results: We develop a novel modification of PCA that is appropriate for analysis of NMR data, entitled Sparse Non-Negative Generalized PCA. This method yields interpretable principal components and loading vectors that select important features and directly account for both the non-negativity of the underlying spectra and dependencies at adjacent chemical shifts. Through the reanalysis of experimental NMR data on five purified neural cell types, we demonstrate the utility of our methods for dimension reduction, pattern recognition, sample exploration and feature selection. Our methods lead to the identification of novel metabolites that reflect the differences between these cell types. Availability: www.stat.rice.edu/~gallen/software.html Contact: gallen@rice.edu Supplementary Information:Supplementary data are available at Bioinformatics online.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "Online mining of data streams is an important data mining problem with broad applications. However, it is also a difficult problem since the streaming data possess some inherent characteristics. In this paper, we propose a new single-pass algorithm, called DSM-FI (data stream mining for frequent itemsets), for online incremental mining of frequent itemsets over a continuous stream of online transactions. According to the proposed algorithm, each transaction of the stream is projected into a set of sub-transactions, and these sub-transactions are inserted into a new in-memory summary data structure, called SFI-forest (summary frequent itemset forest) for maintaining the set of all frequent itemsets embedded in the transaction data stream generated so far. Finally, the set of all frequent itemsets is determined from the current SFI-forest. Theoretical analysis and experimental studies show that the proposed DSM-FI algorithm uses stable memory, makes only one pass over an online transactional data stream, and outperforms the existing algorithms of one-pass mining of frequent itemsets.",
    "actual_venue": "Knowl Inf Syst"
  },
  {
    "abstract": "Statements about entities occur everywhere, from newspapers and web pages to structured databases. Correlating references to entities across systems that use different identifiers or names for them is a widespread problem. In this paper, we show how shared knowledge between systems can be used to solve this problem. We present \"reference by description\", a formal model for resolving references. We provide some results on the conditions under which a randomly chosen entity in one system can, with high probability, be mapped to the same entity in a different system.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "A novel method is described for verifying the equivalence between a combinational circuit and its specification, when both are given in a modular (e.g., factored) form. It is based on the notion of cross-controllability and cross-observability relations that exist between the internal logic values across a cut of the joint composition of the circuit and the specification. It is proven that even after abstracting input and other internal variables the relations are sufficient to verify the equivalence. The abstraction allows reduction of the size of the relation, thus permitting the verification of much larger circuits. A report is presented on the verification of an 8*8 parallel multiplier using at most 527 BDD (binary decision diagram) cells of 21 variables. Extensions to sequential circuits are also discussed.<>",
    "actual_venue": "Santa Clara, Ca, Usa"
  },
  {
    "abstract": "Many quantitative structure-activity relationships (QSARs) have been published in pure organic chemistry as well as from biological sources since 1935 when Hammett introduced his equation. We have organized a database of about 6,000 QSARs about evenly divided from both sources. We discuss how the database can be searched for different types of QSAR and how this facilitates comparison of new QSARs with published results.<>",
    "actual_venue": "System Sciences, 1994. Proceedings of the Twenty-Seventh Hawaii International Conference Â "
  },
  {
    "abstract": "In this paper, a Chebyshev spectral collocation domain decomposition (DD) semi-discretization by using a grid mapping, derived by Kosloff and Tal-Ezer in space is applied to the numerical solution of the generalized Burger's-Huxley (GBH) equation. To reduce roundoff error in computing derivatives we use the above mentioned grid mapping. In this work, we compose the Chebyshev spectral collocation domain decomposition and Kosloff and Tal-Ezer grid mapping, elaborately. Firstly, the theory of application of the Chebyshev spectral collocation method with grid mapping and DD on the GBH equation is presented. This method yields a system of ordinary differential algebraic equations (DAEs). Secondly, we use a fourth order Runge-Kutta formula for the numerical integration of the system of DAEs. Application of this modified method to the GBH equation show that this method (M-DD) is faster and more accurate than the standard Chebyshev spectral collocation DD (S-DD) method.",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "The leader-following consensus problem of multiple uncertain Euler-Lagrange systems under switching network topology was studied using the distributed observer approach in one of our recent papers. However, the distributed observer in that paper assumed that all the followers knew the system matrix of the leader system. In this paper, we first propose a distributed adaptive observer which only assumes those followers who are the children of the leader know the system matrix of the leader system. Then, we further develop a distributive adaptive controller utilizing this distributed adaptive observer to solve the same problem as studied in our previous paper. As a result, we have removed the assumption that all the followers need to know the system matrix of the leader system.",
    "actual_venue": "Ram/Cis"
  },
  {
    "abstract": "We modelled the cortical columnar organisation to design a neuromimetic architecture for topological spatial learning and action planning. Here, we first introduce the biological constraints and the hypotheses upon which our model was based. Then, we describe the learning architecture, and we provide a series of numerical simulation results. The system was validated on a classical spatial learning task, the Tolman & Honzik's detourprotocol, which enabled us to assess the ability of the model to build topological representations suitable for spatial planning, and to use them to perform flexible goal-directed behaviour (e.g., to predict the outcome of alternative trajectories avoiding dynamically blocked pathways). We show that the model reproduced the navigation performance of rodents in terms of goal-directed path selection. In addition, we present a series of statistical and information theoretic analyses to study the neural coding properties of the learnt space representations.",
    "actual_venue": "Spatial Cognition"
  },
  {
    "abstract": "We study the Hospitals/Residents with Couples problem, a variant of the classical Stable Marriage problem. This is the extension of the Hospitals/Residents problem where residents are allowed to form pairs and submit joint rankings over hospitals. We use the framework of parameterized complexity, considering the number of couples as a parameter. We also apply a local search approach, and examine the possibilities for giving FPT algorithms applicable in this context. Furthermore, we also investigate the matching problem containing couples that is the simplified version of the Hospitals/Residents with Couples problem modeling the case when no preferences are given.",
    "actual_venue": "Discrete Optimization"
  },
  {
    "abstract": "Processor caches play a critical role in the performance of today\\\"s computer systems. As technology scales, due to manufacturing defects and process variations a large number of cells in a cache is expected to be faulty. The number of faulty cells varies from die to die and in the field of the application depends on the operating conditions (e.g., supply voltage, frequency). Several techniques have been proposed to tolerate faults in caches. A drawback of the redundancy based techniques is that the amount of redundancy is decided at the design time targeting a maximum number of faults, so in cases of a small number of faults (e.g., in the nominal supply voltage in a system with DVS) only a part of the redundant resources is used. In this paper we propose a new reconfigurable-self adaptive fault tolerant cache scheme. The unique characteristic of our scheme is that it uses its resources for both the reduction of the misses caused by the faulty blocks as well as for the reduction of conflict misses, depending on the number of faults, their distribution in the cache, and the running application. Our experimental results for a wide range of scientific applications and a plethora of fault maps with different SRAM failure probabilities reveal that our proposal can achieve significant benefits.",
    "actual_venue": "Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "In this paper we present a graph-based optimization method for information diffusion and attack durability in networks using properties of Complex Networks. We show why and how Complex Networks with Scale Free and Small World features can help optimize the topology of networks or indicate weak or strong elements of the network. We define some efficiency measures of information diffusion and attack durability in networks. Using these measures we formulate multicriteria optimization problem to choose the best network. We show a practical example of using the method based on an analysis of a few social networks.",
    "actual_venue": "Rsctc"
  },
  {
    "abstract": "Ubiquitous parallel computing aims to make parallel programming accessible to a wide variety of programming areas using deterministic and scale-free programming models built on a task abstraction. However, it remains hard to reconcile these attributes with pipeline parallelism, where the number of pipeline stages is typically hard-coded in the program and defines the degree of parallelism. This paper introduces hyperqueues, a programming abstraction that enables the construction of deterministic and scale-free pipeline parallel programs. Hyperqueues extend the concept of Cilk++ hyperobjects to provide thread-local views on a shared data structure. While hyperobjects are organized around private local views, hyperqueues require shared concurrent views on the underlying data structure. We define the semantics of hyperqueues and describe their implementation in a work-stealing scheduler. We demonstrate scalable performance on pipeline-parallel PARSEC benchmarks and find that hyperqueues provide comparable or up to 30% better performance than POSIX threads and Intel's Threading Building Blocks. The latter are highly tuned to the number of available processing cores, while programs using hyperqueues are scale-free.",
    "actual_venue": "SC"
  },
  {
    "abstract": "An edge-coloring of a graph G = ( V , E ) is a function c that assigns an integer c(e) (called color) in { 0 , 1 , 2 , Â¿ } to every edge e Â¿ E so that adjacent edges are assigned different colors. An edge-coloring is compact if the colors of the edges incident to every vertex form a set of consecutive integers. The deficiency problem is to determine the minimum number of pendant edges that must be added to a graph such that the resulting graph admits a compact edge-coloring. We propose and analyze three integer programming models and one constraint programming model for the deficiency problem. HighlightsWe describe integer and constraint programming models for the deficiency problem.We obtain bounds on the number of colors in an edge-coloring with minimum deficiency.We clearly show that symmetry breaking constraints decrease the computing time.",
    "actual_venue": "Computers And Operations Research"
  },
  {
    "abstract": "â¢We examine two channel structures in contract farming: Farmer-Firm; Farmer-Cooperative-Firm.â¢We characterize the conditions and opportunities for channel members to reach contract farming agreements.â¢We study one-farmer-one-firm, multi-farmer-one-firm, and multi-farmer-multi-firm cases.â¢Cost-sharing can yield a win-win outcome for farmer and firm, compared to wholesale price contract.â¢A win-win-win outcome can be shown when a cooperative is engaged.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Every year around the world, we suffer from disasters. For some countries, earthquakes are the largest threat among natural disasters. It is difficult to forecast them, and their impacts are huge. An earthquake occurred in March, 2011 in Japan, and the massive tsunami triggered by that earthquake was a stark reminder of the severe damage that is possible with such a huge earthquake. Similar damage occurs every few years worldwide. In addition, global warming results in stronger and more devastating natural disasters. Consequently, disaster management has become even more crucial for many network operators.",
    "actual_venue": "Design Of Reliable Communication Networks"
  },
  {
    "abstract": "In a relational database, tuples are called âduplicateâ if they describe the same real-world entity. If such duplicate tuples are observed, it is recommended to remove them and to replace them with one tuple that represents the joint information of the duplicate tuples to a maximal extent. This remove-and-replace operation is called a fusion operation. Within the setting of a relational database management system, the removal of the original duplicate tuples can breach referential integrity. In this paper, a strategy is proposed to maintain referential integrity in a semantically correct manner, thereby optimizing the quality of relationships in the database. An algorithm is proposed that is able to propagate a fusion operation through the entire database. The algorithm is based on a framework of first and second order fusion functions on the one hand, and conflict resolution strategies on the other hand. It is shown how classical strategies for maintaining referential integrity, such as DELETE cascading, are highly specialized cases of the proposed framework. Experimental results are reported that (i) show the efficiency of the proposed algorithm and (ii) show the differences in quality between several second order fusion functions. It is shown that some strategies easily outperform DELETE cascading.",
    "actual_venue": "Knowledge and Data Engineering, IEEE Transactions Â "
  },
  {
    "abstract": "Excellent performance, real time and strong robustness are three vital requirements for infrared small target detection. Unfortunately, many current state-of-the-art methods merely achieve one of the expectations when coping with highly complex scenes. In fact, a common problem is that real-time processing and great detection ability are difficult to coordinate. Therefore, to address this issue, a robust infrared patch-tensor model for detecting an infrared small target is proposed in this paper. On the basis of infrared patch-tensor (IPT) model, a novel nonconvex low-rank constraint named partial sum of tensor nuclear norm (PSTNN) joint weighted l(1) norm was employed to efficiently suppress the background and preserve the target. Due to the deficiency of RIPT which would over-shrink the target with the possibility of disappearing, an improved local prior map simultaneously encoded with target-related and background-related information was introduced into the model. With the help of a reweighted scheme for enhancing the sparsity and high-efficiency version of tensor singular value decomposition (t-SVD), the total algorithm complexity and computation time can be reduced dramatically. Then, the decomposition of the target and background is transformed into a tensor robust principle component analysis problem (TRPCA), which can be efficiently solved by alternating direction method of multipliers (ADMM). A series of experiments substantiate the superiority of the proposed method beyond state-of-the-art baselines.",
    "actual_venue": "Remote Sensing"
  },
  {
    "abstract": "Since there is a wide range of applications requiring image color difference (CD) assessment (e.g. color quantization, color mapping), a number of CD measures for images have been proposed. However, the performance evaluation of such measures often suffers from the following major flaws: (1) test images contain primarily spatial- (e.g. blur) rather than color-specific distortions (e.g. quantization noise), (2) there are too few test images (lack of variability in color content), and (3) test images are not publicly available (difficult to reproduce and compare). Accordingly, the performance of CD measures reported in the state-of-the-art is ambiguous and therefore inconclusive to be used for any specific color-related application.",
    "actual_venue": "Signal Processing: Image Communication"
  },
  {
    "abstract": "This paper investigates the problem of power allocation for distributed estimation via diffusion in wireless sensor networks (WSNs) with simultaneous wireless information and power transfer (SWIPT). We consider a WSN consisting of smart sensor nodes (SNs) and common sensor nodes (CNs), and each SN is capable of performing SWIPT via multi-antenna beamforming to its neighboring (i.e., near-tier) CNs. In each diffusion iteration, all nodes collect measurements and exchange intermediate estimates with their neighbors. We first analyze the effect of each SN's beamforming design and each near-tier CN's harvested power allocation on the steady-state network-wide mean square deviation (MSD) of the diffusion least-mean-squares (LMSs) strategy. Then, we formulate a problem to minimize an upper bound MSD by jointly optimizing the global power allocation weights for each SN to perform beamforming, and the local power allocation proportion for each CN to perform measurement collection. We further show that the formulated non-convex problem is decomposable and propose a gradient-based iterative algorithm to find the optimal solution. In addition, for practical implementation, we propose adaptive online approaches to estimate some parameters required for system optimization. Finally, extensive simulation results demonstrate that with optimal power allocation, our proposed scheme improves the MSD performance significantly, compared to the conventional diffusion LMS strategy without wireless power transfer (WPT).",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "The purchasing function of large firms has slowly evolved from the operational task of ordering products and services towards a strategic part of business. However, the full implications of strategic purchasing for firm innovation have yet to be explored. The contribution of this study is two-fold. First, we develop a relational view of strategic purchasing embracing both the quality of the purchasing function and the properties of its internal and external relations. Second, based on exploratory empirical analysis, we propose four patterns of the link between strategic purchasing and innovation. We find that strategic purchasing is an enabling, but not necessary, condition for a contribution of purchasing to innovation, which in turn mostly relates to the relational resources of purchasing.",
    "actual_venue": "Technology Analysis And Strategic Management"
  }
]