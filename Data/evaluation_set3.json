[
  {
    "abstract": "In this study, a new non-linear recursive mechanism for Volterra least mean square (VLMS) algorithm is proposed in the domain of non-linear adaptive signal processing and control. The proposed adaptive scheme is developed by applying concepts and theories of fractional calculus in weight adaptation structure of standard VLMS approach. The design scheme based on fractional VLMS (F-VLMS) algorithm i...",
    "actual_venue": "Iet Signal Processing"
  },
  {
    "abstract": "In dependency parsing of long sentences with fewer subjects than predicates, it is difficult to recognize which predicate governs which subject. To handle such syntactic ambiguity between subjects and predicates, this paper proposes an \"S- clause\" segmentation method, where an S(ubject)- clause is defined as a group of words containing several predicates and their common subject. We propose an automatic S-clause segmentation method using decision trees. The S-clause information was shown to be very effective in analyzing long sentences, with an improved performance of 5 percent.",
    "actual_venue": "Alta"
  },
  {
    "abstract": "3D surfaces are widely employed to model geometric assets (e.g., mountains on a landscape), which are used in digital animations and video games. A single surface commonly needs to be created and modified by a group of collaborators, but most of the 3D content creation applications are essentially single-user. In addition, such surfaces are visualized in 2D projections, causing confusion to new users, when imagining their shape in 3D. In this paper, we propose a novel approach based on Augmented Reality (AR) to the task of collaboratively authoring surfaces on the Web using mobile devices. We rely on AR technology to help collaborators to easily understand the shape of the surface's 3D representation, and we provide them with the basic authoring tools to intuitively modify its shape. To support real time face-to-face interaction, we implement an object sharing scheme, which according to our results is enough in practice. In this way, our approach is able to create a new online collaborative setting in which a group of collocated participants, each one using a mobile device, or connected to the Web, can concurrently modify a surface, while visualizing it in their own real environment through AR.",
    "actual_venue": "Ieee/Wic/Acm International Conference On Web Intelligence"
  },
  {
    "abstract": "In this paper, we have worked on reducing burden on mechanic involving complex automobile maintenance activities that are performed in centralised workshops. We have presented a system prototype that combines Augmented Reality with Kinect. With the use of Kinect, very high quality sensors are available at considerably low costs, thus reducing overall expenditure for system design. The system can be operated either in Speech mode or in Gesture mode. The system can be controlled by various audio commands if user opts for Speech mode. The same controlling can also be done by using a set of Gestures in Gesture mode.   Gesture recognition is the task performed by Kinect system. This system, bundled with RGB and Depth camera, processes the skeletal data by keeping track of 20 different body joints. Recognizing Gestures is done by verifying user movements and checking them against predefined condition. Augmented Reality module captures real-time image data streams from high resolution camera. This module then generates 3D model that is superimposed on real time data.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "he ESRF control system is in the process of being modernised. The present\ncontrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,\nMotif and C. The new control system will be based on compact PCI, 100 MHz\nEthernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main\nfrontend operating system will be GNU/Linux running on Intel/x86 and\nMotorola/68k. Linux will also be used on handheld devices for mobile control.\nThis poster describes how GNU/Linux is being used to modernise the control\nsystem and what problems have been encountered so far",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "A program called Oc2rep has been written for the distributed memory case. It was tested on several Lustre and Esterel programs implemented on a Unix based LAN, and appeared to work satisfactorily, in the sense that the problem of distributed debugging was avoided. Yet some important studies are still to be undertaken: performance and optimization, bounding of the reaction time and finally fault tolerance.",
    "actual_venue": "Parle"
  },
  {
    "abstract": "This paper presents an effective unsupervised sparse feature learning algorithm to train deep convolutional networks on hyperspectral images. Deep convolutional hierarchical representations are learned and then used for pixel classification. Features in lower layers present less abstract representations of data, while higher layers represent more abstract and complex characteristics. We successfully illustrate the performance of the extracted representations in a challenging AVIRIS hyperspectral image classification problem, compared to standard dimensionality reduction methods like principal component analysis (PCA) and its kernel counterpart (kPCA). The proposed method largely outperforms the previous state-of-the-art results on the same experimental setting. Results show that single layer networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels. Regarding the deep architecture, we can conclude that: (1) additional layers in a deep architecture significantly improve the performance w.r.t. single layer variants; (2) the max-pooling step in each layer is mandatory to achieve satisfactory results; and (3) the performance gain w.r.t. the number of layers is upper bounded, since the spatial resolution is reduced at each pooling, resulting in too spatially coarse output features.",
    "actual_venue": "Workshop On Hyperspectral Image And Signal Processing: Evolution In Remote Sensing"
  },
  {
    "abstract": "Smartphones and tablets now include General Purpose Graphics Processing Units (GP-GPUs) that can be used for computation beyond driving the high-resolution screens. In this paper we present a mobile GP-GPU-based object detection algorithm and system, based on the work by Viola and Jones (which is also used in the OpenCV library). This implementation achieved twofold speed up compared to OpenCV running on the CPU of the same smartphone, and up to 84% energy saving. Interestingly, the new implementation saves energy vs. the CPU even when it executes slower than the OpenCV implementation, because the GPU consumes less power than the CPU, something that is not typical in desktop or laptop systems.",
    "actual_venue": "Ieee Africon"
  },
  {
    "abstract": "In this paper, we study the problem of generating synthetic graphs that resemble real-world graphs in terms of their degree correlations and potentially additional properties. We present an algorithmic framework that generates simple undirected graphs with the exact target joint degree matrix, which we refer to as 2K graphs, in linear time in the number of edges. Our framework imposes minimal constraints on the graph structure, which allows us to target additional graph properties during construction, namely, node attributes (2K+A), clustering (both average clustering, 2.25K, and degree-dependent clustering, 2.5K), and number of connected components (2K+CC). We also define, for the first time, the problem of directed 2K graph construction, provide necessary and sufficient conditions for realizability, and develop efficient construction algorithms. We evaluate our approach by creating synthetic graphs that target real-world graphs both undirected (such as Facebook) and directed (such as Twitter), and we show that it brings significant benefits, in terms of accuracy and running time, compared to the state-of-the-art approaches.",
    "actual_venue": "Ieee-Acm Transactions On Networking"
  },
  {
    "abstract": "With the growing importance of parametric (process and environmental) variations in advanced technologies, it has become a serious challenge to design reliable, fast and low-power embedded memories. Adopting a variation-aware design paradigm requires a holistic perspective of memory-wide metrics such as yield, power and performance. However, accurate estimation of such metrics is largely dependent on circuit implementation styles, technology parameters and architecture-level specifics. In this paper, we propose a fully automated tool - INFORMER - that helps high-level designers estimate memory reliability metrics rapidly and accurately. The tool relies on accurate circuit-level simulations of failure mechanisms such as soft-errors and parametric failures. The statistics obtained can then help couple low-level metrics with higher-level design choices. A new technique for rapid estimation of low-probability failure events is also proposed. We present three use-cases of our prototype tool to demonstrate its diverse capabilities in autonomously guiding large SRAM based robust memory designs.",
    "actual_venue": "Design, Automation And Test In Europe Conference And Exhibition"
  },
  {
    "abstract": "Clock and data recovery (CDR) circuits incorporating a bang-bang (BB) phase detector have been widely adopted in high-speed serial links due to their advantages in high speed implementations. However, the heavily non-linear nature of the BB phase detector makes the analysis of the CDR loop difficult. In this paper, we propose a new technique for accurate and efficient estimation of the bit-error rate (BER) for BB CDR circuits. The technique estimates the BER based on the spectral information of jitter and the jitter transfer characteristics of the BB CDR circuit. It eliminates the conventional BER measurement process and, thus, substantially accelerates the jitter tolerance test. In addition, this technique offers insights into the behavior of the non-linear CDR loop and the contribution of the jitter to the BER. We present simulation results that demonstrate the potential usefulness of the method.",
    "actual_venue": "VTS"
  },
  {
    "abstract": "High Performance Computing (HPC) systems are becoming increasingly complex and are also associated with very high operational costs. The cloud computing paradigm, coupled with modern Virtual Machine (VM) technology offers attractive techniques to easily manage large scale systems, while significantly bringing down the cost of computation, memory and storage. However, running HPC applications on cloud systems still remains a major challenge. One of the biggest hurdles in realizing this objective is the performance offered by virtualized computing environments, more specifically, virtualized I/O devices. Since HPC applications and communication middlewares rely heavily on advanced features offered by modern high performance interconnects such as InfiniBand, the performance of virtualized InfiniBand interfaces is crucial. Emerging hardware-based solutions, such as the Single Root I/O Virtualization (SR-IOV), offer an attractive alternative when compared to existing software-based solutions. The benefits of SR-IOV have been widely studied for GigE and 10GigE networks. However, with InfiniBand networks being increasingly adopted in the cloud computing domain, it is critical to fully understand the performance benefits of SR-IOV in InfiniBand network, especially for exploring the performance characteristics and trade-offs of HPC communication middlewares (such as Message Passing Interface (MPI), Partitioned Global Address Space (PGAS)) and applications. To the best of our knowledge, this is the first paper that offers an in-depth analysis on SR-IOV with InfiniBand. Our experimental evaluations show that for the performance of MPI and PGAS point-to-point communication benchmarks over SR-IOV with InfiniBand is comparable to that of the native InfiniBand hardware, for most message lengths. However, we observe that the performance of MPI collective operations over SR-IOV with InfiniBand is inferior to native (non-virtualized) mode. We also evaluate the trade-offs of various - M to CPU mapping policies on modern multi-core architectures and present our experiences.",
    "actual_venue": "Ccgrid"
  },
  {
    "abstract": "The United States Department of Homeland Security (DHS) charge is to, \"Build a safer, more secure, and more resilient America by preventing, deterring, neutralizing, or mitigating the effects of deliberate efforts by terrorists to destroy, incapacitate, or exploit elements of our Nation's CIKR.\" using an all-hazards approach. The effective implementation of this strategy hinges on understanding catastrophes and their potential effect on the functioning of infrastructure. Unfortunately, there has been no unifying theory of catastrophe to guide decision-making, preparedness, or response. In this paper, the authors present a framework based on network science and normal accident theory that can be used to guide policy decisions for homeland security. They show that exceedance probability encompasses operational definitions of risk and resilience and provides a unifying policy framework for homeland security investments. Such an approach allows one to classify hazards as 'high' or 'low' risk, according to the resiliency exponent, and guide investments toward prevention or response. This framework is applied to cyber exploits and electric power grid systems to illustrate its generality.",
    "actual_venue": "International Journal Of Cyber Warfare And Terrorism"
  },
  {
    "abstract": "Reliable people counting is crucial to many urban applications. However, most existing people counting systems are sensor-based and can only work in some fixed gateways or checkpoints where sensors have been installed. This high dependence on the exact locations of sensors leads to low accuracy. To overcome these limitations, in this paper, we propose a smartphone-based people counting system, Wi-Counter, by leveraging the pervasive Wi-Fi infrastructure. To collect comprehensive Wi-Fi signals and people count information based on crowdsource, Wi-Counter first adopts a preprocessor to overcome the noisy, discrepant, and fragile data based on the Wiener filter and Newton interpolation. It then makes use of the designated five-layer neural network to learn the relation model between the Wi-Fi signals and the number of people. By analyzing the received Wi-Fi signals, Wi-Counter can estimate the number of people based on the resulting model. We have conducted experiments by implementing a prototype of Wi-counter based on smartphones and evaluated the system in terms of accuracy and power consumption in an indoor testbed covering an area of 96 m $^2$. Wi-Counter achieved a counting accuracy of up to 93% and exhibited reliable and robust performance resisting temporal environmental changes with negligible power usage.",
    "actual_venue": "Ieee Trans Human-Machine Systems"
  },
  {
    "abstract": "Object recognition algorithms often focus on determining the class of a detected object in a scene. Two significant phases are usually involved in object recognition. The first phase is the object representation phase, in which the most suitable features that provide the best discriminative power under constraints such as lighting, resolution, scale, and view variations are chosen to describe the objects. The second phase is to use this representation space to develop models for each object class using discriminative classifiers. In this paper, we focus on composite objects, i.e., objects with two or more simpler classes that are interconnected in a complicated manner. One classic example of such a scenario is a bicyclist. A bicyclist consists of a bicycle and a human who rides the bicycle. When we are faced with the task of classifying bicyclists and pedestrians, it is counterintuitive and often hard to come up with a discriminative classifier to distinguish the two classes. We explore global image analysis based on bag of visual words to compare the results with local image analysis, in which we attempt to distinguish the individual parts of the composite object. We also propose a unified naive Bayes framework and a combined histogram feature method for combining the individual classifiers for enhanced performance.",
    "actual_venue": "Ieee Transactions Intelligent Transportation Systems"
  },
  {
    "abstract": "As the number of file I/O intensive mobile applications increase, in-memory file systems have gained attention. In-memory file systems provide a fast access to file data, but the main memory of the corresponding mobile devices often suffers due to its capacity limit. To provide mobile applications with more free space, the swap mechanism has been an essential technique for the overcoming of this space insufficiency; however, the existing swap mechanism is inadequate for the file I/O of mobile applications. In this study, a new in-memory file system is devised to improve the current swap mechanism in the legacy in-memory file system, whereby strip-based layouts with separated file-swap partitions are used and unnecessary file- I/O overheads are eliminated. To evaluate the proposed in-memory file system, experiments with the file-I/O benchmark were conducted. The experiment results show that, compared to the current swap scheme, the proposed swap scheme improves the overall performance by ten times on average1.",
    "actual_venue": "Ieee Trans Consumer Electronics"
  },
  {
    "abstract": "Notary is a new hardware and software architecture for running isolated approval agents in the form factor of a USB stick with a small display and buttons. Approval agents allow factoring out critical security decisions, such as getting the user's approval to sign a Bitcoin transaction or to delete a backup, to a secure environment. The key challenge addressed by Notary is to securely switch between agents on the same device. Prior systems either avoid the problem by building single-function devices like a USB U2F key, or they provide weak isolation that is susceptible to kernel bugs, side channels, or Rowhammer-like attacks. Notary achieves strong isolation using reset-based switching, along with the use of physically separate systems-on-a-chip for agent code and for the kernel, and a machine-checked proof of both the hardware's register-transfer-level design and software, showing that reset-based switching leaks no state. Notary also provides a trustworthy I/O path between the agent code and the user, which prevents an adversary from tampering with the user's screen or buttons.\n\nWe built a hardware/software prototype of Notary, using a combination of ARM and RISC-V processors. The prototype demonstrates that it is feasible to verify Notary's reset-based switching, and that Notary can support diverse agents, including cryptocurrencies and a transaction approval agent for traditional client-server applications such as websites. Measurements of reset-based switching show that it is fast enough for interactive use. We analyze security bugs in existing cryptocurrency hardware wallets, which aim to provide a similar form factor and feature set as Notary, and show that Notary's design avoids many bugs that affect them.",
    "actual_venue": "Proceedings Of The Acm Symposium On Operating Systems Principles"
  },
  {
    "abstract": "This paper describes a web-based system for the online delivery of formal examinations and their automated marking. This system was first used in June 2001 in an end-of-year exam for a first year undergraduate programming course. The outcome of this experiment is also described.",
    "actual_venue": "Proceedings Of The Annual Sigcse Conference On Innovation And Technology In Computer Science Education"
  },
  {
    "abstract": "The title is not a statement of fact, of course, but an opinion about how language designers should think about types. There has been a natural tendency to look to mathematics for a consistent, precise notion of what types are. The point of view there is extensional: a type is a subset of the universe of values. While this approach may have served its purpose quite adequately in mathematics, defining programming language types in this way ignores some vital ideas. Some interesting developments following the extensional approach are the ALGOL-68 type system [vW], Scott's theory [S], and Reynolds' system [R]. While each of these lend valuable insight to programming languages, I feel they miss an important aspect of types.Rather than worry about what types are I shall focus on the role of type checking. Type checking seems to serve two distinct purposes: authentication and secrecy. Both are useful when a programmer undertakes to implement a class of abstract objects to be used by many other programmers. He usually proceeds by choosing a representation for the objects in terms of other objects and then writes the required operations to manipulate them.",
    "actual_venue": "Symposium On Principles Of Programming Languages"
  },
  {
    "abstract": "Recent significant advancements in FPGAs have made it viable to explore multiprocessor solutions on a single FPGA chip. An efficient communication architecture that matches the needs of the target application is always critical to the overall performance of multiprocessors. Packet-switching network-on-chip (NoC) approaches are being offered to deal with scalability and complexity challenges coming along with the increasing number of processing elements (PEs). Many FPGA-based NoC designs consume significant resources, leaving little room for PEs. We argue that computation is still the primary task of multiprocessors and sufficient resources should be reserved for PEs. This paper presents our novel design and implementation of a resource-efficient communication architecture for multiprocessors on FPGAs. We reduce not only the required number of routers for a given number of PEs by introducing a new PE-router topology, but also resource requirements of each router, while maintaining good performance for typical injection rates.",
    "actual_venue": "Reconfig"
  },
  {
    "abstract": "For user convenience, processing of document images captured by a digital camera has been attracted much attention. However, most existing processing methods require an upright image such like captured by a scanner. Therefore, we have to cancel perspective distortion of a camera-captured image before processing. Although there are rectification methods of the distortion, most of them work under certain assumptions on the layout; the borders of a document are available, textlines are in parallel, a stereo camera or a video image is required and so on. In this paper, we propose a layout-free rectification method which requires none of the above assumptions. We confirm the effectiveness of the proposed method by experiments.",
    "actual_venue": "DRR"
  },
  {
    "abstract": "The multiple-channel reactions X + CF3CH2OCF3 (X = F, Cl, Br) are theoretically investigated. The minimum energy paths (MEP) are calculated at the MP2/6-31+G(d,p) level, and energetic information is further refined by the MC-QCISD (single-point) method. The rate constants for major reaction channels are calculated by canonical variational transition state theory (CVT) with small-curvature tunneling (SCT) correction over the temperature range 2002000 K. The theoretical three-parameter expressions for the three channels k1a(T) = 1.24 x 10(-15)T1.24exp(-304.81/T), k2a(T) = 7.27 x 10(-15)T0.37exp(-630.69/T), and k3a(T) = 2.84 x 10(-19)T2.51 exp(-2725.17/T) cm3 molecule-1 s-1 are given. Our calculations indicate that hydrogen abstraction channel is only feasible channel due to the smaller barrier height among five channels considered. (C) 2011 Wiley Periodicals, Inc. J Comput Chem, 2012",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "The purpose of the Java memory model is to formalize the behavior of the sharedmemory inmultithreaded Java programs. The subtlest points of its formalization are causality requirements that serve to provide safety and security guarantees for incorrectly synchronized Java programs. In this paper, we consider the problem of verifying whether an execution of amultithreaded Java program satisfies these causality requirements and show that this problem is undecidable.",
    "actual_venue": "Ppam"
  },
  {
    "abstract": "The deterministic shrinking two-pushdown automata characterize the deterministic growing context-sensitive languages, known to be the Church-Rosser languages. Here, we initiate the investigation of reversible two-pushdown automata, RTPDAs, in particular the shrinking variant. We show that as with the deterministic version, shrinking and length-reducing RTPDAs are equivalent. We then give a separation of the deterministic and reversible shrinking two-pushdown automata, and prove that these are incomparable with the (deterministic) context-free languages. We further show that the properties of emptiness, (in) finiteness, universality, inclusion, equivalence, regularity, and context-freeness are not even semi-decidable for shrinking RTPDAs.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "We present the auto- and hetero-associative memories incorporating the exponential encoding scheme in this paper. The condition that the evolution of the system energy can reach the global minimum state is presented. The improvement of the memory storage capacities of the exponential associative memories is discussed. The application of a frequency classifier is presented and the performance of associative memories with and without the exponential correlation function are compared. The exponential associative memory is superior in classifying the cluttered signals to the linear associative memory.",
    "actual_venue": "Inf Process Lett"
  },
  {
    "abstract": "Recent advances in development of wireless communication technologies and embedded computing systems led us into the area of the next generation wireless networks and smart devices. In this context, it is widely argued that security has become a primary concern, in order to ensure dependable, secure communications and services to the end user. There are many open questions for these challenging issues such as security of protocols and applications, secure architecture, frameworks and methodologies for next generation wireless networks and respective smart devices.",
    "actual_venue": "Eurasip J Wireless Comm And Networking"
  },
  {
    "abstract": "The Web is overcrowded with news articles, an overwhelming information source both with its amount and diversity. Document clustering is a powerful technique that has been widely used for organizing data into smaller and manageable information kernels. Several approaches have been proposed which, however, suffer from problems like synonymy, ambiguity and lack of a descriptive content marking of the generated clusters. In this work, we are investigating the application of a great spectrum of clustering algorithms, as well as similarity measures, to news articles that originate from the Web. Also, we are proposing the enhancement of standard k-means algorithm using the external knowledge from WordNet hypernyms in a twofold manner: enriching the ''bag of words'' used prior to the clustering process and assisting the label generation procedure following it. Furthermore, we are examining the effect that text preprocessing has on clustering. Operating on a corpus of news articles derived from major news portals, our comparison of the existing clustering methodologies revealed that k-means, gives better aggregate results when it comes to efficiency. This is amplified when the algorithm is accompanied with preliminary steps for data cleaning and normalizing, despite its simple nature. Moreover, the proposed WordNet-enabled W-k means clustering algorithm significantly improves standard k-means generating also useful and high quality cluster tags by using the presented cluster labeling process.",
    "actual_venue": "Knowl-Based Syst"
  },
  {
    "abstract": "Abstract Data locality and synchronization overhead are two important factors that affect the performance,of applications on multiprocessors. Loop fusion is an effective way for reducing synchronization and improving data locality. Traditional fusion t echniques, however, either can not address the case when fusion-preventing dependencies exist in nested loops, or can not achieve good parallelism after fusion. This paper presents a significant addition to the current loop fusion techniques by presenting several efficient polynomial-time alg orithms to solve these problems. These algorithms, based on multi-dimensional retiming, allow nested loop fusion even in the presence of outmost loop-carried dependencies or fusion-preventing dependencies. The multiple loops are modeled,by a multi-dimensional loop dependence,graph. The algorithms are applied to such a graph in order to perform the fusion and to obtain full parall elism in the innermost loop. Key Words: Loop Fusion, Loop Transformation, Nested Loops, Retiming, Data Locality  This work is partially supported by NSF grants MIP-95-01006 and MIP-97-04276, and by the W. D. Mensch Jr. and",
    "actual_venue": "International Journal Of Computers And Their Applications"
  },
  {
    "abstract": "This paper explores the use of MEMS devices such as bulk acoustic wave (BAW) and low frequency silicon resonators, in combination with digital circuits and techniques, to reach miniaturization and low power dissipation in a 2.4 GHz transceiver targeting wireless body area networks (WBAN) and wireless sensor networks (WSN) applications. Precise phase locking of the BAW digitally controlled oscillator (DCO) to the low- frequency temperature compensated oscillator is demonstrated. Additional cancellation within the ADPLL of the deterministic jitter induced by the bi-frequency mode on the 32 kHz clock is proven, with a residual modulation on the DCO command word corresponding to ±0.5 ppm relative frequency deviation. The RF resulting after frequency up-conversion of the system IF signal with the DCO shows an excellent phase noise of - 136.6 dBc/Hz at 1 MHz offset frequency, for a total synthesizer current consumption of 7.52 mA under 1.6 V supply. In addition, 1 Mb/s GFSK Bluetooth and Bluetooth Low Energy modulations have been successfully validated in transmission.",
    "actual_venue": "Circuits And Systems"
  },
  {
    "abstract": "This paper is concerned with the efficient determination of FIR least squares filters with linear phase. The general unknown statistics case is considered, whereby only sample records of the input and the desired response signals are available. Both linear phase cases with phase delay and group delay are examined. Taking advantage of the underlying near-to-Toeplitz structure of the resulting normal equations, a fast algorithm is developed that recursively produces all linear phase filters orders for a single block of data.",
    "actual_venue": "Acoustics, Speech And Signal Processing, Ieee Transactions"
  },
  {
    "abstract": "•This paper presents a new algorithm based on combining of the improved QPSO-NM algorithms for solving load flow problem.•The modification of both improved QPSO-NM method intends to obtain more accurate convergence and good search performance.•The proposed algorithm is tested on different IEEE test systems for a set of normal and critical operating conditions.•The test results are compared to the conventional NR algorithm, PSO and different versions of QPSO.•The numerical results reveal the superiority of the proposed approach for solving LF problems under different situations.",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "Social science literature has shown a strong connection between the visual appearance of a city's neighborhoods and the behavior and health of its citizens. Yet, this research is limited by the lack of methods that can be used to quantify the appearance of streetscapes across cities or at high enough spatial resolutions. In this paper, we describe 'Streetscore', a scene understanding algorithm that predicts the perceived safety of a streetscape, using training data from an online survey with contributions from more than 7000 participants. We first study the predictive power of commonly used image features using support vector regression, finding that Geometric Texton and Color Histograms along with GIST are the best performers when it comes to predict the perceived safety of a streetscape. Using Streetscore, we create high resolution maps of perceived safety for 21 cities in the Northeast and Midwest of the United States at a resolution of 200 images/square mile, scoring ~1 million images from Google Streetview. These datasets should be useful for urban planners, economists and social scientists looking to explain the social and economic consequences of urban perception.",
    "actual_venue": "Cvpr Workshops"
  },
  {
    "abstract": "In the paper, the status of flood disaster in china and the progress of disaster prevention and reduction in the field of property insurance were analyzed. The characteristics and application fields of 3S (GIS,RS and GPS) were also introduced. According to the current need and future development of property insurance company, which were based on the investigation to the work of disaster prevention and reduction in property insurance and casualty company (abbreviated as PICC) China, the authors decided to apply 3S technologies to the field of property insurance and used the successful methods in foreign property insurance companies for reference to develop a decision support system of flood disaster for property insurance. The system linked well with the operational system of insurance company and realized seamless integration between different data sources. The work of disaster prevention and reduction in property insurance company was better organized both in theory ways and in key technologies. As a result, the economic benefit of property insurance company was really improved. The system was applied in Shenzhen,China and the result was satisfying.",
    "actual_venue": "Igarss : Ieee International Geoscience And Remote Sensing Symposium Proceedings, Vols -: Science For Society: Exploring And Managing A Changing Planet"
  },
  {
    "abstract": "We present a deterministic parallel algorithm for the linked list prefix problem. It computes linked list prefix for an input list of n elements in time O(n/p+logn) on a local memory PRAM model using p processors and p shared memory cells. We also show that a maximal matching for a linked list can be computed in O(logG(n)) time with n processors and n shared cells.",
    "actual_venue": "Acm Conference On Computer Science"
  },
  {
    "abstract": "Side-channel attacks in general and power analysis attacks in particular are becoming a major security concern in embedded systems. Countermeasures proposed against power analysis attacks are data and table masking, current flattening, dummy instruction insertion and bit-flips balancing. All these techniques are either susceptible to multi-order power analysis attack, not sufficiently generic to cover all encryption algorithms, or burden the system with high area, run-time or energy cost. In this article, we propose a randomized instruction injection technique (RIJID) that overcomes the pitfalls of previous countermeasures. RIJID scrambles the power profile of a cryptographic application by injecting random instructions at random points of execution and therefore protects the system against power analysis attacks. Two different ways of triggering the instruction injection are also presented: (1) softRIJID, a hardware/software approach, where special instructions are used in the code for triggering the injection at runtime; and (2) autoRIJID, a hardware approach, where the code injection is triggered by the processor itself via detecting signatures of encryption routines at runtime. A novel signature detection technique is also introduced for identifying encryption routines within application programs at runtime. Further, a simple obfuscation metric (RIJIDindex) based on cross-correlation that measures the scrambling provided by any code injection technique is introduced, which coarsely indicates the level of scrambling achieved. Our processor models cost 1.9&percnt; additional area in the hardware/software approach and 1.2&percnt; in the hardware approach for a RISC based processor, and costs on average 29.8&percnt; in runtime and 27.1&percnt; in energy for the former and 25.0&percnt; in runtime and 28.5&percnt; in energy for the later, for industry standard cryptographic applications.",
    "actual_venue": "Acm Trans Embedded Comput Syst"
  },
  {
    "abstract": "Dynamic networking of brain regions is suggested to be one of the key factors involved in various brain computations. Central executive function typically requires instantaneous coordination among the medial prefrontal regions and other distant regions, depending on the on-going task situation. In human scalp-recorded electroencephalography (EEG), the medial prefrontal area is estimated to be the current source of the theta rhythm, while there is no direct evidence that the theta rhythm is involved in the dynamic networking of central executive circuits. Here we hypothesize that the central executive circuit over the prefrontal and task-related cortices is dynamically linked by theta synchronization. By using simultaneous functional magnetic resonance imaging (fMRI) and EEG, we elucidated cortical circuits emerging with theta phase synchronization during free pacing repeated subtraction. Theta phase synchronization in the scalp EEG was found to emerge at two major clusters of electrode pairs, between the right frontal and left parietal sites and between the frontal and right parietal sites. The phase synchronization of two clusters is accompanied by fMRI responses in the cortical regions responsible for central executive function, working memory, visual imagery and cognitive action sequence. Here we report the first evidence that theta phase synchronization dynamically coordinates the central executive circuits, including the medial prefrontal cortex and relevant cortical regions.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "There are few equations for underwater communications in the related literature. They show that the speed propagation and absorption coefficient in freshwater are independent of the working frequency of the transmitted signals. However, some studies demonstrate that electromagnetic waves present lower losses when they are working at certain frequencies. In this paper, we perform a set of measurements of electromagnetic (EM) waves at 2.4 GHz in the underwater environment. In our study case, we fix the water conditions and we measure the behavior of EM as a function of several network parameters such as the working frequency, data transfer rates and modulations. Our results will show that higher frequencies do not mean worse network performance. We will also compare our conclusion with some statements extracted from other works.",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "In the last few years, Networks Operators (NO) have experienced an increased number of requests for video contents and rich media services, which are becoming increasingly popular. In view of the network scaling limitations, operators are developing their own caching systems to speed up the network performance. Indeed, disseminating caches in the infrastructure not only helps in absorbing the network's congestion, but in addition, brings content closer to users, which allows a reduced latency. Several studies have focused on improving the performance of such caching systems, especially in the context of Content-Centric Networking (CCN). In this paper, we propose a fairly generic model of caching systems that can be adapted very easily to represent different caching strategies, even the most advanced ones. Indeed, the proposed model of a single cache, named MACS, which stands for Markov chain-based Approximation of CCN Caching Systems, can be extended to represent an interconnection of caches under different schemes. In order to demonstrate the accuracy of our model, we proposed to derive the two most effective techniques in the literature, namely LCD and LRU-K, which may adapt to changing patterns of access. Simulation results using a discrete event simulator clearly show the accuracy of the proposed model under different network configurations.",
    "actual_venue": "Journal Of Network And Computer Applications"
  },
  {
    "abstract": "Modern vehicles contain tens of Electronic Control Units (ECUs), several of which communicate over the Controller Area Network (CAN) protocol. As such, in-vehicle networks have become a prime target for automotive network attacks. To understand the security of these networks, we argue that we need tools analogous to network mappers for traditional networks that provide an in-depth understanding of a network's structure. To this end, our goal is to develop an automotive network mapping tool that assists in identifying a vehicle's ECUs and their communication with each other. A significant challenge in designing this tool is the broadcast nature of the CAN protocol, as network messages contain no information about their sender or recipients. To address this challenge, we design and implement CANvas, an automotive network mapper that identifies transmitting ECUs with a pairwise clock offset tracking algorithm and identifies receiving ECUs with a forced ECU isolation technique. CANvas generates network maps in under an hour that identify a previously unknown ECU in a 2009 Toyota Prius and identify lenient message filters in a 2017 Ford Focus.",
    "actual_venue": "Proceedings Of The Usenix Security Symposium"
  },
  {
    "abstract": "Existence status of (96, 20, 4) difference sets in Z 4 × Z 4 × Z 2 × Z 3 has been open so far. In this paper we construct these difference sets, thereby filling a missing entry in Lander's table with the answer “yes,”",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "The interplay of science and industry in the development and exploitation of a mutually beneficial HPCN policy is a non-issue. Everyone, I believe, is willing to subscribe to this view. Just a tad harder is to outline in detail the precise lines along which such a partnership should evolve and what the advantages are for both partners. CRS4 is a research institute based in Cagliari (Sardegna) with a vocation for mathematical modeling and numerical simulation. Research at CRS4 means applied research in close contact with industry. It would seem that we are occupying a privileged position to entertain thoughts and develop theories and models on the interplay of science and industry in HPCN. Nonetheless our experience is too time-limited to allow me to make any general conclusion on the subject. In my paper I will first speculate on what kind of development we will see in the coming years in the field of High Performance Computing and Networks. I will describe the interaction between CRS4 and industry, and extrapolate from this some of the needs of industry from HPCN. I will close by describing our view of the interaction between a HPCN and CRS4.",
    "actual_venue": "Future Generation Comp Syst"
  },
  {
    "abstract": "Markov automata (MA) constitute an expressive continuous-time compositional modelling formalism. They appear as semantic backbones for engineering frameworks including dynamic fault trees, Generalised Stochastic Petri Nets, and AADL. Their expressive power has thus far precluded them from effective analysis by probabilistic (and statistical) model checkers, stochastic game solvers, or analysis tools for Petri net-like formalisms. This paper presents the foundations and underlying algorithms for efficient MA modelling, reduction using static analysis, and most importantly, quantitative analysis. We also discuss implementation pragmatics of supporting tools and present several case studies demonstrating feasibility and usability of MA in practice.",
    "actual_venue": "Qest"
  },
  {
    "abstract": "Abstract Domain ontologies facilitate the organization, sharing and reuse of domain knowledge, and enable various vertical domain applications to operate successfully. Most methods for automatically constructing ontologies focus on taxonomic relations, such as is-kind-of and is-part-of relations. However, much of the domain-specific semantics is ignored. This work proposes a semi-unsupervised approach for extracting semantic relations from domain-specific text documents. The approach effectively utilizes text mining and existing taxonomic relations in domain ontologies to discover candidate keywords that can represent semantic relations. A preliminary experiment on the natural science domain (Taiwan K9 education) indicates that the proposed method yields valuable recommendations. This work enriches domain ontologies by adding distilled semantics.",
    "actual_venue": "Journal Of Intelligent Information Systems"
  },
  {
    "abstract": "Requirements for systems have been rapidly changed and the heterogeneous systems with the different requirement levels have been globally connected from devices level to world-wide level. In this trend of requirements and technologies, the autonomous decentralized systems (ADS) are expected to adapt themselves to the changing environments through convolution of heterogeneity and autonomy of the systems. Here, the requirements of the systems, and the technologies for the ADS on the biological analogy are discussed.",
    "actual_venue": "Isads"
  },
  {
    "abstract": "A Discrete Event System (DES) is a dynamic system whose evolution is governed by the instantaneous occurrence of physical\n events. DES arise in many areas such as robotics, manufacturing, communication networks, and transportation. They are often\n modelled by languages or automata over an alphabet of symbols denoting the events. In 1987, Ramadge and Wonham initiated a very\n successful approach to the control of DES [10, 13], which was subsequently extended by themselves and others. Textbooks or\n course notes on the subject include [1, 7, 12].",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The increasing availability of dynamically changing digital data that can be used for extracting social networks over time has led to an upsurge of interest in the analysis of dynamic social networks. One key aspect of dynamic social network analysis is finding the central nodes in a network. However, dynamic calculation of centrality values for rapidly changing networks can be computationally expensive, with the result that data are frequently aggregated over many time periods and only intermittently analyzed for centrality measures. This paper presents an incremental betweenness centrality algorithm that efficiently updates betweenness centralities or k-betweenness centralities of nodes in dynamic social networks by avoiding re-computations through the efficient storage of information from earlier computations. In this paper, we evaluate the performance of the proposed algorithms for incremental betweenness centrality and k-betweenness centrality on both synthetic social network data sets and on several real-world social network data sets. The presented incremental algorithm can achieve substantial performance speedup (3–4 orders of magnitude faster for some data sets) when compared to the state of the art. And, incremental k-betweenness centrality, which is a good predictor of betweenness centrality, can be carried out on social network data sets with millions of nodes.",
    "actual_venue": "Social Netw Analys Mining"
  },
  {
    "abstract": "In this paper, we report the hotspot and gaze path of users' eye-movements on three different layouts for recommender interfaces. One is the standard list layout, as appearing in most of current recommender systems. The other two are variations of organization interfaces where recommended items are organized into categories and each category is annotated by a title. Gaze plots infer that the organization interfaces, especially the quadrant layout, are likely to arouse users' attentions to more recommendations. In addition, more users chose products from the organization layouts. Combining the results with our prior works, we suggest a set of design guidelines and practical implications to our future work.",
    "actual_venue": "IUI"
  },
  {
    "abstract": "A range of Management Information Base (MIB) modules has been developed to help model and manage the various aspects of Multiprotocol Label Switching (MPLS) networks. These MIB modules are defined in separate documents that focus on the specific areas of responsibility of the modules that they describe. This document describes the management architecture for MPLS and indicates the interrelationships between the different MIB modules used for MPLS network management.",
    "actual_venue": "RFC"
  },
  {
    "abstract": "To date, several types of structure for finite Constraint Satisfaction Problems have been investigated with the goal of either improving the performance of problem solvers or allowing efficient problem solvers to be identified. Our aim is to extend the work in this area by performing a structural analysis in terms of variable connectivity for 3-SAT problems. Initially structure is defined in terms of the compactness of variable connectivity for a problem. Using an easily calculable statistic developed to measure this compactness, a test was then created for identifying 3-SAT problems as either compact, loose or unstructured (or uniform). A problem generator was constructed for generating 3-SAT problems with varying degrees of structure. Using problems from this problem generator and existing problems from SATLIB, we investigated the effects of this type of structure on satisfiability and solvability of 3-SAT problems. For the same problem length, it is demonstrated that satisfiability and solvability are different for structured and uniform problems generated by the problem generator.",
    "actual_venue": "Australian Joint Conference On Artificial Intelligence"
  },
  {
    "abstract": "Experiments on the behaviour of solid materials with respect to the formation of new phases caused by chemical diffusion result in pictures showing the (random) penetration in a certain moment. Since the (surely rather complicated) law of probability of this process is unknown, the usually applied elementary methods from mathematical statistics become pure heuristics. Interpreting the phase (penetration) depth as a fuzzy variable allows a more realistic representation of the experimental results and the evaluation of a fuzzy functional relationship between phase depth and time, from which, e.g., useful conclusions are possible with respect to reasons and time laws of phase formation. The paper contains a real-world example. (C) 1998 Published by Elsevier Science B.V.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "The progress of the Internet of Things (IoT) has enhanced real-time accessibility on smart things, and the advent of online social network (e.g. Facebook, Twitter, and Google+) have attracted people in sharing their knowledge and experiences with their social organization. Recently, several researches have shown that online social network could act as a user interface of smart things and smart things could be shared by using online social network, and especially we expect that the integration of both of them provide us with novel services which we collectively call IoT social network. However, it is possible that developers or researchers will address complexity of jointly utilizing and understanding both. In this paper, we propose a novel social networking platform for the Internet of Things, called Lilliput. Lilliput is an ontology-based platform providing information of online social networks (e.g. people's profiles, people's social relationships) and smart things (e.g. context, location) as a social graph. To architect consistent unified ontology-based platform, we conduct a research on what the social relationship among people, objects, and places will be, and propose it as IoT social relationship. Also, we show how to exploit this ontologically-connected people, objects, and places by mainly focusing on Human-oriented Access Control for IoT social network (HAC). With this platform, application developers can build IoT social network application without having to understand details in both online social networks and the Internet of Things. We show the feasibility of our architecture by implementing a prototype platform, building a testbed, and creating one of IoT social network applications, called Sorcerer's Book.",
    "actual_venue": "Ieee Scc"
  },
  {
    "abstract": "In this study, we leverage Information Technology (IT) readiness literature and resource-based view (RBV) to investigate the impact of firm structural and psychological readiness on firm value creation, as mediated by big data analytics usage. The proposed research model is empirically validated using survey data from 179 senior IT managers. The findings demonstrate the importance of both structural (i.e. IT infrastructure capability, tools functionality, employee analytical capability, and bigness of data) and psychological readiness (i.e. IT proactive climate) in enhancing firm value creation through big data analytics usage. These results provide interesting theoretical and practical insights.",
    "actual_venue": "Enterprise Information Systems"
  },
  {
    "abstract": "En raison de la mobilité inhérente dans l’environnement ad hoc, la topologie du réseau joue un rôle principal dans le fonctionnement des protocoles de routage, sur la capacité ainsi que sur la performance du réseau d’une manière générale. Dans cet article nous proposons de contrôler la morphologie du réseau ad hoc en utilisant la mobilité d’un ensemble de routeurs dédiés. Le principe est d’utiliser positivement la mobilité, qui est habituellement subie, afin d’améliorer les performances du réseau sans fil ad hoc. Des stratégies de déploiement des routeurs dédiés sont étudiées dans le but d’offrir une meilleure connectivité ou pour limiter le diamètre du réseau en vue par la suite d’offrir une certaine QoS. Le problème de déploiement a été formulé comme un problème de programmation linéaire entière mixte. Il est ensuite résolu à l’aide du solveurCplex, et des simulations de ces stratégies de contrôle de topologie sont effectuées sous NS-2 pour montrer la valeur ajoutée d’une telle proposition.",
    "actual_venue": "Annales Des Tlcommunications"
  },
  {
    "abstract": "In this paper, we have developed a model that integrates system dynamics with fuzzy multiple objective programming (SD-FMOP). This model can be used to study the complex interactions in a industry system. In the process of confirming sensitive parameters and fuzzy variables of the SD model, we made use of fuzzy multi-objective programming to help yield the solution. We adopted the chance-constraint programming model to convert the fuzzy variables into precise values. We use genetic algorithm to solve FMOP model, and obtain the Pareto solution through the programming models. It is evident that FMOP is effective in optimizing the given system to obtain the decision objectives of the SD model. The results recorded from the SD model are in our option, reasonable and credible. These results may help governments to establish more effective policy related to the coal industry development.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "The combination of the large-scale multiple-input-multiple-output (MIMO) and orthogonal frequency-division multiplexing (OFDM) transmissions is a promising candidate for future wireless communication systems. We investigate a quality of service (QoS) guaranteed user-scheduling algorithm for large-scale MIMO-OFDM systems, where pilots are assigned to served users. In general, a wireless channel con...",
    "actual_venue": "Ieee Transactions On Vehicular Technology"
  },
  {
    "abstract": "In protein structures the peptide bond is found to be in trans conformation in the majority of the cases. Only a small fraction of peptide bonds in proteins is reported to be in cis conformation. Most of these instances (90%) occur when the peptide bond is an imide (X-Pro) rather than an amide bond (X-nonPro). Due to the implication of cis/trans isomerization in many biologically significant processes, the accurate prediction of the peptide bond conformation is of high interest. In this study, we evaluate the effect of a wide range of features, towards the reliable prediction of both proline and non-proline cis/trans isomerization. We use evolutionary profiles, secondary structure information, real-valued solvent accessibility predictions for each amino acid and the physicochemical properties of the surrounding residues. We also explore the predictive impact of a modified feature vector, which consists of condensed position-specific scoring matrices (PSSMX), secondary structure and solvent accessibility. The best discriminating ability is achieved using the first feature vector combined with a wrapper feature selection algorithm and a support vector machine (SVM). The proposed method results in 70% accuracy, 75% sensitivity and 71% positive predictive value (PPV) in the prediction of the peptide bond conformation between any two amino acids. The output of the feature selection stage is investigated in order to identify discriminatory features as well as the contribution of each neighboring residue in the formation of the peptide bond, thus, advancing our knowledge towards cis/trans isomerization.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "Correlated Nakagami-m fading is commonly encountered in wireless communications. Its generation in a laboratory environment is therefore of theoretical and practical importance. However, no generic technique for this purpose is available in the literature except for the simplest case of correlated Rayleigh fading. In this paper, by introducing a direct-sum decomposition principle and determining the statistical mapping between the correlated Nakagami (1960) process and a set of Gaussian vectors for its generation, a simple general procedure is derived for the generation of correlated Nakagami channels with arbitrary parameters. The validity of the new technique is examined through the generation of a correlated Nakagami sequence, as encountered in US digital cellular, and a multibranch vector channel as encountered in diversity reception.",
    "actual_venue": "Communications, Icc Ieee International Conference"
  },
  {
    "abstract": "•We propose the first model that can supervise Latent Dirichlet Allocation (LDA) by Gaussian Processes (GPs).•LDA and GP are jointly trained by a novel variational inference algorithm that adopts ideas form Deep GPs.•Differently from Supervised LDA (sLDA), our model learns non-linear mappings from topic activations to document classes.•By virtue of this non-linearity, our model outperforms s LDA, as well as a disjointly trained cascade of LDA and GP in three real-world data sets from two different domains.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "A parallel algorithm to perform modified Gram-Schmidt orthogonalisation of a set of vectors using an MIMD architecture is described. An array of transputers configured in a simple pipeline topology is used for the computation. The speedup obtained by using P processors asymptotically approaches P/2 when the size of the problem becomes large.",
    "actual_venue": "Parallel Computing"
  },
  {
    "abstract": "Nowadays, distributed and mobile systems are acquiring greater importance and becoming more widely used to support ubiquitous computing However, developing systems of this kind is a difficult task Instead of concentrating on how problems should be solved developers must worry about implementation details Ambient Calculus is a formalism that provides primitives to describe mobile systems in an abstract way Aspect-oriented software development and software architectures promise to achieve reusability, maintenance and adaptability, which are all essential for the development of distributed systems In this paper, we present how a platform-independent model called Ambient-PRISMA combines both Ambient Calculus and Aspect-Oriented Software Architecture for the specification of distributed and mobile systems A platform-specific model in .Net for supporting Ambient-PRISMA code generation is also presented.",
    "actual_venue": "Otm Conferences"
  },
  {
    "abstract": "We present a new approach to building a solver for a set of geometric constraints represented by multivariate rational functions. The constraints are formulated using inequalities as well as equalities. When the solution set has dimension larger than zero, we approximate it by fitting a hypersurface to discrete solution points. We also consider a variety of constraint solving problems common in geometric modeling. These include computing ray-traps, bisectors, sweep envelopes, and regions accessible during 5-axis machining.",
    "actual_venue": "Symposium On Solid Modeling And Applications"
  },
  {
    "abstract": "New binary covering codes of radius 1, obtained by simulated annealing, are presented. These constructions establish that K(9, 1)&les;62 and K(12, 1)&les;380. The article is concerned with finding upper bounds on K(n,R), the minimum cardinality of any binary code of length n and with covering radius R",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "The global synchronisation problem consists in making a cellular automaton converge to a homogeneous blinking state from any initial condition. We here study this inverse problem for one-dimensional binary systems with periodic boundary conditions (i.e., rings). For small neighbourhoods, we present results obtained with the formulation of the problem as a SAT problem and the use of SAT solvers. Our observations suggest that it is not possible to solve this problem perfectly with deterministic systems. In contrast, the problem can easily be solved with stochastic rules.",
    "actual_venue": "Cellular Automata And Discrete Complex Systems, Automata"
  },
  {
    "abstract": "Three fundamental questions concerning minds are presented. These are about consciousness, intentionality and intelligence. After we present the fundamental framework that has shaped both the philosophy of mind and the Artificial Intelligence research in the last forty years or so regarding the last two questions, we turn to consciousness, whose study still seems evasive to both communities. After briefly illustrating why and how phenomenal consciousness is puzzling, a theoretical diagnosis of the problem is proposed and a framework is presented, within which further research would yield a solution. The diagnosis is that the puzzle stems from a peculiar dual epistemic access to phenomenal aspects (qualia) of our conscious experiences. An account of concept formation is presented such that both the phenomenal concepts (like the concepts RED and SWEET) and the introspective concepts (like the concepts EXPERIENCING RED and TASTING SWEET) are acquired from a first-person perspective as opposed to the third-person one (the standard concept formation strategy about objective features). We explain the first-person perspective in information-theoretic and computational terms.",
    "actual_venue": "Journal Of Experimental And Theoretical Artificial Intelligence"
  },
  {
    "abstract": "Web-based image classification systems aim to provide users with an easy access to image classification function. The existing work mainly focuses on web-based unsupervised classification systems. This paper proposes a web-based supervised classification system framework which includes three modules: client, servlet and service. It comprehensively describes how to combine the procedures of supervised classification into the development of a web system. A series of methods are presented to realize the modules respectively. A prototype system of the framework is also implemented and a number of remote sensing (RS) images are tested on it. Experiment results show that the prototype is capable of accomplishing supervised classification of RS images on the Web. If appropriate algorithms and parameter values are used, the results of the web-based solution could be as accurate as the results of traditional desktop-based systems. This paper lays the foundation on both theoretical and practical aspects for the future development of operational web-based supervised classification systems.",
    "actual_venue": "Geoinformatica"
  },
  {
    "abstract": "We studied and developed a new schedule application, \"Smiley\", for autistic children. We aimed to give the user an opportunity to feel a sense of accomplishment and a motive to try something by him/herself by using this application. We cooperated with special support education schools to correct data and bugs, and to meet the demands of users. In this study, we discuss autism and education and clarify the theory we used to develop the application. Then we explain the usage and functions of the application. In the latter half of this paper, we show the data of the hearing investigation we made at a special support education school in Aichi, Japan.",
    "actual_venue": "Advanced Applied Informatics"
  },
  {
    "abstract": "This paper describes an automated process for designing mechanical systems based on the adaptive multi-agent system theory. At the beginning of the design process, the designer adds the elements of his problem: goal, envelope, constraints, known mechanical components... During the solving process, mechanical components (previously \"agentified\") organize themselves in order to find a solution, by modifying their parameters, by creating new mechanical components fitting in with the needs, or by modifying the system topology. While this paper presents an overview of the theoretical basis of AMAS theory, it primarily focuses on the method for developing the Mechanical Synthesis Solver and the results from the first prototype.",
    "actual_venue": "Lecture Notes In Artificial Intelligence"
  },
  {
    "abstract": "This paper presents a high-performance VLSI architecture for context adaptive variable length coding (CAVLC) used in the MPEG-4 AVC/H.264 video coding. Instead of only the coarse-grained 8times8 zero block skipping in the previous design, the proposed design implements the fine-grained zero skipping at the 4times4 block level and the individual coefficient level. The implementation with 0.18mum CMOS process just needs average 6.88 cycles for one block coding and costs 11.9K gates when working at 100 MHz. This design saves more than half of cycle count and 48% of area cost when compared with the other designs",
    "actual_venue": "Apccas"
  },
  {
    "abstract": "In this paper, we firstly introduce particle swarm optimization to the problem of learning Bayesian networks and propose a novel structure learning algorithm using PSO. To search in DAG spaces efficiently, a discrete PSO algorithm especially for structure learning is proposed based on the characteristics of Bayesian networks. The results of experiments show that our PSO based algorithm is fast for convergence and could obtain better structures compared with GA based algorithms.",
    "actual_venue": "CIS (1)"
  },
  {
    "abstract": "The electronic generation of documents in modern offices will trasform the nature of archives, and also the techniques of historical research. Although considerable attention has been directed to developing research methodologies for social and economic history using computerized numeric data, almost no attention has been paid to the impact of machine readable textual records on historical writing. This article considers the advantages and disadvantages for the historian of the shift from paper records to electronic documents, and suggests a number of approaches to historical research made possible by the new technology.",
    "actual_venue": "Computers And The Humanities"
  },
  {
    "abstract": "A new feature extraction process is proposed for gait representation and recognition. The new system is based on the Radon transform of binary silhouettes. For each gait sequence, the transformed silhouettes are used for the computation of a template. The set of all templates is subsequently subjected to linear discriminant analysis and subspace projection. In this manner, each gait sequence is described using a low-dimensional feature vector consisting of selected Radon template coefficients. Given a test feature vector, gait recognition and verification is achieved by appropriately comparing it to feature vectors in a reference gait database. By using the new system on the Gait Challenge database, very considerable improvements in recognition performance are seen in comparison to state-of-the-art methods for gait recognition",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "Much has been written about adopting agile software development within a large organisation. A key aspect of this significant organisational change is to ensure that a common understanding of the new technology emerges within all stakeholder groups. We propose that an analysis framework based on the concept of Technological Frames (TFs) can identify where understanding is in conflict across different stakeholder groups. We used TFs to analyse data collected in one organisation in the process of adopting an agile development approach. In doing so, we identified several dimensions (called I elements' in TFs) which characterise a group's understanding of agility. In this paper, we present these elements and describe the TFs for four distinct groups. We suggest that these elements may be used by other organisations adopting agile methods to help understand the views of different stakeholder groups.",
    "actual_venue": "Agile Processes In Software Engineering And Extreme Programming, Proceedings"
  },
  {
    "abstract": "This work addresses the coordination issue in distributed optimization problem (DOP) where multiple distinct and time-critical tasks are performed to satisfy a global objective function. The performance of these tasks has to be coordinated due to the sharing of consumable resources and the dependency on non-consumable resources. Knowing that it can be suboptimal to predefine the performance of the tasks for large DOPs, the multi-agent reinforcement learning (MARL) framework is adopted wherein an agent is used to learn the performance of each distinct task using reinforcement learning. To coordinate MARL, we propose a novel coordination strategy integrating Motivated Learning (ML) and the k-Winner-Take-All (k-WTA) approach. The priority of the agents to the shared resources is determined using Motivated Learning in real time. Due to the finite amount of the shared resources, the k-WTA approach is used to allow for the maximum number of the most urgent tasks to execute. Agents performing tasks dependent on resources produced by other agents are coordinated using domain knowledge. Comparing our proposed contribution to the existing approaches, results from our experiments based on a 16-task DOP and a 68-task DOP show our proposed approach to be most effective in coordinating multi-agent reinforcement learning.",
    "actual_venue": "Wi-Iat"
  },
  {
    "abstract": "In this paper, we propose a new Identity-Based Certificateless Proxy Signature scheme, for the grid environment, in order to enable attribute-based authorization, fine-grained delegation and enhanced delegation chain establishment and validation, all without relying on any kind of PKI Certificates or proxy certificates. We show that our scheme is correct and secure. We also give an evaluation of the computational and communication overhead of the proposed scheme. Simulations shows satisfying results.",
    "actual_venue": "Cluster, Cloud And Grid Computing"
  },
  {
    "abstract": "This article discusses the integration of animated pedagogical agent into the development of cognitive tools based upon Scientific Inquiry Model for invoking learners' cognitive conflict and fostering conceptual change. In this study, animated pedagogical agent acts as a catalyst while coaching the learner who interacts individually carrying out the hypothesize-observe-experiment activities. Study shows that through cognitive apprenticeship, the animated pedagogical agent is able to interact actively with the learners by allowing them to make their own sense of imposed ideas, extracting meaningful patterns and integrating new input with their prior belief and ideas in order to foster the conceptual change process.",
    "actual_venue": "Ieee International Conference On Advanced Learning Technologies, Proceedings"
  },
  {
    "abstract": "In this paper we try to investigate the qualitative possibility and/or necessity measures introduced by D. Dubois [Computer and Artificial Intelligence 5 (1986) 403-416] and D, Dubois and H. Prade [Artificial Intelligence 50 (1991) 223-239] as an extension of a qualitative (or subjective) probability studied by T. Fine [Theory of Probability, Academic Press, New York, 1973]. We particularly review some relationship between these concepts and the basic qualitative possibility distribution as introduced by R.R. Yager [IEEE Trans. Fuzzy Systems 1 (3) (1993) 184-193]. This will then be studied in the general framework of preference relations when the qualitative possibility/necessity measures are real valued, graded From 0 to 1. The justification of the proposal real valued relation will be carried out in the framework of measurement theory where a relationship between the difference measurement scale and our proposal will be established. Two kinds of relations will be proposed. The first one is based on some appropriate exponential form, and, the second one is based on a more general. form of R-implication operators.Finally, we briefly investigate some heuristic approach concerning the problem of how to obtain a real valued qualitative possibility from a quantitative possibility distribution. (C) 2000 Elsevier Science Inc. All rights reserved.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "Dynamic reconfiguration provides of powerful mechanism to adapt component-based distributed applications to changing environmental conditions. We have designed and implemented a framework for dynamic component reconfiguration on the basis of the Microsoft .NET environment. Within this paper we present an experimental evaluation of our infrastructure for dynamic reconfiguration of component-based applications. Our framework supports the description of application configurations and profiles and allows for selection of a particular configuration and object/component instantiation based on measured environmental conditions. In response to changes in the environment, our framework will dynamically load new configurations, thus implementing dynamic reconfiguration of an application. Configuration code for components and applications has to interact with many functional modules and therefore is often scattered around the whole application. We use aspect-oriented programming techniques to handle configuration aspects separately from functional code. The timing behavior of dynamic reconfiguration depends heavily on properties of the underlying programming environment and the operating system. We have studied to which extend and with which performance impact the Microsoft .NET Platform1 supports dynamic reconfiguration. The paper thoroughly discusses our experimental results.",
    "actual_venue": "Isorc"
  },
  {
    "abstract": "The Information and Communication Technology (ICT) has been used in wide range of applications, such as smart living, smart health and smart transportation. Among all these applications, smart home is most popular, in which the users/residents can control the operations of the various smart sensor devices from remote sites also. However, the smart devices and users communicate over an insecure communication channel, i.e., the Internet. There may be the possibility of various types of attacks, such as smart device capture attack, user, gateway node and smart device impersonation attacks and privileged-insider attack on a smart home network. An illegal user, in this case, can gain access over data sent by the smart devices. Most of the existing schemes reported in the literature for the remote user authentication in smart home environment are not secure with respect to the above specified attacks. Thus, there is need to design a secure remote user authentication scheme for a smart home network so that only authorized users can gain access to the smart devices. To mitigate the aforementioned isses, in this paper, we propose a new secure remote user authentication scheme for a smart home environment. The proposed scheme is efficient for resource-constrained smart devices with limited resources as it uses only one-way hash functions, bitwise XOR operations and symmetric encryptions/decryptions. The security of the scheme is proved using the rigorous formal security analysis under the widely-accepted Real-Or-Random (ROR) model. Moreover, the rigorous informal security analysis and formal security verification using the broadly-accepted Automated Validation of Internet Security Protocols and Applications (AVISPA) tool is also done. Finally, the practical demonstration of the proposed scheme is also performed using the widely-accepted NS-2 simulation.",
    "actual_venue": "Ieee Transactions On Dependable And Secure Computing"
  },
  {
    "abstract": "We present some personal reflections on the developments in integer and mixed-integer programming from 1968 up to around 1980.",
    "actual_venue": "Annals Or"
  },
  {
    "abstract": "We present a fundamental theory of solute dispersion in porous using (i) critical path analysis and cluster statistics of percolation theory far from the percolation threshold and (ii) the tortuosity and structure of large clusters near the percolation threshold. We use the simplest possible model of porous media, with a single length scale of heterogeneity in which the statistics of local conductances are uncorrelated. This combination of percolation-based techniques allows comprehensive investigation and predictions concerning the process of dispersion. Our predictions, which ignore molecular diffusion and make minimal use of unknown parameters, account for results obtained in a comprehensive set of nearly 1100 experiments performed on systems ranging in size from centimeters to 100 km. The success of our simple treatment overturns many existing notions about transport in porous media, such as (1) multiscale heterogeneity must be accounted for in predictions (single scale is sufficient), (2) geologic correlations are of great importance (the randomness of percolation theory is more appropriate for prediction than the most complicated models in other frameworks), (3) geologic complexity is more important than statistical physics (exactly the reverse), (4) knowledge of the subsurface is more important than knowledge of the initial conditions of the plume (the latter is critical, the former may be virtually irrelevant), (5) diffusion is dominant over advection (diffusion appears seldom to be relevant at all), (6) fracture networks are fundamentally different, and more complex, than porous media (the two are mostly equivalent), (7) the fractal structure of the medium is relevant to power-law behavior of the dispersion (in fact, at short times it is the heterogeneity of the medium, while at long times it is the fractal structure of the critical paths), and (8) there is a relation between an increase in dispersion with scale and a similar increase in the hydraulic conductivity (in fact the present model is consistent with both a diminishing hydraulic conductivity and a diminishing solute velocity with increasing spatial scale). © 2009 Wiley Periodicals, Inc. Complexity, 16,43–55, 2010",
    "actual_venue": "Complexity"
  },
  {
    "abstract": "A neural network method combined with simulated annealing algorithm is proposed for power system harmonic analysis. This method is aimed at the system in which the sampling frequency cannot be locked on the actual fundamental frequency. By updating the relevant parameters including the learning rate of fundamental frequency, fundamental frequency, harmonic phases and amplitudes, the accurate harmonic estimating results can be obtained. The simulating results show that the harmonic estimation accuracy by the proposed approach is relatively better than that by the conventional harmonic analysis methods in the asynchronous case.",
    "actual_venue": "Ieee International Conference On Granular Computing"
  },
  {
    "abstract": "Assuming the existence of a secure probabilistic encryption scheme, we show that every language that admits an interactive proof admits a (computational) zero-knowledge interactive proof. This result extends the result of Goldreich, Micali and Wigderson, that, under the same assumption, all of NP admits zero-knowledge interactive proofs. Assuming envelopes for bit commitment, we show that every language that admits an interactive proof admits a perfect zero-knowledge interactive proof.",
    "actual_venue": "Crypto Proceedings On Advances In Cryptology"
  },
  {
    "abstract": "This paper shows that ROC curves, as a method of visual- izing classifier performance, are inadequate for the needs of Artificial Intelligence researchers in several significant respects, and demon- strates that a different way of visualizing performance - the cost curves introduced by Drummond and Holte at KDD'2000 - over- comes these deficiencies.",
    "actual_venue": "Rocai"
  },
  {
    "abstract": "Understanding business behavior in a city requires acquiring huge amount of data coming from diverse field studies. The growing use of mobile devices in social media provides massive data transactions that can replace such data acquired by field studies. Location-based social networks' (LBSNs') data can be exploited in urban analysis for economic reasons. In this research, the spatial correlation of business turnouts for venues registered in LBSNs is studied for business behavior predictions. A novel similarity embedded (SE)-spatial interpolation technique is proposed for business turnouts' predictions. The proposed technique utilizes diverse features provided by LBSNs in the prediction process to improve prediction performance. Moreover, a local filter is introduced to avoid local extreme involvements in the prediction process issuing better prediction results. To test the proposed techniques, experimental case study is implemented for predicting business behavior of venues registered in Foursquare in Texas. The proposed SE-spatial interpolation has shown better prediction accuracy than classical spatial interpolation predictions. The additional integration of the local filter shows further alleviated prediction errors. Furthermore, this study extends the work for the efficient application of the proposed prediction technique in big datasets. An iterative nearest neighbors first search method is designed for accelerating the execution time of the prediction technique implementation regardless the dataset size. The proposed method was tested over several size datasets. The test results show accelerated execution time for the proposed method when compared with the classical implementation execution time. This article is categorized under: Algorithmic Development > Spatial and Temporal Data Mining Fundamental Concepts of Data and Knowledge > Big Data Mining Technologies > Prediction",
    "actual_venue": "Wiley Interdisciplinary Reviews: Data Mining And Knowledge Discovery"
  },
  {
    "abstract": "We developed an R function named \"microarray outlier filter\" (MOF) to assist in the identification of failed arrays. In sorting a group of similar arrays by the likelihood of failure, two statistical indices were employed: the correlation coefficient and the percentage of outlier spots. MOF can be used to monitor the quality of microarray data for both trouble shooting, and to eliminate bad datasets from downstream analysis. The function is freely avaliable at http://www.wriwindber.org/applications/mof/.",
    "actual_venue": "Genomics, Proteomics And Bioinformatics"
  },
  {
    "abstract": "Introduction. This paper reports on a study undertaken for the UK Joint Information Systems Committee (JISC), which explored the economic implications of alternative scholarly publishing models. Rather than simply summarising the study's findings, this paper focuses on the approach and presents a step-by-step account of the research process, highlighting the combination of process mapping, activity costing and macro modelling.Method. The analysis relies primarily on existing sources, collating activity cost information from the wide-ranging literature on scholarly communication. Where necessary, these sources were supplemented by targeted informal consultation with experts.Analysis. We examine the costs and potential benefits of the major alternative models for scholarly publishing, including subscription publishing, open access publishing and self-archiving. Adopting a formal approach to modelling the scholarly communication process and identifying activity costs, this paper presents activity and system-wide costs for each of the alternative publishing models. It then explores the potential impacts of enhanced access on returns to R&D.Results. We find that different scholarly publishing models could make a material difference to the costs faced by various parties and to the returns on investment in R&D that might be realised.Conclusion. It seems likely that more open access could have substantial benefits in the longer term. While the benefits may be lower during a transitional period they would be likely to be positive for both open access publishing and self-archiving alternatives.",
    "actual_venue": "Information Research-An International Electronic Journal"
  },
  {
    "abstract": "The dimension of a partially-ordered set (poset), introduced by Dushnik and Miller (1941), has been studied extensively in the literature. Recently, Ueckerdt (2016) proposed a variation called local dimension which makes use of partial linear extensions. While local dimension is bounded above by dimension, they can be arbitrarily far apart as the dimension of the standard example is n while its local dimension is only 3. Hiraguchi (1955) proved that the maximum dimension of a poset of order n is n∕2. However, we find a very different result for local dimension, proving a bound of Θ(n∕logn). This follows from connections with covering graphs using difference graphs which are bipartite graphs whose vertices in a single class have nested neighborhoods. We also prove that the local dimension of the n-dimensional Boolean lattice is Ω(n∕logn) and make progress toward resolving an analogue of the removable pair conjecture for local dimension.",
    "actual_venue": "European Journal Of Combinatorics"
  },
  {
    "abstract": "Recently proposed techniques for peak power management (18) involve centralized decision- making and assume quick evaluation of the various power management states. These techniques suf- fer from two limitations. First, they do not pre- vent instantaneous power from exceeding the peak power budget, but instead trigger corrective action when the budget has been exceeded. Second, while these techniques may work for multi-core architec- tures (processors with small number of cores), they are not suitable for many-core architectures (proces- sors with tens or possibly hundreds of cores on the same die) due to an exponential explosion in the number of global power management states. In this paper, we look at three scalable techniques for peak power management for many-core architec- tures. The proposed techniques (mapping the power management problem to a knapsack problem, map- ping it to a genetic search problem, and mapping it to a simple learning problem with condenc e coun- ters) prevent power from exceeding the peak power budget and enable the placement of several more cores on a die than what the power budget would normally allow. We show up to 47% (33% on aver- age) improvements in throughput for a given power budget. Our techniques outperform the static oracle by 22%.",
    "actual_venue": "High Performance Computing"
  },
  {
    "abstract": "This paper presents an image-space approximation technique for real-time subsurface scattering. We first create transmitted irradiance samples on shadow maps and then estimate single scattering efficiently using a method similar to shadow mapping, with adaptive deterministic sampling. We incorporate this single-scattering with a recently proposed technique for multiple scattering. We demonstrate that our technique produces high-quality images of animated scenes. We archived hundreds of frames per second on graphics hardware without lengthy preprocessing.",
    "actual_venue": "Pacific Conference On Computer Graphics And Applications"
  },
  {
    "abstract": "Flash memory becomes ideal storage media for small size embedded systems as well as for large size multimedia applications because of its attractive features like fast access speed, shock resistance, high reliability and further more it's rapidly increasing capacity. However, flash needs expensive erase operations for rewriting data on same memory locations that degrades the system performance. Therefore, for increasing performance by decreasing the number of erase operations, data in flash memory is likely to be handled by access patterns in hot and cold data nature categories. Separately storing data by nature in different blocks however reduces the frequent erasures but as a side effect consumes large main memory space to maintain the memory mapping table at fine granularity on a page level. In this paper, an innovative storage management framework is proposed where data is stored by nature in separate physical blocks but mapping structures are retained at coarse granularity on a block level. Simulation results using real time traces ensure significantly improved overall system performance with 70.5% and 42% reduced erasures, and 39.33 minutes preserved migration cost compared to well known previous schemes.",
    "actual_venue": "Icuimc"
  },
  {
    "abstract": "In a spectrum sharing scenario, when the channel state information (CSI) of interference links is imperfect, the transmission of the secondary user (SU) may cause a harmful interference on the primary user (PU). To quantify the impact of imperfect CSI on the PU, we present a performance metric termed as interference probability and derive the exact closed-form expression for interference probability of the PU. It is shown that the PU's interference probability is always equal to 0.75 when the CSI of interference links is imperfect. In order to guarantee the PU's interference probability below an acceptable value, a back-off power control mechanism is adopted. Then, under the PU's interference probability constraints, the exact outage probability of partial amplify-and-forward (AF) relay selection scheme in the cognitive relay networks is derived. Finally, numerical results are provided to validate our analysis. © 2012 IEEE.",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "Service-oriented systems are an instantiation of open world software, which is characterized by high dynamism and decentralization. These properties strongly impact on how service-oriented systems are engineered, built, and operated, as well as verified. To address the challenges of applying verification to open service-oriented systems, in this position paper we propose to apply verification across the entire life cycle of a service and introduce a verification-oriented service life cycle.",
    "actual_venue": "Vancouver, Bc"
  },
  {
    "abstract": "\\\"What shall I look like after N years?\\\" In this paper, we present an Auto Age Progression system, which automatically renders a series of aging faces in the future age ranges and generates an aging sequence (aging video) covering the entire life for an individual input. In the offline stage, a set of age-range specific dictionaries are learned from the constructed database, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern across different age groups, and a linear combination of these patterns expresses a particular personalized aging process. In the online stage, for an input face of an individual, our system renders the aging faces corresponding to different age ranges through the aging dictionaries, and then generates an age progression by the presented face morphing technology.",
    "actual_venue": "Acm Multimedia"
  },
  {
    "abstract": "A method of generating a Japanese sentence by inferring function words from content words using valency patterns is presented.A procedure for selecting an appropriate function word, on the assumption that correct content words have been selected for a given phrase lattice, is described. A method of inferring a correct verb when verbs are recognized less accurately than nouns by the speech recognition system is described. Sentences are produced from content words as inputs by using the valency patterns obtained from collected dialogue sentences in a restricted task domain. Using the semantic features of preceding nouns and valency patterns allow a fairly restricted number of candidate verbs to be inferred.This method eliminates possible errors at the interface between speech recognition and machine translation (component technologies of an Automatic Telephone Interpretation system) and selects the most appropriate candidate from a lattice of typical phrases output by the speech recognition system.",
    "actual_venue": "Coling"
  },
  {
    "abstract": "The object of this paper is to perform an analysis of the sensitivity for convex vector programs with inequality constraints by examining the quantitative behavior of a certain set of optima according to changes of right-hand side parameters included in the program. The results in the paper prove that the sensitivity of the program depends on the solution of a dual program and its sensitivity.",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "In this paper we give a simple polynomial-time algorithm to exactly count the number of Euler Tours (ETs) of any Eulerian graph of bounded treewidth. The problems of counting ETs are known to be #P-complete for general graphs (Brightwell and Winkler, (Brightwell and Winkler, 2005). To date, no polynomial-time algorithm for counting Euler tours of any class of graphs is known except for the very special case of series-parallel graphs (which have treewidth 2).",
    "actual_venue": "Corr"
  },
  {
    "abstract": "The polar diagram of a set of points in a plane and its extracted dual EDPD were recently introduced for static and dynamic cases. In this paper, the near-pole polar diagram NPPD for a set of points is presented. This new diagram can be considered as a generalization of the polar diagram and has applications in several communication systems and robotics problems. After reviewing the NPPD of points, we solve the problem for a set of line segments and simple polygons in optimal time Θ(nlogn), where n is the number of line segments or polygon vertices. We introduce duality for the NPPD of points and identify some applications.",
    "actual_venue": "Journal Of Computational Science"
  },
  {
    "abstract": "This article proposes an observer-based control strategy for networked multi-agent systems with constant communication delay and stochastic switching topology. First, using the system transformation method, the mean-square consensus problem of multi-agent systems can be converted into the mean-square stability problem of an equivalent system, and some equivalent conditions concerning the mean-square consensus are presented. Then, an example is given to illustrate that the connection weights should be regarded as the parameters to be designed, since they have a great effect on the mean-square consensus of multi-agent systems. By choosing appropriate connection weights, the mean-square consensus problem can be converted into the mean-square stabilisation problem of N-1 delay systems with stochastic switching signal, whose related observer-based stabilisability criteria can be established in the form of linear matrix inequalities (LMIs). Furthermore, if the LMIs are feasible, the multi-agent systems achieve mean-square consensus if and only if the union of graphs in the switching topology set has a directed spanning tree. Finally, numerical simulations are given to illustrate our results.",
    "actual_venue": "International Journal Of Control"
  },
  {
    "abstract": "We propose a new topological data structure for representing a set of polygonal curves embedded in a meshed surface. In this embedding, the vertices of the curve do not necessarily correspond to the vertices of the surface. The partition of the surface yielded by the intersecting curves is efficiently represented as a \"cut-graph\". The cut-graph stores combinatorial information of the network of curves. In our approach, the combinatorial form of information is systematically preferred to geometrical information since it improves both robustness and efficiency. Thanks to the topological data structure and algorithms, the cut-graph can be sketched through iterations of designing and erasing curves on the mesh surface in a \"nondestructive\" way, i.e. without modifying the mesh until the cutting operation is committed. We also demonstrate several prototype curve design tools inspired by 2D vector and bitmap graphics paradigms. We show how to sketch the cut-graph and how these tools can be mixedly used.",
    "actual_venue": "SMI"
  },
  {
    "abstract": "In cloud storage, the digital data is stored in logical storage pools, backed by heterogeneous physical storage media and computing infrastructure that are managed by a cloud service provider (CSP). One of the key advantages of cloud storage is its elastic pricing mechanism, in which the users need only pay for the resources/services they actually use, e.g., depending on the storage capacity consumed, the number of file accesses per month, and the negotiated service level agreement. To balance the tradeoff between service performance and cost, CSPs often employ different storage tiers, for instance, cold storage and hot storage. Storing data in hot storage incurs high storage cost yet delivers low access latency, whereas cold storage is able to inexpensively store massive amounts of data and thus provides lower cost with higher latency. In this paper, we address a major challenge confronting the CSPs utilizing such tiered storage architecture—how to maximize their overall profit over a variety of storage tiers that offer distinct characteristics, as well as file placement and access request scheduling policies. To this end, we propose a scheme where the CSP offers a two-stage auction process for: 1) requesting storage capacity and 2) requesting accesses with latency requirements. Our two-stage bidding scheme provides a hybrid storage and access optimization framework with the objective of maximizing the CSP’s total net profit over four dimensions: file acceptance decision, placement of accepted files, file access decision and access request scheduling policy. The proposed optimization is a mixed-integer nonlinear program that is hard to solve. We propose an efficient heuristic to relax the integer optimization and to solve the resulting nonlinear stochastic programs. The algorithm is evaluated under different scenarios and with different storage system parameters, and insightful numerical results are reported by comparing the proposed approach with other profit-maximization models. We see a profit increase of over 60% of our proposed method compared to other baseline algorithms in certain simulation scenarios.",
    "actual_venue": "Ieee Transactions On Network And Service Management"
  }
]