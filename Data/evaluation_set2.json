[
  {
    "abstract": "Technology convergence has become a fiery phenomenon. More and more technologies can cohabit in the same platform and the information becomes more portable from one platform to another. The phenomenon presses on examining the question of what technology entails in technology acceptance research. If the technology can be decomposed by its dimensions, e.g.. hardware, software, content, will the adoption of one dimension affect the adoption patterns of other dimensions? Addressing this research question in the case of podcasting for educational purposes, this study found that adoption of a mobile device like an iPod can moderate the causal relationships among the core constructs in the Technology Acceptance Model, and between the core constructs and the individual differences antecedents, including subjective norms, self-efficacy and personal innovativeness in information technology. The research and practice implications are also discussed.",
    "actual_venue": "Journal Of Computer Information Systems"
  },
  {
    "abstract": "A new method of recovering the original colors of black-and-white (B&W) halftoned images with homogeneous dot patterns is proposed. The conventional inverse halftoning method, which uses a look-up table (LUT), can establish the relation between the halftoned patterns and the corresponding gray levels, while the conventional reversible color to gray conversion method can recover the original colors from a given color-embedded gray image. To accomplish our goal of original color recovery from B&W halftoned patterns, an approach of combining the conventional inverse halftoning and reversible color to gray conversion is presented in this paper. Differently from the conventional method of inverse halftoning via LUT, four LUTs categorized according to the red, green, blue, and gray reference colors are designed to more accurately map a specific B&W halftone pattern into the corresponding color-embedded gray level based on the observation that the shapes of the halftone patterns depend on input colors, thereby increasing the color recovery accuracy. Also, a color mapping method based on a linear regression which models the relation between the recovered colors and the original colors is introduced to adjust the initially recovered colors more closely to the original colors. Experimental results show that unknown original colors can be recovered from B&W halftoned images via the proposed method.",
    "actual_venue": "Digital Signal Processing"
  },
  {
    "abstract": "This paper demonstrates the use of Behavior Trees and model checking to assess system safety requirements for a system containing substantial redundancy. The case study concerns the hydraulics systems for the Airbus A320 aircraft, which are critical for aircraft control. The system design is supposed to be able to handle up to 3 different components failing individually, without loss of all hydraulic power. Verifying the logic of such designs is difficult for humans because of the sheer amount of detail and number of different cases that need to be considered. The paper demonstrates how model checking can yield insights into what combinations of component failures can lead to system failure.",
    "actual_venue": "Sefm"
  },
  {
    "abstract": "Deep convolutional neural networks (CNNs) form the backbone of many state-of-the-art computer vision systems for classification and segmentation of 2D images. The same principles and architectures can be extended to three dimensions to obtain 3D CNNs that are suitable for volumetric data such as CT scans. In this work,we train a 3D CNN for automatic detection of pulmonary nodules in chest CT images using volumes of interest extracted from the LIDC dataset. We then convert the 3D CNN which has a fixed field of view to a 3D fully convolutional network (FCN) which can generate the score map for the entire volume efficiently in a single pass. Compared to the sliding window approach for applying a CNN across the entire input volume, the FCN leads to a nearly 800-fold speed-up, and there by fast generation of output scores for a single case. This screening FCN is used to generate difficult negative examples that are used to train a new discriminant CNN. The over all system consists of the screening FCN for fast generation of candidate regions of interest, followed by the discrimination CNN.",
    "actual_venue": "Proceedings Of Spie"
  },
  {
    "abstract": "A general framework for studying the transitivity of reciprocal relations is presented. The key feature is the cyclic evaluation of transitivity: triangles (i.e. any three points) are visited in a cyclic manner. An upper bound function acting upon the ordered weights encountered provides an upper bound for the ‘sum minus 1’ of these weights. Commutative quasi-copulas allow to translate a general definition of fuzzy transitivity (when applied to reciprocal relations) elegantly into the framework of cycle-transitivity. Similarly, a general notion of stochastic transitivity corresponds to a particular class of upper bound functions. Ample attention is given to self-dual upper bound functions.",
    "actual_venue": "Social Choice And Welfare"
  },
  {
    "abstract": "Inferring causality using longitudinal observational databases is challenging due to the passive way the data are collected. The majority of associations found within longitudinal observational data are often non-causal and occur due to confounding. The focus of this paper is to investigate incorporating information from additional databases to complement the longitudinal observational database analysis. We investigate the detection of prescription drug side effects as this is an example of a causal relationship. In previous work a framework was proposed for detecting side effects only using longitudinal data. In this paper we combine a measure of association derived from mining a spontaneous reporting system database to previously proposed analysis that extracts domain expertise features for causal analysis of a UK general practice longitudinal database. The results show that there is a significant improvement to the performance of detecting prescription drug side effects when the longitudinal observation data analysis is complemented by incorporating additional drug safety sources into the framework. The area under the receiver operating characteristic curve (AUC) for correctly classifying a side effect when other data were considered was 0.967, whereas without it the AUC was 0.923 However, the results of this paper may be biased by the evaluation and future work should overcome this by developing an unbiased reference set.",
    "actual_venue": "Siam International Conference On Data Mining"
  },
  {
    "abstract": "Let k q ( n ) denote the minimal cardinality of a q-ary code C of length n and covering radius one. The numbers of elements of C that lie in a fixed k-dimensional subspace of {0,…, q −1} n satisfy a certain system of linear inequalities. By employing a technique for dealing with ‘large’ values of k (i.e. unbounded with increasing n) we are able to derive lower bounds for k q ( n ). The method works especially well in cases where the sphere covering bound has not been substantially improved, for example if q =3 and n≡1 ( mod 3) . As an application we show that the difference between k q ( n ) and the sphere covering bound approaches infinity with increasing n if q is fixed and ( q −1) n +1 does not divide q n . Moreover, we present improvements of already known lower bounds for k q ( n ) such as k 3 (10)⩾2835.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "Quantum error correction (QEC) entails the encoding of quantum information into a QEC code space, measuring error syndromes to properly locate and identify errors, and, if necessary, applying a proper recovery operation. Here we compare three syndrome measurement protocols for the [[7,1,3]] QEC code: Shor states, Steane states, and one ancilla qubit by simulating the implementation of 50 logical gates with the syndrome measurements interspersed between the gates at different intervals. We then compare the fidelities for the different syndrome measurement types. Our simulations show that the optimal syndrome measurement strategy is generally not to apply syndrome measurements after every gate but depends on the details of the error environment. Our simulations also allow a quantum computer programmer to weigh computational accuracy versus resource consumption (time and number of qubits) for a particular error environment. In addition, we show that applying syndrome measurements that are unnecessary from the standpoint of quantum fault tolerance may be helpful in achieving better accuracy or in lowering resource consumption. Finally, our simulations demonstrate that the single-qubit non-fault-tolerant syndrome measurement strategy achieves comparable fidelity to those that are fault tolerant.",
    "actual_venue": "Quantum Information Processing"
  },
  {
    "abstract": "Studying the structure of the evolutionary communities in complex networks is essential for detecting the relationships between their structures and functions. Recent community detection algorithms often use the single-objective optimization criterion. One such criterion is modularity which has fundamental problems and disadvantages and does not illustrate complex networks’ structures. In this study, a novel multi-objective optimization algorithm based on ant colony algorithm (ACO) is recommended to solve the community detection problem in complex networks. In the proposed method, a Pareto archive is considered to store non-dominated solutions found during the algorithm’s process. The proposed method maximizes both goals of community fitness and community score in a trade-off manner to solve community detection problem. In the proposed approach, updating the pheromone in ACO has been changed through Pareto concept and Pareto Archive. So, only non-dominated solutions that have entered the Pareto archive after each iteration are updated and strengthened through global updating. In contrast, the dominated solutions are weakened and forgotten through local updating. This method of updating the Pheromone will improve algorithm exploration space, and therefore, the algorithm will search and find new solutions in the optimal space. In comparison to other algorithms, the results of the experiments show that this algorithm successfully detects network structures and is competitive with the popular state-of-the-art approaches.",
    "actual_venue": "Journal Of Ambient Intelligence And Humanized Computing"
  },
  {
    "abstract": "To model flexible objectives for discrete location problems, ordered median functions can be applied. These functions multiply\n a weight to the cost of fulfilling the demand of a customer which depends on the position of that cost relative to the costs\n of fulfilling the demand of the other customers. In this paper a reformulated and more compact version of a covering model\n for the discrete ordered median problem (DOMP) is considered. It is shown that by using this reformulation better solution\n times can be obtained. This is especially true for some objectives that are often employed in location theory. In addition,\n the covering model is extended so that ordered median functions with negative weights are feasible as well. This type of modeling\n weights has not been treated in the literature on the DOMP before. We show that several discrete location problems with equity\n objectives are particular cases of this model. As a result, a mixed-integer linear model for this type of problems is obtained\n for the first time.",
    "actual_venue": "Math Meth Of Or"
  },
  {
    "abstract": "The IEEE 802.11 protocols are used by millions of smartphone and tablet devices to access the Internet via Wi-Fi wireless networks or communicate with one another directly in a peer-to-peer mode. Insider attacks are those originating from a trusted node that had initially passed all the authentication steps to access the network and then got compromised. A trusted node that has turned rogue can easily perform Denial-of-Service (DoS) attacks on the Media Access Control (MAC) layer by illegally capturing the channel and preventing other legitimate nodes from communicating with one another. Insider attackers can alter the implementation of the IEEE 802.11 Distributed Coordination Function (DCF) protocol residing in the Network Interface Card (NIC) to illegally increase the probability of successful packet transmissions into the channel at the expenses of nodes that follow the protocol standards. The attacker fools the NIC to upgrade its firmware and forces in a version containing the malicious code. In this paper, we present a distributed solution to detect and isolate the attacker in order to minimize the impact of the DoS attacks on the network. Our detection algorithm enhances the DCF firmware to enable honest nodes to monitor each other's traffic and compare their observations against honest communication patterns derived from a two-dimensional Markov chain. A channel hopping scheme is then used on the physical layer (PHY) to evade the attacker. To facilitate communication among the honest member stations and minimize network downtime, we introduce two isolation algorithms, one based on identity-based encryption and another based on broadcast encryption. Our simulation results show that the latter enjoys quicker recovery time and faster network convergence.",
    "actual_venue": "Advanced Information Networking And Applications"
  },
  {
    "abstract": "Ranging from temperature control to safety-critical applications, continuous controllers are used in a plethora of applications becoming increasingly complex. In turn, testing continuous control systems also is more complex. Particularly, application-specific manual formal analysis or testing the complete input range becomes infeasible. We present a comprehensive failure-based testing methodology and a respective automated tool for continuous controllers. Our methodology is based on an existing automated approach, testing stability, liveness, smoothness and responsiveness in a single value-response scenario only. We performed a practitioner survey and literature review in the domain revealing the quality criteria steadiness and reliability to be vital for meaningful testing of continuous controllers. In addition, we identified 4 further scenarios including disturbance response for comprehensive testing. We contribute a library of failure models and quality criteria for the automated testing of continuous control systems more complete than in previous approaches. On the grounds of our comprehensive experiments on 9 real-world control systems, our results demonstrate our failure-based testing methodology to provide better worst cases than manual testing (effectiveness) within an adequate time frame (efficiency) for any configuration used in our experiments (reproducibility).",
    "actual_venue": "Ieee International Symposium On Software Reliability Engineering"
  },
  {
    "abstract": "In this paper, we propose a crosslayer approach that explores Tomlinson-Harashima Precoding (THP) at the physical layer to reduce the multiuser scheduling burden at the MAC layer, and improves the sum rate of the downlink multiuser MIMO system. Our proposed scheme is further evaluated with imperfect feedback, obtained by the long range prediction (LRP) technique. Compared to some existing scheduling schemes, the proposed scheme approaches the performance upper bound in certain scenarios, while incurring much less computation complexity. Significant gains are still maintained with imperfect channel state information (CSI), fed back at a rate much lower than the data rate.",
    "actual_venue": "Ieee Wireless Communications And Networking Conference"
  },
  {
    "abstract": "A supervisory control system (SCS) that maximizes the overall powertrain efficiency of a series hybrid electric vehicle (HEV) is developed. A novel approach of formulating the overall power efficiency which considers the idling losses of the engine and also characterizes separately the charging and discharging efficiencies of the battery, is proposed. These features allow the mode of direct power transfer from engine to battery to be included in the optimization. In turn, this enables the formulation of an optimization problem with a charge-sustaining scheme for improved control. A dynamic model of a series HEV is used in the development and testing of the SCS. Simulations with standard driving cycles demonstrate the significant improvement in fuel economy and battery charge sustaining of the designed Efficiency Maximizing and Charge Sustaining Map (EMCSM) over a Thermostat control scheme.",
    "actual_venue": "Decision And Control"
  },
  {
    "abstract": "Advancements of digital image processes (DIP) and availability of multispectral and hyperspectral remote sensing data have greatly benefited mineral investigation, structure geology mapping, fault pattern, and landslide studies: site-specific landslide assessment and landslide quantification. The main objective of this research was to map the geology of the central region of Kenya using remote-sensing techniques to aid rainfall-induced landslide quantification. The study area is prone to landslides geological hazards and, therefore, it was necessary to investigate geological characteristics in terms of structural pattern, faults, and river channels in a highly rugged mountainous terrain. The methodology included application of PCA, band rationing, intensity hue saturation (IHS) transformation, ICA, false color composites (FCC), filtering applications, and thresholding, and performing knowledge-based classification on Landsat ${rm ETM + }$ imagery. PCA factor loading facilitated the choice of bands with the most geological information for band rationing and FCC combination. Band ratios (3/2, 5/1, 5/4, and 7/3) had enhanced contrast on geological features and were the input variables in a knowledge-based geological classification. This was compared to a knowledge-based classification using PCs 2, 5, and IC1, where the band ratio classification performed better at representing geology and matched FCC [IC1, PC5, saturation band of IHS (5,7,3)]. Fault and lineament extraction was achieved by filtering and thresholding of pan-band8 and ratio 5/1 and overlaid on the geology map. However, the best visualization of lineaments and geology was in the FCC [IC1, PC5, saturation band of IHS (5,7,3)], where volcanic extrusions, igneous, sedimentary rocks (eolian and organic), and fluvial deposits were well discriminated.",
    "actual_venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of  "
  },
  {
    "abstract": "In an experiment of multi-trial task to obtain a reward, reward expectancy neurons, which responded only in the non-reward trials that are necessary to advance toward the reward, have been observed in the anterior cingulate cortex of monkeys. In this paper, to explain the emergence of the reward expectancy neuron in terms of reinforcement learning theory, a model that consists of a recurrent neural-network trained based on reinforcement learning is proposed. The analysis of the hidden layer neurons of the model during the learning suggests that the reward expectancy neurons emerge to realize smooth temporal increase of the state value by complementing the neuron that responds only in the reward trial.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "With the recent advances in Internet and mobile technologies and infrastructures, there are increasing demands for ubiquitous access to tourist information systems for service coordination and integration. However, disparate tourist information and service resources such as airlines, hotels, tour operators, etc., make it difficult for tourist to use them effectively when planning their trips and/or during their trips. Motivated by the emerging technologies of multi-agent information system (MAIS) and its ability to aid Internet and mobile users, together with semantic Web that can effectively organize information and service resources. In this paper, we propose a virtual travel agent system (VTAS), which is built upon these technologies. In this paper, we formulate a scalable, flexible, and intelligent MAIS architecture for VTAS with agent clusters based on a case study of a large service-oriented travel agency. Agent clusters may comprise several types of agents to achieve the goals of the major processes of a tourist's trip. We show how agents can make use of ontology from the semantic web help tourists better plan, understand, and specify their requirements. We further illustrate how this can be successfully implemented with Web service technologies to integrate disparate Internet tourist resources.",
    "actual_venue": "Niagara Falls, On"
  },
  {
    "abstract": "Real-time physiological monitoring of athletes during sporting events has tremendous potential for maximizing player performance while preventing burn-out and injury, and also enabling exciting new applications such as referee-assist services and enhanced television broadcast. Emerging advanced monitoring devices have the right combination of light weight and unobtrusive size to allow truly non-intrusive monitoring during competition. However their small battery capacities, limited wireless ranges and susceptibility to body effects make real-time data extraction a challenge, particularly in sports with a large playing area. In this work we present the novel application of body area sensor networks to monitoring soccer players in a soccer field. We begin by outlining the challenges in experimental data collection and elaborate on the design choices we have made. Secondly, we show that the inherent characteristics of the operating environment lead to unacceptably high delays for direct transmissions from the players to the base stations. This leads to our third contribution, namely a multi-hop routing protocol that balances between the competing objectives of resource consumption and delay.",
    "actual_venue": "LCN"
  },
  {
    "abstract": "The harmonic model, i.e., a sum of sinusoids having frequencies that are integer multiples of the pitch, has been widely used for modeling of voiced speech. In microphone arrays, the direction-of-arrival (DOA) adds an additional parameter that can help in obtaining a robust procedure for tracking non-stationary speech signals in noisy conditions. In this paper, a joint DOA and pitch estimation (JDPE) method is proposed. The method is based on the minimum variance distonionless response (MVDR) beamformer in the frequency-domain and is much faster than previous joint methods, as it only requires the computation of the optimal filters once per segment. To exploit that both pitch and DOA evolve piece-wise smoothly over time, we also extend a dynamic programming approach to joint smoothing of both parameters. Simulations show the proposed method is much more robust than parallel and cascaded methods combining existing DOA and pitch estimators.",
    "actual_venue": "Proceedings Of The European Signal Processing Conference"
  },
  {
    "abstract": "This paper deals with the automatic extraction of the road network in dense urban areas using a few-meters-resolution synthetic aperture radar (SAR) images. The first part presents the proposed method, which is an adaptation of previous work to the specific case of urban areas. The major modifications are 1) the clique potentials of the Markov random field that extracts the road network are adapte...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "This document describes the registration of the MIME sub-type image/tiff. This document refines an earlier sub-type registration in RFC 1528. This document obsoletes RFC 2302. 1. Conventions used in this document The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119 (REQ). 2. Overview This document describes the registration of the MIME sub-type image/tiff. The baseline encoding of TIFF (Tag Image File Format) is defined by (TIFF).",
    "actual_venue": "Tag Image File Format - Image/Tiff Mime Sub-Type Registration"
  },
  {
    "abstract": "In this paper, we propose a novel approach called, Reviewers Authority Testing and Evaluation (RATE), to improve the effectiveness of a manuscript review process. In the proposed RATE approach, we define a RATE model to express a manuscript review process mathematically. We then design a RATE algorithm to rank the authority of each reviewer in the RATE model and consequently calculate the quality score for each manuscript. The experimental results demonstrate that the performance of the RATE algorithm is superior to existing approaches. Furthermore, the experiments on testing algorithm's parameter settings also demonstrate that the proposed RATE algorithm behaves effectively and stably.",
    "actual_venue": "Web Intelligence"
  },
  {
    "abstract": "A novel reversible data hiding scheme based on invariability of the sum of pixel pairs and pairwise difference adjustment (PDA) is presented in this letter. For each pixel pair, if a certain value is added to one pixel while the same value is subtracted from the other, then the sum of these two pixels will remain unchanged. How to properly select this value is the key issue for the balance between reversibility and distortion. In this letter, half the difference of a pixel pair plus 1-bit watermark has been elaborately selected to satisfy this purpose. In addition, PDA is proposed to significantly reduce the capacity consumed by overhead information. A series of experiments is conducted to verify the effectiveness and advantages of the proposed approach. © 2008 IEEE.",
    "actual_venue": "Ieee Signal Processing Letters"
  },
  {
    "abstract": "This work considers data query applications in tree-structured networks, where a given set of source nodes generate (or collect) data and forward the data to some halfway storage nodes for satisfying queries that call for data generated by all source nodes. The goal is to determine an optimal set of storage nodes that minimizes overall communication cost. Prior work toward this problem assumed homogeneous channel cost, which may not be the case in many network environments. We generalize the optimal storage problem for a tree-structured network by considering heterogeneous channel costs. The necessary and sufficient conditions for the optimal solution are identified, and an algorithm that incurs a linear time cost is proposed. We have also conducted extensive simulations to validate the algorithm and to evaluate its performance.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "Clustering for wireless sensor networks (WSNs) is an effective scheme in utilizing sensor nodes energy and extending the network lifetime, while coverage preservation is one of the most essential issues to guarantee the quality of service (QoS). However, the coverage problem has not been well understood so far. For mission-critical applications of networks, it is crucial to consider coverage requirements when we select cluster heads and routing nodes for the clustering topology. In this paper, we propose the ECDC (Energy and Coverage-aware Distributed Clustering Protocol), an integrated protocol involving both energy and coverage, which is different from previous clustering protocols. For different practical applications, we design corresponding coverage importance metrics and introduce them into the clustering algorithm. Theoretical analysis and simulation results show that our protocol is effective in improving network coverage performance, reducing nodes energy dissipation and extending the network lifetime.",
    "actual_venue": "Computers And Electrical Engineering"
  },
  {
    "abstract": "With the economy developing, effective financial distress prediction methods of artificial intelligence have got more and more attention of the academia. Concept drift in a data flow is another hot research topic. This paper firstly introduces several kinds of existing batch weighted methods for financial distress prediction modeling, and analyzes their shortages. To find a solution to deal with them, we proposed a new batch weighted method base on classifier combination, which applies different classification algorithms respectively in batch weighting and classifier modeling, and output the financial distress prediction result by weighted voting combination of multiple classifiers. Empirical experiment is carried out with the financial data selected from Chinese listed companies, and the proposed method is proved to be effective.",
    "actual_venue": "CSO"
  },
  {
    "abstract": "Interwinding planar micro-transformers are developed using micro-machining technique. The transformers with a vertically stack coil structure on silicon monolithic substrate are designed to achieve high coupling and high inductance value in a relatively small coil area. In this work, various types of stack interwinding transformer are fabricated, measured and compared. The results show that the metal-to-metal effect of a multi-layer structure contributes to the significant increase of parasitic capacitances and hence limits the operating frequency. Moreover, the lumped element parameters are analyzed by extracting the measured S-parameter. This investigation can give important information for the future development of three-dimensional RF devices.",
    "actual_venue": "Microelectronics Journal"
  },
  {
    "abstract": "The incorporation of sensor readings into RFID tags creates a significant potential for applications which require environment monitoring as well as asset management and tracking, such as blood storage and management, and on-board aircraft part maintenance. On one hand, the environment history is essential in evaluating health and integrity of objects to be protected, such as blood and aircraft part. On the other hand, the periodic sensing information stored in RFID enables identifying the events of displacing and removal of the objects. However, the benefits of integrating RFID with sensor data are achieved only when the authenticity and integrity of data written to and stored at RFID are ensured. In this paper, we present challenges on secure integration of sensor readings and RFID, and compare security solutions of data integrity and authenticity for resource-constrained RFID tags. For mutual authentication between RFID tags and sensors (or readers), the public key based approaches proves efficient and feasible. We further evaluate different public key based approaches in terms of storage and communication efficiency.",
    "actual_venue": "Hetersanet"
  },
  {
    "abstract": "A learning model predictive controller for iterative tasks is presented. The controller is reference-free and is able to improve its performance by learning from previous iterations. A safe set and a terminal cost function are used in order to guarantee recursive feasibility and nondecreasing performance at each iteration. This paper presents the control design approach, and shows how to recursive...",
    "actual_venue": "Ieee Transactions On Automatic Control"
  },
  {
    "abstract": "Crossed cubes are important variants of the hypercubes. It has been proven that crossed cubes have attractive properties in Hamiltonian connectivity and pancyclicity. In this paper, we study two stronger features of crossed cubes. We prove that the n-dimensional crossed cube is not only node-pancyclic but also edge-pancyclic for n ≥ 2. We also show that the similar property holds for hypercubes. © 2004 Elsevier B.V. All rights reserved.",
    "actual_venue": "Information Processing Letters"
  },
  {
    "abstract": "The Standard Generlized Markup Language (SGML) and the Extensible Markup Language (XML) allow users to define document-type definitions (DTDs), which are essentially extended context-free grammars expressed in a notation that is similar to extended Backus-Naur form. The right-hand side of a production, called a content model, is both an extended and a restricted regular expression. The semantics of content models for SGML DTDs can be modified by exceptions (XML does not allow exceptions). Inclusion exceptions allow named elements to appear anywhere within the content of a content model, and exclusion exceptions preclude named elements from appearing in the content of a content model. We give precise definitions of the semantics of exceptions, and prove that they do not increase the expressive power of SGML DTDs when we restrict DTDs according to accepted SGML practice. We prove the following results: 1. Exceptions do not increase the expressive power of extended context-free grammars. 2. For each DTD with exceptions, we can obtain a structurally equivalent extended context-free grammar. 3. For each DTD with exceptions, we can construct a structurally equivalent DTD when we restrict the DTD to adhere to accepted SGML practice. 4. Exceptions are a powerful shorthand notation---eliminating them may cause exponential growth in the size of an extended context-free grammar or of a DTD. Copyright 2001 Academic Press",
    "actual_venue": "Inf Comput"
  },
  {
    "abstract": "Packet classification-the matching of packet headers against a predefined rule set-is a crucial functionality of firewalls, intrusion detection systems, and SDN switches. Most existing classification algorithms trade setup time for classification speed-that is, the packet classification is fast, but the transformation of rules set into the corresponding search data structure takes a considerable amount of time. This preprocessing time, however, poses a significant challenge for systems where rule sets can often change. Hence, these systems often use slow classification algorithms that support frequent rule set updates, which drastically limits their achievable throughput. In this work, we present a novel algorithmic technique which is able to \"upgrade\" an arbitrary existing classification algorithm to support fast updates, while still providing high lookup performance. Our evaluation demonstrates that our proposed technique exceeds the matching performance of existing dynamically updatable algorithms by an order of magnitude while providing the same level of update responsiveness.",
    "actual_venue": "Ieee Conference On Local Computer Networks"
  },
  {
    "abstract": "This paper describes the design and implementation of a PKI-based e-Health authentication architecture. This architecture was developed to authenticate e-Health Professionals accessing RTS (Rede Telematica da Saude), a regional platform for sharing clinical data among a set of affiliated health institutions. The architecture had to accommodate specific RTS requirements, namely the security of Professionals' credentials, the mobility of Professionals, and the scalability to accommodate new health institutions. The adopted solution uses short. lived certificates and cross-certification agreements between RTS and e-Health institutions for authenticating Professionals accessing the RTS. These certificates carry as well the Professional's role at their home institution for role-based authorization. Trust agreements between health institutions and RTS are necessary in order to make the certificates recognized by the RTS. As a proof of concept, a prototype was implemented with Windows technology. The presented authentication architecture is intended to be applied to other medical telematic systems.",
    "actual_venue": "Healthinf : Proceedings Of The International Conference On Health Informatics"
  },
  {
    "abstract": "Enhancing the quality of low light images is a critical processing function both from an aesthetics and an information extraction point of view. This work proposes a novel approach for enhancing images captured under low illumination conditions based on the mathematical framework of Sparse Representations. In our model, we utilize the sparse representation of low light image patches in an appropriate dictionary to approximate the corresponding day-time images. We consider two dictionaries; a night dictionary for low light conditions and a day dictionary for well illuminated conditions. To approximate the generation of low and high illumination image pairs, we generated the day dictionary from patches taken from well exposed images, while the night dictionary is created by extracting appropriate features from under exposed image patches. Experimental results suggest that the proposed scheme is able to accurately estimate a well illuminated image given a low-illumination version. The effectiveness of our system is evaluated by comparisons against ground truth images while compared to other methods for image night context enhancement, our system achieves better results both quantitatively as well as qualitatively.",
    "actual_venue": "Image Analysis And Recognition, Iciar , Pt"
  },
  {
    "abstract": "RNA molecules play major roles in all cell processes, and therefore have been subject to a great attention by biologists, biochemists and bioinformaticians in the recent years. From a computational optimization point of view, two interrelated major issues are on one hand the problem of structure prediction, and the problem of comparing two or several RNA sequences or structures. We present a brief survey of the latter, its variants, its computational complexity issues, and optimization algorithms that have been developed up to now.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "A wide range of wireless system developments require knowledge of the distribution of electromagnetic fields from various\n sources in humans. As experimental assessment is ethically unacceptable, high-resolution numerical dosimetry is needed. The\n finite-difference time-domain method is the most appropriate due to its simplicity and versatility. Reduction in demands on\n computational resources can be achieved using subgridding techniques. This paper rigorously introduces frequency dependency\n to one of the most promising subgridding techniques, Huygens subgridding. The validity of the Huygens surface in lossy media,\n as well as on the physical interface, is intensively studied.",
    "actual_venue": "Annales Des Tlcommunications"
  },
  {
    "abstract": "Goal programming is an important technique for solving many decision/management problems. Fuzzy goal programming involves applying the fuzzy set theory to goal programming, thus allowing the model to take into account the vague aspirations of a decision-maker. Using preference-based membership functions, we can define the fuzzy problem through natural language terms or vague phenomena. In fact, decision-making involves the achievement of fuzzy goals, some of them are met and some not because these goals are subject to the function of environment/resource constraints. Thus, binary fuzzy goal programming is employed where the problem cannot be solved by conventional goal programming approaches. This paper proposes a new idea of how to program the binary fuzzy goal programming model. The binary fuzzy goal programming model can then be solved using the integer programming method. Finally, an illustrative example is included to demonstrate the correctness and usefulness of the proposed model.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "This paper deals with the development of a Web-based integrated system for distance graduate studies of theories of optimization, games, and markets, which is aimed to support core curriculum and to provide students with hands-on experience on effective use of software tools. The main objective is to stimulate and develop creative abilities to work as independent researchers. This can be explained by detailed description of a typical example: the Stock Exchange Game Model (SEGM). That is the purpose of this paper. The web-site http:// mockus.org/optimum includes SEGM and accompanying models. (C) 2003 Wiley Periodicals, Inc.",
    "actual_venue": "Computer Applications In Engineering Education"
  },
  {
    "abstract": "HTTP adaptive streaming (HAS) is a widely used video streaming technology that suffers from a degradation of user’s Quality of Experience (QoE) and network’s Quality of Service (QoS) when many HAS players are sharing the same bottleneck link and competing for bandwidth. The two major factors of this degradation are: the large OFF period of HAS, which causes false bandwidth estimations, and the TCP congestion control, which is not suitable for HAS given that it does not consider the different video encoding bitrates of HAS. This paper proposes a HAS-based TCP congestion control, TcpHas, that minimizes the impact of the two aforementioned issues. It does this using traffic shaping on the server. Simulations indicate that TcpHas improves both QoE, mainly by reducing instability and convergence speed, and QoS, mainly by reducing queuing delay and packet drop rate.",
    "actual_venue": "Multimedia Syst"
  },
  {
    "abstract": "Autonomous robot calibration is defined as the process of determining a robot's model by using only its internal sen sors. It is shown that autonomous calibration of a manip ulator and stereo camera system is possible. The pro posed autonomous calibration algorithm may obtain the manipulator kinematic parameters, external kinematic camera parameters, and internal camera parameters. To do this, only joint angle readings and camera image plane data are used. A condition for the identifiability of the manipulator/camera parameters is derived. The method is a generalization of a recently developed scheme for self- calibrating a manipulator by forming it into a mobile closed-loop kinematic chain.",
    "actual_venue": "International Journal Of Robotic Research"
  },
  {
    "abstract": "Vehicular Ad Hoc Networks (VANETs) are considered as a promising scheme to actively guarantee vehicle safety, and broadcast is a key technology for warning message dissemination in VANETs. This paper proposes a novel Position-based Multi-hop Broadcast (PMB) protocol for VANETs in view of some shortcomings of existing broadcast protocols for VANETs, such as ignoring the differences of transmission range among different nodes (vehicles), and disseminating warning messages only with the help of nodes in the one-way lane, PMB calculates waiting time to select the rebroadcast nodes based on additional coverage area of adjacent nodes considering the transmission ranges of nodes together with the inter-vehicle spacing, to guarantee less nodes used to rebroadcast warning packets. Besides, it guarantees the reliability of warning message dissemination by adopting the alternative answering mechanism named implicit ACK and explicit ACK adaptively and rebroadcast packets based on nodes in the two-way lane. The simulation results show that PMB outperforms existing broadcast protocols for warning message dissemination in VANETs in terms of suppression of broadcast redundancy, real-time performance and reliability even if all nodes have different transmission ranges. © 2011 ACADEMY PUBLISHER.",
    "actual_venue": "JNW"
  },
  {
    "abstract": "In this paper, we introduce a new class of frequency-filtering IBLU decompositions that use continued-fraction approximation for the diagonal blocks. This technique allows us to construct efficient frequency-filtering preconditioners for discretizations of elliptic partial differential equations on domains with non-trivial geometries. We prove theoretically for a class of model problems that the application of the proposed preconditioners leads to a convergence rate of up to 1-O(h(1/4)) of the CG iteration. Copyright (C) 2008 John Wiley & Sons, Ltd.",
    "actual_venue": "Numerical Linear Algebra With Applications"
  },
  {
    "abstract": "In order to find relationships between Noise-Transfer-Function (NTF) characteristics and stability of a Incremental-Delta-Sigma ADC (I-DS-ADC) 40.000 different NTF has been investigated. A fast and easy to use criterion to determine, if an NTF of an I-DS-ADC of 4th-order is likely to be stable was found. The novel criterion is fast and easy to use and covers a much bigger variety of possible NTFs compared to recent criteria.",
    "actual_venue": "Norchip"
  },
  {
    "abstract": "This paper presents an experimental \"morphological analysis\" retrieval system for mammograms, using Relevance-Feedback techniques. The features adopted are first-order statistics of the Normalized Radial Distance, extracted from the annotated mass boundary. The system is evaluated on an extensive dataset of 2274 masses of the DDSM database, which involves 7 distinct classes. The experiments verify that the involvement of the radiologist as part of the retrieval process improves the results, even for such a hard classification task, reaching the precision rate of almost 90%. Therefore, Relevance-Feedback can be employed as a very useful complementary tool to a Computer Aided Diagnosis system.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "Encrypted files captured by acquiring a bit-by-bit image in the process of conventional forensic investigation are practically impossible to decrypt without knowing the key and the method of encryption. The Windows operating system provides the option to encrypt files using an encryption driver bundled with the New Technology File System (NTFS) file system, the so-called encrypting file system (EFS). EFS files can be manipulated transparently by the owner and the system administrator as long as they reside in an NTFS file system. In this article we demonstrate the methodology of extracting EFS-decrypted files from a live system. The method of extraction is built around a software utility, Robocopy, which does not modify any metadata of the file system during extraction. The hash value for the encrypted data calculated before and after the extraction is identical, so this approach can be considered to be forensically sound. We present a scenario that shows that live system investigation is indispensable in obtaining complete information about the system being examined. This information would be lost if conventional methods were applied, even when supplemented by the capture and analysis of physical memory.",
    "actual_venue": "J Digital Forensic Practice"
  },
  {
    "abstract": "In this paper, the statistical cyclostationary characteristics of a controlled continuous-time system which result as a consequence of implementing a digital controller are investigated. A minimax quadratic cost function which takes into account the periodic nature of the statistics is defined and optimized. The resulting linear state feedback law improves the intersample behavior of the controlled system when compared to discrete-time optimization. In particular, the minimax variance regulator which is proposed as a new design method for regulating the output variance offers a significant improvement over the classical minimum variance regulator.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "The technique of distributed speech recognition (DSR) has recently become an interesting area of research. One of the main issues with DSR is the need to compress the feature vector stream, produced on the terminal device, into a sufficiently low bit-rate such that it can be sent across low bandwidth channels. This work proposes a compression technique based upon first transforming a block of feature vectors into a more compact matrix representation. Columns of the resulting matrix that correspond to faster temporal variation can be removed without loss in recognition performance. The number of bits allocated to the remaining coefficients in the matrix is determined automatically, based on a measure of the information present. Experiments show that the transform-based compression gives good recognition accuracy at bit rates of 4800, 2400 and 1200bps. For example at 1200bps the recognition performance is 98.03% compared to 98.57% with uncompressed speech.",
    "actual_venue": "Interspeech"
  },
  {
    "abstract": "A prototype-based initialization methodology is proposed to approximate functions that are used to characterize nonlinear stress-strain, moment-curvature, and load-displacement relationships, as well as restoring forces and time histories in engineering mechanics applications. Three prototypes are defined by exploiting the capabilities of linear sums of sigmoidal functions. By using the proposed prototypes either individually or combinatorially, successful training can take place for ten specific types of nonlinear functions and far beyond when the required number of hidden nodes and initial values of weights and biases can always be derived before the training starts. Some mathematical insights to this initialization methodology and a few prototypes are offered, while training examples are provided to demonstrate a clear procedure that is used to implement this methodology. With the derived numbers of hidden nodes in each approximation, applying the Nguyen-Widrow algorithm is enabled and the training performance is compared between the existing and the proposed initialization options.",
    "actual_venue": "Ieee International Joint Conference On Neural Networks, Vols"
  },
  {
    "abstract": "Quality metrics, within the field of laser range imaging, are used to quantify by how much some aspect of a measurement deviates from a predefined standard. Measurement quality evaluations are becoming increasingly important in laser range imaging for range image registration, merging measurements, and planning the next best view. Spatial uncertainty and resolution are the primary metrics of image quality; however, spatial uncertainty is affected by a variety of environmental factors. A review how contemporary researchers have attempted to quantify these environmental factors is presented, along with spatial uncertainty and resolution, resulting in a wide range of quality metrics. (C) 2008 SPIE and IS&T. [DOI: 10.11/1.2955245]",
    "actual_venue": "Journal Of Electronic Imaging"
  },
  {
    "abstract": "We discuss an adaptation of the famous primal-dual 1-matching algorithm to balanced network flows which can be viewed as a network flow description of capacitated matching problems. This method Is endowed with a sophisticated start-up procedure which eventually makes the algorithm strongly polynomial. We apply the primal-dual algorithm to the shortest valid path problem with arbitrary are lengths, and so end up with a new complexity bound for this problem. (C) 2002 John Wiley Sons, Inc.",
    "actual_venue": "Networks"
  },
  {
    "abstract": "Multi-reference frame motion estimation improves the accuracy of motion compensation in video coding. However, it also increases computational complexity dramatically. In this paper, we propose a different approach for multi-reference motion estimation via downhill simplex search. Additionally, an adaptive reference frame selection algorithm is developed based on spatial and temporal smoothness of motion vectors. We first apply single-reference downhill simplex search to the previous frame. Then, temporal smoothness of motion vectors in collocated blocks is calculated to decide the number of reference frames to be included for motion estimation. Spatial smoothness of motion vectors in the neighboring blocks is used as a criterion for termination. Experimental results show that the proposed algorithm provides better PSNR than that of original multi-reference downhill simplex search in all testing sequences with similar computational speed. In addition, it outperforms several representative single-reference frame block matching methods in terms of estimation speed and coding quality.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing, , Pts -, Proceedings"
  },
  {
    "abstract": "Develops and evaluates an automated process sequential manufacturing system using stochastic Petri nets (SPN) theory. The proposed Petri net-based graphic model is used to generate a reachability tree. The tree is, in turn, converted to a state table representation of a Markov process model to investigate the model for liveness, boundedness, reversibility, and performance. Other important characteristics of a dynamic system, such as conflict, blocking, and deadlock can also be examined. Different configurations of an automated system are studied by SPN models to design an improved system",
    "actual_venue": "SMC"
  },
  {
    "abstract": "Cloud computing has been regarded as an emerging approach to provisioning resources and managing applications. It provides attractive features, such as an on-demand model, scalability enhancement, and management cost reduction. However, cloud computing systems continue to face problems such as hardware failures, overloads caused by unexpected workloads, or the waste of energy due to inefficient resource utilization, which all result in resource shortages and application issues such as delays or saturation. A paradigm, the brownout, has been applied to handle these issues by adaptively activating or deactivating optional parts of applications or services to manage resource usage in cloud computing system. Brownout has successfully shown that it can avoid overloads due to changes in workload and achieve better load balancing and energy saving effects. This article proposes a taxonomy of the brownout approach for managing resources and applications adaptively in cloud computing systems and carries out a comprehensive survey. It identifies open challenges and offers future research directions.",
    "actual_venue": "Acm Computing Surveys"
  },
  {
    "abstract": "Linda is an elegant parallel and distributed programming model. It is based on a shared associative memory, structured in tuples. We show in this paper that this model suffers from the false matching phenomenon. We explain under which conditions this problem occurs, we examine the solutions already proposed to solve it, and we show why they are not sufficient. In this framework, our goal is to propose an extension to the Linda model in order to eliminate the false matching phenomenon. This model驴Linda with bound types or B-Linda驴suitable for modern programming paradigms, adds an extended-type notion into the basic Linda model. It is first introduced in an informal manner, then we present an implementation of it. Some formal aspects are specified in the appendix: The definition of the model's elements and operational semantics.",
    "actual_venue": "Parallel and Distributed Systems, IEEE Transactions  "
  },
  {
    "abstract": "A blind source separation problem in a cyclostationary context is addressed. The mixture stems from several telecommunication signals, the baud-rates of which are possible different. Cost functions are introduced, the optimization of which achieves the equalization for a user (i.e. estimation of the baud-rate and the sequence of symbols). The method is iterated by implementing a deflation. The theoretical results are validated by simulations",
    "actual_venue": "Icassp ) Ieee International Conference"
  },
  {
    "abstract": "Drowsy driver alert systems have been developed to minimize and prevent car accidents. Existing vision-based systems are usually restricted to using visual cues, depend on tedious parameter tuning, or cannot work under general conditions. One additional crucial issue is the lack of public datasets that can be used to evaluate the performance of different methods. In this paper, we introduce a novel hierarchical temporal Deep Belief Network (HTDBN) method for drowsy detection. Our scheme first extracts high-level facial and head feature representations and then use them to recognize drowsiness-related symptoms. Two continuous-hidden Markov models are constructed on top of the DBNs. These are used to model and capture the interactive relations between eyes, mouth and head motions. We also collect a large comprehensive dataset containing various ethnicities, genders, lighting conditions and driving scenarios in pursuit of wide variations of driver videos. Experimental results demonstrate the feasibility of the proposed HTDBN framework in detecting drowsiness based on different visual cues.",
    "actual_venue": "Computer Vision - Eccv Workshops, Pt"
  },
  {
    "abstract": "The relationship between software models and metrics is examined. The opportunities for CASE application measurement are seen to be much greater than for conventional development. CASE metrics requirements are also seen to be different in a number of important ways from conventional software metrics requirements. CASE application metrics are examined under the headings of process metrics and product metrics. CASE measurement issues and methods are considered together with some potential uses of CASE development measurements",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "Vector quantization (VQ) using exhaustive nearest neighbor (NN) search is the speed bottleneck in classic bag of visual words (BOV) models. Approximate NN (ANN) search methods still cost great time in VQ, since they check multiple regions in the search space to reduce VQ errors. In this paper, we propose ExVQ, an exclusive NN search method to speed up BOV models. Given a visual descriptor, a portion of search regions is excluded from the whole search space by a linear projection. We ensure that minimal VQ errors are introduced in the exclusion by learning an accurate classifier. Multiple exclusions are organized in a tree structure in ExVQ, whose VQ speed and VQ error rate can be reliably estimated. We show that ExVQ is much faster than state-of-the-art ANN methods in BOV models while maintaining almost the same classification accuracy. In addition, we empirically show that even with the VQ error rate as high as 30%, the classification accuracy of some ANN methods, including ExVQ, is similar to that of exhaustive search (which has zero VQ error). In some cases, ExVQ has even higher classification accuracy than the exhaustive search.",
    "actual_venue": "Lecture Notes In Computer Science (Including Subseries Lecture Notes In Artificial Intelligence And Lecture Notes In Bioinformatics"
  },
  {
    "abstract": "In this paper we consider a functional language with recursively defined types and a weak form of polymorphism. For this language a strictness analysis is developed, based on abstract interpretation in a category of complete algebraic lattices.",
    "actual_venue": "Information And Computation"
  },
  {
    "abstract": "The paper presents a novel slicing based method for computation of volume fractions in multi-material solids given as a B-rep whose faces are triangulated and shared by either one or two materials. Such objects occur naturally in geoscience applications and the said computation is necessary for property estimation problems and iterative forward modeling. Each facet in the model is cut by the planes delineating the given grid structure or grid cells. The method, instead of classifying the points or cells with respect to the solid, exploits the convexity of triangles and the simple axis-oriented disposition of the cutting surfaces to construct a novel intermediate space enumeration representation called slice-representation, from which both the cell containment test and the volume-fraction computation are done easily. Cartesian and cylindrical grids with uniform and non-uniform spacings have been dealt with in this paper. After slicing, each triangle contributes polygonal facets, with potential elliptical edges, to the grid cells through which it passes. The volume fractions of different materials in a grid cell that is in interaction with the material interfaces are obtained by accumulating the volume contributions computed from each facet in the grid cell. The method is fast, accurate, robust and memory efficient. Examples illustrating the method and performance are included in the paper.",
    "actual_venue": "Computers And Geosciences"
  },
  {
    "abstract": "In this paper, we create a framework for trainingbased channel estimation under different channel and interference statistics. The minimum mean square error (MMSE) estimator for channel matrix estimation in Rician fading multi-antenna systems is analyzed, and especially the design of mean square error (MSE) minimizing training sequences. By considering Kronecker-structured systems with a combination of noise and interference and arbitrary training sequence length, we collect and generalize several previous results in the framework. We clarify the conditions for achieving the optimal training sequence structure and show when the spatial training power allocation can be solved explicitly. We also prove that spatial correlation improves the estimation performance and establish how it determines the optimal training sequence length. The analytic results for Kronecker-structured systems are used to derive a heuristic training sequence under general unstructured statistics. The MMSE estimator of the squared Frobenius norm of the channel matrix is also derived and shown to provide far better gain estimates than other approaches. It is shown under which conditions training sequences that minimize the non-convex MSE can be derived explicitly or with low complexity. Numerical examples are used to evaluate the performance of the two estimators for different training sequences and system statistics. We also illustrate how the optimal length of the training sequence often can be shorter than the number of transmit antennas.",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "Natural multiped gaits are believed to evolve from countless generations of natural selection. However, do they also prove to be better choices for walking machines? This paper compares two surefooted gaits, one natural and the other artificial, for six-legged animals or robots. In these gaits four legs are used to support the body, enabling greater stability and tolerance for faults. A standardized hexapod model was carefully examined as it moved in arbitrary directions. The study also introduced a new factor in addition to the traditional stability margin criterion to evaluate the equilibrium of such gaits. Contrary to the common belief that natural gaits would always provide better stability during locomotion, these results show that the artificial gait is superior to the natural gait when moving transversely in precarious conditions.",
    "actual_venue": "Ijimr"
  },
  {
    "abstract": "Native-sounding vs. intelligible. This has been a controver- sial issue for a long time in language learning and many teach- ers claim that the intelligible pronunciation should be the goal. What is the physical definition of the intelligibility? The cur- rent work shows a very good candidate answer to this ques- tion. The author proposed a new paradigm of observing speech acoustics based upon structural phonology, where all the kinds of speech events are viewed as an entire structure and this struc- ture was shown to be mathematically invariant with any static non-linguistic features such as age, gender, size, shape, micro- phone, room, line, and so on. This acoustic structure is purely linguistic and the phoneme-level structure is regarded as the pronunciation structure of individual students. This structure is matched with another linguistic structure, the lexical structure of the target language, and degree of compatibility between the two different levels of structures is calculated, which is defined as the intelligibility in this work. To increase the intelligibility, different instructions should be prepared for different students because no two students are the same. The proposed method can show the order of phonemes to learn, which is appropriate to a student and different from that of the others.",
    "actual_venue": "Interspeech"
  },
  {
    "abstract": "Deep neural networks are a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that are ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations in a deep neural network architecture can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet using less than half the number of neural connections as a conventional deep neural network achieves comparable accuracy and reduces over fitting on the CIFAR-10, MNIST, and SVHN data sets. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of similar to 6% compared to ConvNet) on the STL-10 data set than a conventional deep neural network. Finally, the StochasticNets have faster operational speeds while achieving better or similar accuracy performances.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "Video segmentation has drawn increasing interest in multimedia applications. This paper proposes a novel joint space-time-range domain adaptive mean shift filter for video segmentation. In the proposed method, segmentation of moving/static objects/background is obtained through inter-frame mode-matching in consecutive frames and motion vector mode estimation. Newly appearing objects/regions in the current frame due to new foreground objects or uncovered background regions are segmented by intra-frame mode estimation. Simulations have been conducted to several image sequences, and results have shown the effectiveness and robustness of the proposed method. Further study is continued to evaluate the results.",
    "actual_venue": "PCM"
  },
  {
    "abstract": "•Introduction of fusion functions extending the notion of aggregation functions.•Introduction of directional monotonicity of fusion functions.•Study of directional monotonicity of composite and transformed fusion functions.•Study of the set D↑(F) of all directions making a fusion function F increasing.•A complete description of D↑(F) for continuous piecewise linear fusion functions.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "LTE Network Function Virtualization (LTE-NFV) scales user services in a low cost fashion by transforming the centralized legacy LTE Core architecture to a distributed architecture. This distributed architecture makes multiple instances of LTE Network Functions (NFs) and virtualizes them on commodity data-center network. The functionality of LTE-NFV architecture breaks however, since the distributed NF instances connected via unreliable IP links delay the execution of critical events. The failure of time-critical events results in users' quality of service degradation and temporary service unavailability. In this paper, we propose a new way to virtualize LTE core network. We argue that logic-based NFs segregation should be done for NFV, instead of instance-based NFs segregation done in current NFV implementation. Our approach of `logic-based NFs segregation' combines the logic of an event into a single NF, thus localizing the execution of critical events to one virtual machine. This way, only the localized entities exchange signalling messages, and the events do not experience large delays. We further reduce the delays by exploiting the parallelism in LTE network protocols; and partition these protocols such that their signalling messages run in parallel. In addition, we eliminate unnecessary messages to reduce the signalling overhead. We build our system prototype over OpenEPC LTE core network in virtualized platform. Our results show that we can reduce event execution time and signalling overhead up to 50% and 40%, respectively.",
    "actual_venue": "Ieee International Conference On Network Protocols"
  },
  {
    "abstract": "Effectively managing the data generated by community-driven mobile geo-sensor networks is a new and challenging problem. One important step for managing and querying sensor network data is to create abstractions of the data in the form of models. These models can then be stored, retrieved, and queried, as required. There has been significant amount of prior literature on using models for query processing [1]-[5]. On the contrary, however, there has been a lack of understanding on developing reliable models, considering the unique characteristics of community-driven geo-sensor networks. In an effort to correct this situation, this paper proposes various approaches for modeling the data from a community-driven mobile geo-sensor network. This data is typically collected over a large geographical area with mobile sensors having uncontrolled or semi-controlled mobility. Therefore, we propose adaptive techniques that take into account such mobility patterns and produce an accurate representation of the sensed spatiotemporal phenomenon. To substantiate our proposals, we perform extensive evaluation of our methods on two real datasets.",
    "actual_venue": "Sensor, Mesh And Ad Hoc Communications And Networks"
  },
  {
    "abstract": "An explicit formula for the dual basis functions of the Bernstein basis is derived. The dual basis functions are expressed\n as linear combinations of Bernstein polynomials.",
    "actual_venue": "Adv Comput Math"
  },
  {
    "abstract": "A DE approach based on a new measure of population diversity and a novel parameter control mechanism is proposed with the aim of introducing a good behavior of the algorithm. The ratio of the new defined population diversity of different generations is equal to that of the population variance, therefore the adaption of parameter can use some theoretical results in(19). Combining with the method in(18), we can adjust the mutation factor F and the crossover rate CR at each generation in the searching process. The performance of the proposed algorithm (DE-F&CR) is compared to the basic DE and other four DE algorithms over 25 standard numerical benchmarks provided by the IEEE Congress on Evolutionary Computation 2005 special session on real parameter optimization. The results and its statistical analysis show that the DE-F&CR generally outperforms the other algorithms in multi-modal optimization.",
    "actual_venue": "International Journal Of Computational Intelligence Systems"
  },
  {
    "abstract": "Indicator-dilution curves (IDCs) for estimation of pulmonary transit times (PTTs) can be generated non-invasively using contrast echocardiography. Currently, these IDCs are analyzed by manual inspection, which is not feasible in a clinical setting, or fit to a statistical model to derive parameters of interest. However, IDCs generated from patients are frequently subject to significant low frequency noise and recirculation artifacts that obscure the first-pass signal and render model fitting impractical or inaccurate. Thus, the objective of this work was to develop alternative computational methods to determine PTT using noisy clinical data in which the signal decay is not adequately visible. Methods: We report on a method that uses a model fit to the rise portion of the IDCs to determine the signal inflection point. Additionally, a signal truncation algorithm was developed that enables automated analysis of the IDCs. Results: We compare PTTs derived from our inflection point method to those obtained by manual inspection in 25 patients (R2 = 0.86) and to those obtained by mean transit time calculation following fitting to a local density random walk model (R2 = 0.80) in a subset of this cohort. Conclusion: Combined with a signal truncation algorithm, the inflection point method provides robust, automated determination of PTT from noisy IDCs containing re-circulation artifacts. Significance: The inflection point method addresses the need for computational analysis of IDCs obtained from contrast echocardiograms that are not amenable to first-pass model fitting.",
    "actual_venue": "Biomedical Engineering, IEEE Transactions  "
  },
  {
    "abstract": "The 0–1 quadratic knapsack problem (QKP) consists in maximizing a quadratic pseudo-Boolean function with positive coefficients subject to a linear capacity constraint. In this paper we present an exact method to solve this problem. This method makes use of the computation of an upper bound for (QKP) which is derived from Lagrangian decomposition. It allows us to find the optimum of instances with up to 150 variables whatever their density, and with up to 300 variables for medium and low density.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Differential input-ouput buffers are frequently utilized in high-speed communication systems. Often, a transmit buffer is connected to a 100-Ω differential receive buffer through a passive interconnect link with no means of verifying the true differential signal quality of the transmitter. Additionally, many electrical protocol specifications are defined assuming differential transmitter terminati...",
    "actual_venue": "Ieee Transactions On Instrumentation And Measurement"
  },
  {
    "abstract": "We present a new deterministic algorithm for the Steiner tree problem in weighted graphs. Its running time is O(nk2^k^+^(^l^o^g^\"^2^k^)^(^l^o^g^\"^2^n^)), where n is the number of vertices and k is the number of terminals. This is faster than all previously known algorithms if logn(loglogn)^3",
    "actual_venue": "Inf Process Lett"
  },
  {
    "abstract": "While the introduction of object-oriented programming slowly moves down the age groups - starting from advanced university courses, to introductory courses, and now into high schools - many attempts are being made to make object-oriented programming introduction less abstract and theoretical. Visualisation and interaction techniques are being applied in an attempt to give students engaging and concrete experiences with objects. Recently, the greenfoot environment has been proposed as another step in this development. In this paper, we describe new functionality in the greenfoot environment, especially the addition of user interaction programming via direct state manipulation. Direct state manipulation provides very low overhead graphical I/O handling at a level that makes it feasible to guide students to simple graphical game programming within a few weeks, while concentrating on fundamental object-oriented concepts in the structure of the program.",
    "actual_venue": "Acm Sigice Bulletin"
  },
  {
    "abstract": "Cryptographic certificates are a powerful tool for security concerned applications where the participants must be authenticated in order to access some resources or commit a transaction. However, due to various reasons, the validity of such certificates can change over time, introducing the risk of an invalid certificate being used to authenticate an entity. Various methods of mitigating this risk have been devised, known broadly as \"certificate revocation\" schemes. In this paper, we categorize and analyze them based on our identified characteristics. We further discuss tradeoffs among them and suggest how system designers might apply the analyses.",
    "actual_venue": "Computer Communication Review"
  },
  {
    "abstract": "In this paper, the synchronization problem of complex networks is investigated by pinning control method. A kind of novel hybrid topologies is presented, which includes directed network and undirected network. Under the hybrid topologies, the pinning controllers on the chosen nodes of undirected network are designed to reach synchronization behavior with convex combination of the nodes in direct network. The synchronization criterion is established for reaching pinning control on networks. The results can also extend to networks of networks. The pinning scheme is designed by Cholesky Decomposition and triangularization, which shows that the nodes with low degrees and high degrees should be controlled.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Background: Analyzing next-generation sequencing data is difficult because datasets are large, second generation sequencing platforms have high error rates, and because each position in the target genome (exome, transcriptome, etc.) is sequenced multiple times. Given these challenges, numerous bioinformatic algorithms have been developed to analyze these data. These algorithms aim to find an appropriate balance between data loss, errors, analysis time, and memory footprint. Typical analysis pipelines require multiple steps. If one or more of these steps is unnecessary, it would significantly decrease compute time and data manipulation to remove the step. One step in many pipelines is PCR duplicate removal, where PCR duplicates arise from multiple PCR products from the same template molecule binding on the flowcell. These are often removed because there is concern they can lead to false positive variant calls. Picard (MarkDuplicates) and SAMTools (rmdup) are the two main softwares used for PCR duplicate removal. Results: Approximately 92 % of the 17+ million variants called were called whether we removed duplicates with Picard or SAMTools, or left the PCR duplicates in the dataset. There were no significant differences between the unique variant sets when comparing the transition/transversion ratios (p = 1.0), percentage of novel variants (p = 0.99), average population frequencies (p = 0.99), and the percentage of protein-changing variants (p = 1.0). Results were similar for variants in the American College of Medical Genetics genes. Genotype concordance between NGS and SNP chips was above 99 % for all genotype groups (e.g., homozygous reference). Conclusions: Our results suggest that PCR duplicate removal has minimal effect on the accuracy of subsequent variant calls.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "The field of Artificial Intelligence in Education (AIED) has undergone significant developments over the last twenty-five years. As we reflect on our past and shape our future, we ask two main questions: What are our major strengths? And, what new opportunities lay on the horizon? We analyse 47 papers from three years in the history of the Journal of AIED (1994, 2004, and 2014) to identify the foci and typical scenarios that occupy the field of AIED. We use those results to suggest two parallel strands of research that need to take place in order to impact education in the next 25 years: One is an evolutionary process, focusing on current classroom practices, collaborating with teachers, and diversifying technologies and domains. The other is a revolutionary process where we argue for embedding our technologies within students’ everyday lives, supporting their cultures, practices, goals, and communities.",
    "actual_venue": "J Artificial Intelligence In Education"
  },
  {
    "abstract": "Increasingly more internet-applications require the supports of multiple multicasts, while existing multicast routing algorithms only aim to establish a single multicast routing tree (MRT). Sequentially optimizing the co-existing MRTs each is a common practice of handling the situation of multiple multicast session being concurrent, but it results in link-congestion on low-cost routes, or leads to non-optimal solution even if bandwidth reservation strategy is adopted. This paper proposes to optimize multiple co-existing MRTs as a whole via one-off optimization instead of sequentially optimizing each in isolation. To carry out the one-off optimization, a discrete artificial fish school algorithm (DAFSA) is proposed. The simulation results show that the proposed DAFSA is capable of optimally packing co-existing MRTs and exhibits remarkably better ability than several the most representative state-of-the-art algorithms in the sense of avoiding the link-congestion and minimizing the overall tree cost. The running time of the proposed DAFSA fully meets the requirements of the practical IP multicasting. Besides, Monte Carlo test proves that the convergence of the proposed DAFSA is not sensitive to its parameters.",
    "actual_venue": "Knowledge Based Systems"
  },
  {
    "abstract": "This paper presents a symbolic formalism for modeling and retrieving video data via the moving objects contained in the video images. The model integrates the representations of individual moving objects in a scene with the time-varying relationships between them by incorporating both the notions of object tracks and temporal sequences of PIRs (projection interval relationships). The model is supported by a set of operations which form the basis of a moving object algebra. This algebra allows one to retrieve scenes and information from scenes by specifying both spatial and temporal properties of the objects involved. It also provides operations to create new scenes from existing ones. A prototype implementation is described which allows queries to be specified either via an animation sketch or using the moving object algebra.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Gait recognition is an important biometric technique relevant to video surveillance, where the task is to identify people at a distance by their walking patterns captured in the video. Most of the current approaches for gait recognition either use a pair of gait images to form a cross-gait representation or rely on a single gait image for unique-gait representation. These two types of representations empirically complement one another. In this paper, we propose a new Joint Unique-gait and Cross-gait Network (JUCNet) representation, to combine the advantages of both schemes, leading to significantly improved performance. A second contribution of this work is a tailored quintuplet loss function, which simultaneously boosts inter-class differences by pushing different subjects further apart and contracts intra-class variations by pulling same subjects closer. Extensive tests demonstrate that our method achieves the best performance tested on multiple standard benchmarks, compared with other state-of-the-art methods.",
    "actual_venue": "Ieee/Cvf Conference On Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "Regular exercise is one of the most important factors in maintaining a good state of health. In the past, different systems have been proposed to assist people when exercising. While most of those systems focus only on cardio exercises such as running and cycling, we exploit smartphones to support leisure activities with a focus on resistance training. We describe how off-the-shelf smartphones without additional external sensors can be leveraged to capture resistance training data and to give reliable training feedback. We introduce a dynamic time warping-based algorithm to detect individual resistance training repetitions from the smartphone's acceleration stream. We evaluate the algorithm in terms of the number of correctly recognized repetitions. Additionally, for providing feedback about the quality of repetitions, we use the duration of an individual repetition and analyze how accurately start and end times of repetitions can be detected by our algorithm. Our evaluations are based on 3,598 repetitions performed by ten volunteers exercising in two distinct scenarios, a gym and a natural environment. The results show an overall repetition miscount rate of about 1 % and overall temporal detection error of about 11 % of individual repetition duration.",
    "actual_venue": "Personal And Ubiquitous Computing"
  },
  {
    "abstract": "Traffic congestion prediction is an important precondition to promote urban sustainable development. Nevertheless, there is a lack of a unified prediction method to address the performance metrics, such as accuracy, instantaneity and stability, systematically. In the paper, we propose a novel approach to predict the urban traffic congestion efficiently with floating car trajectory data. Specially, an innovative traffic flow prediction method utilizing particle swarm optimization algorithm is responsible for calculating the traffic flow parameters. Then, a congestion state fuzzy division module is applied to convert the predicted flow parameters to citizens' cognitive congestion states. We conduct extensive experiments with real floating car data and the experimental results show that our proposed method has advantage in terms of accuracy, instantaneity and stability.",
    "actual_venue": "Ica3Pp"
  },
  {
    "abstract": "Although direct volume rendering is a powerful tool for visualizing complex structures within volume data, the size and complexity of the parameter space controlling the rendering process makes generating an informative rendering challenging. In particular, the specification of the transfer function-the mapping from data values to renderable optical properties-is frequently a time consuming and unintuitive task. Ideally, the data being visualized should itself suggest an appropriate transfer function that brings out the features of interest without obscuring them with elements of little importance. We demonstrate that this is possible for a large class of scalar volume data, namely that where the regions of interest are the boundaries between different materials. A transfer function which makes boundaries readily visible can be generated from the relationship between three quantities: the data value and its first and second directional derivatives along the gradient direction. A data structure we term the histogram volume captures the relationship between these quantities throughout the volume in a position independent, computationally efficient fashion. We describe the theoretical importance of the quantities measured by the histogram volume, the implementation issues in its calculation, and a method for semiautomatic transfer function generation through its analysis. We conclude with results of the method on both idealized synthetic data as well as real world datasets.",
    "actual_venue": "Research Triangle Park, Nc, Usa"
  },
  {
    "abstract": "Many recent studies show that data center networks usually mix with a large amount of latency-agnostic background flows and a large number of latency-sensitive flows. The former, although contributing most of the traffic, is deadline-agnostic, while the latter always confronts a soft-real-time constraint, namely deadline. Missing this deadline may cause performance impairment. For this reason, directly using the traditional TCP in data center networks, which is deadline-agnostic, may suffer from performance and efficiency problems. Recently, many works improve on TCP to address this problem. But all of them focus on the latency-sensitive flows themselves. Because latency-sensitive flows, although large in number, only account for a small part of traffic, these methods cannot effectively ensure deadline for the latency-sensitive flows. In this paper, we propose Make-way, a new data center network transport protocol for satisfying the deadlines of latency-sensitive flows. In Make-way, just as what the name depicts, once a latency-sensitive flow encounters congestion, the latency-agnostic background flows will make way for it. Especially, Make-way does not need any special hardware support of modification. For the latency-agnostic flows in data center networks usually contribute the majority traffic, by doing so, the latency-sensitive flows may be transported nonblockingly in data center networks and thus can meet their deadlines. Extensive simulation results show that Make-way can meet the deadlines of latency-sensitive flows with a probability of more than 97%. Copyright (C) 2015 John Wiley & Sons, Ltd.",
    "actual_venue": "Concurrency And Computation: Practice And Experience"
  },
  {
    "abstract": "The issues of multilingual access to legal information are examined, and strategies of cross-language retrieval to legal information resources are illustrated as additional services of the Portal to legal literature created by ITTIG. Consideration is given to the peculiarities of legal language as a technical language closely related to the diverse legal systems. The paper describes a methodological approach planned for the Portal to provide a single point of access into disparate repositories where categories of law (i.e. trade law, constitutional law, criminal law) are the essential metadata to point to relevant material irrespective of the language used in a query. Categories of law of a specific legal system represent the way how retrieval can be satisfactorily achieved. Strategies and techniques for translating legal queries to different target languages, eventually disambiguating ambiguous words by a machine learning approach are illustrated.",
    "actual_venue": "Dublin Core Conference"
  },
  {
    "abstract": "The multiple traveling salesperson problem (MTSP) involves scheduling m>1 salespersons to visit a set of n>m locations so that each location is visited exactly once while minimizing the total (or maximum) distance traveled by the salespersons. The MTSP is similar to the notoriously difficult traveling salesperson problem (TSP) with the added complication that each location may be visited by any one of the salespersons. Previous studies investigated solving the MTSP with genetic algorithms (GAs) using standard TSP chromosomes and operators. This paper proposes a new GA chromosome and related operators for the MTSP and compares the theoretical properties and computational performance of the proposed technique to previous work. Computational testing shows the new approach results in a smaller search space and, in many cases, produces better solutions than previous techniques.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Estimation of aircraft aerodynamic parameters from flight or wind-tunnel data has been an important modeling activity for aerodynamicists over several decades. In this work, we propose to apply the genetic algorithms (GAs) to estimate the longitudinal aerodynamic parameters of an airplane. We further show that the efficiency of classical GAs can be greatly improved by combining it with the simulated annealing (SA) technique. Simulation results illustrate the effectiveness and efficiency of the GA based algorithms in aerodynamic parameter estimation.",
    "actual_venue": "Ieee Congress On Evolutionary Computation"
  },
  {
    "abstract": "The increasingly connected world has catalyzed the fusion of networks from different domains, which facilitates the emergence of a new network model—multi-layered networks. Examples of such kind of network systems include critical infrastructure networks, biological systems, organization-level collaborations, cross-platform e-commerce, and so forth. One crucial structure that distances multi-layered network from other network models is its cross-layer dependency, which describes the associations between the nodes from different layers. Needless to say, the cross-layer dependency in the network plays an essential role in many data mining applications like system robustness analysis and complex network control. However, it remains a daunting task to know the exact dependency relationships due to noise, limited accessibility, and so forth. In this article, we tackle the cross-layer dependency inference problem by modeling it as a collective collaborative filtering problem. Based on this idea, we propose an effective algorithm F<scp;>ascinate</scp;> that can reveal unobserved dependencies with linear complexity. Moreover, we derive F<scp;>ascinate</scp;>-ZERO, an online variant of F<scp;>ascinate</scp;> that can respond to a newly added node timely by checking its neighborhood dependencies. We perform extensive evaluations on real datasets to substantiate the superiority of our proposed approaches.",
    "actual_venue": "Tkdd"
  },
  {
    "abstract": "Replacement rules have played an important role in the study of monotone boolean function complexity. In this paper, notions of replaceability and computational equivalence are formulated in an abstract algebraic setting, and examined in detail for finite distributive lattices — the appropriate algebraic context for monotone boolean functions. It is shown that when computing an element f of a finite distributive lattice D, the elements of D partition into classes of computationally equivalent elements, and define a quotient of D in which all intervals of the form [tf, tf] are boolean. This quotient is an abstract simplicial complex with respect to ordering by replaceability. Possible applications of computational equivalence in developing upper and lower bounds on monotone boolean function complexity are indicated, and new directions of research, both abstract mathematical and computational, are suggested.",
    "actual_venue": "Fsttcs"
  },
  {
    "abstract": "The need for annotating the continuously increasing volume of medical image data is recognized from medical experts for a variety of purposes, regardless if this is medical practice, research or education. The rich information content latent in medical images can be made explicit and formal with the use of well-defined ontologies. Evolution of the Semantic Web now offers a unique opportunity of a web-based, service-oriented approach. Remote access to FMA and ICD-10 reference ontologies provides the ontological annotation framework. The proposed system utilizes this infrastructure to provide a customizable and robust annotation procedure. It also provides an intelligent search mechanism indicating the advantages of semantic over keyword search. The common representation layer discussed facilitates interoperability between institutions and systems, while semantic content enables inference and knowledge integration.",
    "actual_venue": "Embc"
  },
  {
    "abstract": "Seizure detection and classification using signal processing methods has been an important issue of research for the last two decades. In the present study, a novel scheme was presented to detect epileptic seizure activity with very fast and highest accuracy from background electro encephalogram (EEG) data recorded from epileptic and normal subjects. The proposed scheme is based on discrete wavelet packet transform (DWT) with energy, entropy, standard deviation, mean, kurtosis, skewness and entropy estimation at each node of the decomposition tree followed by application of probabilistic neural network (PNN). Normal as well as epileptic EEG epochs were decomposed into approximation and details coefficients till sixth-level using DWT packet. Discrete harmony search with modified differential operator was used to select the optimal features out of all above mentioned statistical and non-statistical parameters. In order to demonstrate the efficacy of the proposed algorithm for classification purpose using PNN, we have implemented 10-fold cross validation. Clinical EEG data recorded from normal as well as epileptic subjects are used to test the performance of this new scheme. It is found that the detection rate is 100% accurate with same level of sensitivity and specificity.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "We improve Knudsen-Preneel’s constructions for cryptographic hash functions based on block ciphers with error correcting codes.\n We first modify to extend original constructions, which are effective only for non-binary codes, to the case with binary codes\n (e.g. BCH codes). We also revise the original method by introducing convolutional codes, whereas the previous adapts only\n block codes. This reduces the circuit complexity of the hardware-implementation 1/N times in terms of the number of (Davies-Meyer’s) module functions than that based block error correcting codes.",
    "actual_venue": "Public Key Cryptography"
  },
  {
    "abstract": "Although many NP-hard graph optimization problems can be solved in polynomial time on graphs of bounded tree-width, the adoption of these techniques into mainstream scientific computation has been limited due to the high memory requirements of the dynamic programming tables and excessive runtimes of sequential implementations. This work addresses both challenges by proposing a set of new parallel algorithms for all steps of a tree decomposition-based approach to solve the maximum weighted independent set problem. A hybrid OpenMP/MPI implementation includes a highly scalable parallel dynamic programming algorithm leveraging the MADNESS task based runtime, and computational results demonstrate scaling. This work enables a significant expansion of the scale of graphs on which exact solutions to maximum weighted independent set can be obtained, and forms a framework for solving additional graph optimization problems with similar techniques.",
    "actual_venue": "Parallel And Distributed Processing Symposium Workshops And Phd Forum"
  },
  {
    "abstract": "We describe an elastic matching procedure between plane curves based on computing a minimal deformation cost between the curves. The design of the deformation cost is based on a geodesic distance defined on an infinite dimensional group acting on the curves. The geodesic paths also provide an optimal deformation process, which allows interpolation between any plane curves.",
    "actual_venue": "Image And Vision Computing"
  },
  {
    "abstract": "According to the language of thought (LOT) approach and the related computational theory of mind (CTM), thinking is the processing of symbols in an inner mental language that is distinct from any public language. Herein, I explore a deep problem at the heart of the LOT/CTM program—it has yet to provide a plausible conception of a mental symbol.",
    "actual_venue": "Synthese"
  },
  {
    "abstract": "This paper presents a method to estimate the position of object using contextual information. Although convention methods used only shape contextual information, color contextual information is also effective to describe scenes. Thus we use both shape and color contextual information. To estimate the object position from only contextual information, the Support Vector Regression is used. We choose the Pyramid Match Kernel which measures the similarity between histograms because our contextual information is described as histogram. When one kernel is applied to a feature vector which consists of color and shape, the similarity of each feature is not used effectively. Thus, kernels are applied to color and shape independently, and the weighted sum of the outputs of both kernels is used. We confirm that the proposed method outperforms conventional methods.",
    "actual_venue": "Iciap"
  },
  {
    "abstract": "We report here on an extensive redesign and unification of the Introductory Computer Programming sequences offered to computer science, computer engineering, information science and digital media majors. The redesign is intended to improve student learning while reducing costs. The approach makes use of substantial Web-based course material and course management tools, including multi-level online modules that individualize instruction and enable students to self-schedule learning each week. Each module covers a particular aspect of computer programming at different levels of knowledge. Students are assigned work and reading from the module at a level appropriate to the objectives of the long-term goals of their major. This allows students in different majors to acquire the appropriate skill level for each technique and concept. Peer mentors and teaching assistants provide assistance online or in person. In the future, we plan to expand the self-scheduling aspect of the course to allow students to enter the course at different modules, depending on their previous knowledge.",
    "actual_venue": "Sigcse Proceedings Of The Eighteenth Sigcse Technical Symposium On Computer Science Education"
  },
  {
    "abstract": "This research investigates the privacy issues that exist on social networking sites. It is reasonable to assume that many Twitter users are unaware of the dangers of uploading a tweet to their timeline which can be seen by anyone. Enabling geo-location tagging on tweets can result in personal information leakage, which the user did not intend to be public and which can seriously affect that user's privacy and anonymity online. This research demonstrates that key information can easily be retrieved using the starting point of a single tweet with geo-location turned on. A series of experiments have been undertaken to determine how much information can be obtained about a particular individual using only social networking sites and freely available mining tools. The information gathered enabled the target subjects to be identified on other social networking sites such as Foursquare, Instagram, LinkedIn, Facebook and Google+, where more personal information was leaked. The tools used are discussed, the results of the experiments are presented and the privacy implications are examined.",
    "actual_venue": "Future Internet"
  },
  {
    "abstract": "Recently, the interests of solving large-scale optimization problems have increased in the field of evolutionary algorithms. This paper presents a novel differential evolution, namely EOBDE, to solve these kinds of problems by using elite opposition-based learning strategy. In the proposed algorithm, the opposite solutions of some selected elite individuals from the current population are generated at a certain probability for generation jumping. Then a corresponding opposite population is constructed to compete with the current population for providing more chances of finding out the global optimum. This approach is helpful to obtain a tradeoff between exploration and exploitation ability of DE. As another contribution, a parallel version of the proposed algorithm is implemented on Graphics Processing Units (GPU) based on CUDA platform for accelerating computing speed. The experiments are carried out on a set of representative problems with D=500 and 1000. The results of EOBDE are compared with other four state-of-the-art evolutionary algorithms in order to investigate the performance, which show that our proposed algorithm outperform the compared algorithms in terms of solution accuracy. Also the parallel version based on GPU shows promising performance in terms of the computational time.",
    "actual_venue": "Pdcat"
  },
  {
    "abstract": "$epsilon$  -Minimum Storage Regenerating ( $epsilon$  -MSR) codes form a special class of Maximum Distance Separable (MDS) codes, providing mechanisms for exact regeneration of a single code block in their codewords by downloading slightly suboptimal amount of information from the remaining code blocks. The key advantage of these codes is a significantly lower sub-packetization that grows only logarithmically with the length of the code, while providing optimality in storage and error-correcting capacity. However, from an implementation point of view, these codes require each remaining code block to be available for the repair of any single code block. In this paper, we address this issue by constructing  $epsilon$  -MSR codes that can repair a failed code block by contacting a fewer number of available code blocks. When a code block fails, our repair procedure needs to contact a few compulsory code blocks and is free to choose any subset of a fixed size for the remaining choices (from the available code blocks). Further, our construction requires a field size linear in code length and ensures load balancing among the contacted code blocks in terms of information downloaded from them.",
    "actual_venue": "Isit"
  },
  {
    "abstract": "Traffic spikes in big data cloud have led to great challenges for middlebox management in the behind datacenter networks, especially for load distribution among varieties of middleboxes. Contemporary flat load distributing strategies are inadequate to handle traffic spikes in terms of time and resource efficiency. Based on the spike patterns, a hybrid load distributing solution is proposed with hierarchical architecture, which conducts static distributing and dynamic collaborative distributing in respect of flow types. The evaluation results from the prototype system and simulation show that our solution is superior to existing load distributing strategies in terms of both time and resource efficiency.",
    "actual_venue": "Icnc"
  },
  {
    "abstract": "In this submission, I discuss my design research and fieldwork investigating mediated crafts' - digital practices around creative handwork. Specifically, I study how creating and sharing digital information around knitting or crochet activity affects the social and material relationships enacted through craft. Here I review my qualitative research on the role of digital resources in creative handwork and the iterative design of Spyn, mobile phone software that associates digital records of the creative process' - captured through audio/visual media, text, and geographic data' - with physical locations on handmade fabric.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "This paper reports on a study involving the design of a virtual community for informal learning about Thai herbs. The community relied on social networking tools and a database of expert knowledge as well as community coordinators. One group of coordinators (Community A) concentrated efforts in recruitment of members on those individuals most likely to be interested in Thai herbs. The second group of coordinators (Community B) recruited from among friends, family and acquaintances. Analysis and t-tests of measurement of membership, participation and knowledge building revealed higher rates for Community A. Results pertaining to the design of the virtual community, which showed that members mostly used the database, provide evidence to support the hypothesis that members' access to expert knowledge positively influences participation and knowledge building in a virtual community for informal learning. Results of the comparison of Community A versus Community B provide evidence to support the hypothesis that interest in the subject of the community positively influences membership, participation and knowledge building in a virtual community for informal learning.",
    "actual_venue": "British Journal Of Educational Technology"
  },
  {
    "abstract": "The ACORN architecture is a multi-agent based information retrieval and provision system which uses the concept of 'document as agent' coupled with a community-based approach to information sharing. In the system, with the minimum of effort, users can expect to be provided with timely information on subjects in their areas of interest, whilst being able to search for information as they wish. The system, then, can be proactive in providing information, and both proactive and reactive in searching for information. This paper describes the ACORN architecture and discusses its current implementation.",
    "actual_venue": "Bcs-Irsg Annual Colloquium On Ir Research"
  },
  {
    "abstract": "Techniques for generating data structures for isotropic vector matrix grids (or face-centered cubic lattices) are presented. Grid basics and some background mathematical foundations are also provided.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": ". The performance and hardware complexity of super-scalararchitectures is hindered by conditional branch instructions. When conditionalbranches are encountered in a program, the instruction fetchunit must rapidly predict the branch predicate and begin speculativelyfetching instructions with no loss of instruction throughput. Speculativeexecution has a high hardware cost, is limited by dynamic branch predictionaccuracies, and does not scale well for increasingly super-scalar...",
    "actual_venue": "Lcpc"
  },
  {
    "abstract": "While research on collaborative tagging systems has largely been the purview of computer scientists, the behavior of these systems is driven by the psychology of their users. Here we explore how simple models of boundedly rational human decision making may partly account for the high-level properties of a collaborative tagging environment, in particular with respect to the distribution of tags used across the folksonomy. We discuss several plausible heuristics people might employ to decide on tags to use for a given item, and then describe methods for testing evidence of such strategies in real collaborative tagging data. Using a large dataset of annotations collected from users of the social music website Last.fm with a novel crawling methodology (approximately one millions total users), we extract the parameters for our decision-making models from the data. We then describe a set of simple multi-agent simulations that test our heuristic models, and compare their results to the extracted parameters from the tagging dataset. Results indicate that simple social copying mechanisms can generate surprisingly good fits to the empirical data, with implications for the design and study of tagging systems.",
    "actual_venue": "Websci"
  },
  {
    "abstract": "In this paper we will explore a number of approaches to developing digital ecosystems research into addressing some of the key research questions that are emerging in the Cloud Computing domain. The approach of this paper is to build an energy consumption model over the cloud computing. The model is based on mathematical methods for optimizing energy consumption under constraints. We will solve the optimization problem with AMPL/CPLEX [2] tool. The aim of this model is to provide an analytical tool that can be used to explore different aspects of the network in terms of energy-reduction.",
    "actual_venue": "Dest"
  },
  {
    "abstract": "The traditional artificial neural network (ANN) inversion of electrical resistivity imaging (ERI) based on gradient descent algorithm is known to be inept for its low computation efficiency and does not ensure global convergence. In order to solve above problems, a kernel principal component wavelet neural network (KPCWNN) trained by an improved shuffled frog leaping algorithm (ISFLA) method is proposed in this study. An additional kernel principal component (KPC) layer is applied to reduce the dimensionality of apparent resistivity data and increase the computational efficiency of wavelet neural network (WNN). Meanwhile, a novel ISFLA algorithm is adopted for improving the learning ability and inversion quality of WNN. In the proposed ISFLA, a hybrid LC mutation attractor is used to enhance the exploitation ability and a differential updating rule is used to enhance the exploration ability. Four groups of experiments are considered to demonstrate the feasibility of the proposed inversion method. The inversion results of the synthetic and field examples show that the introduced method is superior to other algorithms in terms of prediction accuracy and computational efficiency, which contribute to better inversion results.",
    "actual_venue": "Neural Networks"
  },
  {
    "abstract": "Controller Area Network (CAN) is a bus communication protocol which defines a standard for reliable and efficient transmission between in-vehicle nodes in real-time. Since CAN message is broadcast from a transmitter to the other nodes on a bus, it does not contain information about the source and destination address for validation. Therefore, an attacker can easily inject any message to lead system malfunctions. In this paper, we propose an intrusion detection method based on the analysis of the offset ratio and time interval between request and response messages in CAN. If a remote frame having a particular identifier is transmitted, a receiver node should respond to the remote frame immediately. In attack-free state, each node has a fixed response offset ratio and time interval while these values vary in attack state. Using this property, we can measure the response performance of the existing nodes based on the offset ratio and time interval between request and response messages. As a result, our methodology can detect intrusions by monitoring offset ratio and time interval, and it allows quick intrusion detection with high accuracy.",
    "actual_venue": "Annual Conference On Privacy, Security And Trust"
  },
  {
    "abstract": "We describe an extension to ordinary patch-based edge detection in images using spatio-temporal volumetric patches from video. The inclusion of temporal information enables us to estimate motion normal to edges in addition to edge strength and spatial orientation. The method can handle complex edges in clutter by comparing distributions of data on either half of an extracted patch, rather than modeling the intensity profile of the edge. An efficient approach is provided for building the necessary histograms which samples candidate edge orientations and motions. Results are compared to classical spatio-temporal filtering techniques.",
    "actual_venue": "Cvprw Proceedings Of The Conference On Computer Vision And Pattern Recognition Workshop"
  },
  {
    "abstract": "Experience in teaching engineering related subjects has shown that a complementary approach combining theoretical and practical exercises is vital for effective learning. Increasingly, teaching institutions are offering remote access to distant laboratories as part of an overall e-learning strategy. However, the majority of remote access laboratories developed to date have suffered from a major deficiency, namely the provision of a web based environment that accurately recreates the collaborative group working and tutor driven experiences of traditional on-campus based laboratories. New collaborative remote experimentation environments and architectures are required to enable students in disparate locations to simultaneously and collaboratively complete complex experimental exercises. This paper presents several clientserver paradigms that facilitate single user remote access, collaborative working and lecturer led approaches to the provision of remote experimentation facilities.",
    "actual_venue": "International Journal Of Online Engineering"
  },
  {
    "abstract": "A version of weighted coloring of a graph is introduced: each node v of a graph G = (V, E) is provided with a positive integer weight w(v) and the weight of a stable set S of G is w(S) = max{w(v) : v 驴 V 驴 S}. A k-coloring S = (S1, . . . , Sk) of G is a partition of V into k stable sets S1, . . . , Sk and the weight of S is w(S1) + . . . + w(Sk). The objective then is to find a coloring S = (S1, . . . , Sk) of G such that w(S1) + . . . + w(Sk) is minimized. Weighted node coloring is NP-hard for general graphs (as generalization of the node coloring problem). We prove here that the associated decision problems are NP-complete for bipartite graphs, for line-graphs of bipartite graphs and for split graphs. We present approximation results for general graphs. For the other families of graphs dealt, properties of optimal solutions are discussed and complexity and approximability results are presented.",
    "actual_venue": "WG"
  },
  {
    "abstract": "This paper examines the detection problem of the preamble sequence index in the orthogonal frequency division multiplexing (OFDM) system. The receiver knows all the possible preamble sequences and should estimate which preamble sequence has been transmitted. Preamble sequence detection is straightforward with the knowledge of channel conditions. However, since the preamble OFDM symbol is usually the first symbol that is received, the channel is unknown to the receiver, which makes this problem complicated. In this paper, the joint maximum likelihood (ML) estimator of the preamble sequence and the channel is derived and compared with a simple decoupled estimator. The estimators are applied to IEEE 802.16e systems, and it is shown by simulation that the joint ML estimator detects the preamble sequence index very well in the absence of the channel knowledge.",
    "actual_venue": "Globecom"
  },
  {
    "abstract": "The causation that why the capacity of information hiding algorithm based on DCT domain is small is analyzed, a large capacity blind information hiding algorithm based on DCT domain is proposed. First, uniform spectrum processing act on the carrier image,then the image is processed globe DCT transform. Finally, the information, which is to be hidden, is embedded into the low frequency coefficients in DCT domain. Moreover, the information hiding is implemented by using segment quantification method, extracting the hidden information does not need the original image. Experiment shows that our scheme has a large Capacity of information hiding and better imperceptibility.",
    "actual_venue": "Isecs"
  },
  {
    "abstract": "We present a new approach to the handling and interrogating of large flow cytometry data where cell status and function can be described, at the population level, by global descriptors such as distribution mean or co-efficient of variation experimental data. Here we link the \"real\" data to initialise a computer simulation of the cell cycle that mimics the evolution of individual cells within a larger population and simulates the associated changes in fluorescence intensity of functional reporters. The model is based on stochastic formulations of cell cycle progression and cell division and uses evolutionary algorithms, allied to further experimental data sets, to optimise the system variables. At the population level, the in-silico cells provide the same statistical distributions of fluorescence as their real counterparts; in addition the model maintains information at the single cell level. The cell model is demonstrated in the analysis of cell cycle perturbation in human osteosarcoma tumour cells, using the topoisomerase II inhibitor, ICRF-193. The simulation gives a continuous temporal description of the pharmacodynamics between discrete experimental analysis points with a 24 hour interval; providing quantitative assessment of inter-mitotic time variation, drug interaction time constants and sub-population fractions within normal and polyploid cell cycles. Repeated simulations indicate a model accuracy of +/- 5%. The development of a simulated cell model, initialized and calibrated by reference to experimental data, provides an analysis tool in which biological knowledge can be obtained directly via interrogation of the in-silico cell population. It is envisaged that this approach to the study of cell biology by simulating a virtual cell population pertinent to the data available can be applied to \"generic\" cell-based outputs including experimental data from imaging platforms.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "Spectrum handoff has a negative impact on the performance of cognitive users (CUs) in terms of handoff delay in cognitive radio (CR) networks. In this study, a pre-emptive resume priority (PRP) M/G/1/K queuing network model is proposed with a finite number of allowable interruptions for proactive decision spectrum handoff scheme in order to minimise the cumulative handoff delay (CHD) and total service time (TST) for CUs. The CHD and TST for different proactive decision handoff schemes: non-switching spectrum handoff, switching spectrum handoff, and random spectrum handoff are modeled under the proposed PRP M/G/1/K queuing network model. Comprehensive results of CHD and TST are obtained to compare the performances of the proactive decision handoff schemes under the proposed PRP M/G/1/K queuing network model. This study also presents an analytical framework to examine the effect of primary users activity and buffer size on spectrum handoff delay performance with a finite number of allowable interruptions in a CR network. Thereafter, the optimal buffer size (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">K</italic>\n) is estimated for the proposed PRP M/G/1/K queuing network model, which gives performance similar to the infinite buffer size PRP M/G/1 queuing model with negligible (<1%) error.",
    "actual_venue": "Iet Communications"
  },
  {
    "abstract": "Despite the apparent success of the Java Virtual Machine, its lackluster performance makes it ill-suited for many speed-critical applications. Although the latest just-in-time compilers and dedicated Java processors try to remedy this situation, optimized code compiled directly from a C program source is still considerably faster than software transported via Java byte-codes. This is true even if the Java byte-codes are subsequently further translated into native code. In this paper, we claim that these performance penalties are not a necessary consequence of machine-independence, but related to Java's particular intermediate representation and runtime architecture. We have constructed a prototype and are further developing a software transportability scheme founded on a tree-based alternative to Java byte-codes. This tree-based intermediate representation is not only twice as compact as Java byte-codes, but also contains more high-level information, some of which is critical for advanced code optimizations. Our architecture not only provides on-the-fly code generation from this intermediate representation, but also continuous re-optimization of the existing code-base by a low-priority background process. The re-optimization process is guided by up-to-the-minute profiling data, leading to superior runtime performance.",
    "actual_venue": "International Journal Of Parallel Programming"
  },
  {
    "abstract": "This paper introduces a new method for surface reconstruction from multiple calibrated images. The primary contribution of this work is the notion of local prior to combine the flexibility of the carving approach with the accuracy of graph-cut optimization. A progressive refinement scheme is used to recover the topology and reason the visibility of the object. Within each voxel, a detailed surface patch is optimally reconstructed using a graph-cut method. The advantage of this technique is its ability to handle complex shape similarly to level sets while enjoying a higher precision. Compared to carving techniques, the addressed problem is well-posed, and the produced surface does not suffer from aliasing. In addition, our approach seamlessly handles complete and partial reconstructions: If the scene is only partially visible, the process naturally produces an open surface; otherwise, if the scene is fully visible, it creates a complete shape. These properties are demonstrated on real image sequences",
    "actual_venue": "Iccv"
  },
  {
    "abstract": "We propose a concurrent error detection (CED) scheme for a sequential circuit implemented using both embedded memory blocks and LUT-based programmable logic blocks available in todayýs FPGAs. The proposed scheme is proven to detect each permanent or transient fault associated with a single input or output of any component of the circuit that results in an incorrect state or output of the circuit. Such faults are detected with no latency. The experimental results show that despite the heterogeneous structure of the proposed CED scheme, the overhead is very reasonable. For the examined benchmark circuits, thecombined overhead, that accounts for both extra EMBs and extra logic cells, is in the range of 25.6% to 61.0%, with an average value of 38.6%.",
    "actual_venue": "DFT"
  },
  {
    "abstract": "Dynamic Probabilistic Networks (DPNs) are exploited for modeling the temporal relationships among a set of different object temporal events in the scene for a coherent and robust scene-level behaviour interpretation. In particular, we develop a Dynamically Multi-Linked Hidden Markov Model (DML-HMM) to interpret group activities involving multiple objects captured in an outdoor scene. The model is based on the discovery of salient dynamic interlinks among multiple temporal events using DPNs. Object temporal events are detected and labeled using Gaussian Mixture Models with automatic model order selection. A DML-HMM is built using Schwarz's Bayesian Information Criterion based factorisation resulting in its topology being intrinsically determined by the underlying causality and temporal order among different object events. Our experiments demonstrate that its performance on modelling group activities in a noisy outdoor scene is superior compared to that of a Multi-Observation Hidden Markov Model (MOHMM), a Parallel Hidden Markov Model (PaHMM) and a Coupled Hidden Markov Model (CHMM).",
    "actual_venue": "Iccv"
  },
  {
    "abstract": "EAST-ADL is a domain specific Architecture Description Language (ADL) for safety-critical and software-intensive embedded systems. The language allows a formalized and traceable description of a wide range of engineering concerns throughout the entire lifecycle of system development. This makes it possible to fully utilize the leverage of state-of-the-art methods and tools for the development of correct-by-construction system functions and components in a seamless and cost efficient way. This paper focuses on the recent advancement of EAST-ADL in supporting an architecture-centric analysis, verification&validation of complex behaviors for the purposes of requirements engineering, application design, and safety engineering. The approach is architecture centric because all behavior descriptions are formalized and connected to a set of standardized design artifacts sitting at multiple levels of abstractions. We present the language design to support this, the theoretical underpinning and tool implementation. To show the capability of EAST-ADL, we also introduce an algorithm and its implementation for transforming the EAST-ADL behavior models to SPIN models for logic model checking. Exploiting mature state-of-the-art technologies from computer science, electronic engineering, and other related domains for a model-based incremental system development, the contribution enables the developers of embedded systems and software to maintain various engineering concerns coherently using EAST-ADL.",
    "actual_venue": "Computing"
  },
  {
    "abstract": "Acoustic images captured by side scan sonar are normally affected by speckle noise for which the enhancement is required in different domain. The underwater acoustic images obtained using sound as a source, basically contain seafloor, sediments, living and non-living resources. The Multiresolution based image enhancement techniques nowadays play a vital role in improving the quality of the low resolution image with repeated patterns. Image pyramid is the representation of an image at various scales. In this work, a three level Gaussian and Laplacian pyramids are constructed to represent the image in different resolution. The multiscale representation requires different filters at different scales. The contrast of each image in Gaussian and Laplacian pyramids are improved by applying both histogram equalization and unsharp masking method. The sharpened images are used to reconstruct the enhanced image. The performance measure, peak signal to noise ratio proves that the unsharp masking method applied to difference images of Laplacian pyramid outperforms the other image enhancement methods.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Xen virtualization is a powerful tool for encapsulating services and providing seamless migration of tasks on hardware failure. This tutorial shows how to set up multiple Xen instances in a network using the MLN management tool.One of the main challenges in virtual machine administration on a large scale is the specification of complex and repeatable virtualized scenarios. Creating a singe new virtual machine, boot it and install an operating system is straight forward with many of todays tools. But what if you need to deploy 50 identical virtual machines across 25 servers and manage them as an atomic unit? How do you at a later point make consistent design adjustments such as migrating a subset of the virtual machines to a new server or adjusting memory levels? These issues are at the heart of this tutorial.MLN is a virtual machine management tool supporting both User-Mode Linux and Xen. It has been developed at the Oslo University College in conjunction with its research on system administration and resource management. MLN and its research has previously been presented at the Norwegian Unix User Group (NUUG) and the 20th USENIX Large Installation System Administration conference LISA. This tutorial is interesting for all who want to look beyond the typical one-vm-on-my-desktop scenario. Teachers interested in virtual student labs should also attend.We start with a short introduction to the Xen virtual machine technology and then proceed to MLN and its own configuration language. In the second part of the tutorial, we will talk about installation and configuration of MLN into a virtual infrastructure across several servers.",
    "actual_venue": "Aims"
  },
  {
    "abstract": "The benchmark for computation is typically given as Turing computability; the ability for a computation to be performed by a Turing Machine. Many languages exploit (indirect) encodings of Turing Machines to demonstrate their ability to support arbitrary computation. However, these encodings are usually by simulating the entire Turing Machine within the language, or by encoding a language that does an encoding or simulation itself. This second category is typical for process calculi that show an encoding of lambda-calculus (often with restrictions) that in turn simulates a Turing Machine. Such approaches lead to indirect encodings of Turing Machines that are complex, unclear, and only weakly equivalent after computation. This paper presents an approach to encoding Turing Machines into intensional process calculi that is faithful, reduction preserving, and structurally equivalent. The encoding is demonstrated in a simple asymmetric concurrent pattern calculus before generalised to simplify infinite terms, and to show encodings into Concurrent Pattern Calculus and Psi Calculi.",
    "actual_venue": "Electronic Proceedings In Theoretical Computer Science"
  },
  {
    "abstract": "Linear and nonlinear convection-diffusion problems are considered. The numerical solution of these problems via the Schwarz alternating method is studied A new class of parallel asynchronous iterative methods with flexible communication is applied. The implementation of parallel asynchronous and synchronous algorithms on distributed memory multiprocessors is described Experimental results obtained on an IBM SP2 by using PVM are presented and analyzed. The interest of asynchronous iterative methods with flexible communication is clearly shown.",
    "actual_venue": "Eleventh Euromicro Conference On Parallel, Distributed And Network-Based Processing, Proceedings"
  },
  {
    "abstract": "Abstract: A review of interpolation to values of a function f(x), x 2 R, byradial basis function methods is given. It addresses the nonsingularity of theinterpolation equations, the inclusion of polynomial terms, and the accuracy of theapproximation sf , where s is the interpolant. Then some numerical experimentsinvestigate the situation when the data points are on a low dimensional nonlinearmanifold in R. They suggest that the number of data points that are necessaryfor good accuracy on...",
    "actual_venue": "Hercma"
  },
  {
    "abstract": "The present study investigated the neural correlates of morphological priming in overt Dutch language production using a long-lag priming paradigm. Compound words were read out loud as primes that were morphologically related to picture names (e.g. the word jaszak, ‘coat pocket’ was used for a picture of a coat; Dutch jas), or primes were form-related, but not morphologically related monomorphemic words (e.g. jasmijn, ‘jasmine’). The morphologically related compounds could be semantically transparent (e.g. eksternest, ‘magpie nest’) or opaque (e.g. eksteroog, lit. ‘magpie eye,’ ‘corn,’ for a picture of a magpie, Dutch ekster). These four priming conditions were complemented by two matched, unrelated conditions. The production of morphologically related, complex words but not the production of form-related words facilitated subsequent picture naming. Also, morphologically related but not form-related words led to a neural priming effect in the left inferior frontal gyrus (LIFG). The effects did not differ for transparent and opaque relations. The results point to a functional role of LIFG in morphological information processing during language production contrary to previous meta-analytic findings. Specifically, morphological priming effects in language production seem to be independent from semantic overlap. However, further research should confirm the independence of morphological and phonological factors. It is suggested that LIFG subserves word form encoding in language production.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "In this paper we prove an implicit-function theorem for a class of generalized equations defined by a monotone set-valued mapping in Hilbert spaces. We give applications to variational inequalities, single-valued functions and a class of nonsmooth functions.",
    "actual_venue": "Kybernetika"
  },
  {
    "abstract": "Even for simple perceptual decisions, the mechanisms that the brain employs are still under debate. Although current consensus states that the brain accumulates evidence extracted from noisy sensory information, open questions remain about how this simple model relates to other perceptual phenomena such as flexibility in decisions, decision-dependent modulation of sensory gain, or confidence about a decision. We propose a novel approach of how perceptual decisions are made by combining two influential formalisms into a new model. Specifically, we embed an attractor model of decision making into a probabilistic framework that models decision making as Bayesian inference. We show that the new model can explain decision making behaviour by fitting it to experimental data. In addition, the new model combines for the first time three important features: First, the model can update decisions in response to switches in the underlying stimulus. Second, the probabilistic formulation accounts for top-down effects that may explain recent experimental findings of decision-related gain modulation of sensory neurons. Finally, the model computes an explicit measure of confidence which we relate to recent experimental evidence for confidence computations in perceptual decision tasks.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "In this paper we propose a fully-parallel, bit-sliced unified architecture designed to perform modular multipication/exponentiation and GF(2^M) multiplication as the core operations of RSA and EC cryptography. The architecture uses radix-2 Montgomery technique for modular arithmetic, and a radix-4 MSD-first approach for GF(2^M) multiplication. To the best of our knowledge, it is the first unified proposal based on such a hybrid approach. The architecture structure is bit-sliced and is highly regular, modular, and scalable, as virtually any datapath length can be obtained at a linear cost in terms of hardware resources and no costs in terms of critical path. Our proposal outperforms all similar unified architectures found in the technical literature in terms of clock count and critical path. The architecture has been implemented on a Field-Programmable Gate Array (FPGA) device. A highly compact and efficient design was obtained taking advantage of the architectural characteristics.",
    "actual_venue": "Date"
  },
  {
    "abstract": "This paper presents an approach to specialising logic programs which is based on abstract interpretation. Program specialisation involves two stages, the construction of an abstract computation tree and a program construction stage. For the tree construction stage, abstract OLDT resolution is defined and used to construct a complete and finite tree corresponding to a given logic program and a goal. In the program construction stage, a specialised program is extracted from this tree.We focus on two logic programming languages: sequential Prolog and Flat Concurrent Prolog. Although the procedural reading of concurrent logic programs is very different from that of sequential programs, the techniques presented provide a uniform approach to the specialisation of both languages. We present the results for Prolog rigorously, and extend them less formally to Flat Concurrent Prolog.There are two main advantages of basing program specialisation on abstract interpretation. Firstly, termination can be ensured by using abstract interpretations over finite domains, while performing a complete flow analysis of the program. Secondly, correctness of the specialised program is closely related to well-defined consistency conditions on the concrete and abstract interpretations.",
    "actual_venue": "New Generation Computing"
  },
  {
    "abstract": "Nomos is a framework for modelling law-compliant solutions in software system design. It provides a core set of concepts to enable exploring and selecting alternatives in a variability space defined by laws, a graphical notation to visualize models, and tool support for compliance analysis. This short paper illustrates the features above and sketches some of the research challenges ahead.",
    "actual_venue": "Ieee International Workshop On Requirements Engineering And Law"
  },
  {
    "abstract": "We focus on agent-based simulations where a large number of agents move in the space, obeying to some simple rules. Since such kind of simulations are computational intensive, it is challenging, for such a contest, to let the number of agents to grow and to increase the quality of the simulation. A fascinating way to answer to this need is by exploiting parallel architectures. In this paper, we present a novel distributed load balancing schema for a parallel implementation of such simulations. The purpose of such schema is to achieve an high scalability. Our approach to load balancing is designed to be lightweight and totally distributed: the calculations for the balancing take place at each computational step, and in?uences the successive step. To the best of our knowledge, our approach is the ?rst distributed load balancing schema in this context. We present both the design and the implementation that allowed us to perform a number of experiments, with up-to 1, 000, 000 agents. Tests show that, in spite of the fact that the load balancing algorithm is local, the workload distribution is balanced while the communication overhead is negligible.",
    "actual_venue": "PDP"
  },
  {
    "abstract": "•We created a cohort study of cardiovascular disease from electronic health records.•Plots of the “baseline” timeframe suggested the potential for bias.•Researchers must address biases introduced by healthcare processes.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "Nowadays, industrial maintenance is one of the most important tasks in the industry because its cost is too high, usually due to poor maintenance decisions. Traditionally, corrective maintenance and preventive maintenance are performed, but both of them, the excessive and the lacking maintenance can be harmful. In the last years, CBM (Condition Based Maintenance) technology or predictive maintenance has appeared in order to establish whether the system will fail during some future period and then take actions to avoid consequences. This paper shows the e-maintenance platform nicknamed DYNAWeb which is part of DYNAMITE project. DYNAWeb develops a CBM system based on OSA-CBM standard over MIMOSA comprising broad of capabilities like sensing and data acquisition, signal processing, health assessment, prognosis... This platform ensures the integration of all the components (software and hardware) using different technologies (sensor technologies, wireless communication technology) and providing them with agents and (Semantic) Web Services to allow the integration and the reuse among different applications.",
    "actual_venue": "Otm Workshops"
  },
  {
    "abstract": "Recognizing diseases from theoretical perspective can help ordinary people have a general understanding of medicine. The usual process of identifying syndromes or diseases in Traditional Chinese Medicine (TCM) is by confirming the frequently symptom patterns. Semantic Web and ontologies introduce well-structured controlled vocabularies for biomedical science. The direct correspondence between symptoms and syndromes can be formatted to semantic inference rules as a additional knowledge upon a medical ontology. In this paper, we present a simplified rule-based diagnostic system for febrile disease theory in TCM, which make use of the capability of semantic inference based on medical ontology. Actually the method is rather general for logic-based medical diagnosis, and we show that without interpreting clinical data, the medical knowledge itself can be applied to do basic clinical diagnosis.",
    "actual_venue": "Lecture Notes In Computer Science (Including Subseries Lecture Notes In Artificial Intelligence And Lecture Notes In Bioinformatics"
  },
  {
    "abstract": "Hardware obfuscation has been proposed as a hardware security measure against reverse engineering, intellectual property (IP) piracy and integrated circuits (IC) overbuilding. In this paper, we present a novel method of obfuscation using a hierarchical approach. In the design flow, IP vendors obfuscate their designs using a set of keys and provide these keys to the design house. The design house then integrates all the IPs and adds its own keys to create a complete obfuscated system. This prevents both misuse of IPs and illegal use of ICs since only secure parties have access to the correct keys. The obfuscation at each level is performed using a mode-based approach in which the design can operate in meaningful and non-meaningful modes. The design is functionally correct in only one mode. An attacker needs to work through different levels of the design to correctly decipher its operation and correct working mode. Since each of the IPs can work in multiple meaningful modes, the attack becomes more difficult as the number of IPs increases. These ideas are demonstrated using a convolution architecture with fast Fourier transform (FFT) blocks. With only about 13% area and 15% power overhead over an unobfuscated design, it is shown that the proposed design has the flexibility to be obfuscated with different key sizes and overheads depending on the level of security.",
    "actual_venue": "Ieee International Symposium On Circuits And Systems"
  },
  {
    "abstract": "Academic researches, government reports, and international organization surveys have indicated that outcomes of many digital\n divide projects fail to accomplish their strategic goals, and so that more exertions of strategic management in the government\n project planning process are expected. In order to ensure the achievement of bridging digital divides, governments need to\n be aware of their absence and weakness in strategy formulation and implementation in addition to simply launching projects\n towards strategic visions and objectives. However, the literature shows little help for the governments to detect the shortcomings\n of their strategies and to identity gaps between strategic objectives and performance measures. The goal of this research\n is to propose generic strategic gaps models based on the balanced scorecard framework for identifying and analyzing the strategic\n gap situations. Both horizontal and vertical strategic gaps regarding project efforts for reducing digital divides will be\n determined and discussed. The proposed approach and analysis models are then applied to the case of Taiwan for demonstrating\n their feasibility and usefulness.",
    "actual_venue": "I3E"
  },
  {
    "abstract": "Formal Concept Analysis (FCA) considers attributes as a non-ordered set. This is appropriate when the data set is not structured. When an attribute taxonomy exists, existing techniques produce a completed context with all attributes deduced from the taxonomy. Usual algorithms can then be applied on the completed context for finding frequent concepts, but the results systematically contain redundant information. This article describes an algorithm which allows the frequent concepts of a formal context with taxonomy to be computed. It works on a non-completed context and uses the taxonomy information when needed. The results avoid the redundancy problem with equivalent performance.",
    "actual_venue": "Concept Lattices And Their Applications"
  },
  {
    "abstract": "This paper presents the development of a wearable accelerometry system for real-time gait cycle parameter recognition. Using a tri-axial accelerometer, the wearable motion detector is a single waist-mounted device to measure trunk accelerations during walking. Several gait cycle parameters, including cadence, step regularity, stride regularity and step symmetry can be estimated in real-time by using autocorrelation procedure. For validation purposes, five Parkinson's disease (PD) patients and five young healthy adults were recruited in an experiment. The gait cycle parameters among the two subject groups of different mobility can be quantified and distinguished by the system. Practical considerations and limitations for implementing the autocorrelation procedure in such a real-time system are also discussed. This study can be extended to the future attempts in real-time detection of disabling gaits, such as festinating or freezing of gait in PD patients. Ambulatory rehabilitation, gait assessment and personal telecare for people with gait disorders are also possible applications.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "Following the ideas of Frontini and Sormani, we present a modified Newton method in Banach space which is used to solve the nonlinear operator equation. We establish the Newton–Kantorovich convergence theorem for the modified Newton method with third-order convergence in Banach space by using majorizing function. We also get the error estimate. Finally, two examples are provided to show the application of our theorem.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "The problem of annual production scheduling in surface mining consists of determining an optimal sequence of extracting the mineralized material from the ground. The main objective of the optimization process is usually to maximize the total Net Present Value of the operation. Production scheduling is typically a mixed integer programming (MIP) type problem. However, the large number of integer variables required in formulating the problem makes it impossible to solve. To overcome this obstacle, a new algorithm termed “Fundamental Tree Algorithm” is developed based on linear programming to aggregate blocks of material and decrease the number of integer variables and the number of constraints required within the MIP formulation. This paper proposes the new Fundamental Tree Algorithm in optimizing production scheduling in surface mining. A case study on a large copper deposit summarized in the paper shows substantial economic benefit of the proposed algorithm compared to existing methods.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "In cellular wireless networks, the choice of Call Admission Control scheme impacts the performance of the system, particularly as how calls are managed when a mobile user is handed off from one cell to another. Non-prioritized schemes treat handoff calls and new calls equally, while, prioritized schemes give higher priority to handoff calls. In this paper, some of the popular non-prioritized and prioritized Call Admission Control schemes were investigated and their behavior was simulated and analyzed. They are evaluated based on call dropping probability, call blocking probability and system utilization parameters.",
    "actual_venue": "Wireless Personal Communications: An International Journal"
  },
  {
    "abstract": "Identifying the finger used for touching and measuring the force of the touch provides valuable information on manual interactions. This information can be inferred from electromyography (EMG) of the forearm, measuring the activation of the muscles controlling the hand and fingers. We present Touch-Sense, which classifies the finger touches using a novel neural network architecture and estimates their force on a smartphone in real time based on data recorded from the sensors of an inexpensive and wireless EMG armband. Using data collected from 18 participants with force ground truth, we evaluate our system's performance and limitations. Our system could allow for new interaction paradigms with appliances and objects, which we exemplarily showcase in four applications.",
    "actual_venue": "Ubicomp : The Acm International Joint Conference On Pervasive And Ubiquitous Computing Singapore Singapore October"
  },
  {
    "abstract": "Most existing research on the job shop scheduling problem has been focused on the minimization of makespan (i.e., the completion time of the last job). However, in the fiercely competitive market nowadays, delivery punctuality is more important for maintaining a high service reputation. So in this paper, we aim at solving job shop scheduling problems with the total weighted tardiness objective. Several dispatching rules are adopted in the Giffler-Thompson algorithm for constructing active schedules. It is noticeable that the rule selections for scheduling consecutive operations are not mutually independent but actually interrelated. Under such circumstances, a probabilistic model-building genetic algorithm (PMBGA) is proposed to optimize the sequence of selected rules. First, we use Bayesian networks to model the distribution characteristics of high-quality solutions in the population. Then, the new generation of individuals is produced by sampling the established Bayesian network. Finally, some elitist individuals are further improved by a special local search module based on parameter perturbation. The superiority of the proposed approach is verified by extensive computational experiments and comparisons.",
    "actual_venue": "Journal Of Applied Mathematics"
  },
  {
    "abstract": "In this paper we report on a study of implicit feedback models for unobtrusively tracking the information needs of searchers. Such models use relevance information gathered from searcher interaction and can be a potential substitute for explicit relevance feedback. We introduce a variety of implicit feedback models designed to enhance an Information Retrieval (IR) system's representation of searchers' information needs. To benchmark their performance we use a simulation-centric evaluation methodology that measures how well each model learns relevance and improves search effectiveness. The results show that a heuristic-based binary voting model and one based on Jeffrey's rule of conditioning [5] outperform the other models under investigation.",
    "actual_venue": "Advances In Information Retrieval, Proceedings"
  },
  {
    "abstract": "This paper presents a logistic regression based approach to predict the soft-response for a challenge using the total delay-difference as an input. This approach enables us to determine whether a challenge is stable or not. Soft-response is the probability of response bit corresponding to the challenge being 1. The total delay-difference is computed from the input challenge by assuming that the delay-difference of the stages are known. The approach learns a logistic function based on the total delay-difference which has just 3 parameters. Therefore, this is a simple approach which gives comparable performance against a more complex approach based on artificial neural network (ANN) models. The model demonstrates good sensitivity and precision but poor specificity. Furthermore, we use scaling parameter of the logistic function to study its relation to the arbiter's timing parameters like setup and hold time.",
    "actual_venue": "Ieee International Symposium On Circuits And Systems"
  },
  {
    "abstract": "This article presents a flexible, extendable system called Tracktivity that can capture gameplay metrics in any type of leaderboard-based video game. This system incorporates novel visualizations, including a dynamic competition balancing (DCB) measure and driver progression, to enhance the resulting gameplay experience. This article also presents use cases of the system and the techniques used to visualize and study the data gathered.",
    "actual_venue": "Ieee Software"
  },
  {
    "abstract": "We demnstrate an adaptation strategy,for adjusting the stride period in a hexapedal running robot. The robot is inspired by discoveries about the self-stabilizing properties of insects and uses a sprawled posture, a bouncing alternating-tripod gait, and passive compliance and damping in the limbs to achieve fast (over four body-lengths per second). stable locomotion. The robot is controlled by an open-loop motor pattern that activates the legs at fixed intervals. For maximum speed and efficiency the stride period of the pattern should be adjusted to match changes in terrain (e.g., slopes) or loading conditions (e.g.. carrying an object). An ideal adaptation strategy will complement philosophy behind the robot and take advantage of self-stabilizing role of the mechanical system. In this paper we describe an adaptation scheme based on measurements of ground contact tuning obtained from binary sensors on the robot's feet. We discuss the motivation for the approach, putting it in the context of previous research on the dynamic properties of running machines and bouncing multi-legged animals, and we show the results of the experiments.",
    "actual_venue": "International Journal Of Robotic Research"
  },
  {
    "abstract": "Hochberg et al. (1995) obtained the bandwidth of triangulated triangles. In this paper, we consider a more general class of graphs, called convex triangulation meshes and denoted by T l , m , n . We show that the bandwidth of T l , m , n is min { l , m , n }.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "This paper introduces basic principles for extending the classical systolic synthesis method- ology to multi-dimensional time. Multi-dimensional scheduling enables complex algorithms that do not admit linear schedules to be parallelized, but it also implies the use of memories in the architecture. The paper explains how to obtain compatible allocation and memory functions for vlsi (or simd-like code) generation. It also presents an original mechanism for controlling a vlsi architecture which has a multi-dimensional schedule. A structural vhdl code has been derived and synthesized (for implementation on fpga platform) using these systematic design principles. These results are preliminary steps to the possibility of a systematic hardware synthesis for multi-dimensional time.",
    "actual_venue": "Asap"
  },
  {
    "abstract": "A new kind of robotic mechanism is proposed to be used for inspection tasks in complex setups of industrial plants. We propose a multiarticulated snake-like mobile robot, with a body consisting of repeating modules, capable of both efficiently moving and reaching points inside complicated or unstructured areas, where human personnel cannot reach or work properly. An analysis of the basic design along with most of the component specifications is presented. This mechanical system is subject to nonholonomic constraints. The kinematic model for motion on-plane of the mobile robot is derived by taking into consideration these constraints. The nonholonomic motion planning is partially solved by converting the multiple-input system to a multiple-chain, single-generator chained form via state feedback and a coordinate transformation. Stabilization and trajectory tracking issues are also considered. We also consider the general case of the n-trailer (or n-module) robotic snake. Simulation results are provided for various test cases. (C) 1999 John Wiley & Sons, Inc.",
    "actual_venue": "Journal Of Robotic Systems"
  },
  {
    "abstract": "Nonlinearity, among other factors, is often the root cause of difficulties in nonlinear problems. It is important to quantify a problem's degree of nonlinearity to decide a proper solution. For example, a full-blown nonlinear filter is needed in general if the estimation problem is highly nonlinear, but a quasi-linear filter (e.g., an extended Kalman filter) is sufficient for a weakly nonlinear case. This paper first surveys various measures of nonlinearity (MoNs) for different applications. For nonlinear estimation, we conclude that these MoNs are not suitable and a better measure is needed. In view of this, we propose a general MoN for estimation. It measures the mean-square closeness between a point and a subspace in a functional space. Properties and computation of this measure are studied. Numerical examples of static models for parameter estimation and dynamic models for process estimation are given to illustrate our measure.",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "A mobile ad hoc network is a kind of popular self-configuring network, in which multicast routing under the quality of service constraints, is a significant challenge. Many researchers have proved that such problem can be formulated as a NP-complete problem and proposed some swarm-based intelligent algorithms to solve the optimal solution, such as the genetic algorithm (GA), bees algorithm. However, a lower efficiency of local search ability and weak robustness still limit the computational effectiveness. Aiming to those shortcomings, a new hybrid algorithm inspired by the self-organization of Physarum, is proposed in this paper. In our algorithm, an updating scheme based on Physarum network model (PM) is used for improving the crossover operator of traditional GAs, in which the same parts of parent chromosomes are reserved and the new offspring by the PM is generated. In order to estimate the effectiveness of our proposed optimized scheme, some typical genetic algorithms and their updating algorithms (PMGAs) are compared for solving the multicast routing on four different datasets. The simulation experiments show that PMGAs are more efficient than original GAs. More importantly, the PMGAs are more robustness that is very important for solving the multicast routing problem. Moreover, a series of parameter analyses is used to find a set of better setting for realizing the maximal efficiency of our algorithm.",
    "actual_venue": "Natural Computing"
  },
  {
    "abstract": "The task of programming instruments in a test system has always been a concern for end users and a major cost for the overall system development. Many users know that programming can often be the most time-consuming part of developing a system. The developer spends much valuable time learning the specific programming requirements of each instrument in the system. Almost all instruments are designed for interactive use through a physical front panel and also offer remote control capability via a communication port on the back of the instrument. An instrument driver, in the simplest definition, is a set of software routines that handles the programmatic details of controlling and communicating with a specific instrument. The most successful instrument driver concepts have always distributed instrument drivers in source code and provided end users with access to the same tools developers use to write drivers. With this philosophy, new instrument drivers were often easily developed by end users through modifying an existing driver for another instrument.",
    "actual_venue": "Idaacs), Ieee International Conference"
  },
  {
    "abstract": "A new electromagnetic sensor using the atomic Rabi frequency is presented. Gaseous cesium-133 atoms enclosed in a compact glass cell work as an electromagnetic sensor by detecting the Rabi frequency. In this paper, we measured the electromagnetic field radiated from a standard gain horn antenna using the new sensor. This result was compared with a numerical simulation based on the method of moment...",
    "actual_venue": "Ieee Transactions On Instrumentation And Measurement"
  },
  {
    "abstract": "Safety is a very important property for safetycritical software systems. We introduce an approach to test safety-critical software systems based on safety requirement by using model-based testing. The model is extracted from the safety requirement and system under test. In our approach, we use model checking techniques to generate test cases. To find appropriate test cases in less time, we divide the states spaces of model into different subsets. By using the approach, we have designed an automatic testing framework and developed an automatic testing platform, which has been used for testing the safety of high speed train control system in china",
    "actual_venue": "Ifita"
  },
  {
    "abstract": "Control-flow visual languages are often criticized as generating diagrams that look like a spaghetti plate. In this paper, we describe a solution we implemented to manage the visual complexity of control-flow diagrams with a very large number of nodes. Our solution is based on subdividing the display window real-estate into a sort of grid, with areas where mostly nodes are displayed and other areas where mostly edges are displayed.",
    "actual_venue": "Tokyo"
  },
  {
    "abstract": "This paper analyzes collaboration in a distributed, multiparty information system (IS) development and implementation project. Existing research on collabo- ration in IS projects, and particularly in distributed, multiparty IS projects, is reviewed. In addition, col- laboration is empirically analyzed by utilizing a phe- nomenographic approach. Empirical analysis shows that collaboration as a theme emerged strongly. Dif- ferent organizations viewed the project and collabo- ration in the project in divergent ways, but there were also varied and contradictory views within the or- ganizations. Implications both for research and prac- tice are discussed. .",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "Previous RGB-D fusion systems based on convolutional neural networks typically employ a two-stream architecture, in which RGB and depth inputs are learned independently. The multi-modal fusion stage is typically performed by concatenating the deep features from each stream in the inference process. The traditional two-stream architecture might experience insufficient multi-modal fusion due to two following limitations: 1) the cross-modal complementarity is rarely studied in the bottom–up path, wherein we believe the cross-modal complements can be combined to learn new discriminative features to enlarge the RGB-D representation community and 2) the cross-modal channels are typically combined by undifferentiated concatenation, which appears ambiguous to selecting cross-modal complementary features. In this paper, we address these two limitations by proposing a novel three-stream attention-aware multi-modal fusion network. In the proposed architecture, a cross-modal distillation stream, accompanying the RGB-specific and depth-specific streams, is introduced to extract new RGB-D features in each level in the bottom–up path. Furthermore, the channel-wise attention mechanism is innovatively introduced to the cross-modal cross-level fusion problem to adaptively select complementary feature maps from each modality in each level. Extensive experiments report the effectiveness of the proposed architecture and the significant improvement over the state-of-the-art RGB-D salient object detection methods.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "There is extensive evidence that the classical, stuck-at fault model, operating at the gate level, is inadequate for testing MOS VLSI circuits. By contrast, a ``nonclassical,'' switch-level model驴directly representing open and short circuits in the interconnect and transistors stuck-open or stuck-on驴allows important effects such as MOSFET bidirectionality and tristate behavior to be taken into account. This paper describes a new switch-level method for generating the singular cover of an MOS primitive gate using path algebras. The method yields tests to cover all specified interconnect open and short circuits, and all irredundant transistor stuck-open and stuck-on faults, should such tests exist. It relies on specification of an appropriate algebra by redefinition of the operators used in computing powers of a matrix. Two such algebras are required驴one to generate tests for open circuit faults and the other for short circuit faults. Test generation for networks of primitive gates is achieved in two stages: after deriving singular covers for the primitives, a variant of the D-algorithm with modified ``D-drive'' is used. The approach is unified and powerful, having the potential to detect parasitic latch behavior and to generate tests for any of the current MOS VLSI technologies. In common with most present automatic test generation methods, it is restricted to combinational logic. Practical limits on the size of circuit which can be dealt with appear comparable to those set by use of the classical D-algorithm.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "The probabilistic relation model has been used for the compact representation of uncertain data in relational databases. In this paper we present the extended probabilistic relation model, a compact representation for uncertain information that admits efficient information integration. We present an algorithm for data integration using this model and prove its correctness. We also explore the complexity of query evaluation under the probabilistic and extended probabilistic models. Finally, we study the problem of obtaining a (pure) probabilistic relation that is equivalent to a given extended probabilistic relation, and present approaches and algorithms for this task. This work is the first and critical step towards practical and efficient uncertain information integration.",
    "actual_venue": "Ideas"
  },
  {
    "abstract": "Compressive sensing (CS) has been viewed as a promising technology to greatly improve the communication efficiency of data gathering in wireless sensor networks. However, this new data collection paradigm may bring in new threats but few study has paid attention to prevent information leakage during compressive data gathering. In this paper, we identify two statistical inference attacks and demonstrate that traditional compressive data gathering may suffer from serious information leakage under these attacks. In our theoretical analysis, we quantitatively analyze the estimation error of compressive data gathering through extensive statistical analysis, based on which we propose a new secure compressive data aggregation scheme by adaptively changing the measurement coefficients at each sensor and correspondingly at the sink without the need of time synchronization. In our analysis, we show that the proposed scheme could significantly improve data confidentiality at light computational and communication overhead.",
    "actual_venue": "Infocom"
  },
  {
    "abstract": "This paper describes a vision system that detects cracks in glass bottles production. The first step consists in collecting prototypes of bottles with and without defects. A sequence of 16 images is captured by a matrix camera while each bottle rotates in front of a specific lighting system. The second step is concerned with morphometric and photometric features extraction. The subsequent decision step is performed by different neural networks, such as MLP, RBF, PNN and LVQ. Finally, performances of these networks have been compared. All the images of bottles without defects have been recognized but a few images with small cracks, which are very important defects, have not been identified. However, since each bottle is represented by a sequence of 16 images, cracks will appear in at least three or four images, so that a defective bottle can be detected at least one time through the sequence. Therefore the decision system recognizes good and defective bottles with a very high rate of success.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "Nowadays, the popular Android is so closely involved in people's daily lives that people rely on Android to perform critical operations and trust Android with sensitive information. It is of great importance to guarantee the usability and security of Android which, however, is such a huge system that a potential threat may arise from any part of it. In this paper, we focus on the Free Floating window (FF window) which is a category of windows that can appear freely above any other applications. It can share the screen space with other FF windows, dialogs, and activities. An FF window is flexible in both its appearance and behaviour features. We analyse the behaviour features of FF windows, including the priority in display layer and the capability of processing user-generated events. Three types of attacks via FF windows with delicate design in their appearance and behaviour features are demonstrated, i.e., DoS attack against Android system, GUI hijacking by targeting overlap, and input inference using FF windows as a side channel. To address the threat caused by FF windows, we design a priority framework for FF windows, which protects a sensitive activity/FF window declared by developers from being attacked by any malicious FF windows. A complementary solution is proposed to mitigate the confusion attack from malicious activities. Finally, we provide Android with suggestions on how to manage FF windows.",
    "actual_venue": "Asiaccs"
  },
  {
    "abstract": "This paper examines the users' perspective on the security of Internet banking in Australia within the social context. This user-centered design approach supplements the technological and industrial approaches to security. The user-centered research on banking was conducted at the Royal Melbourne University of Technology University and Griffith University, both of which are part of the Smart Internet Technology Cooperative Research Centre. We conclude that the most effective way to increase the perception of Internet banking security is to increase ease of use, convenience, personalisation and trust. Without the perception of security, there will be little trust in banking and transactions on the Internet. This will impede the use of Internet banking and e-commerce which are increasingly important aspects of the nation's critical infrastructure.",
    "actual_venue": "Jtaer"
  },
  {
    "abstract": "Rational function model (RFM) faces difficulty in extracting accurate geometric information from remotely sensed images, which is mainly due to the problems of overparameterization and ill-posedness. These problems can be addressed via variable selection methods, in which an optimum subset of rational polynomial coefficients is identified via an optimization algorithm, usually metaheuristic method...",
    "actual_venue": "Ieee Geoscience And Remote Sensing Letters"
  },
  {
    "abstract": "At present, most of small grain crops Such as millet and forage seeds arc sown by external force feed in mainland China, which has some problems of wasting seeds, uneven seed distribution and so on. Therefore, it is needed to design a precise feed device for small grain corps. The author's of this paper established a three-dimensional model to design the spiral groove precision seed-metering device using Pro/Engineer. The movement simulation and clearance were tested between the seeding roller and the shell by mechanism module. According to the analysis results, the defaults Of the feed device were modified, so that the model structure was optimized. The simulation results showed that this device is simple and suitable and suitable for the seeding requirements of small grains.",
    "actual_venue": "Computer And Computing Technologies In Agriculture"
  },
  {
    "abstract": "Cancer classification, through gene expression data analysis, has produced remarkable results, and has indicated that gene expression assays could significantly aid in the development of efficient cancer diagnosis and classification platforms. However, cancer classification, based on DNA array data, remains a difficult problem. The main challenge is the overwhelming number of genes relative to the number of training samples, which implies that there are a large number of irrelevant genes to be dealt with. Another challenge is from the presence of noise inherent in the data set. It makes accurate classification of data more difficult when the sample size is small. We apply genetic algorithms (GAs) with an initial solution provided by t statistics, called t-GA, for selecting a group of relevant genes from cancer microarray data. The decision-tree-based cancer classifier is built on the basis of these selected genes. The performance of this approach is evaluated by comparing it to other gene selection methods using publicly available gene expression data sets. Experimental results indicate that t-GA has the best performance among the different gene selection methods. The Z-score figure also shows that some genes are consistently preferentially chosen by t-GA in each data set.",
    "actual_venue": "Expert Systems"
  },
  {
    "abstract": "We construct a scale space of shape of closed Riemannian manifolds, equipped with metrics derived from spectral representations and the Hausdorff distance. The representation depends only on the intrinsic geometry of the manifolds, making it robust to pose and articulation. The computation of shape distance involves an optimization problem over the 2^p-element group of all p-bit strings, which is approached with Markov chain Monte Carlo techniques. The methods are applied to cluster surfaces in 3D space.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "As it is difficult to detect small ship targets from complex background in an IR image because of the impact of kinds of noises, complex background and the smallness of the target, this paper puts forward a method of background prediction utilizing the neural network. The method estimates the background of the input image nonlinearly, gains the residual error image, and detects the target. By comparison, NARX whose hidden layer transfer function utilizing L-M algorithm can achieve more excellent target detection than other neural networks like BP in training speed and stability, thus it is research valuable in the field of small target detection in complex background.",
    "actual_venue": "Icarcv"
  },
  {
    "abstract": "•An information theoretic framework to quantify semantic correlations is presented.•This is used to measure the disclosure risk of correlated terms in document sanitization.•An efficient algorithm providing a practical implementation of the framework is provided.•Evaluation results show a significant decrease of disclosure risk against standard sanitizers.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Cell detection plays a significant role in automated biomedical image analysis. However, it is challenging to achieve accurate detection due to dense crowding/touching of cells. In this paper, we propose a robust decomposition algorithm for cell detection on adipocyte images. It formulates the decomposition into a cut selection problem using the polygon triangulation approximation and a specific-defined concavity measurement, and utilizes combinatorial optimization to select the optimal cuts. The proposed algorithm can effectively handle contour noises, large shape variance and size difference. In addition, the decomposition seeds preserve original cell shapes to facilitate the subsequent segmentation. The algorithm is well tested with 231 adipocyte microscopic image patches, which contain 2-17 touching cells with a variety of morphological complexity. The comparative experiments with the recent state of the arts demonstrate the superior performance of the proposed method.",
    "actual_venue": "Isbi"
  },
  {
    "abstract": "Physics engines have created a whole new source of emergence and fun for digital games. Water simulation could add another similar emergent interaction element, but it is currently only rarely used as a part of gameplay. The reasons for this are analysed and different water simulation methods are compared, focusing on actual game usage. Based on this, we suggest using the extremely simple but fast pipe model. The simplicity of the underlying simulation can be masked in many ways using modern shader effects and other tricks. An example of the pipe method in action is given and compared to more sophisticated fluid solvers.",
    "actual_venue": "Mindtrek"
  },
  {
    "abstract": "Delay Tolerant Networks (DTNs) are generally characterized by long and variable delays due to intermittent connectivity. This lack of connectivity make most of the existing routing protocols in WSNs not applicable in DTNs (The performance of such networks depends on connectivity and mobility of some nodes). We propose in this paper a new routing mechanism for a DTN using WSN nodes for localization or traceability applications. Our routing mechanism exploits node mobility by allowing stations that come into contact with fixed or mobile nodes, to collect, to exchange and even to pass around information called contact events, that need to be fetched towards a collect point. We defined rules (filters) to decide which contact event must be sent or dropped by nodes. We also define the modeling approach to reproduce mobile activities in confined environments such as mines. We also evaluate our routing mechanism through simulations representing mining activity for which a set of mobile nodes plays the role of miners.",
    "actual_venue": "Networking, Sensing And Control"
  },
  {
    "abstract": "Prioritized sweeping is a model-based reinforcement learning method that attempt to focus the agent's limited computational resources to achieve a good estimate of the v alue of environment states. The classic account of prioritized sweeping uses an explicit, state-b ased representation of the value, reward, and model parameters. Such a representation is unwieldy for dealing with complex environments and there is growing interest in learning with more compact representations. We claim that classic prioritized sweeping is ill-suited for learning wit h such representations. To overcome this deficiency, we introducegeneralized prioritized sweeping , a principled method for generating representation-specific algorithms for model-based reinforcement learning. We then apply this method for several representations, including state-based mod els and generalized model approximators (such as Bayesian networks). We describe preliminary experiments that compare our approach with classical prioritized sweeping.",
    "actual_venue": "Nips"
  },
  {
    "abstract": "In the last years, scientific workflows have emerged as a fundamental abstraction for structuring and executing scientific experiments in computational environments. Scientific workflows are becoming increasingly complex and more demanding in terms of computational resources, thus requiring the usage of parallel techniques and high performance computing (HPC) environments. Meanwhile, clouds have emerged as a new paradigm where resources are virtualized and provided on demand. By using clouds, scientists have expanded beyond single parallel computers to hundreds or even thousands of virtual machines. Although the initial focus of clouds was to provide high throughput computing, clouds are already being used to provide an HPC environment where elastic resources can be instantiated on demand during the course of a scientific workflow. However, this model also raises many open, yet important, challenges such as scheduling workflow activities. Scheduling parallel scientific workflows in the cloud is a very complex task since we have to take into account many different criteria and to explore the elasticity characteristic for optimizing workflow execution. In this paper, we introduce an adaptive scheduling heuristic for parallel execution of scientific workflows in the cloud that is based on three criteria: total execution time (makespan), reliability and financial cost. Besides scheduling workflow activities based on a 3-objective cost model, this approach also scales resources up and down according to the restrictions imposed by scientists before workflow execution. This tuning is based on provenance data captured and queried at runtime. We conducted a thorough validation of our approach using a real bioinformatics workflow. The experiments were performed in SciCumulus, a cloud workflow engine for managing scientific workflow execution.",
    "actual_venue": "J Grid Comput"
  },
  {
    "abstract": "A fundamental role of the Hsp90 chaperone system in mediating maturation of protein clients is essential for the integrity of signaling pathways involved in cell cycle control and organism development. Molecular characterization of Hsp90 interactions with client proteins is fundamental to understanding the activity of many tumor-inducing signaling proteins and presents an active area of structural and biochemical studies. In this work, we have probed mechanistic aspects of allosteric regulation of Hsp90 by client proteins via a detailed computational study of Hsp90 interactions with the tumor suppressor protein p53. Experimentally guided protein docking and molecular dynamics structural refinement have reconstructed the recognition-competent states of the Hsp90-p53 complexes that are consistent with the NMR studies. Protein structure network analysis has identified critical interacting networks and specific residues responsible for structural integrity and stability of the Hsp90-p53 complexes. Coarse-grained modeling was used to characterize the global dynamics of the regulatory complexes and map p53-induced changes in the conformational equilibrium of Hsp90. The variations in the functional dynamics profiles of the Hsp90-p53 complexes are consistent with the NMR studies and could explain differences in the functional role of the alternative binding sites. Despite the overall similarity of the collective movements and the same global interaction footprint, p53 binding at the C-terminal interaction site of Hsp90 may have a more significant impact on the chaperone dynamics, which is consistent with the stronger allosteric effect of these interactions revealed by the experimental studies. The results suggest that p53-induced modulation of the global dynamics and structurally stable interaction networks can target the regulatory hinge regions and facilitate stabilization of the closed Hsp90 dimer that underlies the fundamental stimulatory effect of the p53 client.",
    "actual_venue": "Journal Of Chemical Information And Modeling"
  },
  {
    "abstract": "Hebbian learning in cortical networks during development and adulthood relies on the presence of a mechanism to detect correlation between the presynaptic and the postsynaptic spiking activity. Recently, the calcium concentration in spines was experimentally shown to be a correlation sensitive signal with the necessary properties: it is confined to the spine volume, it depends on the relative timing of pre- and postsynaptic action potentials, and it is independent of the spine's location along the dendrite. NMDA receptors are a candidate mediator for the correlation dependent calcium signal. Here, we present a quantitative model of correlation detection in synapses based on the calcium influx through NMDA receptors under realistic conditions of irregular pre- and postsynaptic spiking activity with pairwise correlation. Our analytical framework captures the interaction of the learning rule and the correlation dynamics of the neurons. We find that a simple thresholding mechanism can act as a sensitive and reliable correlation detector at physiological firing rates. Furthermore, the mechanism is sensitive to correlation among afferent synapses by cooperation and competition. In our model this mechanism controls synapse formation and elimination. We explain how synapse elimination leads to firing rate homeostasis and show that the connectivity structure is shaped by the correlations between neighboring inputs.",
    "actual_venue": "Frontiers In Computational Neuroscience"
  },
  {
    "abstract": "Increasingly, women elect breast reconstruction after mastectomy. However, their expectations of surgery are often not met, and dissatisfaction with outcome and ongoing psychosocial concerns and distress are common. We developed a patient-centered intervention, PEGASUS:(Patients' Expectations and Goals: Assisting Shared Understanding of Surgery) which supports shared decision making by helping women clarify their own, individual goals about reconstruction so that they can discuss these with their surgeon. Our acceptability/feasibility work has shown it is well received by patients and health professionals alike. We now need to establish whether PEGASUS improves patients' experiences of breast reconstruction decision making and outcomes. The purpose of this study is, therefore, to examine the effectiveness of PEGASUS, an intervention designed to support shared decision making about breast reconstruction.A multi-centered sequential study will compare the impact of PEGASUS with usual care, in terms of patient reported outcomes (self-reported satisfaction with the outcome of surgery, involvement in decision making and in the consultation) and health economics. Initially we will collect data from our comparison (usual care) group (90 women) who will complete standardized measures (Breast-Q, EQ5D -5 L and ICECAP- A) at the time of decision making, 3, 6 and 12 months after surgery. Health professionals will then be trained to use PEGASUS, which will be delivered to the intervention group (another 90 women completing the same measures at the time of decision making, and 3, 6 and 12 months after surgery). Health professionals and a purposefully selected sample of participants will be interviewed about whether their expectations of reconstruction were met, and their experiences of PEGASUS (if appropriate).PEGASUS may have the potential to provide health professionals with an easily accessible tool aiming to support shared decision making and improve patients' satisfaction with breast reconstruction. Results of this study will be available at the end of 2019.ISRCTN 18000391 (DOI 10.1186/ISRCTN18000391) 27/01/2016.",
    "actual_venue": "Bmc Med Inf And Decision Making"
  },
  {
    "abstract": "Detecting epistatic interactions plays a significant role in improving pathogenesis, prevention, diagnosis, and treatment of complex human diseases. Applying machine learning or statistical methods to epistatic interaction detection will encounter some common problems, e.g., very limited number of samples, an extremely high search space, a large number of false positives, and ways to measure the association between disease markers and the phenotype.To address the problems of computational methods in epistatic interaction detection, we propose a score-based Bayesian network structure learning method, EpiBN, to detect epistatic interactions. We apply the proposed method to both simulated datasets and three real disease datasets. Experimental results on simulation data show that our method outperforms some other commonly-used methods in terms of power and sample-efficiency, and is especially suitable for detecting epistatic interactions with weak or no marginal effects. Furthermore, our method is scalable to real disease data.We propose a Bayesian network-based method, EpiBN, to detect epistatic interactions. In EpiBN, we develop a new scoring function, which can reflect higher-order epistatic interactions by estimating the model complexity from data, and apply a fast Branch-and-Bound algorithm to learn the structure of a two-layer Bayesian network containing only one target node. To make our method scalable to real data, we propose the use of a Markov chain Monte Carlo (MCMC) method to perform the screening process. Applications of the proposed method to some real GWAS (genome-wide association studies) datasets may provide helpful insights into understanding the genetic basis of Age-related Macular Degeneration, late-onset Alzheimer's disease, and autism.",
    "actual_venue": "Bmc Systems Biology"
  },
  {
    "abstract": "The Sindice Semantic Web index provides search capabilities over 260 million documents. Reasoning over web data enables to make explicit what would otherwise be implicit knowledge: it adds value to the information and enables Sindice to ultimately be more competitive in terms of precision and recall. However, due to the scale and heterogeneity of web data, a reasoning engine for the Sindice system must (1) scale out through parallelisation over a cluster of machines; and (2) cope with unexpected data usage. In this paper, we report our experiences and lessons learned in building a large scale reasoning engine for Sindice. The reasoning approach has been deployed, used and improved since 2008 within Sindice and has enabled Sindice to reason over billions of triples.",
    "actual_venue": "RR"
  },
  {
    "abstract": "Audio frequency magnetotullric (AMT) is widely used in the exploration of mineral and underground water. Three-dimension (3D) AMT exploration makes the imaging of underground geological body or structure with the best precision and resolution. And it needs a lot of AMT acquisition units to carry out exploration. Current commercial AMT unit was suffered from high power consumption and low work efficiency for 3D AMT exploration. We design and realize a low power AMT acquisition network based on ZigBee, GPS and ARM based embedded control system. Each AMT acquisition unit is composed of power supply module, GPS module, ARM module, and data acquisition module. The power supply module is controlled by the GPIO of the ZigBee Pro module. Power supply for GPS module, ARM module, data acquisition module, and induction coils can be switched on or off remotely by ZigBee network. The GPS module offers clocks and timing signals for the data acquisition module. The ARM based embedded control module is composed of AT91RM9200, 1GB NAND flash, 64 MB NOR flash, WI-FI, Ethernet and 64 MB SDRAM. It controls the data acquisition, calibration, and self-testing in AMT exploration. The data acquisition module is composed of 4 channels for the signal conditioning of weak AMT signals, 4 channels 24-bit ADC, and 24-bit fixed DSP. The low power differential amplifiers and low power audio amplifiers are used for the amplifying and filtering of the input signals. All AMT acquisition units are configured as ZigBee routers to build a wireless sensor network in mountain area successfully. A notebook with ZigBee router is used to control the AMT acquisition network. Software is developed in notebook to monitor network, send control command, retrieve the status of each AMT acquisition unit, and control the data acquisition process. One hundred AMT acquisition units are made, and tested in Tibet. The tests were successful in most area, but it was difficult to build a ZigBee network in some area where - he tough topology was faced.",
    "actual_venue": "Instrumentation And Measurement Technology Conference"
  },
  {
    "abstract": "In this paper, a novel watermarking scheme in the compressed domain is proposed for video copyright protection. The robust watermark is embedded in motion vectors of the bitstream based on the relationships between one-pixel accuracy and half-pixel accuracy motion vectors at the encoder. Watermark can be blindly extracted by parsing the properties of motion vectors at the decoder. Simulation results show that the proposed algorithm can perform the real-time watermark embedding process without affecting visual quality of the video sequence and it is also standard-compliant. The proposed algorithm can be applied to any video compression standard with half-pixel accuracy motion vectors such as MPEG-2 and H.263.",
    "actual_venue": "KES (2)"
  },
  {
    "abstract": "In 3GPP LTE-Advanced standard forum, Mobile Relay Node (MRN) occupies an important place. It is well known that cell-edge users suffer from bad Signal Level strength. To improve the signal quality and network coverage at the cell-edge it is proposed to use Mobile Relay Node. For the moment, MRN communicates to the UE devices in the vehicle. In this paper, we propose to implement MRN on the top of public transportation having known path and low speed to serve data hungry users outside the vehicles.",
    "actual_venue": "Ieee International Conference On Wireless And Mobile Computing, Networking And Communications"
  },
  {
    "abstract": "Convolutional neural networks are known to be powerful image classifiers. In this work, a method is proposed for training convolutional networks for implementation on an existing mixed digital-analog VLSI hardware architecture. The binary threshold neurons provided by this architecture cannot be trained using gradient-based methods. The convolutional layers are trained with a clustering method, locally in each layer. The output layer is trained using the Perceptron learning rule. Competitive results are obtained on hand-written digits (MNIST) and traffic signs. The analog hardware enables high integration and low power consumption, but inherent error sources affect the computation accuracy. Networks trained as suggested are highly robust against random changes of synaptic weights occuring on the hardware substrate, and work well even with only three distinct weight values (-1, 0, +1), reducing computational complexity to mere counting. I. INTRODUCTION",
    "actual_venue": "Vancouver, Bc"
  },
  {
    "abstract": "Mirrored mutations and active covariance matrix adaptation are two recent ideas to improve the well-known covariance matrix adaptation evolution strategy (CMA-ES)---a state-of-the-art algorithm for numerical optimization. It turns out that both mechanisms can be implemented simultaneously. In this paper, we investigate the impact of mirrored mutations on the so-called IPOP active CMA-ES. We find that additional mirrored mutations improve the IPOP active CMA-ES statistically significantly, but by only a small margin, on several functions while never a statistically significant performance decline can be observed. Furthermore, experiments on different function instances with some algorithm parameters and stopping criteria changed reveal essentially the same results.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "Latency insensitive communication offers many potential benefits for FPGA designs, including easier timing closure by enabling automatic pipelining, and easier interfacing with embedded NoCs. However, it is important to understand the costs and trade-offs associated with any new design style. This paper presents optimized implementations of latency insensitive communication building blocks, quantifies their overheads in terms of area and frequency, and provides guidance to designers on how to generate high-speed and area-efficient latency insensitive systems.",
    "actual_venue": "Fpga"
  },
  {
    "abstract": "In this paper, the fast technique for image colorization is considered. The proposed method transfers colors from the color image (source) to the gray level image (target). For the source image, we use the segmented uniformly colored regions (dielectric surfaces) under single color illumination. This method maps the gray level image into the color space by means of parametrical mapping learnt using PCA and principal components regression. The experiments show the method's feasibility for colorizing the objects, and textures, as well.",
    "actual_venue": "Cciw"
  },
  {
    "abstract": "In this work we present a runtime threading system which provides an efficient substrate for fine-grain parallelism, suitable for deployment in multicore platforms. Its architecture encompasses a number of optimizations that make it particularly effective in managing a large number of threads and with low overheads. The runtime system has been integrated into an OpenMP implementation to allow for transparent usage under a high level programming paradigm. We evaluate our implementation on two multicore systems using synthetic microbenchmarks and a real-time face detection application.",
    "actual_venue": "Scientific Programming"
  },
  {
    "abstract": "We present a methodology for improving credit scoring models by distinguishing two forms of rational behaviour of loan defaulters. It is common knowledge among practitioners that there are two types of defaulters, those who do not pay because of cash flow problems ('Can't Pay'), and those that do not pay because of lack of willingness to pay ('Won't Pay'). This work proposes to differentiate them using a game theory model that describes their behaviour. This separation of behaviours is represented by a set of constraints that form part of a semi-supervised constrained clustering algorithm, constructing a new target variable summarizing relevant future information. Within this approach the results of several supervised models are benchmarked, in which the models deliver the probability of belonging to one of these three new classes (good payers, 'Can't Pays', and 'Won't Pays'). The process improves classification accuracy significantly, and delivers strong insights regarding the behaviour of defaulters.",
    "actual_venue": "Journal Of The Operational Research Society"
  },
  {
    "abstract": "•SYBA is built on the basis of the compressed sensing theory.•The descriptor is robust, simple and computationally efficient.•Evaluated the descriptor performance statistically on BYU feature matching dataset.",
    "actual_venue": "Computer Vision And Image Understanding"
  },
  {
    "abstract": "The prototype of a content based search engine for mathematical knowledge supporting a small set of queries requiring matching and/or typing operations is described. The prototype — called Whelp — exploits a metadata approach for indexing the information that looks far more flexible than traditional indexing techniques for structured expressions like substitution, discrimination, or context trees. The prototype has been instantiated to the standard library of the Coq proof assistant extended with many user contributions.",
    "actual_venue": "Types"
  },
  {
    "abstract": "We consider markets in which firms offer supply functions, rather than a quantity or price alone: the most important examples are wholesale electricity markets. The equilibria in such markets can be hard to characterize. In many cases, whole families of supply function equilibria occur so there are difficulties in determining which equilibrium will be chosen. In this paper, we consider supply function equilibria, when firms hold forward contracts, which is common in electricity markets. Under the assumption that contract positions have been fixed in advance, we characterize the families of supply function equilibria in a duopoly. The existence of forward contracts implies a tightening of the conditions for an equilibrium, and a greater likelihood that no equilibrium solution exists. In the case of three firms, there can be at most one supply function equilibrium, provided that the lowest demand be small enough.",
    "actual_venue": "J Optimization Theory And Applications"
  },
  {
    "abstract": "Data hiding is an important technique in multimedia security. Multimedia data hiding techniques enable message senders to disguise secret data by embedding them into cover media. Thus, delivering secret messages is as easy as sending the cover media. Recently, many researchers have studied reversible data hiding for images. Those methods can reconstruct the original cover image and extract the embedded secret data from a stego-image. This study proposes a novel reversible steganographic method of embedding secret data into a vector quantization (VQ) compressed image by applying the concept of side match. The proposed method uses extra information, namely the hit pattern, to achieve reversibility. Moreover, its small hit pattern enables the embedding of the entire hit pattern along with the secret data in most cases. To optimize visual quality of the output stego-image, the method applies the concept of partitioned codebooks (state codebooks). The partition operation on the codebook uses a look-up table to minimize embedding and extraction time. We also propose the use of diagonal seed blocks to embed the entire hit pattern into the cover image without producing any extra control messages. Compared to the Chang and Lin method, the experimental results show that the proposed method has higher capacity, better visual quality, and shorter execution time.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Improving architectural energy efficiency is important to address diminishing energy efficiency gains from technology scaling. At the same time, limiting hardware complexity is also important. This paper presents a new processor architecture, the idempotent processor architecture, that advances both of these directions by presenting a new execution paradigm that allows speculative execution without the need for hardware checkpoints to recover from mis-speculation, instead using only re-execution to recover. Idempotent processors execute programs as a sequence of compiler-constructed idempotent (re-executable) regions. The nature of these regions allows precise state to be reproduced by re-execution, obviating the need for hardware recovery support. We build upon the insight that programs naturally decompose into a series of idempotent regions and that these regions can be large. The paradigm of executing idempotent regions, which we call idempotent processing, can be used to support various types of speculation, including branch prediction, dependence prediction, or execution in the presence of hardware faults or exceptions. In this paper, we demonstrate how idempotent processing simplifies the design of in-order processors. Conventional in-order processors suffer from significant complexities to achieve high performance while supporting the execution of variable latency instructions and enforcing precise exceptions. Idempotent processing eliminates much of these complexities and the resulting inefficiencies by allowing instructions to retire out of order with support for re-execution when necessary to recover precise state. Across a diverse set of benchmark suites, our quantitative results show that we obtain a geometric mean performance increase of 4.4% (up to 25% and beyond) while maintaining an overall reduction in power and hardware complexity.",
    "actual_venue": "Micro"
  },
  {
    "abstract": "In this study, we investigated the effect of avatar design on the perception of sign language animation display. Signed sentence expressions were comparatively evaluated using a natural and an anime-style avatar model of three different clothing colors and patterns to determine each design parameter's influence on comprehensibility, naturalness and user affinity. Results show that plain settled color clothing was perceived as most favorable, and that the natural avatar was clearly preferred over the anime-style avatar as an informant of the signed sentence content.",
    "actual_venue": "Acm International Conference On Intelligent Virtual Agents"
  },
  {
    "abstract": "The reception of multimedia applications often depends on the quality of processed and displayed visual content. This is the main reason for the development of automatic image quality assessment (IQA) techniques which try to mimic properties of human visual system and produce objective scores for evaluated images. Most of them require a training step in which subjective scores, obtained in tests with human subjects, are used for parameters tuning. In this paper, it is shown that pairwise score differences (PSD) can be successfully used for training a full-reference hybrid IQA measure based on the least absolute shrinkage and selection operator (lasso) regression. The results of extensive experimental evaluation on four largest IQA benchmarks show that the proposed IQA technique is statistically better than its version trained using raw scores, and both approaches are statistically better than state-of-the-art full-reference IQA measures. They are also better than other hybrid approaches. In the paper, the evaluation protocol is extended with tests using PSD.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "This paper proposes two passive reference methods of position finding for an automatic guidance system of a ground vehicle using laser beam and corner cube(retro-reflector). One is a positioning method for a vehicle guided along a flexible and arbitrary path. This system is composed of corner cubes set on pre-determined three reference points of ground and a laser beam scanned on a vehicle. Another is a positioning method for a vehicle (automobile) guided within a traffic lane on roadway, which is used for a compensations of the positional errors caused by a self- contained measuring system. This method is realized by a chain of corner cube pairs set on both sides of roadway and four laser beams installed on a vehicle. The experimental results using our prototype system and the simulation results are shown with some discussions about the feasibility of real system using our methods.",
    "actual_venue": "Icra"
  },
  {
    "abstract": "This paper describes a novel approach to the problem of reaching an object in space under visual guidance, The approach is characterized by a great robustness to calibration errors, such that virtually no calibration is required. Servoing is based on binocular vision: a continuous measure of the end-effector motion field, derived from real-time computation of the binocular optical flow over the stereo images, is compared with the actual position of the target and the relative error in the end-effector trajectory is continuously corrected, The paper outlines the general framework of the approach, shows how visual measures are obtained and discusses the synthesis of the controller along with its stability analysis, Real-time experiments are presented to show the applicability of the approach in real 3-D applications.",
    "actual_venue": "Ieee Transactions On Robotics And Automation"
  },
  {
    "abstract": "Distinctiveness of users in biometric authentication can be limited when multiple samples of a user's biometric information differ due to intra-class variability in the acquisition, thus resulting in random correspondence between users. Most of the currently known methods based on error correcting codes, fuzzy vault have been proposed for protecting biometric data against the intra-class variability. These methods require a binary representation from the real-valued biometric data to measure the security by a discrete model. In this work, we analytically formulate probability of random correspondence (PRC) for biometrics by developing a discrete noisy source model using statistics of real-valued features that helps us to propose error exponent of biometrics. The quantized template of real-valued features represented by Ns bits inherently has t error bits occurring due to the intra-class variability and modeled by a binary symmetric noise channel. The values of t, error probability of binary symmetric channel and information rate R for the discrete noisy source are combined in the framework of error exponents to develop analytic expression for PRC of biometrics. We illustrate our approach with simulations, using available data and approximations, to validate the analytic expression of PRC for palmprint and iris biometrics.",
    "actual_venue": "Coimbatore"
  },
  {
    "abstract": "Security considerations play an increasingly important role for distributed computing. In today's Internet age, academia requires sharing, distributing, merging, changing information, linking applications and other resources within and among universities and other related organizations. Because e-learning systems are open, distributed and interconnected, then security becomes an important challenge in order to insure that interested actors only have access to the right information at the appropriate time. The purpose of this paper is to give an in-depth understanding of most important security challenges that can be relevant for distributed e-learning systems.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "We present a study of the fluid–structure interaction in an idealized end-to-end anastomosis of a vascular bypass-graft and an artery. Special attention is paid to the impact of geometric imperfections in the artery and the flow path of the upstream vessel segment on the hemodynamics. A partitioned solution approach is applied and developed further to solve the coupled problem in an implicit manner. To stabilize and accelerate the convergence of the staggered coupling iterations, an interface quasi-Newton least squares method is applied. While the finite volume method is used for the fluid mechanics subproblem, high-order finite elements serve to discretize the structural subproblem. A convergence study shows the efficiency of the high-order elements in the context of nearly incompressible, anisotropic materials used to model circular and irregular-shaped segments of an artery. The fluid–structure interaction simulations reveal a dominant influence of the upstream vessel’s curvature, which, however, decays rapidly in straight sections where the influence of geometric imperfections is dominant. Based on the proposed simulation approach, hemodynamic parameters such as the oscillating shear index can be directly linked to the shape and the intensity of the imperfections.",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "If we hope to provide an Internet of Things that's useful, we must understand users' privacy expectations of their smart devices and the environment in which they operate. Samsung's Smart TV situation throws that into clear relief.",
    "actual_venue": "Ieee Security And Privacy"
  },
  {
    "abstract": "In this paper, we propose Modulobe: a creation and sharing platform for articulated 3D models with complex motion. Modulobe has two components: a 3D modeling application and a model sharing web site. The former is intended to achieve complex motion simulation of a 3D model and provide a user-friendly interface. Motion pattern of angle for each hinge can be set easily by specifying pattern on a graph. The latter has a feature to show uploaded models on bases of tags, comments and popularity which is designed to stimulate users to create new models. This paper presents design concept of Modulobe and show the user interface and model sharing site. We are delivering Modulobe application and opened the model sharing site for about two years. Modulobe application had been downloaded about 170,000 times and the web site had been accessed 686 million times from various sites whose unique IP is about 100,000 variations. More than 3,000 models were uploaded.",
    "actual_venue": "Advances In Computer Entertainment Technology"
  },
  {
    "abstract": "The freshness of web page indices is the key to improving searching quality of search engines. In Baidu, the major search engine in China, we have developed DirectLoad, an index updating system for efficiently delivering the webscale indices to nationwide data centers. However, the web-scale index updating suffers from increasingly high data volumes during network transmission and inefficient I/O transactions due to slow disk operations. DirectLoad accelerates the index updating streams from two aspects: 1) DirectLoad effectively cuts down the overwhelmingly high volume of indices in transmission by removing the redundant data across versions, and mutates regular operations in a key-value storage system for successful accesses to the deduplicated datasets. 2) DirectLoad significantly improves the I/O efficiency by replacing the LSMTree with a memory-resident table (memtable) and appendingonly-files (AOFs) on disk. Specifically, the write amplification stemming from sorting operations on disk is eliminated, and a lazy garbage collection policy further improves the I/O performance at the software level. In addition, DirectLoad directly manipulates the SSD native interfaces to remove the write amplification at the hardware level. In practice, 63% updating bandwidth has been saved due to the deduplication, and the write throughput to SSDs is increased by 3x. The index updating cycle of our production workloads has been compressed from 15 days to 3 days after deploying DirectLoad. In this paper, we show the effectiveness and efficiency of an in-memory index updating system, which is disruptive to the framework in a conventional memory hierarchy. We hope that this work contributes a strong case study in the system research literature.",
    "actual_venue": "Ieee International Conference On Data Engineering"
  },
  {
    "abstract": "This paper evaluates the application of minimum classification error (MCE) training to online-handwritten text recognition based on hidden Markov models. We describe an allograph-based, character level MCE training aimed at minimizing the character error rate while enabling flexibility in writing style. Experiments on a writer-independent discrete character recognition task, covering all alpha-numerical characters and keyboard symbols, show that MCE achieves more than 30% character error rate reduction compared to the baseline maximum likelihood-based system.",
    "actual_venue": "Icassp ) Ieee International Conference"
  },
  {
    "abstract": "Optimal decentralized decision making in a team of cooperative agents as formalized by decentralized POMDPs is a notoriously hard problem. A major obstacle is that the agents do not have access to a sufficient statistic during execution, which means that they need to base their actions on their histories of observations. A consequence is that even during off-line planning the choice of decision rules for different stages is tightly interwoven: decisions of earlier stages affect how to act optimally at later stages, and the optimal value function for a stage is known to have a dependence on the decisions made up to that point. This paper makes a contribution to the theory of decentralized POMDPs by showing how this dependence on the 'past joint policy' can be replaced by a sufficient statistic. These results are extended to the case of k-step delayed communication. The paper investigates the practical implications, as well as the effectiveness of a new pruning technique for MAA* methods, in a number of benchmark problems and discusses future avenues of research opened by these contributions.",
    "actual_venue": "Ijcai"
  },
  {
    "abstract": "With over a billion vehicles on the road of the world traffic and traffic related issues are becoming extremely important topics of research. The identification of dangerous sites can help with managing the traffic and introduction of new policies, but sometimes lack of detailed data can preclude the use of sophisticated methods. In this paper we use probe car taxi data and retrospective accident data for evaluation of road danger in the city of Sapporo in winter time. We discuss the relationship between weather and traffic accidents and the difficulty of predicting the location of new accidents from available retrospective data alone. Then we show a correlation between different types of traffic, obtained by clustering probe car data, and the accident rates. We propose a method for estimating the danger levels of road segments in a broad area of the city based on the traffic data. Finally we give an evaluation of the proposed method and discuss improvements that can be made in the future.",
    "actual_venue": "Cyber-Enabled Distributed Computing And Knowledge Discovery"
  },
  {
    "abstract": "The ability to combine sensory information Is an important attribute of the brain. Multisensory integration in natural systems suggests that a similar approach in artificial systems may be important. Multisensory integration is exemplified in mammals by the superior colliculus (SC), which combines visual, auditory and somatosensory stimuli to shift gaze. However, although we have a good understanding of the overall architecture of the SC, as yet we do not fully understand the process of integration. While a number of computational models of the SC have been developed, there has not been a larger scale implementation that can help determine how the senses are aligned and integrated across the superficial and deep layers of the SC. In this paper we describe a prototype implementation of the mammalian SC consisting of self-organizing maps linked by Hebbian connections, modeling visual and auditory processing in the superficial and deep layers. The model is trained on artificial auditory and visual stimuli, with testing demonstrating the formation of appropriate spatial representations, which compare well with biological data. Subsequently, we train the model on multisensory stimuli, testing to see if the unisensory maps can be combined. The results show the successful alignment of sensory maps to form a multisensory representation. We conclude that, while simple, the model lends itself to further exploration of integration, which may give insight into whether such modeling is of benefit computationally.",
    "actual_venue": "Ieee International Joint Conference On Neural Networks, Vols"
  },
  {
    "abstract": "A major issue in software maintenance is . A software engineer should be able to assess the impact of a change in a software system, so that the effort to accomplish the maintenance may be properly estimated. We define a novel model, named K3B, for estimating change propagation impact. The model aims to predict how far a set of changes will propagate throughout the system. K3B is a stochastic model that has input parameters about the system and the number of modules which will be initially changed. K3B returns the estimated number of , considering that a module may be changed more than once during a modification process. We provide the implementation of K3B for object-oriented programs. We compare our implementation with data from an artificial scenario, given by simulation, as well as with data from a real scenario, given by historical data. We found strong correlation between the results given by K3B and the results observed in the simulation, as well as with historical data of change propagation. K3B may be used for comparing software systems from the viewpoint of change impact. The model may aid software engineers in allocating proper resources to the maintenance tasks.",
    "actual_venue": "Software Quality Journal"
  },
  {
    "abstract": "Abstract Many parallel applications from scientific computing,use co llective MPI communication,operations to distribute or collect data. The execution time of c ollective MPI communication,operations can be significantly reduced by a restructuring based on orthogona l processor structures or by using specific point-topoint algorithms based on virtual communication,topologies. The performance,improvement,depends strongly on numerous factors, like the collective MPI communication operation, the specific group layout, the message size, the specific MPI library, and the architecture paramet ers of the parallel target platform. In this paper we describe an adaptive approach to determine and select a specific processor group layout or communication algorithm for the realization of collective communication operations with the objective of minimizing the communication,overhead. In the case that a communication,method is faster than the original implementation of the collective MPI communication operation, the specific communication method is applied to perform the",
    "actual_venue": "Parco"
  },
  {
    "abstract": "Learning from interactions between agents is a key component for inference in multiagent systems. Depending on the downstream task, there could be multiple criteria for evaluating the generalization performance of learning. In this work, we propose a novel framework for evaluating generalization in multiagent systems based on agent-interaction graphs. An agent-interaction graph models agents as nodes and interactions as hyper-edges between participating agents. Using this abstract data structure, we define three notions of generalization for principled evaluation of learning in multiagent systems.",
    "actual_venue": "Proceedings Of The International Conference On Autonomous Agents And Multiagent Systems"
  },
  {
    "abstract": "This paper deals with numerical methods for solving unilateral contact problems with friction. Although these problems are usually defined in terms of the displacement, a stress based approach to the problem is developed here. The ''equilibrium'' finite elements method is therefore used. Using these elements make it possible to satisfy the local equilibrium condition a priori, but on the other hand, prescribed and contact forces have to be introduced using Lagrangian multipliers. The problem obtained is therefore a non-linear, constrained problem and the global system matrix is non-positive definite. Various solution algorithms are thus proposed and compared. Comparisons between the classical method and that developed here show that the stress formulation gives very satisfactory results in terms of the stresses.",
    "actual_venue": "Advances In Engineering Software"
  },
  {
    "abstract": "TPC-Energy specification augments the existing TPC Benchmarks with Energy Metrics. The TPC-Energy specification is designed to help hardware buyers identify energy efficient equipment that meets both their computational and budgetary requirements. In this paper we discuss our experience is publishing industry's first-ever TPC-Energy metric publication.",
    "actual_venue": "Tpctc"
  },
  {
    "abstract": "Understanding the correct self-posture is known to improve the performance of motor skills in sports, dance, ballet, walking, and running. However, it is difficult to understand the accurate self-posture and move one's body as intended for imitation or learning, especially in-situation. Based on previous physiological research, we herein propose the addition of a new assisting viewpoint to the human body interface for understanding the correct self-posture without interrupting the training process. This new viewpoint enables the users to see their current posture as a three-dimensional skeletal image that is an avatar which synchronizes with the owner's movements. The Optical See-Through Head Mounted Display (OST-HMD) and Full-Body Motion Capture (MoCap) are used for creating the proposed viewpoint. The position and the angle of view of an avatar is determined by the rotation of the user's head interactively. Moreover, visualizing the avatar's trail helps users to understand how to moved their own body correctly. We conducted several experiments to confirm the avatar system's validity and the availability for an expert athlete. Our results showed that the proposed viewpoint was successful in enabling the user to understand the accurate self-posture in-situation without occurring interruption.",
    "actual_venue": "Proceedings Of The International Acm In-Cooperation Hci And Ux Conference"
  },
  {
    "abstract": "AbstractThe paper reviews certain integral equation approaches and related numerical methods used in studies of biomedical applications of electromagnetic fields pertaining to transcranial magnetic stimulation TMS and nerve fiber stimulation. TMS is analyzed by solving the set of coupled surface integral equations SIEs, while the numerical solution of governing equations is carried out via Method of Moments MoM scheme. A myelinated nerve fiber, stimulated by a current source, is represented by a straight thin wire antenna. The model is based on the corresponding homogeneous Pocklington integro-differential equation solved by means of the Galerkin Bubnov Indirect Boundary Element Method GB-IBEM. Some illustrative numerical results for the TMS induced fields and intracellular current distribution along the myelinated nerve fiber active and passive, respectively, are presented in the paper.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "Using wireless communication networks, information between moving systems can be exchanged if their relative position requires a coordination of their control. The most important question to be answered is: Which information links are suitable and which improvements of the control performance can be achieved with the help of exchanged information? This question will be answered for systems consisting of serially coupled identical subsystems for which the control objectives can be satisfied with only local information if the subsystems are far away from each other while cooperation between the controllers is necessary to achieve the control objectives if the subsystems are close together.",
    "actual_venue": "Automatisierungstechnik"
  },
  {
    "abstract": "Two state of charge estimation methods using fractional order extended and unscented Kalman filter and a nonlinear variable fractional order battery model are implemented. Both, battery model and Kalman filters are evaluated and compared using measurements of an actual lithium-ion polymer battery cell. The observability of the battery model and the influence of an initialization function on the estimation algorithms is investigated.",
    "actual_venue": "Annual American Control Conference"
  },
  {
    "abstract": "Pathfinding is becoming more and more common in autonomous vehicle navigation, robot localization, and other computer vision applications. In this paper, a novel approach to mapping and localization is presented that extracts visual landmarks from a robot dataset acquired by a Kinect sensor. The visual landmarks are detected and recognized using the improved scale-invariant feature transform (I-SIFT) method. The methodology is based on detecting stable and invariant landmarks in consecutive (red-green-blue depth) RGB-D frames of the robot dataset. These landmarks are then used to determine the robot path, and a map is constructed by using the visual landmarks. A number of experiments were performed on various datasets in an indoor environment. The proposed method performs efficient landmark detection in various environments, which includes changes in rotation and illumination. The experimental results show that the proposed method can solve the simultaneous localization and mapping (SLAM) problem using stable visual landmarks, but with less computation time.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Rate control (RC) is crucial in controlling compression bitrates and encoding qualities for networked video applications. In this paper, we propose a direct non-buffer real-time rate control algorithm for video encoding, which has two unique features. First, unlike traditional algorithms which adopt buffers in rate control, the proposed algorithm does not use a buffer in rate regulation which can reduce the delay and improve real-time response. Second, we propose a new Proportional-Integral-Derivative (PID) bit controller to directly control encoding bitrates. In addition, we also develop a simple but effective method for real-time target bit allocation. To the best of our knowledge, this is the first work that conducts video rate control without using a buffer. Our extensive experimental results have demonstrated that the proposed algorithm outperforms the MPEG-4 rate control algorithm by achieving more accurate rate regulation and improving overall coding quality.",
    "actual_venue": "Multimedia Tools And Applications"
  },
  {
    "abstract": "Distributed applications should be able to make use of an object group service in a number of application specific ways. Three main modes of interactions can be identified: (i) request-reply: a client issues a request to multiple servers and waits for their replies; this represents a commonly occurring scenario when a service is replicated; (ii) group-to-group request-reply: a generalization of the previous case, where clients are themselves groups; and (iii) Peer Participation: here all the members are regularly multicasting messages (asynchronous invocation); this represents a commonly occurring scenario when the purpose of an application is to share information between members, (e.g., a teleconferencing application). Customization within each class of interaction is frequently required for obtaining better performance. This paper describes the design and implementation of a flexible CORBA object group service that supports the three types of interactions and enables application specific customization. Performance figures collected over low latency LAN and high latency WAN are presented to support the case for flexibility.",
    "actual_venue": "DSN"
  },
  {
    "abstract": "A collection of networks is considered a network ensemble if its members originate from a common natural or technical process such as repeated measurements, replication and mutation, or massive parallelism, possibly under varying conditions. We propose a spectral approach to identify structural trends, i.e. prevalent patterns of connectivity, in an ensemble by delineating classes of networks with similar role structure. Formal, experimental, and practical evidence of its potential is given.",
    "actual_venue": "Studies In Computational Intelligence"
  },
  {
    "abstract": "AbstractParcel services route vehicles to pick up parcels in the service area. Pickup requests occur dynamically during the day and are unknown before their actual request. Because of working hour restrictions, service vehicles only have a limited time to serve dynamic requests. As a result, not all requests can be confirmed. To achieve an overall high number of confirmed requests, dispatchers have to budget their time effectively by anticipating future requests. To determine the value of a decision, i.e., the expected number of future confirmations given a point of time and remaining free time budget, we present an anticipatory time budgeting heuristic ATB drawing on methods of approximate dynamic programming. ATB frequently simulates a problem's realization to subsequently approximate the values for every vector of point of time and free time budget to achieve an approximation of an optimal decision policy. Since the number of vectors is vast, we introduce the dynamic lookup table DLT, a general approach adaptively partitioning the vector space to the approximation process. Compared with state-of-the-art benchmark heuristics, ATB allows an effective use of the time budget resulting in anticipatory decision making and high solution quality. Additionally, the DLT significantly strengthens and accelerates the approximation process.The online appendix is available at https://doi.org/10.1287/trsc.2016.0719.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "In this paper, a comparison of the Choquet and Sugeno integrals is presented. The proposed methods enable the calculation of the Choquet and Sugeno integrals for combining multiple source of information with a degree of uncertainty. The methods are used to combine the modules output of a modular neural network for face recognition. In this paper, the focus is on aggregation operators that use measures as inputs, in particular the Choquet and Sugeno integrals. Recognition results with the Choquet integral are better or comparable to results produced by Sugeno integral.",
    "actual_venue": "Ieee International Conference On Fuzzy Systems"
  },
  {
    "abstract": "We consider the problem of reasoning about the probability of assertion violations in straight-line, nonlinear computations involving uncertain quantities modeled as random variables. Such computations are quite common in many areas such as cyber-physical systems and numerical computation. Our approach extends probabilistic affine forms, an interval-based calculus for precisely tracking how the distribution of a given program variable depends on uncertain inputs modeled as noise symbols. We extend probabilistic affine forms using the precise tracking of dependencies between noise symbols combined with the expectations and higher order moments of the noise symbols. Next, we show how to prove bounds on the probabilities that program variables take on specific values by using concentration of measure inequalities. Thus, we enable a new approach to this problem that explicitly avoids subdividing the domain of inputs, as is commonly done in the related work. We illustrate the approach in this paper on a variety of challenging benchmark examples, and thus study its applicability to uncertainty propagation.",
    "actual_venue": "Tacas"
  },
  {
    "abstract": "In this paper, we provide a polynomial-time algorithm for solving an important class of metric temporal problems that involve simple temporal constraints between various events (variables) and piecewise constant preference functions over variable domains. We are given a graph G = (χ, ε) where χ = {X0, X1... Xn} is a set of events (X0 is the \"beginning of the world\" node and is set to 0 by convention) and e = (Xi, Xj) ∈ ε. annotated with the bounds [LB(e), UB(e)], is a simple temporal constraint between Xi and Xj indicating that Xj must be scheduled between LB(e) and UB(e) seconds after Xi is scheduled (LB(e) ≤ UB(e)). A family of stepwise constant preference functions F = {fxi (t) : R → R} specifies the preference attached with scheduling Xi at time t. The goal is to find a schedule for all the events such that all the temporal constraints are satisfied and the sum of the preferences is maximized. Our polynomial-time algorithm for solving such problems (which we refer to as extended simple temporal problems (ESTPs)) has important consequences in dealing with limited forms of disjunctions and preferences in metric temporal reasoning that would otherwise require an exponential search space.",
    "actual_venue": "Aaai"
  },
  {
    "abstract": "AbstractIn this paper, we investigate the outage and error performances of a full duplex relaying cooperative system in a multicell environment, considering the possibility of co-channel interference from the users located in the nearby cells. The decode-and-forward cooperative strategy is assumed at the relay. Throughout the analysis, we consider all the channels to be independent but not necessarily identically distributed as generalised Nakagami-m fading. We find an accurate closed-form expression of outage probability and average symbol error rate for integer values of fading parameter m. The analysis considers symmetric M-ary phase shift keying modulation scheme with equal energy constellation. Copyright © 2014 John Wiley & Sons, Ltd.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "Strong illumination variation is a key challenge in the Human Epithelial Type 2 (HEp-2) cell classification task. Aiming to improve the robustness of the HEp-2 classification system to the illumination variation, this paper deeply explores discriminative and illumination robust descriptors. Specifically, we propose a novel Spatial Shape Index Descriptor (SSID) to capture spatial layout information of the second-order structures, and utilize a Local Orientation Adaptive Descriptor (LOAD), which was originally designed for texture classification, to the HEp-2 cell classification task. Both SSID and LOAD show strong discrimination and great complementarity to each other.Four different sets of experiments were carried out to evaluate SSID, LOAD and their combination. Our two submissions achieved superior performance on the new Executable Thematic of Pattern Recognition Techniques for Indirect Immunofluorescence images analysis. Compared to the rank 1st method in the ICPR 2014 HEp-2 cell classification contest, both of our submissions achieved a better performance when only using the provided training data. Our approaches also demonstrated superior performance on a newly compiled large-scale HEp-2 data set with 63,445 cell images. HighlightsTwo introduced features achieve superior performance on the HEp-2 cell classification task.We propose a novel Spatial Shape Index Descriptor (SSID) to capture spatial layout of the second-order texture structures.We utilize a multi-scale Local Orientation Adaptive Descriptor (LOAD) to the HEp-2 cell classification task.We introduce a new large-scale HEp-2 data set that contains 63,445 cell images from the I3A Task-2 data set.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "Lines provide important information in images, and line detection is crucial in many applications. However, most of the existing algorithms focus only on the extraction of line positions, ignoring line thickness. This paper presents a novel wide line detector using an isotropic nonlinear filter. Unlike most existing edge and line detectors which use directional derivatives, our proposed wide line detector applies a nonlinear filter to extract a line completely without any derivative. The detector is based on the isotropic responses via circular masks. A general scheme for the analysis of the robustness of the proposed wide line detector is introduced and the dynamic selection of parameters is developed. In addition, this paper investigates the relationship between the size of circular masks and the width of detected lines. A sequence of tests has been conducted on a variety of image samples and our experimental results demonstrate the feasibility and effectiveness of the proposed method.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "One of the biggest obstacles in the wide-spread industrial take-up of Grid technology is the existence of a large amount of legacy code that is not accessible as Grid services. The paper describes a new approach (GEMLCA: Grid Execution Management for Legacy Code Architecture) to deploy legacy codes as Grid services without modifying the original code. Moreover, we show a workflow execution oriented Grid portal technology (P-GRADE portal) by which such legacy code based Grid services can be applied in complex business processes. GEMLCA has been implemented as GT-3 services but can be easily ported into the new WSRF Grid standards.",
    "actual_venue": "Grid"
  },
  {
    "abstract": "Based on the structure of general pan-multiplication operators and the theory of first order linear partial differential equations, we study the structure of pan-addition operators with pre-determined pan-multiplication operators. At first, we describe the structure of pan-addition operators under the ordinary multiplication. Then, with this structure in place, we derive the general representation for the structure of pan-addition operators.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "The cluster architecture has played an important role in high-end computing for the past 20 years. With the advent of Internet services, big data, and cloud computing, traditional clusters face three challenges: 1) providing flexible system balance among computing, memory, and I/O capabilities; 2) reducing resource pooling overheads; and 3) addressing low performance-power efficiency. This position paper proposes a software-defined cluster (SDC) architecture to deal with these challenges. The SDC architecture inherits two features of traditional cluster: its architecture is multicomputer and it has loosely-coupled interconnect. SDC provides two new mechanisms: global I/O space (GIO) and hardware-supported native access (HNA) to remote devices. Application software can define a virtual cluster best suited to its needs from resources pools provided by a physical cluster, and traditional cluster ecosystems need no modification. We also discuss a prototype design and implementation of a 32-processor cloud server utilizing the SDC architecture.",
    "actual_venue": "J Comput Sci Technol"
  },
  {
    "abstract": "We present a logic-based framework that is able to model semantic e-services and to verify some of properties supporting the design and maintenance of cooperative information systems. This framework is based upon a formal foundation of the Semantic Web , as the Description Logic family, that provides an expressive specification language, allowing for complex application domains. We adopt the well-known IOPE (Input, Output, Preconditions, and Effects) paradigm for the description of e-service contracts, providing a suitable operational semantics and we are able to reason about update effects also in case of under-specified e-services, using a repair-based approach. On this base, we firstly define some basic consistency and correctness properties, and then we characterize the adequacy of an e-service to achieve a user goal as foundational task in service discovery. We present decidable checking procedures for the devised properties using a reduction technique to First-Order Logic reasoning tasks, including an analysis in terms of computational complexity.",
    "actual_venue": "Ws-Fm"
  },
  {
    "abstract": "Recently, deep learning has been playing a central role in machine learning research and applications. Since AlexNet, increasingly more advanced networks have achieved state-of-the-art performance in computer vision, speech recognition, language processing, game playing, medical imaging, and so on. In our previous studies, we proposed quadratic/second-order neurons and deep quadratic neural networks. In a quadratic neuron, the inner product of a vector of data and the corresponding weights in a conventional neuron is replaced with a quadratic function. The resultant second-order neuron enjoys an enhanced expressive capability over the conventional neuron. However, how quadratic neurons improve the expressing capability of a deep quadratic network has not been studied up to now, preferably in relation to that of a conventional neural network. In this paper, we ask three basic questions regarding the expressive capability of a quadratic network: (1) for the one-hidden-layer network structure, is there any function that a quadratic network can approximate much more efficiently than a conventional network? (2) for the same multi-layer network structure, is there any function that can be expressed by a quadratic network but cannot be expressed with conventional neurons in the same structure? (3) Does a quadratic network give a new insight into universal approximation? Our main contributions are the three theorems shedding light upon these three questions and demonstrating the merits of a quadratic network in terms of expressive efficiency, unique capability, and compact architecture respectively.",
    "actual_venue": "Arxiv: Learning"
  },
  {
    "abstract": "We study an optimization-based approach to construct a mean-reverting portfolio of assets. Our objectives are threefold: (1) design a portfolio that is well-represented by an Ornstein-Uhlenbeck process with parameters estimated by maximum likelihood, (2) select portfolios with desirable characteristics of high mean reversion, and (3) select a parsimonious portfolio, i.e. find a small subset of a larger universe of assets that can be used for long and short positions. We present the full problem formulation, a specialized algorithm that exploits partial minimization, and numerical examples using both simulated and empirical price data.",
    "actual_venue": "Ieee Conference On Decision And Control"
  },
  {
    "abstract": "In previous work [10], we have proposed a new signal representation for audio coding, where the signal is decomposed in a union of MDCT bases using matching pursuit. The resulting coder gave better performance than a transform coder at low bitrates but slightly worse at high bitrates. In this paper, we propose an adaptive matching pursuit algorithm that in the first iterations decomposes the signal into the redundant union of MDCT bases, and then, when the residual energy decay becomes too low, switches to an orthogonal basis (one of the MDCT bases). We investigate simple strategies to determine in which iteration switching is near-optimal in terms of rate-distortion. We present in this paper a prototype audio coder based on this algorithm, that reaches the performance of the previous approach at low bitrates and the one of transform coding at high bit rates.",
    "actual_venue": "Lausanne"
  },
  {
    "abstract": "Abstract Organizations have recognized the need for high quality information and academics have  proposed several methods to measure and improve information quality One such method is to  manage information as an information product (IP) Many modeling methods for information  manufacturing systems have been described Almost all of these lack the ability to systematically  represent the manufacturing processes and are deficient in constructs offered to explicitly  represent manufacturing details In this paper we propose the information product map (IP - MAP)  as a method to systematically model the manufacture of an IP The IP - MAP is an extension of  the Information Manufacturing System (IMS) proposed earlier It offers several advantages  including the ability to visualize the manufacture, implement continuous improvement and  quality - at - source, and measure the quality of the IP using appropriate quality dimensions We  submit that the IP - MAP would serve as a foundation upon which a suite of quality dimensions  may be identified and implemented for managing the IP quality",
    "actual_venue": "IQ"
  },
  {
    "abstract": "We consider opportunistic spectrum access for secondary users over multiple channels whose occupancy by primary users is modeled as discrete-time Markov processes. Due to hardware limitations and energy constraints, a secondary user can choose, in each slot, one channel to sense and decide whether to access based on the sensing outcome. The design of sensing strategies that govern channel selections in each slot for optimal throughput performance of the secondary user can be formulated as a partially observable Markov decision process (POMDP). We exploit the structure of this problem when channels are independently and identically distributed. We reveal that the myopic sensing policy has a simple structure: channel selection is reduced to a counting process with little complexity. Further, for the two-channel case, we prove that the myopic sensing policy is in fact the optimal policy. Numerical results have also demonstrated the optimality of the myopic sensing policy when there are more than two channels.",
    "actual_venue": "ICC"
  },
  {
    "abstract": "Social networks are often modelled by graphs, with nodes representing individuals, and positively weighted edges representing the strength of the relationships between them. Working directly with such graphs is difficult, and it has become common to use spectral techniques that embed graphs in a geometry, and then work with the geometry instead. In a good embedding, edges that are heavily (positively) weighted, and so represent strong interactions, cause the vertices they connect to be embedded close to one another. However, in some social networks there are also antagonistic relationships that are naturally represented by negatively weighted edges. The resulting graphs are called signed graphs. Clearly an embedding of such a signed graph should place nodes connected by positively weighted edges close together, but nodes connected by negatively weighted edges far apart. Existing spectral techniques to embed signed graphs have serious drawbacks. We derive two normalized spectral analysis methods for signed graphs and show, using realworld data, that they produce robust embeddings.",
    "actual_venue": "SDM"
  },
  {
    "abstract": "We present a novel approach for interactive content-aware image resizing. The resizing is performed on warping a triangular mesh over the image, which captures the image saliency information as well as the underlying image features. The warped triangular mesh and the horizontal and vertical scales of all triangles are simultaneously obtained by a quadratic optimization which can be achieved by solving a sparse linear system. Our approach can preserve the shapes of curved features in the resized images. The resizing operation can be performed in an interactive rate which makes the proposed approach practically useful for realtime image resizing. To guarantee a foldover free resizing result, we modify the optimization to a standard quadratic programming. A number of experimental results have shown that our approach has obtained pleasing results and outperforms the previous approaches.",
    "actual_venue": "The Visual Computer"
  },
  {
    "abstract": "Traditionally, the robotic visual servoing/tracking problem has received attention from researchers for its interesting control and computer vision issues. However many visual servoing tasks also require the ability to automatically detect moving objects. Until recently, very few efforts have been reported in the area of automatic detection of servoing targets. This paper presents a robust detection scheme for use in robotic visual servoing experiments. It detects and tracks moving objects through the use of a “figure/ground” approach. Experimentation has shown the feasibility of this approach under general conditions. This paper provides a description of the authors system implementation in an experimental robotic system, along with a collection of results. The paper also contains a discussion of problems and issues for future work",
    "actual_venue": "Intelligent Robots And Systems Human Robot Interaction And Cooperative Robots, Proceedings Ieee/Rsj International Conference"
  },
  {
    "abstract": "Elliptic curve cryptosystems([19,25]) are based on the elliptic curve discrete logarithm problem (ECDLP). If elliptic curve cryptosystems avoid FR-reduction([11,17]) and anomalous elliptic curve over Fq ([34,3,36]), then with current knowledge we can construct elliptic curve cryptosystems over a smaller definition field. ECDLP has an interesting property that the security deeply depends on elliptic curve traces rather than definition fields, which does not occur in the case of the discrete logarithm problem (DLP). Therefore it is important to characterize elliptic curve traces explicitly from the security point of view. As for FR-reduction, supersingular elliptic curves or elliptic curve E/Fq with trace 2 have been reported to be vulnerable. However unfortunately these have been only results that characterize elliptic curve traces explicitly for FR- or MOV-reductions. More importantly, the secure trace against FR-reduction has not been reported at all. Elliptic curves with the secure trace means that the reduced extension degree is always higher than a certain level.In this paper, we aim at characterizing elliptic curve traces by FR-reduction and investigate explicit conditions of traces vulnerable or secure against FR-reduction. We show new explicit conditions of elliptic curve traces for FR-reduction. We also present algorithms to construct such elliptic curves, which have relation to famous number theory problems.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The fast development of the World Wide Web makes searching and retrieving of information to become a not easy task. Two algorithms proposed around the fall of 1996, Page Rank [3] and HITS [9], became the center of majority research efforts. They try to remedy the abundance of results, bringing order with the help of notions related to prestige in social network analysis. Some approaches use the link structure of the web to find the importance of the web pages (Page Rank method [3]) or to determine their authority related to a particular topic (Hub and Authority concept). In this paper we propose a new method for calculating the authority of a web page.",
    "actual_venue": "Atlanta, Ga, Usa"
  },
  {
    "abstract": "Multi-constrained routing is a key driver to support quality-of-service (QoS) for real-time multimedia applications in wireless mesh networks (WMNs). Due to the difficulty of applying strict admission control into a public WMN, it is inevitable to accommodate multiple application flows with different QoS requirements exceeding the capacity of a certain link shared by multiple flows. However, existing multi-constrained routing protocols under such an environment find the QoS degradation based on end-to-end path quality probing and trigger flooding-based route discovery from a scratch for resolving the QoS degradation, which incurs a longer recovery time and much routing overhead. In this paper, we propose a novel multi-constrained routing protocol for WMNs that finds problematic links that may affect QoS degradation to end-to-end paths and replaces them with a detour path using a local repair principle. We model congestion threshold estimation for finding problematic links and design algorithms for quickly finding detour paths and selecting an optimal path by minimizing the negative effect on existing flows nearby the detour path. Simulation results show that the proposed routing protocol achieves up to 19.6% more goodput of live video streaming applications with up to 33% reduced routing overhead compared with an existing work.",
    "actual_venue": "Wireless Communications And Mobile Computing"
  },
  {
    "abstract": "This forum is dedicated to personal health in all its many facets: decision-making, goal setting, celebration, discovery, reflection, coordination---even entertainment. We'll look at innovations in interactive technologies and how they help address current critical healthcare challenges. Elizabeth D. Mynatt, Editor",
    "actual_venue": "Interactions"
  },
  {
    "abstract": "We examine the problem of how to ensure behavioral consistency of an object-oriented systemafter its schema has been updated. The problem is viewed from the perspective of both thestrongly typed and the untyped language model. Solutions are compared in both models usingC++ and CLOS as examples.Keywords: Schema Evolution, Schema Transformations, Refactoring, Software Reuse, ObjectOrientedProgramming Languages.1 IntroductionSchema evolution and transformations have recently received...",
    "actual_venue": "Isotas"
  },
  {
    "abstract": "Skyline queries return a set of interesting data points that are not dominated on all dimensions by any other point. Most of the existing algorithms focus on skyline computation in centralized databases, and some of them can progressively return skyline points upon identification rather than all in a batch. Processing skyline queries over the Web is a more challenging task because in many Web applications, the target attributes are stored at different sites and can only be accessed through restricted external interfaces. In this paper, we develop PDS (progressive distributed skylining), a progressive algorithm that evaluates skyline queries efficiently in this setting. The algorithm is also able to estimate the percentage of skyline objects already retrieved, which is useful for users to monitor the progress of long running skyline queries. Our performance study shows that PDS is efficient and robust to different data distributions and achieves its progressive goal with a minimal overhead.",
    "actual_venue": "Data Knowl Eng"
  },
  {
    "abstract": "For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the “success” of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Our mining analysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.",
    "actual_venue": "Data Min Knowl Discov"
  },
  {
    "abstract": "In the early days of network and service management, researchers paid much attention to the design of management frameworks and protocols. Since then the focus of research has shifted from the development of management technologies towards the analysis of management data. From the five FCAPS areas, security of networks and services has become a key challenge. For example, brute-force attacks against Web applications, and compromises resulting thereof, are widespread. Talks with several Top-10 Web hosting companies in the Netherlands reflect that detection of these attacks is often done based on log file analysis on servers, or by deploying host-based intrusion detection systems (IDSs) and firewalls. However, such host-based solutions have several problems. In this paper we therefore investigate the feasibility of a network-based monitoring approach, which detects brute-force attacks against and compromises of Web applications, even in encrypted environments. Our approach is based on per-connection histograms of packet payload sizes in flow data that are exported using IPFIX. We validate our approach using datasets collected in the production network of a large Web hoster in the Netherlands.",
    "actual_venue": "J Network Syst Manage"
  },
  {
    "abstract": "To protect user privacy in the search engine context, most current approaches, such as private information retrieval and privacy preserving data mining, require a server-side deployment, thus users have little control over their data and privacy. In ...",
    "actual_venue": "CSE (3)"
  },
  {
    "abstract": "In this paper, we propose a modified version of the well-known Delay Looked Loop (DLL) timing synchronizer, which is specifically designed to take into account the characteristics of a class of receiver architectures that has been experiencing a renewed interest in the frame of Ultra Wide Band (UWB) transmission systems, i.e., the Differential Transmitted Reference (DTR) receiver. The proposed solution requires Average Channel State Information (A-CSI) for system and parameters optimization, but it is shown in this paper that it can efficiently work in typical multipath fading channels, where UWB systems are expected to operate. Simulation results will be also provided to show the performance of the proposed method.",
    "actual_venue": "EW"
  },
  {
    "abstract": "We propose a novel 3D space representation for multi-view video, using epipolar plane depth images (EPDI). Multi-view video plus depth (MVD) is used as common data format for FTV(Free-viewpoint TV), which enables synthesizing virtual view images. Due to the large amount of data and complexity of the multi-view video coding (MVC), compression of MVD is a challenging issue. We address this problem and propose a new representation that is constructed from MVD using ray-space. MVD is converted into image and depth ray-spaces. The proposed representation is obtained by converting each of ray-spaces into a global depth map and a global view using EPDI. Experiments demonstrate the analysis of this representation.",
    "actual_venue": "Picture Coding Symposium"
  },
  {
    "abstract": "In this paper, we describe how the IBM z14 processor, together with Db2 for z/OS Version 12, can improve data compression rates and thus reduce data storage requirements and cost for large databases. The new processor improves on the compression hardware accelerator available in earlier IBM Z generations by adding new hardware algorithms that increase the compression ratio and extend the applicabi...",
    "actual_venue": "Ibm Journal Of Research And Development"
  },
  {
    "abstract": "We implement CUILT, a scalable mix-and-match framework for Local Iterative Approximate Best-Response Algorithms for DCOPs, using the graph processing framework SIGNAL/COLLECT, where each agent is modeled as a vertex and communication pathways are represented as edges. Choosing this abstraction allows us to exploit the generic graph-oriented distribution/optimization heuristics and makes our proposed framework scalable, configurable, as well as extensible. We found that this approach allows us to scale to problems more than 3 orders of magnitude larger than results commonly published so far, to easily create hybrid algorithms by mixing components, and to run the algorithms fast, in a parallel fashion.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "Although several Engineering Change Order (ECO) routers had been proposed to obtain a routing solution based on different design objectives, mask re-spin cost still cannot be effectively reduced because the ECO routing problem is handled in a sequential manner. This paper presents a three-stage ECO routing flow which can simultaneously route all ECO nets while considering routing layer minimization. Experimental results demonstrate that our proposed ECO routing flow can effectively reduce the number of changed masks with only negligible wirelength and via overhead.",
    "actual_venue": "Asp-Dac"
  },
  {
    "abstract": "In this paper, we study the problem of state feedback stabilization of a linear time-invariant (LTI) discrete-time multi-input system with imperfect input channels. Each input channel is modeled in three different ways. First it is modeled as an ideal transmission system together with an additive norm bounded uncertainty, introducing a multiplicative uncertainty to the plant. Then it is modeled as an ideal transmission system together with a feedback norm bounded uncertainty, introducing a relative uncertainty to the plant. Finally it is modeled as an additive white Gaussian noise channel. For each of these models, we properly define the capacity of each channel whose sum yields the total capacity of all input channels. We aim at finding the least total channel capacity for stabilization. Different from the single-input case that is available in the literature and boils down to a typical H or H 2 optimal control problem, the multi-input case involves allocation of the total capacity among the input channels in addition to the design of the feedback controller. The overall process of channel resource allocation and the controller design can be considered as a case of channel-controller co-design which gives rise to modified nonconvex optimization problems. Surprisingly, the modified nonconvex optimization problems, though appear more complicated, can be solved analytically. The main results of this paper can be summarized into a universal theorem: The state feedback stabilization can be accomplished by the channel-controller co-design, if and only if the total input channel capacity is greater than the topological entropy of the open-loop system.",
    "actual_venue": "Ieee Trans Automat Contr"
  },
  {
    "abstract": "Modern hardware and software systems promote a view of parallel systems in which interprocessor communications are uniform and rather expensive in cost. Such systems demand efficient clustering algorithms that aggregate atomic tasks in a way that diminishes the impact of the high communication costs. We develop here a linear-time algorithm that optimally clusters computations that comprise a sequence of disjoint complete up- and/or down-sweeps on a complete binary tree for such parallel environments. Such computations include, for instance, those that implement broadcast, accumulation, and the parallel-prefix operator; such environments include, for instance, networks of workstations or BSP-based programming systems. The schedules produced by our clustering are optimal in the sense of having the exact minimum makespan驴not just an approximation thereof驴accounting for both computation and communication time. We show by simulation that the makespans of the schedules produced by our algorithm are close to half of those produced by the algorithm that yielded the best schedules previously known.",
    "actual_venue": "Ieee Trans Parallel Distrib Syst"
  },
  {
    "abstract": "In this paper, we discuss the effects of spatial simplicial meshes on the stability and the conditioning of fully discrete approximations of a parabolic equation using a general finite element discretization in space with explicit or implicit marching in time. Based on the new mesh dependent bounds on extreme eigenvalues of general finite element systems defined for simplicial meshes, we derive a new time step size condition for the explicit time integration schemes presented, which provides more precise dependence not only on mesh size but also on mesh shape. For the implicit time integration schemes, some explicit mesh-dependent estimates of the spectral condition number of the resulting linear systems are also established. Our results provide guidance to the studies of numerical stability for parabolic problems when using spatially unstructured adaptive and/or possibly anisotropic meshes.",
    "actual_venue": "Mathematics Of Computation"
  },
  {
    "abstract": "Autism spectrum disorder has become one of the most prevalent developmental disorders. A difficulty with communication is one of the main impairments. We are developing a digital library of images that will be used to help children with autism communicate without the need for reading or writing skills. Images will be displayed on Pocket PCs to convey messages. We are currently developing and evaluating the first prototype.",
    "actual_venue": "Amia"
  },
  {
    "abstract": "We propose the use of commute distance, a random walk metric, to discover anomalies in network traffic data. The commute distance based anomaly detection approach has several advantages over Principal Component Analysis (PCA), which is the method of choice for this task: (i) It generalizes both distance and density based anomaly detection techniques while PCA is primarily distance-based (ii) It is agnostic about the underlying data distribution, while PCA is based on the assumption that data follows a Gaussian distribution and (iii) It is more robust compared to PCA, i.e., a perturbation of the underlying data or changes in parameters used will have a less significant effect on the output of it than PCA. Experiments and analysis on simulated and real datasets are used to validate our claims.",
    "actual_venue": "Data Mining Workshop"
  },
  {
    "abstract": "The Weyl--Horn theorem characterizes a relationship between the eigenvalues and the singular values of an arbitrary matrix. Based on that characterization, a fast recursive algorithm is developed to construct numerically a matrix with prescribed eigenvalues and singular values. Besides being of theoretical interest, the technique could be employed to create test matrices with desired spectral features. Numerical experiment shows this algorithm to be quite efficient and robust.",
    "actual_venue": "Siam J Numerical Analysis"
  },
  {
    "abstract": "Students have difficulty learning 3D geometry; spatial thinking is an important aspect of the learning processes in this academic area. In light of the unique features of virtual environments and the influence of metacognitive processes (e.g., self-regulating questions) on the teaching of mathematics, we assumed that a combination of self-regulating questions and virtual environments would enhance spatial thinking through the exercise of certain spatial abilities with the VR Spaces 1.0 software. These two methods primarily focus on the cognitive domain. In terms of learning styles, we define different cognitive characteristics. The main objective of the present study was to examine whether students with a certain learning style would benefit more from this exercise than other students. To assess the effect of these methods, a sample of 192 10th graders were randomly assigned to four groups, two of which used Virtual Spaces 1.0 (Group 1 with virtual reality and self-regulating questions, N = 52; Group 2 with virtual reality only, N = 52) while the other two used non-Virtual Spaces 1.0 (Group 3 with self-regulating questions only, N = 45; Group 4 was the non-treatment group; N = 45). The findings indicate a differential impact of virtual environments on students with different modal and personal learning styles. The post-test scores for all students (except audio students on the Aptitude Profile Test Series - Educational) were significantly higher than the pre-test scores. The unique nature of this study's findings expresses itself in the fact that the ''sensing'' students (S type) scored higher than the ''intuitive'' students (N type) on the Mental Rotation Test group 2 alone. Additionally, the scores of the visual students were higher then those of the kinesthetic style but not significantly. These findings suggest that virtual environment decreases the gap in performance results between the visual and kinesthetic students and highlight the importance of virtual environments to the ''sensing'' and kinesthetic styles.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "The current software development process in common use within industry is inefficient, in that the time required to incorporate results from competitive, beta, and previous releases into new versions available to customers is typically measured in years. Further, the accuracy of customer feedback returned to the development team is frequently weak or incomplete, with samples often drawn from only a small, self- selected set of customers. This paper argues that we can automate this feedback process and, in so doing, drive an order of magnitude improvement in the rate at which software evolves and improves.",
    "actual_venue": "Cidr"
  },
  {
    "abstract": "BackgroundSystems biology experiments generate large volumes of data of multiple modalities and this information presents a challenge for integration due to a mix of complexity together with rich semantics. Here, we describe how graph databases provide a powerful framework for storage, querying and envisioning of biological data.",
    "actual_venue": "Biodata Mining"
  },
  {
    "abstract": "Full reconstruction of neuron morphology is of fundamental interest for the analysis and understanding of their functioning. We have developed a novel method capable of automatically tracing neurons in three-dimensional microscopy data. In contrast to template-based methods, the proposed approach makes no assumptions about the shape or appearance of neurite structure. Instead, an efficient seeding approach is applied to capture complex neuronal structures and the tracing problem is solved by computing the optimal reconstruction with a weighted graph. The optimality is determined by the cost function designed for the path between each pair of seeds and by topological constraints defining the component interrelations and completeness. In addition, an automated neuron comparison method is introduced for performance evaluation and structure analysis. The proposed algorithm is computationally efficient and has been validated using different types of microscopy data sets including Drosophila’s projection neurons and fly neurons with presynaptic sites. In all cases, the approach yielded promising results.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "1 kd with d 2 N, d 2 and k1 kd 1. We prove that these elements dene Rauzy fractals that are stable under a central symmetry. We use this result to compute, for several cases of cubic Pisot units, the maximal length among the lengths of the nite -fractional parts of sums of two -integers, denoted by L. In particular, we prove that L = 5 in the Tribonacci case.",
    "actual_venue": "Discrete Mathematics And Theoretical Computer Science"
  },
  {
    "abstract": "The movement of a knuckleball depends on the seams of the baseball strongly, as a consequence a lift force proportional to the square of the ball velocity is occasioned. In this work we develop a model as a function of the position of the seams to compute the lift coefficient that appears in the lift force acting in upward direction, for four-seam (4S) and two-seam (2S) orientations of the ball. The result of the model is consistent with experimental data. In addition, deviations caused by lift force are calculated for all angles in the 4S and 2S orientations and some trajectories are generated.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "Results useful in the calculation of the exact closed-form expression for the minimum average probability of errorP_eof a binary pulse linear communication system with2Nintersymbol interferences and additive colored Gaussian noise are given. By first performing statistical averaging and then using the variational method, a compact form ofP_ethat depends only on the overall system impulse response taken overNtime instants is obtained. TheseNunknowns are given as the solution ofNsimultaneous nonlinear equations. Specific examples illustrating this approach are considered.",
    "actual_venue": "Information Theory, IEEE Transactions  "
  },
  {
    "abstract": "A generalmethodology for design of biometric verification system is presented. It is based on linear feature discrimination using sequential compositions of several types of feature vector transformations: data centering, orthogonal projection onto linear subspace, vector component scaling, and orthogonal projection onto unit sphere. Projections refer to subspaces in global, within-class, and between-class error spaces. Twelve basic discrimination schemes are identified by compositions of subspace projections interleaved by scaling operations and single projection onto unit sphere. For the proposed discriminant features, the Euclidean norm of difference between query and average personal feature vectors is compared with the threshold corresponding to the required false acceptance rate. Moreover, the aggregation by geometric mean of distances in two schemes leads to better verification results. The methodology is tested and illustrated for the verification system based on facial 2D images.",
    "actual_venue": "Rskt"
  },
  {
    "abstract": "Session types have emerged as a powerful paradigm for structuring communication-based programs. They guarantee type soundness and session fidelity for concurrent programs with sophisticated communication protocols. As type soundness proofs for languages with session types are tedious and technically involved, it is rare to see mechanized soundness proofs for these systems.\n\nWe present an executable intrinsically typed small-step semantics for a realistic functional session type calculus. The calculus includes linearity, recursion, and recursive sessions with subtyping. Asynchronous communication is modeled with an encoding.\n\nThe semantics is implemented in Agda as an intrinsically typed, interruptible CEK machine. This implementation proves type preservation and a particular notion of progress by construction.",
    "actual_venue": "Proceedings Of The International Symposium On Principles And Practice Of Programming Languages"
  },
  {
    "abstract": "In editing source code of a program on modern integrated development environments, automated recording of editing operations has become popular. These operations enable past program modifications to be investigated in detail. However, such investigation of enormous amount of operations is troublesome for a human. Moreover, each of the recorded operations does not indicate what code changes were totally done. To address these problems, this paper proposes OperationReplayer, which replays recorded operations in chronological order and arbitrarily restores past snapshots of source code. It employs a plug-in mechanism that allows its user to flexibly highlight particular operations in their visualization. This mechanism provides the user with various overviews of vast operations and alleviates burden on his/her investigation. The paper also shows three case studies of effective examinations using highlight plug-ins.",
    "actual_venue": "Evol/Iwpse"
  },
  {
    "abstract": "With the rapid expansion of Internet in recent years, computer systems are facing increased number of security threats. Despite numerous technological innovations for information assurance, it is still very difficult to protect computer systems. Therefore, unwanted intrusions take place when the actual software systems are running. Different soft computing based approaches have been proposed to detect computer network attacks. This paper presents a genetic algorithm (GA) based approach to network intrusion detection, and the software implementation of the approach. The genetic algorithm is employed to derive a set of classification rules from network audit data, and the support-confidence framework is utilized as fitness function to judge the quality of each rule. The generated rules are then used to detect or classify network intrusions in a real-time environment. Unlike most existing GA-based approaches, because of the simple representation of rules and the effective fitness function, the proposed method is easier to implement while providing the flexibility to either generally detect network intrusions or precisely classify the types of attacks. Experimental results show the achievement of acceptable detection rates based on benchmark DARPA data sets on intrusions, while no other complementary techniques or relevant heuristics are applied.",
    "actual_venue": "Snpd"
  },
  {
    "abstract": "The gradient method for the symmetric positive definite linear system $$Ax=b$$ is as follows 1 $$x_{k + 1}=x_{k}-\\alpha_{k} g_{k}$$ where $$g_{k}=Ax_{k}-b$$ is the residual of the system at xk and 驴k is the stepsize. The stepsize $$\\alpha_{k} = \\frac{2}{{\\lambda_{1}+\\lambda_{n}}}$$ is optimal in the sense that it minimizes the modulus $$||I - \\alpha A||_{2}$$ , where 驴1 and 驴n are the minimal and maximal eigenvalues of A respectively. Since 驴1 and 驴n are unknown to users, it is usual that the gradient method with the optimal stepsize is only mentioned in theory. In this paper, we will propose a new stepsize formula which tends to the optimal stepsize as $$k \\to \\infty$$ . At the same time, the minimal and maximal eigenvalues, 驴1 and 驴n, of A and their corresponding eigenvectors can be obtained.",
    "actual_venue": "Comp Opt And Appl"
  },
  {
    "abstract": "In this paper, a microwave cavity resonator is presented for chemical sensing applications. The proposed resonator is comprised of a three dimensional (3D) split-ring resonator (SRR) residing in an external cavity and capacitively coupled by a pair of coaxial probes. 3D-printing technology with polylactic acid (PLA) filament is used to build the 3D SRR and cavity. Then, the surfaces of the SRR and the inside walls of cavity are silver-coated. The novelty of our proposed structure is its light weight and inexpensive design, owing to the utilization of low density and low-cost PLA. A Teflon tube is passed through the split-gap of the SRR so that it is parallel to the applied electric field. With an empty tube, the resonance frequency of the structure is measured at 2.56 GHz with an insertion loss of 13.6 dB and quality factor (Q) of 75. A frequency shift of 205 MHz with respect to the empty channel was measured when deionized water (DIW) was injected into the tube. Using volume occupied by the structure, the weight of the proposed microwave resonator is estimated as 22.8 g which is significantly lighter than any metallic structure of comparable size.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "Underwater Acoustic Sensor Networks (UW-ASNs) have found a wide range of applications from ocean monitoring to military surveillance. The underwater environment is energy constrained and hence it is very important to improve the life expectancy of sensor nodes. In this paper we propose a new MAC protocol (RMAC-PC) which uses transmission power control to enhance the energy efficiency. The protocol is developed as an extension to the RMAC protocol which schedules the transmissions of control and data packets depending on the latency calculations. Here, we utilize a cross-layer interaction between the MAC and physical layers to compute the optimum transmit power based on inter-nodal distance.",
    "actual_venue": "Advances In Computing And Communications"
  },
  {
    "abstract": "When the number of base station (BS) antennas is considerably larger than the number of user terminals (UTs), a simple linear minimum mean-square-error (LMMSE) data detection algorithm is nearoptimal for uplink massive multiuser multiple-input and multiple-output (MIMO) systems. However, the LMMSE detector suffers significant performance loss when the number of UTs is comparable to the number of B...",
    "actual_venue": "Ieee Transactions On Vehicular Technology"
  },
  {
    "abstract": "This paper addresses the problem of vehicle tracking under a single static, uncalibrated camera without any constraints on the scene or on the motion direction of vehicles. We introduce an explicit contour model, which not only pro- vides a good approximation to the contours of all classes of vehicles but also embeds the contour dynamics in its para- meterized template. We integrate the model into a Bayesian framework with multiple cues for vehicle tracking, and eval- uate the correctness of a target hypothesis, with the infor- mation implied by the shape, by monitoring any conflicts within the hypothesis of every single target as well as be- tween the hypotheses of all targets. We evaluated the pro- posed method using some real sequences, and demonstrated its effectiveness in tracking vehicles, which have their shape changed significantly while moving on curly, uphills roads.",
    "actual_venue": "Image Processing, Icip Ieee International Conference"
  },
  {
    "abstract": "The choice of radix is crucial for multivalued logic synthesis. Practical examples, however, reveal that it is not always possible to find the optimal radix when taking into consideration actual physical parameters of multivalued operations. In other words, each radix has its advantages and disadvantages. Our proposal is to synthesize logic in different radices, so it may benefit from their combination. The theory presented in this paper is based on Reed-Muller expansions over Galois field arithmetic. The work aims to first estimate the potential of the new approach and to second analyze its impact on circuit parameters down to the level of physical gates. The presented theory has been applied to real-life examples focusing on cryptographic circuits where Galois Fields find frequent application. The benchmark results show that the approach creates a new dimension for the trade-off between circuit parameters and provides information on how the implemented functions are related to different radices.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "Metabolic fluxes provide invaluable insight on the integrated response of a cell to environmental stimuli or genetic modifications. Current computational methods for estimating the metabolic fluxes from 13C isotopomer measurement data rely either on manual derivation of analytic equations constraining the fluxes or on the numerical solution of a highly nonlinear system of isotopomer balance equations. In the first approach, analytic equations have to be tediously derived for each organism, substrate or labelling pattern, while in the second approach, the global nature of an optimum solution is difficult to prove and comprehensive measurements of external fluxes to augment the 13C isotopomer data are typically needed.We present a novel analytic framework for estimating metabolic flux ratios in the cell from 13C isotopomer measurement data. In the presented framework, equation systems constraining the fluxes are derived automatically from the model of the metabolism of an organism. The framework is designed to be applicable with all metabolic network topologies, 13C isotopomer measurement techniques, substrates and substrate labelling patterns. By analyzing nuclear magnetic resonance (NMR) and mass spectrometry (MS) measurement data obtained from the experiments on glucose with the model micro-organisms Bacillus subtilis and Saccharomyces cerevisiae we show that our framework is able to automatically produce the flux ratios discovered so far by the domain experts with tedious manual analysis. Furthermore, we show by in silico calculability analysis that our framework can rapidly produce flux ratio equations--as well as predict when the flux ratios are unobtainable by linear means--also for substrates not related to glucose.The core of 13C metabolic flux analysis framework introduced in this article constitutes of flow and independence analysis of metabolic fragments and techniques for manipulating isotopomer measurements with vector space techniques. These methods facilitate efficient, analytic computation of the ratios between the fluxes of pathways that converge to a common junction metabolite. The framework can been seen as a generalization and formalization of existing tradition for computing metabolic flux ratios where equations constraining flux ratios are manually derived, usually without explicitly showing the formal proofs of the validity of the equations.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "This paper summarizes commonly accepted theories of corporate governance and extends them to information technology governance in United States (US) hospitals. It goes on to argue that currently accepted theories of governance may not apply in rural US hospital settings. Rather, this work posits that governance is a pattern of social relations integrating organizational activities. This position is supported in the adjoining case study1 examining a small, rural US hospital's use of coordination and collaboration to integrate IT activities and to align IT strategy and functions with corporate strategy. Thus, contrary to the dominant paradigms of hierarchy, power, and resource based governance, coordination can provide an effective mechanism for information technology governance.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "This paper presents the TerraMax vision systems used during the 2007 DARPA Urban Challenge. First, a description of the different vision systems is provided, focusing on their hardware configuration, calibration method, and tasks. Then, each component is described in detail, focusing on the algorithms and sensor fusion opportunities: obstacle detection, road marking detection, and vehicle detection. The conclusions summarize the lesson learned from the developing of the passive sensing suite and its successful fielding in the Urban Challenge.",
    "actual_venue": "Ieee Transactions Intelligent Transportation Systems"
  },
  {
    "abstract": "The prevalence of high performance mobile devices such as smartphones and tablets has brought fundamental changes to existing wireless networks. The growth of multimedia and location-based mobile services has exponentially increased network congestion and the demands for more wireless access. This has led to the development of advanced techniques to address the resulting challenges based on the co...",
    "actual_venue": "Ieee Wireless Communications"
  },
  {
    "abstract": "Lower-bound eye widths of multilevel minimum-bandwidth (MB) systems are presented as function of a parameter indicating the grade of the MB property. They are then compared with actual eye widths of some MB partial-response systems, a linear subclass of the generalized MB system. Our result can be used in securing the minimum guaranteed eye widths for real implemented MB systems.<>",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Participatory simulations are conducted to improve our knowledge of human behaviors, to help in solving conflicts, to shape interaction protocols between humans and to teach some aspects of collective management.Agent-based participatory simulations differ from other kinds of participatory simulations including role playing games and experimental economics simulations. The control architecture of the agents, in these simulations, is more or less integrally replaced by a human player and the interactions between players are limited by the communication protocols designed for the agents, usually the exchange of electronic messages logged for further analysis. Such systems can be considered as ideal multi-agent systems featuring cognitive and intelligent agents. Previous work demonstrated that running this kind of simulations helps to design and improve multi-agent simulations.In this paper, we present a series of agent-based participatory experiments studying negotiation in an abstract case of common resource pool management. The roles were designed in such a way that conflicts should emerge during the negotiations. Observing the behavior of human players, we noticed the apparition of power relations between players. We observed that this power in negotiations was unrelated to any a priori dependence between agents or between roles but was instead drawn from strategies and, more surprisingly, this power was built on an emerging ontology.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "This paper develops a simple analytical model for determining the hierarchical system of road networks. The model is based\n on a grid road network where roads are classified into three types according to road widths and travel speeds. We derive the\n optimal ratios of road areas that minimize the average and maximum travel time. Minimizing the average travel time provides\n an efficient solution, whereas minimizing the maximum travel time provides an equitable solution. Both of the solutions are\n expressed in terms of road widths and travel speeds. As an application of the grid network model, we evaluate the hierarchical\n system of the road network of Tokyo.",
    "actual_venue": "Annals Or"
  },
  {
    "abstract": "Cognitive decisions are best described by quantum mathematics. Do quantum information devices operate in the brain? What would they look like? Fuss and Navarro (2013) describe quantum lattice registers in which quantum superpositioned pathways interact (compute/integrate) as 'quantum walks' akin to Feynman's path integral in a lattice (e. g. the 'Feynman quantum chessboard'). Simultaneous alternate pathways eventually reduce (collapse), selecting one particular pathway in a cognitive decision, or choice. This paper describes how quantum walks in a Feynman chessboard are conceptually identical to 'topological qubits' in brain neuronal microtubules, as described in the Penrose-Hameroff 'Orch OR' theory of consciousness.",
    "actual_venue": "Topics In Cognitive Science"
  },
  {
    "abstract": "Abstract   Periodic/aperiodic sequence with good correlation and stopband properties has received widespread attention and been widely applied in radar and communication systems. To meet the hardware requirement and maximize the transmitter efficiency, the unimodular or peak-to-average ratio (PAR) constraint is always required in sequence design. In this paper, we consider the problem of designing PAR-constrained periodic/aperiodic sequence with good properties. After establishing the corresponding criterions for both correlation and stopband properties, the unified PAR-constrained problem is formulated and then transformed into an unconstrained minimization problem via sequence synthesis. To solve the problem, an efficient gradient-based algorithm is proposed to minimize the objective function directly. As the main steps can be implemented by fast Fourier transform (FFT) operations and Hadamard product, the whole algorithm is computationally efficient. In addition, the proposed algorithm can be applied to design both the periodic and aperiodic sequences by choosing proper parameters. Numerical experiments show that the proposed algorithm has better performance than the state-of-the-art algorithms in terms of the sequence quality and the running time.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "This paper discusses how to apply the approach used in the Chinese Abacus to implement digital arithmetic. Firstly, we examine the representations and the basic techniques used in the Chinese Abacus; then, we propose a MOS realization of the basic functions required; finally, we discuss a novel 12 bit full adder based on the Chinese Abacus method. Simulations of 0.5 mu m CMOS realizations showed that a parallel solution can run at 200 MHz while a pipeline realization can achieve: 1 GHz of clock frequency. The complexity of the circuit is quite limited; thus, the use of the Chinese Abacus approach results a competitive technique with respect to conventional methodologies.",
    "actual_venue": "Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "Since the first stage models of e-government were proposed around 2000, there have been at least 15 variants published in the academic and professional literature. This paper is a critical examination of these models which places them in the wider historical context of information systems stage and maturity modeling. It is argued that, with a small number of exceptions, most e-government stage models are theoretically weak being descriptive, not well grounded in empirical evidence and/or normative. If such models are to be useful, a different approach is required. A number of ideas for doing this are proposed.",
    "actual_venue": "System Science"
  },
  {
    "abstract": "Cloud computing is a new paradigm for the delivery of IT services. It has enabled many promising opportunities for features that cannot be easily implemented in traditional IT environments, such as elastic scalability, self-service deployment, resiliency and recovery, and so forth. Benchmarking the cloud requires a well-defined set of cloud performance metrics that should be able to sensitively distinguish the capabilities of cloud systems that enable those features. One way of defining benchmark metrics is based on observations of the internal mechanisms in a cloud. For example, an elasticity evaluation may be based on measuring a resource provisioning interval in the cloud. However, a more meaningful evaluation should be based on user-centric metrics. In this article, we will introduce a set of performance metrics that can be directly measured, calculated and compared by the cloud users, including workload consumers and the users who deploy and manage the workload life cycles. We will also discuss ways to organize the user-centric metrics, with different emphasis, into a benchmark that represents different use cases.",
    "actual_venue": "Ic2E"
  },
  {
    "abstract": "Image quality and algorithm efficiency are the two core problems of super resolution (SR) from a single image. In this paper, we propose a novel single image SR method by using multiscale local similarity and neighbor embedding method. The proposed algorithm utilizes the self similarity redundancy in the original input image, and does not depend on external example images or the whole input image to search and match patches. Instead, we search and match patches in a localized region of the image in each level, which can improve the algorithm efficiency. The neighbor embedding method is used to generate more accurate patches for reconstruction. Finally, we use the original image and filters we design to control the iterate errors which caused by layered reconstruction, which can further improve the quality of SR results. Experimental results demonstrate that our method can ensure the quality of SR images and improve the algorithm efficiency.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "This paper presents an approach of shape retrieval using linear regression. This approach extracts object edges from decomposed images in different scale levels and approximates them by the linear regression algorithm Matching Pursuit (MP). MP uses linear combination of circular Gaussian basis functions to reconstruct target edges. Each Gaussian basis function contains several features: scale, position and amplitude, which are used to compute distance of two contours. Shapes are retrieved according to the distances between query image and all images in a database.",
    "actual_venue": "Avss"
  },
  {
    "abstract": "Heterogeneous platforms integrating several CPU cores and GPU accelerators have established in several application domains, from desktop, server and mobile. To take full advantage of such platforms, video encoders/decoders have to exploit a broader design space, by cooperatively executing in all the available CPU and GPU cores. To attain such objective, three novel contributions that aim the exploitation of the maximum parallelism level in an HEVC deblocking filter are presented: i) a highly optimized CPU parallel implementation, which outperforms the current state of the art; ii) the first known GPU implementation of the HEVC deblocking filter; and iii) an hybrid and load-balanced CPU+GPU implementation, where all the available resources cooperatively execute, in order to maximize the attained performance. The obtained experimental results demonstrated the ability to achieve processing times as low as 0.8 ms and 0.5 ms to filter 1080p I-type and B-type frames, respectively, corresponding to speedup factors as high as 17 and 9.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "Differential evolution (DE) is a simple yet effective metaheuristic specially suited for real-parameter optimization. The most advanced DE variants take into account the feedback obtained in the self-optimization process to modify their internal parameters and components dynamically. In recent years, some controversies have arisen regarding the adaptive schemes that incorporate feedback from the search process to guide the adaptation of the mutation scale factor. Some researchers have claimed that no significant benefits are obtained with these kinds of schemes. However, other studies have shown that they are highly effective. In this paper, we show that there is a relationship between the effectiveness of these adaptive schemes and the balance between exploration and exploitation induced by the trial vector generation strategy considered. State-of-the-art adaptive schemes are not useful for the trial vector generation strategies with the highest levels of exploration, which in fact seems to be the reason behind the controversies of recent years.",
    "actual_venue": "Optimization Letters"
  },
  {
    "abstract": "In this paper we develop a globally stabilizing stability-based switching controller for a three-state lumped parameter centrifugal compressor surge model. The proposed model involves pressure and mass flow compression system dynamics as well as spool dynamics to account for the influence of speed transients on the compression surge dynamics. The proposed nonlinear switching controller architecture involves throttle and compressor torque regulation and is directly applicable to compression systems with actuator amplitude and rate saturation constraints.",
    "actual_venue": "Ieee Trans Contr Sys Techn"
  },
  {
    "abstract": "Typical data analytics systems abstract jobs as directed acyclic graphs (DAG5). It is crucial to maximize throughput and speedup completions for DAG jobs in practice. Existing works propose clairvoyant schedulers optimizing these goals, however, they assume complete job information as a prior knowledge which limits their applicability. Instead, we remove the complete prior knowledge assumption and rely solely on a partial prior information, which is more practical. And we design a semi-clairvoyant task scheduler COBRA working within each job. COBRA adaptively adjusts its resource desires in a multiplicative-increase multiplicative-decrease (MIMD) manner according to nearly past resource utilizations and the current waiting tasks. On the other hand, COBRA seeks to satisfy task locality preferences by allowing each task to wait for some time that is bounded by a parameterized threshold. Surprisingly, even with the partial prior job information, we theoretically prove, COBRA, when working with the widely used fair job scheduler, is O(1)-competitive with respect to both makespan and average job response time. We experimentally validate that the performance promotion of COBRA in both real system deployment and trace driven simulations.",
    "actual_venue": "Ieee Infocom"
  },
  {
    "abstract": "Hadoop is widely used in many application scenarios, where a massive computation is required. The Hadoop framework is an open-source distributed computing system adopting the Map-Reduce paradigm for data processing, which is gaining more and more popularity. Indeed, recently, many Big Data solutions benefited from the Hadoop framework. One of the main issues in using Hadoop is related to its impossibility to dynamically scale and re-configure the environment, e.g, adding/removing nodes in a cluster for an efficient resource usage. This paper presents a new approach to dynamically setup Hadoop using a Message Oriented Middleware for Cloud computing (MOM4C), in order to make the system much more suitable to Cloud providers' requirements.",
    "actual_venue": "ISCC), 2014 IEEE Symposium  "
  },
  {
    "abstract": "The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.",
    "actual_venue": "Iccvw Proceedings Of The Ieee International Conference On Computer Vision Workshop"
  },
  {
    "abstract": "ABSTRACT. Beginning with the gap in France between the institutional support for the use of technology in mathematics teaching and its weak integration into teacher practice, this paper claims that integrating technology into teaching is a long process. The aim of the paper is to identify and analyse the steps in this integration using as an example the evolu- tion over time (3 years) in the design of teaching scenarios based on Cabri-géomètre for high school students. The analysis indicates that the role played by the technology moved from being a visual amplifier or provider of data towards being an essential constituent of the meaning,of tasks and as a consequence,affected the conceptions of the mathematical objects that the students might construct. KEY WORDS: dynamic geometry, teacher beliefs, integration of technology, teaching",
    "actual_venue": "J Computers For Math Learning"
  },
  {
    "abstract": "We present a robotic system for collecting data from wireless devices dispersed across a large environment. In such applications, deploying a network of stationary wireless sensors may be infeasible because many relay nodes must be deployed to ensure connectivity. Instead, our system utilizes robots that act as data mules and gather the data from wireless sensor network nodes. We address the problem of planning paths of multiple robots so as to collect the data from all sensors in the shortest time. In this new routing problem, which we call the data gathering problem (DGP), the total download time depends on not only the robots' travel time but also the time to download data from a sensor and the number of sensors assigned to the robot. We start with a special case of DGP in which the robots' motion is restricted to a curve that contains the base station at one end. For this version, we present an optimal algorithm. Next, we study the two-dimensional version and present a constant factor approximation algorithm for DGP on the plane. Finally, we present field experiments in which an autonomous robotic data mule collects data from the nodes of a wireless sensor network deployed over a large field. © 2011 Wiley Periodicals, Inc. © 2011 Wiley Periodicals, Inc.",
    "actual_venue": "J Field Robotics"
  },
  {
    "abstract": "Agile Software Development (ASD) is gaining in popularity in today's business world. Industry is adopting agile methodologies both to accelerate value delivery and to enhance the ability to deal with changing requirements. However, ASD has a great impact on how Requirements Engineering (RE) is carried out in agile environments. The integration of Human-Centered Design (HCD) plays an important role due to the focus on user and stakeholder involvement. To this end, we aim to introduce agile RE patterns as main objective of this paper. On the one hand, we will describe our pattern mining process based on empirical research in literature and industry. On the other hand, we will discuss our results and provide two examples of agile RE patterns. In sum, the pattern mining process identifies 41 agile RE patterns. The accumulated knowledge will be shared by means of a web application.",
    "actual_venue": "Europlop"
  },
  {
    "abstract": "We show how to efficiently smooth a polygon with an approximating spline that stays to one side of the polygon. We also show how to find a smooth spline path between two polygons that form a channel. Problems of this type arise in many phys- ical motion planning tasks where not only forbidden regions have to be avoided but also a smooth traver- sal of the motion path is required. Both algorithms are based on a new tight and efficiently computable bound on the distance of a spline from its control polygon and employ only standard linear and qua- dratic programming techniques. 1. MOTIVATION",
    "actual_venue": "Symposium On Computational Geometry"
  },
  {
    "abstract": "We present an exact quantum algorithm for solving the Exact Satisfiability problem, which belongs to the important NP-complete complexity class. The algorithm is based on an intuitive approach that can be divided into two parts: the first step consists in the identification and efficient characterization of a restricted subspace that contains all the valid assignments of the Exact Satisfiability; while the second part performs a quantum search in such restricted subspace. The quantum algorithm can be used either to find a valid assignment (or to certify that no solution exists) or to count the total number of valid assignments. The query complexities for the worst-case are respectively bounded by O(root 2(n-M')) and O(2(n-M')), where n is the number of variables and M' the number of linearly independent clauses. Remarkably, the proposed quantum algorithm results to be faster than any known exact classical algorithm to solve dense formulas of Exact Satisfiability. As a concrete application, we provide the worst-case complexity for the Hamiltonian cycle problem obtained after mapping it to a suitable Occupation problem. Specifically, we show that the time complexity for the proposed quantum algorithm is bounded by O(2(n/4)) for 3-regular undirected graphs, where n is the number of nodes. The same worst-case complexity holds for(3, 3)-regular bipartite graphs. As a reference, the current best classical algorithm has a (worst-case) running time bounded by O(2(31n/96)). Finally, when compared to heuristic techniques for Exact Satisfiability problems, the proposed quantum algorithm is faster than the classical WalkSAT and Adiabatic Quantum Optimization for random instances with a density of constraints close to the satisfiability threshold, the regime in which instances are typically the hardest to solve. The proposed quantum algorithm can be straightforwardly extended to the generalized version of the Exact Satisfiability known as Occupation problem. The general version of the algorithm is presented and analyzed.",
    "actual_venue": "New Journal Of Physics"
  },
  {
    "abstract": "Caching and prefetching have often been studied as separate tools for enhancing the access to the World Wide Web. The goal of this work is to propose integrated Caching and Prefetching Algorithms for improving the performances of web navigation. We propose a new prefetching algorithm that uses a limited form of user cooperation to establish which documents to prefetch in the local cache at the client side. We show that our prefetching technique is highly beneficial only if integrated with a suitable caching algorithm.We consider two caching algorithms, Greedy-Dual-Size [6,17] and Least Recently Used, and demonstrate on trace driven simulation that Greedy-Dual-Size with prefetching outperforms both LRU with prefetching and a set of other popular caching algorithms.",
    "actual_venue": "Alenex"
  },
  {
    "abstract": "The Japanese government tweeted to calm public fear, as the public generally listened to tweets expressing alarm.",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "In this paper we compare the sliding-window (SW) and leaky-bucket (LB) input regulators. These regulators reject, or treat as lower priority, certain arrivals to a queueing system, so as to reduce congestion in the queueing system. Such regulators are currently of interest for access control in emerging high-speed communication networks. The SW admits no more than a specified numberW of arrivals in any interval of specified lengthL. The LB is a counter that increases by one up to a maximum capacityC for each arrival and decreases continuously at a given drain rate to as low as zero; an arrival is admitted if the counter is less than or equal toC-1. To indirectly represent the impact of the regulator on the performance of the queueing system, we focus on the maximum bursts admissible at the peak rate. We show that the SW admits larger bursts than the LB at any given peak rate and admissible average rate. To make the comparison, we use a special construction: We start with a sample path of an arrival process with a given peak rate. We choose a window lengthL for the SW and find the minimum window contentW that is just conforming (so there are no rejections). We then set the LB drain rate equal toW/L, so that the two admissible average rates are identical. Finally, we choose the LB capacityC so that the given arrival process is also just conforming for the LB. With this construction, we show that the SW will admit larger bursts at the peak rate than the LB. We also develop approximations for these maximum burst sizes and their ratio over long time intervals based on extreme-value asymptotics. We use simulations to confirm that these approximations do indeed enable us to predict the burst ratios for typical stochastic arrival processes.",
    "actual_venue": "Queueing Syst"
  },
  {
    "abstract": "In this paper we articulate the idea of utilizing Artificial Immune System (AIS) for the prediction of bankruptcy of companies. Our proposed AIS model considers the financial ratios as input parameters. The novelty of our algorithms is their hybrid nature, where we use modified Negative Selection, Positive Selection and the Clonal Selection Algorithms adopted from Human Immune System. Finally we compare our proposed models with a few existing statistical and mathematical sickness prediction methods.",
    "actual_venue": "Icaris"
  },
  {
    "abstract": "In multichannel system, user could keep transmitting over an instantaneous “on peak” channel by opportunistically accessing and switching among channels. Previous studies rely on constant transmission duration, which would fail to leverage more opportunities in time and frequency domain. In this paper, we consider opportunistic channel accessing/releasing scheme in multichannel system with Rayleigh fading channels. Our main goal is to derive a throughput-optimal strategy for determining when and which channel to access and when to release it. We formulate this real-time decision-making process as a two-dimensional optimal stopping problem. We prove that the two-dimensional optimal stopping rule can be reduced to a simple threshold-based policy. Leveraging the absorbing Markov chain theory, we obtain the optimal threshold as well as the maximum achievable throughput with computational efficiency. Numerical and simulation results show that our proposed channel utilization scheme achieves up to 140 percent throughput gain over opportunistic transmission with a single channel and up to 60 percent throughput gain over opportunistic channel access with constant transmission duration.",
    "actual_venue": "Ieee Trans Parallel Distrib Syst"
  },
  {
    "abstract": "Vessel detection is an important process in many medical imaging applications. In this paper, an edge tracking scheme is proposed for the detection of blood vessels in retinal images. This method detects edge points iteratively based on a Bayesian approach using local grey levels statistics and continuity properties of blood vessels. Combining the grey level profile and vessel geometric properties improves the accuracy and robustness of the tracking process. Experiments on both synthetic and real retinal images show promising results.",
    "actual_venue": "Icip"
  },
  {
    "abstract": "This paper proposes an evolved architecture from 3G networks to provide basic and advanced positioning methods for Location Based Services in Mobile IPv6-based Radio Access Networks. We start analyzing current status of Location-Based Services (i.e. LBS or LCS) and architectures in 3G networks as well as state-of-the-art research on LBS and Mobile Internet. Next we set the requirements the solution should fulfill. We continue proposing the evolved architecture for support of basic and advanced positioning methods, using MIPv6 and HMIPv6 as mobility scenario for the Mobile IPv6 based RAN, describing element's functions and changes from current approaches as well as description of the dynamic behavior. We complete the proposal with a bandwidth analysis of the signaling, identifying issues when planning implementation of LCS services in the network.",
    "actual_venue": "Ieee International Symposium On Personal, Indoor And Mobile Radio Communications, Vols -, Proceedings"
  },
  {
    "abstract": "Given the data ( p i , t i , y i ), i =1,…, m , m ⩾3, we give necessary and sufficient conditions which guarantee the existence of the least squares estimate for a 3-parametric exponential function. To this end, we suggest a choice of the initial approximation and give some numerical examples.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "Modern virtual machines, debuggers, and sandboxing solutions lend themselves towards more and more inconspicuous ways to run honeypots, and to observe and analyze malware and other malicious activity. This analysis yields valuable data for threat-assessment, malware identification and prevention. However, the use of such introspection methods has caused malware authors to create malicious programs with the ability to detect and evade such environments. This paper presents an overview on existing research of anti-honeypot and anti-introspection methods. We also propose our own taxonomy of detection vectors used by malware.",
    "actual_venue": "Advances In Information Systems And Technologies"
  },
  {
    "abstract": "The lack of developing platforms and the incompatibilities of protocols result in difficulties in rapidly developing P2P applications, and JXTA proposed by SUN corporation is an opensource platform to solve these problems, but it still needs to be improved. The characteristics between JXTA and mobile agent are analyzed in this paper. A novel JXTA architecture based on mobile agent is proposed. A new resource expressing method referring to the Dublin Core metadata method and the corresponding message format are designed, and a resource searching algorithm is designed. At last a prototype system is implemented to verify the validity of the forenamed designs.",
    "actual_venue": "Wgec"
  },
  {
    "abstract": "We use the algorithms Bottomup and Topdown to study the finitistic dimensions of a class of extension algebras: the trivially twisted extensions of monomial algebras. A hybrid algorithm for determination of the finitistic dimension of the aforementioned extension is presented, and in particular, conditions for construction of such extensions that have relatively larger finitistic dimension are investigated.",
    "actual_venue": "International Journal Of Algebra And Computation"
  },
  {
    "abstract": "Arrow is a prominent distributed protocol which globally orders requests initiated by the nodes in a distributed system. In this paper we present a dynamic analysis of the Arrow protocol. We prove that Arrow is O(log D)-competitive, where D is the diameter of the spanning tree on which Arrow operates. In addition, we show that our analysis is almost tight by proving that for all trees the competitive ratio of Arrow is Ω(log D/log log D).",
    "actual_venue": "Spaa"
  },
  {
    "abstract": "Machine learning (ML) and image processing techniques have been applied together to various scenarios for the development of Intelligent Vehicles. Among these scenarios, pedestrian detection has received growing interest in recent years, since high concern for safety applications in traffic has arisen. Several ML methods were successfully applied to solve this problem. However, because pedestrian detection is in general computationally intensive, a good trade off between accuracy and processing time is desirable, particularly if the methods are directed to real-time applications. Optimum Path Forest (OPF) classifier is a recently developed non-parametric classifier method. This work contribution is the performance assessment of a novel OPF application to pedestrian detection. Results have shown that it is fast and competitive against established methods and a viable alternative to be considered for machine learning and pedestrian detection applications.",
    "actual_venue": "Ieee International Conference On Robotics And Biomimetics"
  },
  {
    "abstract": "From a theoretical viewpoint, the (tree-)decomposition methods offer a good approach when the (tree)-width of constraint networks (CSPs) is small. In this case, they have often shown their practical interest. However, sometimes, a bad choice for the root cluster (a tree-decomposition is a tree of clusters) may drastically degrade the performance of the solving. In this paper, we highlight an explanation of this degradation and we propose a solution based on restart techniques. Then, we present a new version of the BTD algorithm (for Backtracking with Tree-Decomposition [8]) integrating restart techniques. From a theoretical viewpoint, we prove that reduced nld-nogood can be safely recorded during the search and that their size is smaller than ones recorded by MAC+RST+NG [9]. We also show how structural (no) goods may be exploited when the search restarts from a new root cluster. Finally, from a practical viewpoint, we show experimentally the benefits of using restart techniques for solving CSPs by decomposition methods.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "In line with a number of other countries, Norway has decided to base their ICT solutions in the public sector on a common ICT architecture. This article discusses some challenges related to this work. The theoretical basis for the discussions is our understanding of information infrastructures, which we claim offers a fruitful perspective to the building of ICT architectures. Of particular relevance is its installed base: the history of technical and nontechnical components that determines its further development. We argue that an ICT architecture for the public sector should be seen as an important element of a government information infrastructure. However, it has to be adapted to other principles and fulfil a wider range of needs than traditional types of infrastructures, including the specific political, regulatory and organizational context that it targets",
    "actual_venue": "Egov"
  },
  {
    "abstract": "This paper describes our attempt to de- velop an NLP-enhanced CALL program for Japanese learners. This CALL pro- gram focuses on zero anaphora acquisition. Based upon our previous work (Ya- mura-Takei et al., 2001), this paper will analyze the revision-based zero anaphora generation task. We first discuss the problems students face in treating zero anaphora by providing some empirical data from a language classroom. We then propose an anaphora generation al- gorithm within the Centering framework and evaluate its feasibility by comparing the algorithm output with human judg- ments. The evaluation result provides insights for the empirical validity of the algorithm and its future extensions.",
    "actual_venue": "Nlprs"
  },
  {
    "abstract": "The endpoint toward which reconfigurable systems should develop is programmable matter, that is, complex systems whose physical properties and structure can be controlled in a systematic way. This can be accomplished by recognizing that computational processes can be used to assemble and reconfigure complex, hierarchically structured systems. Programmable matter may be programmatically controlled externally or internally, which includes self-assembly. The best approach to the self-assembly of complex, hierarchical systems, such as future robots with capabilities comparable to those of animals, is by artificial morphogenesis, which adapts embryological morphogenesis to artificial systems. We review the requirements of self-assembling morphogenetic components.",
    "actual_venue": "Proceedings Of The Ieee"
  },
  {
    "abstract": "Bit-flipping (BF) decoding of low-density parity-check codes is of low complexity but gives inferior performance in general. To improve performance and provide new BF decoder options for complexity-performance tradeoffs, we propose new designs for the flipping function (FF), the flipped bit selection (FBS) rule and the checksum weight updating schedule. The new FF adjusts the checksum weights in every iteration while our FBS rules take more information into account. These two modifications represent efforts to track more closely the evolutions of both check and variable nodes' reliabilities. Two selective update schedules are proposed to offer more performance and complexity tradeoffs. The combinations of the new FBS rule and known FFs result in new BF decoders with improved performance and a modest complexity increase. On the other hand, combining the new FF and FBS rule gives a new decoder with performance comparable to that of the normalized min-sum algorithm while if we use a much simpler FBS rule instead, the decoder suffers little performance loss with reduced complexity. We also present a simple decision-theoretical argument to justify the new checksum weight formula and a time-expanded factor graph model to explain the proposed selective weight-updating schedules.",
    "actual_venue": "Ieee Trans Communications"
  },
  {
    "abstract": "The Electromagnetism-like algorithm is a relatively modern metaheuristic based on the attraction-repulsion mechanism of particles in the context of electromagnetism theory. This paper focuses on improving performance of this metaheuristic when solving binary problems. To this end, we incorporate three elements: pre-processing, repairing, and transfers functions. The pre-processing allows to reduce the size of instances, while repairing eliminates those potential solutions that violate the constraints. Finally, the incorporation of a transfer function adapts the solutions to a binary domains. We illustrate experimental results where the incorporation of these elements improve the resolution phase, when solving a set of 65 non-unicost set covering problems.",
    "actual_venue": "Artificial Intelligence Perspectives And Applications"
  },
  {
    "abstract": "We present an improved algorithm for selecting variable precision coefficients for FIR filters that produces a reduced space implementation with no degradation in frequency response. The algorithm is based on the fact that the frequency response of a filter has different sensitivities to different coefficients depending on the response itself (i.e. the coefficient value). The method provided here makes it possible to predict the variable precision of the quantized coefficients that are required to meet the specification. This approach, along with other technologies such as CSD and the scaling method used in our realizations, has opened exciting implementation possibilities for FPGAs, ASICs, and custom VLSI. The example study in this paper shows that using variable precision to exploit redundancy across the coefficients results in significant reductions in complexity and area over the uniform wordlength method.",
    "actual_venue": "Iscas"
  },
  {
    "abstract": "In this paper, we investigate a cooperative routing problem in time-varying Wireless Sensor Networks (WSNs) targeting the achievement of quality-of-service guarantees in delay and reliability domains. We develop a distributed adaptive cooperative routing protocol, called DACR, that exploits cooperative communication on top of delay- and energy-aware end-to-end routes and optimizes the trade-off between the reliability and delay through Lexicographic Optimization at each hop. We employ a lightweight reinforcement learning method to update the routing nodes with knowledge of expected performances that could be provided by the candidate relay nodes, helping to determine the optimal relay with the least overhead. The decision of selecting a transmission mode (i.e., direct or relayed transmission) at each hop is taken adaptively so that the reliability is maximized. The performances of our DACR have been evaluated through ns-2 simulations for a wide range of link failure rates and data traffic generation rates and the results show that the DACR outperforms a number of state-of-the-art protocols.",
    "actual_venue": "Ad Hoc Networks"
  },
  {
    "abstract": "The estimation of the maximum Doppler spread or, equivalently, the vehicle velocity, is useful in improving handoff algorithms and necessary for the optimal tuning of parameters for systems that adapt to changing channel conditions. We provide a novel velocity estimator based on the spectral moments of the in-phase and the quadrature phase components or the envelope of the received signal. We characterize the joint effects of the Ricean K factor, the directivity and the angle of nonisotropic scattering, and the effects of additive white noise on our estimator and other covariance-based velocity estimators analytically. We also prove the mean-square consistency of the covariance-based velocity estimators under some assumptions on the angle of arrival distribution. Simulations illustrate our approach and compare with existing techniques",
    "actual_venue": "Vehicular Technology, Ieee Transactions"
  },
  {
    "abstract": "SEAM is an enterprise architecture method that defines a visual lan- guage for modeling. Our goal is to provide formal semantics for SEAM. Model simulation, model comparison, and refinement verification are practical benefits we expect from this formalization. This paper complements the existing SEAM semantics by formalizing property-property relations . This formalization is based on the theory of multi-relations and Relation Partition Algebra (RPA).",
    "actual_venue": "Msvveis"
  },
  {
    "abstract": "An iterative receiver, proposed in our previous works, employs decision directed phase noise compensation (DD-PNC) and decision directed channel estimation (DDCE), and thus can effectively reduce the effects of phase noise on millimeter-wave (mm-wave) orthogonal frequency division multiplexing (OFDM) systems. When the iterative receiver is applied to mm-wave communications between fixed points and high speed trains (HSTs), not only phase noise but also Doppler shift should be considered. In this paper, we evaluate the iterative DD-PNC and DDCE scheme in the HST environment by using a dynamic multipath channel model. Computer simulations show that the proposed scheme can reduce not only the effects of phase noise but also that of Doppler shift, when angle spread (AS) is small. However, when AS becomes larger, it is demonstrated that the performance of the proposed scheme degrades. For alleviating the degradation, we modify DDCE into a symbol by symbol manner, symbol-DDCE (SDDCE) that improves the performance remarkably.",
    "actual_venue": "RWS"
  },
  {
    "abstract": "Extracting information from large amounts of data by using tables of numbers is difficult. Often, such data can be presented more effectively with graphics. The reduction in the cost of memory has allowed more powerful display systems to provide for the simultaneous display of hundreds, thousands, and even millions of colors. Effective and efficient manipulation of the colors in the display system is necessary to manage the use of such a large number of colors. These extended color capabilities can also be used to enrich the understanding of presentations of complex data sets. Applications which previously might have required the user to mentally correlate several displays can now display the same information in a single image with a corresponding increase in user understanding and accuracy of interpretation.",
    "actual_venue": "Workshop On Applied Computing"
  },
  {
    "abstract": "Some of the brain areas in the ventral temporal lobe, such as the fusiform face area (FFA), are critical for face perception in humans, but what determines this specialization is a matter of debate. The face specificity hypothesis claims that faces are processed in a domain-specific way. Alternatively, the expertise hypothesis states that the FFA is specialized in processing objects of expertise. To disentangle these views, some previous experiments used an artificial class of novel objects called Greebles. These experiments combined a learning and fMRI paradigm. Given the high impact of the results in the literature, we replicated and further investigated this paradigm. In our experiment, eight participants were trained for ten 1-hr sessions at identifying Greebles. We scanned participants before and after training and examined responses in FFA and lateral occipital complex. Most importantly and in contrast to previous reports, we found a neural inversion effect for Greebles before training. This result suggests that people process the \"novel\" Greebles as faces, even before training. This prediction was confirmed in a postexperimental debriefing. In addition, we did not find an increase of the inversion effect for Greebles in the FFA after training. This indicates that the activity in the FFA for Greebles does not depend on the degree of expertise acquired with the objects but on the interpretation of the stimuli as face-related.",
    "actual_venue": "J Cognitive Neuroscience"
  },
  {
    "abstract": "This paper proposes an initial synchronization architecture for the sector search process in third-generation partnership project (3GPP) long-term evolution (LTE) communications. The proposed system, consisting of three techniques, takes intercell interference, inter-carrier interference and multipath fading into consideration with assistance from inherent diversity. The outage and detection probabilities are derived by taking multisector diversity into account in the coarse timing alignment. The long-lag differential correlator can achieve approximately 4.7- dB signal-to-noise ratio (SNR) gain in primary synchronization sequence (PSS) acquisition probability. Joint estimation of the residual timing error and the fractional frequency offset is achieved by evaluating the short-lag autocorrelation at an orthogonal frequency division multiplexing (OFDM) symbol duration. Mean-square errors obtained via simulations are compared with the modified Cram´er-Rao lower bounds derived here. The proposed technique can achieve approximately 6.9-dB SNR gain when it is compared with that assisted from the synchronization signalling. Joint detection of the integral frequency offset (IFO) and the sector identification (SID) is achieved by exploiting a frequency-domain (FD) matched filter (MF) to deal with frequency selectivity. Differential detection on a segmental FD MF is considered in frequency-selective environments. Computer simulations are conducted to verify that the proposed technique can achieve high IFO and SID detection probabilities.",
    "actual_venue": "Wireless Communications, Ieee Transactions"
  },
  {
    "abstract": "Purpose - This paper aims to examine the views of the global knowledge management (KM) community on the research area of KM and business performance and identify key future research themes. Design/methodology/approach - An interview study spanning 222 informants in 38 countries was launched to collect data on KM expert views concerning the future research needs of the KM field. Findings - The value contribution of KM requires more research despite experts agreeing on the complexities involved in solving this challenge. Further research areas identified were related to the influence of KM to support business strategy, intellectual capital, decision-making, knowledge sharing, organizational learning, innovation performance, productivity and competitive advantage. Research limitations/implications - The sample is dominated by European-based KM experts and the self-selecting sampling approach that was used by relying on the networks of each partner could have biased the structure of this sample. Practical implications - The recognition of the complexity to demonstrate the value contribution of KM could prevent practitioners from using over-simplified approaches and encourage them to use more advanced measurement approaches. Originality/value - The paper is unique, in that it reports on the views of 222 KM experts from 38 countries representing both academia and practice, on the issue of future research needs in terms of KM and business outcomes. As such it provides valuable guidance for future studies in the KM field and related subjects.",
    "actual_venue": "Journal Of Knowledge Management"
  },
  {
    "abstract": "This paper presents an algorithm for calibrating erroneous tri-axis magnetometers in the magnetic field domain. Unlike existing algorithms, no simplification is made on the nature of errors to ease the estimation. A complete error model, including instrumentation errors (scale factors, nonorthogonality, and offsets) and magnetic deviations (soft and hard iron) on the host platform, is elaborated. An adaptive least squares estimator provides a consistent solution to the ellipsoid fitting problem and the magnetometer's calibration parameters are derived. The calibration is experimentally assessed with two artificial magnetic perturbations introduced close to the sensor on the host platform and without additional perturbation. In all configurations, the algorithm successfully converges to a good estimate of the said errors. Comparing the magnetically derived headings with a GNSS/INS reference, the results show a major improvement in terms of heading accuracy after the calibration.",
    "actual_venue": "Journal Of Sensors"
  },
  {
    "abstract": "Information visualization is valuable in learning abstract ideas and new concepts. The existing technology for publishing online papers is lacking in terms of user interaction as well as dynamic visualization of abstract concepts. Currently, the presentation of electronic documents does not fully exploit what the current technology is able to offer. Often, the visualization techniques employed are static 2-D illustrations.We propose a web-based system that allows authors of electronic documents to create their own interactive 3-D visualizations. Our target audience includes researchers from various fields. The proposed system allows researchers to share their ideas with other users in an interactive 3D virtual environment over the Internet.",
    "actual_venue": "Vrcai"
  },
  {
    "abstract": "This paper presents four distributed motion controllers to enable a group of robots to collectively transport an object towards a guide robot. These controllers include: rotation around a pivot robot, rotation in-place around an estimated centroid of the object, translation, and a combined motion of rotation and translation in which each manipulating robot follows a trochoid path. Three of these controllers require an estimate of the centroid of the object, to use as the axis of rotation. Assuming the object is surrounded by manipulator robots, we approximate the centroid of the object by measuring the centroid of the manipulating robots. Our algorithms and controllers are fully distributed and robust to changes in network topology, robot population, and sensor error. We tested all of the algorithms in real-world environments with 9 robots, and show that the error of the centroid estimation is low, and that all four controllers produce reliable motion of the object.",
    "actual_venue": "International Conference On Robotics And Automation"
  },
  {
    "abstract": "Embodied Conversational Agents that can express emotions are a popular topic. Yet, despite recent attempts, reliable methods are still lacking to assess the quality of facial displays. This paper extends and refines the work in [6], focusing on the role of the upper and the lower portions of the face. We analysed the recognition rates and errors from the responses of 74 subjects to the presentations of dynamic (human and synthetic) faces. The results points to the possibility of: a) addressing the issue of the naturalness of synthetic faces, and b) a greater importance of the upper part.",
    "actual_venue": "IUI"
  },
  {
    "abstract": "This study presents an efficient algorithm for the detection of scrolling text on continuous frames. The proposed scheme adopts both spatial and temporal computation as well as pre-processing to differentiate among noise, text and background information. Scrolling text is detected using temporal differentiation among inter-frames and then the region in which the scrolling text appears is identified as a rectangle. Simulation results demonstrate that the proposed algorithm is capable of precisely differentiating scrolling text in any direction, along any of the frame boundaries without false detection or missed text. In a comparison using quantised measurements, the proposed method outperforms all competing algorithms.",
    "actual_venue": "Iet Image Processing"
  },
  {
    "abstract": "Protecting our environment and natural resources is a major global challenge. \\\"Protectors\\\" (law enforcement agencies) try to protect these natural resources, while \\\"extractors\\\" (criminals) seek to exploit them. In many domains, such as illegal fishing, the extractors know more about the distribution and richness of the resources than the protectors, making it extremely difficult for the protectors to optimally allocate their assets for patrol and interdiction. Fortunately, extractors carry out frequent illegal extractions, so protectors can learn about the richness of resources by observing the extractor's behavior. This paper presents an approach for allocating protector assets based on learning from extractors. We make the following four specific contributions: (i) we model resource conservation as a repeated game; (ii) we transform this repeated game into a POMDP by adopting a fixed model for the adversary's behavior, which cannot be solved by the latest general POMDP solvers due to its exponential state space; (iii) in response, we propose GMOP, a dedicated algorithm that combines Gibbs sampling with Monte Carlo tree search for online planning in this POMDP; (iv) for a specific class of our game, we can speed up the GMOP algorithm without sacrificing solution quality, as well as provide a heuristic that trades off solution quality for lower computational cost.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "For expensive constrained optimization problems (ECOPs), the computation of objective function and constraints is very time-consuming. This paper proposes a novel global and local surrogate-assisted differential evolution (DE) for solving ECOPs with inequality constraints. The proposed method consists of two main phases: 1) global surrogate-assisted phase and 2) local surrogate-assisted phase. In the global surrogate-assisted phase, DE serves as the search engine to produce multiple trial vectors. Afterward, the generalized regression neural network is used to evaluate these trial vectors. In order to select the best candidate from these trial vectors, two rules are combined. The first is the feasibility rule, which at first guides the population toward the feasible region, and then toward the optimal solution. In addition, the second rule puts more emphasis on the solution with the highest predicted uncertainty, and thus alleviates the inaccuracy of the surrogates. In the local surrogate-assisted phase, the interior point method coupled with radial basis function is utilized to refine each individual in the population. During the evolution, the global surrogate-assisted phase has the capability to promptly locate the promising region and the local surrogate-assisted phase is able to speed up the convergence. Therefore, by combining these two important elements, the number of fitness evaluations can be reduced remarkably. The proposed method has been tested on numerous benchmark test functions from three test suites and two real-world cases. The experimental results demonstrate that the performance of the proposed method is better than that of other state-of-the-art methods.",
    "actual_venue": "Ieee Transactions On Cybernetics"
  },
  {
    "abstract": "Gamification is an emerging design principle for information systems where game design elements are applied to non-game contexts. IS researchers have suggested that the IS discipline must study this area but there are other applications such as serious games, and simulations that also use games in non-game contexts. Specifically, the management field has been using games and simulations for years and these applications are now being supported by information systems. We propose in this paper that we must think beyond gamification, towards other uses of games in non-gaming contexts, which we call purposeful gaming.   In this paper we identify how the IS discipline can adapt to purposeful gaming. Specifically, we show how IT artifacts, IS design, and IS theories can be used in the purposeful gaming area. We also provide three conceptual dimensions of purposeful gaming that can aid IS practitioners and researchers to classify and understand purposeful games.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "This paper evaluates anomaly detection approaches for drinking-water quality. Two major machine learning techniques are compared. One is manual feature engineering with feature subset selection for dimensionality reduction. The other is automatic feature learning through a recurrent neural network. Both methods incorporate the time domain for change detection. Preliminary results show a superior performance of automatic feature learning with an F1 score of 80%. While the feature set proposed in this work out-performs naive classification with original features, it needs further analysis to reach comparable performance to the automatic approach.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "This study focuses on the negotiation in the financial markets, specifically in programming an algorithm to trade automatically (without human intervention) in the foreign exchange market (Forex). The platform used in this study was the Meta Trader (version 5), which allows for this kind of negotiation. The main objective was to conclude about the effectiveness of a newly developed strategy for automatic negotiation. The developed strategies must have the ability to identify situations with lucrative potential based on several types of trading strategies used around the world. Methods were created based on technical and fundamental analysis as well as correlations. Fundamental analysis, in particular, is a novelty in this type of algorithms since, most part of the times, it is hard to quantify the information necessary to make decisions based on this type of analysis. Moreover, some other functions were developed in order to optimize the overall performance of the implemented strategies.",
    "actual_venue": "Intelligent Systems Design And Applications"
  },
  {
    "abstract": "This paper presents a system, named CAPTOP, for authoring and checking operating procedures for plant operations. It consists of a knowledge base of plant unit operations that can be linked to a graphical front end for inputting operating instructions. The system then builds a formal model of the instruction set as an interlingua and then uses it to output multilingual operating procedures. It avoids the problems of natural language understanding that make machine translation so difficult. Furthermore, the system could also generate output in a formal syntax that can be used as input to another knowledge based component, CHECKOP, for checking the procedure for operability and safety problems.",
    "actual_venue": "Iea/Aie"
  },
  {
    "abstract": "In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision.",
    "actual_venue": "J Artif Intell Res"
  },
  {
    "abstract": "Problems dealing with assignment of clients to servers have been widely studied. However, they usually do not model the fact that the delay incurred by a client is a function of both the distance to the assigned server and the load on this server, under a given assignment. We study a problem referred to as the load-distance balancing (LDB) problem, where the objective is assigning a set of clients to a set of given servers. Each client suffers a delay, that is, the sum of the network delay (which is proportional to the distance to its server) and the congestion delay at this server, a nondecreasing function of the number of clients assigned to the server. We address two flavors of LDB-the first one seeking to minimize the maximum incurred delay, and the second one targeted for minimizing the average delay. For the first variation, we present hardness results, a best possible approximation algorithm, and an optimal algorithm for a special case of linear placement of clients and servers. For the second one, we show the problem is NP-hard in general, and present a 2-approximation for concave delay functions and an exact algorithm, if the delay function is convex. We also consider the game theoretic version of the second problem and show the price of stability of the game is at most 2 and at least 4/3. (C) 2011 Wiley Periodicals, Inc. NETWORKS, Vol. 59(1), 22-29 2012",
    "actual_venue": "Networks"
  },
  {
    "abstract": "This memo describes a simple method of encoding Global Switched Telephone Network (GSTN) addresses (commonly called \\\"telephone numbers\\\") in the local-part of Internet email addresses, along with an extension mechanism to allow encoding of additional standard attributes needed for email gateways to GSTN-based services.",
    "actual_venue": "RFC"
  },
  {
    "abstract": "In this paper, we consider the development of central discontinuous Galerkin methods for solving the nonlinear shallow water equations over variable bottom topography in one and two dimensions. A reliable numerical scheme for these equations should preserve still-water stationary solutions and maintain the non-negativity of the water depth. We propose a high-order technique which exactly balances the flux gradients and source terms in the still-water stationary case by adding correction terms to the base scheme, meanwhile ensures the non-negativity of the water depth by using special approximations to the bottom together with a positivity-preserving limiter. Numerical tests are presented to illustrate the accuracy and validity of the proposed schemes.",
    "actual_venue": "J Sci Comput"
  },
  {
    "abstract": "Gleason and Mallows and Sloane characterized the weight enumerators of maximal self-orthogonal codes with all weights divisible by 4. We apply these results to obtain a new necessary condition for the existence of 2 − (v, k , λ) designs where the intersection numbers s 1 …, s n satisfy s 1 ≡ s 2 ≡ … ≡ s n (mod 2). Non-existence of quasi-symmetric 2−(21, 18, 14), 2−(21, 9, 12), and 2−(35, 7, 3) designs follows directly from the theorem. We also eliminate quasi-symmetric 2−(33, 9, 6) designs. We prove that the blocks of quasi-symmetric 2−(19, 9, 16), 2−(20, 10, 18), 2-(20,8, 14), and 2−(22, 8, 12) designs are obtained from octads and dodecads in the [24, 12] Golay code. Finally we eliminate quasi-symmetric 2−(19,9, 16) and 2-(22, 8, 12) designs.",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "\"…[S]oftware remains NIT's [Networking and Information Technology] greatest weakness. Although reliable and robust software is central to activities throughout society, much software is brittle, full of bugs and flaws. Software development remains a labor-intensive process in which delays and cost overruns are common, and responding to installed software's errors, anomalies, vulnerabilities, and lack of interoperability is costly to organizations throughout the U.S. economy.\" \"…[T]he science of software development must be a focus of Federal NIT R&D. As software's complexity continues to rise, today's design, development, and management problems will become intractable unless fundamental breakthroughs are made…\"[2] Current understanding of software development---largely based on anecdotes---is inadequate for this \"science of software development.\" Achieving the deeper understanding needed to transform software production requires collecting and using evidence on a large scale. This paper proposes some steps toward that outcome.",
    "actual_venue": "International Conference On Software Engineering"
  },
  {
    "abstract": "This paper proposes an asymptotic rejection algorithm for rejecting exotic disturbances in nonlinear systems.Disturbances produced by nonlinear exosystems are nonharmonic and periodic.A new internal model is proposed to deal with these disturbances.Furthermore,an adaptive output feedback controller is designed to ensure that the system's state variables asymptotically converge to zero,and disturbances can be completely rejected.The proposed algorithm has various applications,such as active vibration control.The proposed algorithm is demonstrated to completely reject the nonharmonic periodic disturbances produced by a van der Pol oscillator.",
    "actual_venue": "Science China-Information Sciences"
  },
  {
    "abstract": "Models of biochemical systems are typically complex, which may complicate the discovery of cardinal biochemical principles. It is therefore important to single out the parts of a model that are essential for the function of the system, so that the remaining non-essential parts can be eliminated. However, each component of a mechanistic model has a clear biochemical interpretation, and it is desirable to conserve as much of this interpretability as possible in the reduction process. Furthermore, it is of great advantage if we can translate predictions from the reduced model to the original model.In this paper we present a novel method for model reduction that generates reduced models with a clear biochemical interpretation. Unlike conventional methods for model reduction our method enables the mapping of predictions by the reduced model to the corresponding detailed predictions by the original model. The method is based on proper lumping of state variables interacting on short time scales and on the computation of fraction parameters, which serve as the link between the reduced model and the original model. We illustrate the advantages of the proposed method by applying it to two biochemical models. The first model is of modest size and is commonly occurring as a part of larger models. The second model describes glucose transport across the cell membrane in baker's yeast. Both models can be significantly reduced with the proposed method, at the same time as the interpretability is conserved.We introduce a novel method for reduction of biochemical models that is compatible with the concept of zooming. Zooming allows the modeler to work on different levels of model granularity, and enables a direct interpretation of how modifications to the model on one level affect the model on other levels in the hierarchy. The method extends the applicability of the method that was previously developed for zooming of linear biochemical models to nonlinear models.",
    "actual_venue": "Bmc Systems Biology"
  },
  {
    "abstract": "Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO.",
    "actual_venue": "Ieee/Cvf Conference On Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "This paper describe a package written in MATHEMATICA that automatizes typical operations performed during evaluation of Feynman graphs with Mellin–Barnes (MB) techniques. The main procedure allows to analytically continue a MB integral in a given parameter without any intervention from the user and thus to resolve the singularity structure in this parameter. The package can also perform numerical integrations at specified kinematic points, as long as the integrands have satisfactory convergence properties. It is demonstrated that, at least in the case of massive graphs in the physical region, the convergence may turn out to be poor, making naïve numerical integration of MB integrals unusable. Possible solutions to this problem are presented, but full automatization in such cases may not be achievable.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "Likey is an unsupervised statistical approach for keyphrase extraction. The method is language-independent and the only language-dependent component is the reference corpus with which the documents to be analyzed are compared. In this study, we have also used another language-dependent component: an English-specific Porter stemmer as a preprocessing step. In our experiments of keyphrase extraction from scientific articles, the Likey method outperforms both supervised and unsupervised baseline methods.",
    "actual_venue": "Semeval@Acl"
  },
  {
    "abstract": "Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.",
    "actual_venue": "Assets"
  },
  {
    "abstract": "eParticipation tools aim to facilitate intrinsic engagement from communities of stakeholders and citizens to develop more effective, bottom-up and inclusive policies, raising the potential to become an efficient engagement tool. It is argued that eParticipation tools such as the Electronic Town Meeting (eTM) have the potential to efficiently engage communities of sought-after 'lead users' to leverage economically valuable 'sticky knowledge'. While the lead user method has been demonstrated to be very effective, challenges remain around the sustainability of such an approach, particularly on a large-scale. A possible mediating tool that might be able to efficiently leverage communities of lead users is the eTM eParticipation tool. Findings from eight case studies show that the eTM has had a positive effect on appropriation and engagement of lead users as well as providing users benefits such as enhanced peer learning, and there is potential to extrapolate to innovation activities.",
    "actual_venue": "Technology Analysis And Strategic Management"
  },
  {
    "abstract": "This paper considers the problem of container inspection at a port-of-entry. Containers are inspected through a specific sequence to detect the presence of nuclear materials, biological and chemical agents, and other illegal shipments. The threshold levels of sensors at inspection stations of the port-of-entry affect the probabilities of incorrectly accepting or rejecting a container. In this paper, we present several optimization approaches for determining the optimum sensor threshold levels, while considering misclassification errors, total cost of inspection, and budget constraints. In contrast to previous work which determines threshold levels and sequence separately, we consider an integrated system and determine them simultaneously. Examples applying the approaches in different sensor arrangements are demonstrated.",
    "actual_venue": "Automation Science And Engineering, Ieee Transactions"
  },
  {
    "abstract": "Combinatorial optimization problems are often very difficult to solve and the choice of a search strategy has a tremendous influence over the solver's performance. A search strategy is said to be adaptive when it dynamically adapts to the structure of the problem instance and identifies the areas of the search space that contain good solutions. We introduce an algorithm (RLBS) that learns to efficiently backtrack when searching non-binary trees. Branching can be carried on using any usual variable/value selection strategy. However, when backtracking is needed, the selection of the node to target involves reinforcement learning. As the trees are non-binary, we have the opportunity to backtrack many times to each node during the search, which allows learning which nodes generally lead to the best rewards (that is, to the most interesting leaves). RLBS is evaluated for a scheduling problem using real industrial data. It outperforms classic (nonadaptive) backtracking strategies (DFS, LDS) as well as an adaptive branching strategy (IBS).",
    "actual_venue": "Ieee International Conference On Tools With Artificial Intelligence"
  },
  {
    "abstract": "This article proposes a novel video abstraction framework for online review services of story-oriented videos such as dramas. Among the many genres of TV programs, a drama is one of the most popularly watched on the Web. The abstracts generated by the proposed framework not only give a summary of a video but also effectively help viewers understand the overall story. In addition, our method is duration-flexible. We get clues about human understanding of a story from scenario writing rules and editorial techniques that are popularly used in the process of video production to explicitly express a narrative, and propose a new video abstraction model, called a Narrative Abstraction Model. The model effectively captures the narrative structure embedded in a story-oriented video and articulates the progress of the story in a weighted directed graph, called a Narrative Structure Graph (NSG). The model provides a basis for a flexible framework for abstract generation using the NSG as the intermediary representation of a video. Different abstracts can be appropriately generated based upon different user requirements. To show the effectiveness of the proposed model and method, we developed a video abstraction system realizing the framework, and successfully applied it to large volumes of TV dramas. The evaluation results show that the proposed framework is a feasible solution for online review services.",
    "actual_venue": "Tomccap"
  },
  {
    "abstract": "A synthetic (X) over bar chart is a combination of a conforming run-length chart and an (X) over bar chart, or equivalently, a 2-of-(H+1) runs-rules (RR) chart with a head-start feature. However, a synthetic (X) over bar chart combined with an (X) over bar chart is called a Synthetic-(X) over bar chart. In this article, we build a framework for Shewhart Synthetic-(X) over bar and improved RR (i.e., 1-of-1 or 2-of-(H+1) without head-start) charts by conducting an in-depth zero-state and steady-state study to gain insight into the design of different classes of these schemes and their average run-length performance using the Markov chain imbedding technique. More importantly, we propose a modified side-sensitive Synthetic-(X) over bar chart, and then using overall performance measures (i.e., the extra quadratic loss, average ratio of average run-length, and performance comparison index), we show that this new chart has a uniformly better performance than its Shewhart competitors. We also provide easy-to-use tables for each of the chart's design parameters to aid practical implementation. Moreover, a performance comparison with their corresponding counterparts (i.e., synthetic (X) over bar and RR charts) is conducted. Copyright (C) 2015 John Wiley & Sons, Ltd.",
    "actual_venue": "Quality And Reliability Engineering International"
  },
  {
    "abstract": "Temperature measurement is becoming increasingly important in integrated circuits and microsystems; nevertheless, existing techniques for the integration of high accuracy, high precision temperature sensors are not optimal for deep sub-micron CMOS processes. Here we describe a low voltage, low power, compact, high accuracy, high precision temperature sensor for deep sub-micron CMOS systems; our approach takes advantage of charge balancing and charge sharing for low current consumption, does not use resistors for compactness, and takes advantage of both PTAT and autozero techniques for high accuracy and high precision; the circuit can be operated at low supply voltages. As a proof of concept, we report transistor level simulations in a standard 0.13 mum process; the sensor only sinks about 6 muA from a 1.2 V supply voltage, achieving a power dissipation as low as 7.2 muW.",
    "actual_venue": "Seattle, Wa"
  },
  {
    "abstract": "Satellite systems are going to build a part of the future personal communications infrastructure. The first-generation candidates for satellite personal communication networks (S-PCN) will rely on low Earth orbit (LEO) and medium Earth orbit (MEO) constellations. A noticeable trend in this field is toward broadband services and the use of ATM. For LEO satellite systems employing intersatellite links (ISLs), this paper proposes an overall networking concept that introduces the strengths of ATM to their operation. The core of the paper is the design of a new routing scheme for the periodically time-variant ISL subnetwork, discrete-time dynamic virtual topology routing (DT-DVTR), and its ATM implementation. DT-DVTR works completely off line, i.e., prior to the operational phase of the system. In a first step, a virtual topology is set up for all successive time intervals of the system period, providing instantaneous sets of alternative paths between all source-destination node pairs. In the second step, path sequences over a series of time interval are chosen from that according to certain optimization procedures. An ATM-based implementation of DT-DVTR in LEO satellite ISL networks is presented with some emphasis on the optimization alternatives, and the performance in terms of delay jitter is evaluated for an example ISL topology",
    "actual_venue": "Ieee Journal On Selected Areas In Communications"
  },
  {
    "abstract": "Obtaining successful operation rules for dam and reservoir systems is crucial for improving water management to meet the increase in agricultural, domestic and industrial activities. Several research efforts have been developed to generate optimal operation rules for dam and reservoir systems utilizing different optimization algorithms. The main purpose of an operation rule is to minimize the gap between water supply and water demand patterns. To examine the optimized model performance, the simulation of a dam and reservoir system is usually carried out for a particular period utilizing the generated operation rule. During the simulation procedure, although reservoir inflow and evaporation are stochastic variables that are required to be forecasted during simulation, they are considered deterministic variables. This study attempts to integrate a forecasting model for reservoir inflow and evaporation with the operation rules generated from optimization models during the simulation procedure. The present study employs several optimization models to generate an optimal operation rule and two different forecasting models for reservoir inflow and reservoir evaporation. The three different optimization algorithms used in this study are the genetic algorithm (GA), particle swarm optimization (PSO) algorithm and shark machine learning algorithm (SMLA). Two different forecasting models have been developed for reservoir inflow and evaporation using the radial basis function neural network (RBF-NN) and support vector regression (SVR). It is necessary to analyze the proposed simulation procedure for examining the operation rule to comprehend the analysis under different optimal operation rules and levels of accuracy for both hydrological variables. The suggested models have been applied to generate optimal operation policies and reservoir inflow and evaporation forecasts for the Timah Tasoh dam (TTD) located in Malaysia. The results show that the major findings regarding the model performance during the simulation period indicate the necessity to pay attention to evaluating the optimized model performance by considering the results of the forecasting model for both the hydrological variables of reservoir inflow and reservoir evaporation rather than the deterministic values.",
    "actual_venue": "Knowledge Based Systems"
  },
  {
    "abstract": "This paper describes a new method to utilize redundancy of hyper redundant robots. In order to use redundancy effectively, the contribution of all actuators for the achievement of the target motions and capability for unpredictable target motions in the future should be considered. From this point of view, two evaluation indices, the movability index of each joint and the assistability index for the target motions are proposed. These indices are formulated with the angle and amplitude distribution of the column vectors of Jacobian matrices of each joint, including the end-effector. Then joint input to optimize these indices simultaneously are derived based on the improved gradient projection method. Motion control simulations and experiments with a planar 10R serial manipulator revealed the effectiveness of the proposed method.",
    "actual_venue": "Journal Of Advanced Mechanical Design Systems And Manufacturing"
  },
  {
    "abstract": "Graph-theoretic aggregation problems have been considered both in OLAP (grid graph) and XML (tree). This paper gives new results for MIN aggregation in a tree, where we want the MIN in a query subtree consisting of the nodes reachable from a node u along paths of length ≤ k (u and k are query parameters). The same problem is well solved when the aggregation is SUM rather than MIN, but the solutions rely on additive inverses for the \"+\" operator, and they fail for the MIN aggregation which is the topic of this paper. For the directed (rooted tree) case, we give an O(n) space, constant query time solution. For the undirected case, the space complexity is O(n log n) and the query time is O(log n).",
    "actual_venue": "Icdt"
  },
  {
    "abstract": "•High-level multimodal video-data fusion solution for sensitive scene localization.•It can easily combine varied data types (e.g., frames, motion, audio, subtitles).•It can straightforwardly be applied to localize diverse types of sensitive scenes.•Results for pornographic and violent scene localization are reported.•The Pornography-2k dataset is introduced as a new benchmark for porn localization.",
    "actual_venue": "Information Fusion"
  },
  {
    "abstract": "Cognitive radio is a technological concept pushing for the introduction of intelligent radio operation that goes beyond traditional\n system adaptation. So far, a rather limited amount of work has been published on the cognitive mechanisms that should be embedded\n into communicating equipments to achieve such an intelligent behavior. This paper presents a generic cognitive framework for\n autonomous decision making with regard to multiple, possibly conflicting, operational objectives in a time-varying environment.\n The framework is based on the definition of two scales introducing order relationships between the configurations that help\n the reasoning and learning processes. The resulting cognitive engine learns to progressively identify the optimal configurations\n for the design objectives imposed given the current radio environment. The proposed approach is illustrated for a case of\n cognitive waveform design and extensive simulation results validate the cognitive engine behavior.",
    "actual_venue": "Annales Des Tlcommunications"
  },
  {
    "abstract": "In this paper, a localization based approach of audio signal separation from binary mixtures is carried out. The audio sources are localized in the spatial domain (azimuth plane) using the delay and amplitude variation cues between two microphones' signals. A coherence based technique is introduced here to localize the audio sources in adverse acoustical environment. The mixture signals are decomposed into a desired number of sub-bands with empirical mode decomposition (EMD) which is a data adaptive filtering scheme suitable for nonlinear and non-stationary signals. Data independent minimum variance beamforming is employed to separate the component sources in underdetermined condition (more sources than sensors). The experimental results of the proposed algorithm show noticeable separation efficiency. It is also found that the sub-band implementation improves the performance compared with and full-band approach..",
    "actual_venue": "Ieee International Symposium On Circuits And Systems, Vols -, Proceedings"
  },
  {
    "abstract": "Contributing editor Paul F. Dubois takes a light-hearted look at how much computer science resembles customer service",
    "actual_venue": "Computing In Science And Engineering"
  },
  {
    "abstract": "In this paper, we propose a general technique for removing symmetries in CSPs during search. The idea is to record no-goods, during the exploration of the search tree, whose symmetric counterpart (if any) should be removed. The no-good, called Global Cut Seed (GCS), is used to generate Symmetry Removal Cuts (SRCs), i.e., constraints that are dynamically generated during search and hold in the entire search tree. The propagation of SRCs removes symmetric configurations with respect to already visited states. We present a general, correct and complete filtering algorithm for SRCs. The main advantages of the proposed approach are that it is not intrusive in the problem-dependent search strategy, treats symmetries in an additive way since GCSs are symmetry independent, and enables to write filtering algorithms which handle families of symmetries together. Finally, we show that many relevant previous approaches can be seen as special cases of our framework.",
    "actual_venue": "CP"
  },
  {
    "abstract": "Modern technology has allowed real-time data collection in a variety of domains, ranging from environmental monitoring to\n healthcare. Consequently, there is a growing need for algorithms capable of performing inferential tasks in an online manner,\n continuously revising their estimates to reflect the current status of the underlying process. In particular, we are interested\n in constructing online and temporally adaptive classifiers capable of handling the possibly drifting decision boundaries arising\n in streaming environments. We first make a quadratic approximation to the log-likelihood that yields a recursive algorithm\n for fitting logistic regression online. We then suggest a novel way of equipping this framework with self-tuning forgetting\n factors. The resulting scheme is capable of tracking changes in the underlying probability distribution, adapting the decision\n boundary appropriately and hence maintaining high classification accuracy in dynamic or unstable environments. We demonstrate\n the scheme’s effectiveness in both real and simulated streaming environments.",
    "actual_venue": "Adv Data Analysis And Classification"
  },
  {
    "abstract": "This paper presents proposal of a universal computational theory of Collective Intelligence (CI),. The toll for formalization, analysis, and modeling is a quasi-chaotic model of computations RPP. In the RPP, molecules (CMs) of facts, rules, goals, or higher-level logical structures enclosed by membranes, move quasi-randomly in structured Computational _Space (CS). When CMs rendezvous, an inference process can occur if and only if the logical conditions are fulfilled. It is proposed that Collective Intelligence can be measured as follows: 1) the mapping is done of a given structure of beings into the RPP; 2) the beings and their behavior are translated into expressions of mathematical logic, carried by CMs; 3) the goal(s) of the social structure is(are) translated into N-Element Inferences (NEI); 4) the efficiency of the NEI is evaluated and given as the Intelligence Quotient of a Social Structure (IQS) projected onto NEI. IQS is computed as a probability function over time, what implies various possibilities, e.g.: to order social structures according to their IQS, to optimize social structures with IQS as a quality measure, or even to compare single beings with social structures. The use of probability allows estimation of IQS either by simulation, or on the basis of analytical calculations.",
    "actual_venue": "Ceemas"
  },
  {
    "abstract": "This paper proposes a two-stage registration method for SAR and optical images based on multi-features and multi-constraints. In the first stage, closed regions are extracted automatically to achieve the coarse mapping parameters as geometrical restriction. In the second stage, Harris corner points and cross-road features are extracted, and then correlation analysis and mutual information are utilized to match the corresponding control points. After that, multi-constraints are used to delete the false matched points. The retained ones are served as ground control points for registration. The experimental results show that the method can reduce the possibility of false matching effectively and the registration error is within one pixel.",
    "actual_venue": "Igarss"
  },
  {
    "abstract": "Currently, the design, deployment, and refinement of new network architectures is a manual, ad hoc, and time-consuming process. We present the design, implementation, and evaluation of the Genesis Kernel, a programming system that automates the life cycle process for the creation, deployment, management, and architecting of network architectures. We discuss our experiences in building a spawning network that is capable of creating distinct virtual network architectures on-demand. The Genesis Kernel is based on a methodology that allows a child virtual network to operate on top of a subset of its parent's network resources and in isolation from other spawned virtual networks. We show through experimentation how a number of diverse network architectures can be spawned and architecturally refined. These spawned network architectures include a parent network that supports IP forwarding, and interior and exterior routing. We discuss how two child networks based on Cellular IP and Mobiware architectures can be spawned on the parent network to support wireless access to data and continuous media services, respectively",
    "actual_venue": "Ieee Journal On Selected Areas In Communications"
  },
  {
    "abstract": "In the present paper, we propose a new identification method by adapting the “fictitious controller” to the obtained data in the closed loop experiment. A fictitious controller, which is introduced in this paper, consists of the nominal model with unknown plant parameters and the implemented controller used in the closed loop. One of the key points of the present method is the adaptation of such a controller to the actual experiment data. Moreover, the adaptation can be performed by using an off-line nonlinear optimization. Since the required material in the proposed method is only one-shot experimental data under the normal operation like step responses, this method has a practical advantage in the sense that the costs and time for the identification can be reduced.",
    "actual_venue": "Ifac Proceedings Volumes"
  },
  {
    "abstract": "To help fulfill the promises of ITS (intelligent transportation system), the ATLAS (Advanced Traffic and Logistics Algorithms and Systems) research center is developing and testing the RHODES (real-time hierarchical optimized distributed effective system) traffic control system. We believe that RHODES play a major role in the realization of future Advanced Traffic Management Systems, a major component of ITS.",
    "actual_venue": "Intelligent Systems, Ieee"
  },
  {
    "abstract": "Temperature has proven to be an effectivestress condition, commonly used to stress memory devicesand to detect special types of failure mechanisms. In thispaper, a new approach is presented where temperature isused as a test parameter to increase the fault coverageof specific tests. This is done using defect injection andsimulation of a memory model at different temperatures.The analysis presents new types of detection conditions formemories and evaluates the impact of temperature on theseconditions.",
    "actual_venue": "ITC"
  },
  {
    "abstract": "In this paper, we present a method for the integration of nonlinear holonomic constraints in deformable models and its application to the problems of shape and illuminant direction estimation from shading. Experimental results demonstrate that our method performs better than previous Shape from Shading algorithms applied to images of Lambertian objects under known illumination. It is also more general as it can be applied to non-Lambertian surfaces and it does not require knowledge of the illuminant direction. In this paper, 1) we first develop a theory for the numerically robust integration of nonlinear holonomic constraints within a deformable model framework. In this formulation, we use Lagrange multipliers and a Baumgarte stabilization approach. We also describe a fast new method for the computation of constraint based forces, in the case of high numbers of local parameters. 3) We demonstrate how any type of illumination constraint, from the simple Lambertian model to more complex highly nonlinear models can be incorporated in a deformable model framework. 4) We extend our method to work when the direction of the light source is not known. We couple our shape estimation method with a method for light estimation, in an iterative process, where improved shape estimation results in improved light estimation and vice versa. 5) We perform a series of experiments on both synthetic and real data. The synthetic data come from a standardized set of images. Our results compare favorably with results of previous SfS algorithms on the same data and our light direction estimation to a previous method by Zheng and Chellapa.",
    "actual_venue": "Ieee Trans Pattern Anal Mach Intell"
  },
  {
    "abstract": "In this work we present a biologically motivated framework for the modelling of the visual scene exploration preference. We aim at capturing the statistical patterns that are elicited by the subjective visual selection and reproduce them via a computational system. The low level visual features are encoded through the projection of the image patches on a learned basis of linear filters reproducing the typical response properties of the primary visual cortex (V1) receptive fields of mammals. The resulting training set is typically high-dimensional and sparse. We exploit the sparse structure by clustering together patterns of channel activation which are similar on the basis of a binary activation map and finally deriving a pooling over the set of the original linear filters in terms of active (on) and non-active (off) channels for each cluster. The system has been tested on a dataset of natural images by comparing the fixation density maps recorded from human subjects observing the pictures and the saliency maps computed by our system obtaining promising results.",
    "actual_venue": "Bica"
  },
  {
    "abstract": "In previous work, the authors have been developing a stochastic model based approach for on-line segmentation of whole body human motion patterns during human motion observation and learning, using a simplified kinematic model of the human body. In this paper, we extend the proposed approach to larger, more realistic kinematic models, which can better represent a larger variety of human motions. These larger models may include spherical in addition to revolute joints. We examine the effects on segmentation performance due to motion representation choice, and compare the segmentation efficacy when Cartesian or joint angle data is used. The approach is tested on whole body human motion data modeled with a 42DoF kinematic model. The results indicate that Cartesian data seems to correspond most closely to the human evaluation of segment points. The experiments also demonstrate the efficacy of the segmentation approach for large kinematic models and a variety of human motions.",
    "actual_venue": "St Louis, Mo"
  },
  {
    "abstract": "In three sites of boreal and temperate forests, P band HH, HV, and W polarization data combined estimate total aboveground dry woody biomass within 12 to 27% of the values derived from allometric equations, depending on forest complexity. Biomass estimates derived from HV-polarization data only are 2 to 14% less accurate. When the radar operates at circular polarization, the errors exceed 100% over flooded forests, wet or damaged trees and sparse open tall forests because double-bounce reflections of the radar signals yield radar signatures similar to that of tall and massive forests. Circular polarizations, which minimize the effect of Faraday rotation in spaceborne applications, are therefore of limited use for measuring forest biomass. In the tropical rain forest of Manu, in Peru, where forest biomass ranges from 4 kg m-2 in young forest succession up to 50 kg m-2 in old, undisturbed floodplain stands, the P band horizontal and vertical polarization data combined separate biomass classes in good agreement with forest inventory estimates. The worldwide need for large scale, updated, biomass estimates, achieved with a uniformly applied method, justifies a more in-depth exploration of multi-polarization long wavelength imaging radar applications for tropical forests inventories",
    "actual_venue": "Geoscience And Remote Sensing, Ieee Transactions"
  },
  {
    "abstract": "Asphaltene can precipitate in oil reservoirs as a result of natural depletion and/or gas injection crippling the oil production performance. Most of the conventional models for asphaltene precipitation cannot precisely capture the asphaltene precipitation at a wide pressure range and for different oil types. To have a precise model that can be used for various oil types at a wide range of pressure conditions, a comprehensive artificial neural network (ANN) model was proposed to estimate the weight percent of precipitated asphaltene in different oil types (three oil types, namely light, medium and heavy). The dilution ratio, pressure, molecular weight of solvent, API gravity and resin-to-asphaltene ratio were considered as the model input parameters. The oil samples were thus categorized based on the differences in their API gravity and resin-to-asphaltene ratio. Five hundred and fifty experimental precipitation datapoints were obtained from our experimental apparatus in a wide range of pressure, dilution ratio and injected fluid molecular weight, and used to make a comprehensive databank for model calibration and verification. At the test stage, the coefficient of correlation (R2) was higher than 0.98 and mean square error was less than 0.04 indicating the good performance of the proposed model. Furthermore, a comparison between the prediction of ANN model and two types of alternative approaches, namely the thermodynamic and the fractal/aggregation approaches, was performed. For this purpose, the prediction of two of the widely used solubility models, Flory---Huggins and Modified Flory---Huggins and also a polydisperse thermodynamic model was compared to the prediction of the proposed ANN model. In addition to those, as a fractal/aggregation model, a scaling model was also selected and employed to compare its performance against that of the proposed ANN model. The ANN model showed a better performance as compared to the other conventional models. The results demonstrated that the proposed model provides acceptable prediction for different oil types over a wide range of pressure which is a difficult task for most of the conventional techniques.",
    "actual_venue": "Neural Computing And Applications"
  },
  {
    "abstract": "In this paper we present a panoramic depth imaging system. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect which enables stereo reconstruction. The camera is rotating on a circular path with a step defined by the angle, equivalent to one pixel column of the captured image. The equation for depth estimation can be easily extracted from the system geometry. To find the corresponding points on a stereo pair of panoramic images the epipolar geometry needs to be determined. It can be shown that the epipolar geometry is very simple if we are doing the reconstruction based on a symmetric pair of stereo panoramic images. We get a symmetric pair of stereo panoramic images when we take symmetric pixel columns on the left and on the right side from the captured image center column. Epipolar lines of the symmetrical pair of panoramic images are image rows. The search space on the epipolar line can be additionaly constrained. The focus of the paper is mainly on the system analysis. Results of the stereo reconstruction procedure and quality evaluation of generated depth images are quite promissing. The system performs well for reconstruction of small indoor spaces. Our finall goal is to develop a system for automatic navigation of a mobile robot in a room.",
    "actual_venue": "International Journal Of Computer Vision"
  },
  {
    "abstract": "Pie menus offer several features which are advantageous especially for gaze control. Although the optimal number of slices per pie and of depth layers has already been established for manual control, these values may differ in gaze control due to differences in spatial accuracy and congitive processing. Therefore, we investigated the layout limits for hierarchical pie menu in gaze control. Our user study indicates that providing six slices in multiple depth layers guarantees fast and accurate selections. Moreover, we compared two different methods of selecting a slice. Novices performed well with both, but selecting via selection borders produced better performance for experts than the standard dwell time selection.",
    "actual_venue": "Etra"
  },
  {
    "abstract": "While great progress has been made in digitizing the US health care system, today's health information technology (IT) infrastructure remains largely a collection of systems that are not designed to support a transition to value-based care. In addition, the pursuit of value-based care, in which we deliver better care with better outcomes at lower cost, places new demands on the health care system that our IT infrastructure needs to be able to support. Provider organizations pursuing new models of health care delivery and payment are finding that their electronic systems lack the capabilities needed to succeed. The result is a chasm between the current health IT ecosystem and the health IT ecosystem that is desperately needed. In this paper, we identify a set of focal goals and associated near-term achievable actions that are critical to pursue in order to enable the health IT ecosystem to meet the acute needs of modern health care delivery. These ideas emerged from discussions that occurred during the 2015 American Medical Informatics Association Policy Invitational Meeting. To illustrate the chasm and motivate our recommendations, we created a vignette from the multistakeholder perspectives of a patient, his provider, and researchers/innovators. It describes an idealized scenario in which each stakeholder's needs are supported by an integrated health IT environment. We identify the gaps preventing such a reality today and present associated policy recommendations that serve as a blueprint for critical actions that would enable us to cross the current health IT chasm by leveraging systems and information to routinely deliver high-value care.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "Advances in high-performance architectures and networking have made it possible to build complex systems with several parallel and distributed interacting components. Unfortunately, the software needed to support such complex interactions has lagged behind. The parallel language's API should provide both algorithmic and run-time system support to optimize the performance of its operations. Some developers, however. choose to play clever and start from the language's primitive operations and write their own versions of the parallel operations. In this paper we have used a number of benchmarks to test performance improvement over current Unified Parallel C (UPC) collective implementations and prove that in some circumstances, it is wiser for developers to optimize starting from UPC's primitive operations. We also pin point specific optimizations at both the algorithmic and the runtime support levels that developers could use to uncover missed optimization opportunities.",
    "actual_venue": "Parallel Computing: Architectures, Algorithms And Applications"
  },
  {
    "abstract": "Modern data center solid state drives (SSDs) integrate multiple general-purpose embedded cores to manage flash translation layer, garbage collection, wear-leveling, and etc., to improve the performance and the reliability of SSDs. As the performance of these cores steadily improves there are opportunities to repurpose these cores to perform application driven computations on stored data, with the aim of reducing the communication between the host processor and the SSD. Reducing host-SSD bandwidth demand cuts down the I/O time which is a bottleneck for many applications operating on large data sets. However, the embedded core performance is still significantly lower than the host processor, as generally wimpy embedded cores are used within SSD for cost effective reasons. So there is a trade-off between the computation overhead associated with near SSD processing and the reduction in communication overhead to the host system.\n\nIn this work, we design a set of application programming interfaces (APIs) that can be used by the host application to offload a data intensive task to the SSD processor. We describe how these APIs can be implemented by simple modifications to the existing Non-Volatile Memory Express (NVMe) command interface between the host and the SSD processor. We then quantify the computation versus communication tradeoffs for near storage computing using applications from two important domains, namely data analytics and data integration. Using a fully functional SSD evaluation platform we perform design space exploration of our proposed approach by varying the bandwidth and computation capabilities of the SSD processor. We evaluate static and dynamic approaches for dividing the work between the host and SSD processor, and show that our design may improve the performance by up to 20% when compared to processing at the host processor only, and 6X when compared to processing at the SSD processor only.",
    "actual_venue": "Micro-: The Annual Ieee/Acm International Symposium On Microarchitecture Cambridge Massachusetts October"
  },
  {
    "abstract": "A conventional autonomous mobile robot is introduced. The main idea isthe integration of many conventional and sophisticated sensor fusiontechniques, introduced by several authors in recent years. We show theactual possibility of integrating all these techniques together, rather thananalyzing implementation details. The topics of multisensor fusion,observation integration and sensor coordination are widely used throuhoutthe article. The final goal is to demonstrate the validity of bothmathematical and artificial intelligence techniques in guaranteeing vehiclesurvival in a dynamic environment, while the robot carries out a specifictask. We review conventional techniques for the management of uncertaintywhile we describe an implementation of a mobile robot which combines on-lineheterogeneous sensors in its navigation and localisation tasks.",
    "actual_venue": "Journal Of Intelligent And Robotic Systems"
  },
  {
    "abstract": "In this paper, a new tunneling field effect transistor with comb-shaped gate (CG-TFET) is proposed and experimentally demonstrated. Source implantation masked by the comb-shaped gate can result in combshaped channel region. With this comb-shaped gate configuration, increased tunneling area can be obtained and hence higher on-state current without area penalty. Moreover, an optimized CG-TFET with silicide source is fabricated to further improve the on-state current by introducing Schottky barrier tunneling current which is much more efficient than band-to-band tunneling. In addition, the electron barrier in the comb-shaped channel region is raised due to the self depleted effect, which can effectively suppress the Schottky injection leakage from the silicide source in the off-state. 1.5 orders of magnitude higher than conventional TFET with the same footprint on-state current and more than 106 on-off current ratio can be achieved.",
    "actual_venue": "Science China-Information Sciences"
  },
  {
    "abstract": "Autonomic computing has become an important paradigm for dealing with large scale network management. However, changes operated by administrators and self-governed entities may generate vulnerable configurations increasing the exposure to security attacks. In this paper, we propose a novel approach for supporting collaborative treatments in order to remediate known security vulnerabilities in autonomic networks and systems. We put forward a mathematical formulation of vulnerability treatments as well as an XCCDF-based language for specifying them in a machine-readable manner. We describe a collaborative framework for performing these treatments taking advantage of optimized algorithms, and evaluate its performance in order to show the feasibility of our solution.",
    "actual_venue": "Cnsm"
  },
  {
    "abstract": "Mobile cloud computing (MCC) as an emerging and prospective computing paradigm, can significantly enhance computation capability and save energy for smart mobile devices (SMDs) by offloading computation-intensive tasks from resource-constrained SMDs onto resource-rich cloud. However, how to achieve energy-efficient computation offloading under hard constraint for application completion time remains a challenge. To address such a challenge, in this paper, we provide an energy-efficient dynamic offloading and resource scheduling (eDors) policy to reduce energy consumption and shorten application completion time. We first formulate the eDors problem into an energy-efficiency cost (EEC) minimization problem while satisfying task-dependency requirement and completion time deadline constraint. We then propose a distributed eDors algorithm consisting of three subalgorithms of computation offloading selection, clock frequency control, and transmission power allocation. Next, we show that computation offloading selection depends on not only the computing workload of a task, but also the maximum completion time of its immediate predecessors and the clock frequency and transmission power of the mobile device. Finally, we provide experimental results in a real testbed and demonstrate that the eDors algorithm can effectively reduce EEC by optimally adjusting CPU clock frequency of SMDs in local computing, and adapting the transmission power for wireless channel conditions in cloud computing.",
    "actual_venue": "Ieee Trans Mob Comput"
  },
  {
    "abstract": "A popular theory on IT's impact fails to consider the rich, dynamic, relationship-filled environment in which IT operates.",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "High-level geometry processing has been a hot topic in graphics community. The functionality analysis of 3D models is an essential issue in this area. Existing 3D models often exhibit both large intra-class and inter-class variations in shape geometry and topology, making the consistent analysis of functionality challenging. Traditional 3D shape analysis methods which rely on geometric shape descriptors can not obtain satisfying results on these 3D models. We develop a new 3D shape descriptor based on the interactions between 3D models and virtual human actions, which is called Action-Based 3D Descriptor (AB3D). Due to the implied semantic meanings of virtual human actions, we obtain encouraging results on consistent segmentation based on AB3D. Finally, we present a method for recognition and reconstruction of scanned 3D indoor scenes using our AB3D. Experiments show that AB3D is a promising shape descriptor toward functionality analysis of 3D shapes.",
    "actual_venue": "The Visual Computer: International Journal Of Computer Graphics"
  },
  {
    "abstract": "Identifying protein-protein interactions is critical for understanding cellular processes. Because protein domains represent binding modules and are responsible for the interactions between proteins, computational approaches have been proposed to predict protein interactions at the domain level. The fact that protein domains are likely evolutionarily conserved allows us to pool information from data across multiple organisms for the inference of domain-domain and protein-protein interaction probabilities.We use a likelihood approach to estimating domain-domain interaction probabilities by integrating large-scale protein interaction data from three organisms, Saccharomyces cerevisiae, Caenorhabditis elegans and Drosophila melanogaster. The estimated domain-domain interaction probabilities are then used to predict protein-protein interactions in S.cerevisiae. Based on a thorough comparison of sensitivity and specificity, Gene Ontology term enrichment and gene expression profiles, we have demonstrated that it may be far more informative to predict protein-protein interactions from diverse organisms than from a single organism.The program for computing the protein-protein interaction probabilities and supplementary material are available at http://bioinformatics.med.yale.edu/interaction.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "We present a neuromorphic VLSI device which comprises hybrid analog/digital circuits for implementing networks of spiking neurons. Each neuron integrates input currents from a row of multiple analog synaptic circuit. The synapses integrate incoming spikes, and produce output currents which have temporal dynamics analogous to those of biological post synaptic currents. The VLSI device can be used to implement real-time models of cortical networks, as well as real-time learning and classification tasks. We describe the chip architecture and the analog circuits used to implement the neurons and synapses. We describe the functionality of these circuits and present experimental results demonstrating the network level functionality.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "We propose a lightweight data structure for indexing and querying collections of NGS reads data in main memory. The data structure supports the interface proposed in the pioneering work by Philippe et al. for counting and locating k-mers in sequencing reads. Our solution, PgSA (pseudogenome suffix array), based on finding overlapping reads, is competitive to the existing algorithms in the space use, query times, or both. The main applications of our index include variant calling, error correction and analysis of reads from RNA-seq experiments.",
    "actual_venue": "Plos One"
  },
  {
    "abstract": "Quality of Service for wireless mesh networks is an often requested feature for various kinds of applications. A common approach is the hop-by-hop reservation of bandwidth for individual routes. In this paper we address the problems of the reservation on a single hop. In previous works we used simulation studies to show that various existing approaches suffer from inconsistencies that lead to admission failures. In this paper, we discuss the reasons for these failures and present a protocol for preventing them. This allows to significantly increase the reliability of established communication links in WMNs.",
    "actual_venue": "Aiccsa"
  },
  {
    "abstract": "This paper presents a new architecture for image processing. It consists of a pipeline of identical programmable serial processing stages, referred to as a cytocomputer. Comparisons are made between cytocomputer and parallel array systems. Cytocomputer systems are shown generally to possess the advantages of lower complexity, high bandwidth and greater architectural flexibility. A first generation system is described and examples of processing are illustrated. Finally, current development efforts are described.",
    "actual_venue": "Isca"
  },
  {
    "abstract": "The present paper formulates digital binary filter design for the subtractive-noise restoration problem in terms of the classical boundary-value-problem paradigm. The boundary-value problem involves both operator relations and invariant (fixed-point) boundary conditions. Noise-hole restoration is to be achieved while certain shape-based structures remain invariant, and the boundary-value problem incorporates these conditions. A design approach is formulated that derives an increasing, translation-invariant solution via the morphological basis expansion directly from the statement of the boundary-value problem itself without positing any a priori class of structuring elements over which to search. Existence conditions are analyzed and, when they exist, solutions are found that possess both minimal bases and minimal structuring elements. These solutions are extensive. Owing to duality, antiextensive solutions result for the classical union-noise model and these are discussed.",
    "actual_venue": "Journal Of Mathematical Imaging And Vision"
  },
  {
    "abstract": "In this paper, we study the problem of effective keyword search over XML documents. We begin by introducing the notion of Valuable Lowest Common Ancestor (VLCA) to accurately and effectively answer keyword queries over XML documents. We then propose the concept of Compact VLCA (CVLCA) and compute the meaningful compact connected trees rooted as CVLCAs as the answers of keyword queries. To efficiently compute CVLCAs, we devise an effective optimization strategy for speeding up the computation, and exploit the key properties of CVLCA in the design of the stack-based algorithm for answering keyword queries. We have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "Mobile computers consume significant amounts of energy when receiving large files. The wireless network interface card (WNIC) is the primary source of this energy consumption. One way to reduce the energy consumed is to transmit the packets to clients in a predictable fashion. Specifically, the packets can be sent in bursts to clients, who can then switch to a lower power sleep state between bursts. This technique is especially effective when the bandwidth of a stream is small. This paper investigates techniques for saving energy in a multiple-client scenario, where clients may be receiving either UDP or TCP data. Energy is saved by using a transparent proxy that is invisible to both clients and servers. The proxy implementation maintains separate connections to the client and server so that a large increase in transmission time is avoided. The proxy also buffers data and dynamically generates a global transmission schedule that includes all active clients. Results show that energy savings within 10-15% of optimal are common, with little packet loss.",
    "actual_venue": "Icpp"
  },
  {
    "abstract": "This paper presents a new model for handling nuanced information expressed in an affirmative form like \"x is m驴 A\". In this model, nuanced information are represented in a qualitative way within a symbolic context. For that purpose, vague terms and linguistic modifiers that operate on them are defined. The model presented is based on a symbolic M-valued predicate logic and provides a new deduction rule generalizing the Modus Ponens rule.",
    "actual_venue": "Jelia"
  },
  {
    "abstract": "In this paper, a new class of lower bounds on the mean square error (MSE) of unbiased estimators of deterministic parameters is proposed. Derivation of the proposed class is performed by projecting each entry of the vector of estimation error on a Hilbert subspace of L2. This Hilbert subspace contains linear transformations of elements in the domain of an integral transform of the likelihood-ratio function. The integral transform generalizes the traditional derivative and sampling operators, which are applied on the likelihood-ratio function for computation of performance lower bounds, such as Cramér-Rao, Bhattacharyya, and McAulay-Seidman bounds. It is shown that some well-known lower bounds on the MSE of unbiased estimators can be derived from this class by modifying the kernel of the integral transform. A new lower bound is derived from the proposed class using the kernel of the Fourier transform. In comparison with other existing bounds, the proposed bound is computationally manageable and provides better prediction of the threshold region of the maximum-likelihood estimator, in the problem of single tone estimation.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "In several economic fields, such as those related to health, education, or poverty, the individuals' characteristics are measured by bounded variables. Accordingly, these characteristics may be indistinctly represented by achievements or shortfalls. A difficulty arises when inequality needs to be assessed. One may focus either on achievements or on shortfalls, but the respective inequality rankings may lead to contradictory results. Specifically, this paper concentrates on the poverty measure proposed by Sen. According to this measure, inequality among the poor is captured by the Gini index. However, the rankings obtained by the Gini index applied to either the achievements or the shortfalls do not coincide in general. To overcome this drawback, we show that an ordered weighted averaging (OWA) operator is underlying in the definition of the Sen measure. The dual decomposition of the OWA operators into a self-dual core and antiself-dual remainder allows us to propose an inequality component which measures consistently the achievement and shortfall inequality among the poor. (C) 2011 Wiley Periodicals, Inc.",
    "actual_venue": "International Journal Of Intelligent Systems"
  },
  {
    "abstract": "As discussed in previous papers, belie) conte.~a are a powerful ud appropriate formalism for the represen- tation and implementation of propositional attitudes in a multiagent environment, In this paper we show that a formalization using belief contexts is also elab- oration tolerant. That is, it is able to cope with minor changes to input problems without major revisions. Elaboration tolerance is a vital property for building situated agents: it allows for adapting and re-uling a previous problem representation in different (but re- lated) situations, rather than building a new represen- tation from scratch. We substantiate our claims by discussing a number of variations to a paradigmatic case study, the Three Wise Men problem.",
    "actual_venue": "Icmas"
  },
  {
    "abstract": "Micro-Doppler signatures can be used not only to recognize different targets, such as vehicles, helicopters, animals, and people, but also to classify varying activities, e.g., walking, running, creeping, and crawling. For this purpose, a plethora of features have been proposed in the literature; however, dozens of features are not required to achieve high classification performance. The topic of feature selection has been under addressed in micro-Doppler studies. Moreover, the optimal feature set is not static but varies under different operational conditions, such as signal-to-noise ratio (SNR), dwell time, and aspect angle. The mutual information of features relative to the classification problem at hand offers a measure for assessing the efficacy of features and thus sets a unique framework for feature selection. In this paper, information-theoretic (IT) feature selection techniques are used to identify essential features and minimize the total number of required features, while maximizing classification performance. It is seen that, although some features are consistently preferred, others are never selected. Results show that for SNRs over 10 dB and at least 1 s of data, this approach yields 96% correct classification when the target moves along the radar line-of-sight and over 65% correct classification for tangential motion.",
    "actual_venue": "Ieee Trans Geoscience And Remote Sensing"
  },
  {
    "abstract": "Humans use various types of modalities to express own internal states. If a robot interacting with humans can pay attention to limited signals, it should select more informative ones to estimate the partners' states. We propose an active perception method that controls the robot's attention based on an energy minimization criterion. An energy-based model, which has learned to estimate the latent state from sensory signals, calculates energy values corresponding to occurrence probabilities of the signals; The lower the energy is, the higher the likelihood of them. Our method therefore selects the modality that provides the lowest expectation energy among available ones to exploit more frequent experiences. We employed a multimodal deep belief network to represent relationships between humans' states and expressions. Our method demonstrated better performance for the modality selection than other methods in a task of emotion estimation. We discuss the potential of our method to advance human-robot interaction.",
    "actual_venue": "Proceedings Of The International Conference On Human Agent Interaction"
  },
  {
    "abstract": "BDD-based symbolic techniques and partial-order reduction (POR) are two fruitful approaches to deal with the combinatorial explosion of model checking. Unfortunately, past experience has shown that BDD-based techniques do not work well for loosely-synchronized models, whereas POR methods allow explicit-state model checkers to deal with large concurrent models. This paper presents an algorithm that combines symbolic model checking and POR to verify linear temporal logic properties without the next operator (LTLX), which performs better on models featuring asynchronous processes. Our algorithm adapts and combines three methods: Clarke et al.'s tableau-based symbolic LTL model checking, Iwashita et al.'s forward symbolic CTL model checking and Lerda et al.'s ImProviso symbolic reachability with POR. We present our approach, outline the proof of its correctness, and present a prototypal implementation and an evaluation on two examples.",
    "actual_venue": "Nasa Formal Methods"
  },
  {
    "abstract": "GRASP with path-relinking (GRASP+PR) is a metaheuristic for finding optimal or near-optimal solutions of combinatorial optimization problems. This paper proposes a new automatic parameter tuning procedure for GRASP+PR heuristics based on a biased random-key genetic algorithm (BRKGA). Given a GRASP+PR heuristic with n input parameters, the tuning procedure makes use of a BRKGA in a first phase to explore the parameter space and set the parameters with which the GRASP+PR heuristic will run in a second phase. The procedure is illustrated with a GRASP+PR for the generalized quadratic assignment problem with n=30 parameters. Computational results show that the resulting hybrid heuristic is robust.",
    "actual_venue": "SEA"
  },
  {
    "abstract": "New function of coding genes and lncRNA can be profitably predicted using tissue specific coexpression, as well as expression of orthologous genes in different species. The data are available for download and through a user-friendly web interface at www.funcpred.com .",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "On-street parking, just as any publicly owned utility, is used inefficiently if access is free or priced very far from market rates. This paper introduces a novel demand management solution: using data from dedicated occupancy sensors an iteration scheme updates parking rates to better match demand. The new rates encourage parkers to avoid peak hours and peak locations and reduce congestion and underuse. The solution is deliberately simple so that it is easy to understand, easily seen to be fair and leads to parking policies that are easy to remember and act upon. We study the convergence properties of the iteration scheme and prove that it converges to a reasonable distribution for a very large class of models. The algorithm is in use to change parking rates in over 6000 spaces in downtown Los Angeles since June 2012 as part of the LA Express Park project. Initial results are encouraging with a reduction of congestion and underuse, while in more locations rates were decreased than increased.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "Due to the growing interest in XML security, various access control schemes have been proposed recently. However, little effort has been put forth to facilitate a uniform analysis and comparison of these schemes under the same framework. This paper presents a first attempt toward a flexible framework that can capture the design principles and operations of existing XML access control mechanisms. Under this framework, we observe that most existing XML access control mechanisms share the same design principle with slightly different orderings of underlying building blocks (i.e., data, query, and access control rule). Furthermore, according to the framework, we identify four plausible approaches to implement XML access controls, namely built-in, view-based, pre-processing and post-processing. Finally, we compare the actual performance of different approaches.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "This paper proposes a 2.2 GHz CMOS Power Amplifier (PA) useful to S-Band applications with an effective 3-bit output power control for efficiency improvement. It is composed by a cascode amplifier topology to minimize the voltage stress across the power transistors, being the cascode amplifier composed by four parallel branches, where the state (on or off) of 3 branches is separately activated by a 3-bit input, for efficiency control. It was designed for the 1 W output power range in 130 nm CMOS process. Post-layout simulations resulted a peak output power of 28.1 dBm (near 650 mW) with a maximum output power efficiency around 43% under 3.3 V of supply voltage. The 3-bit control allows a total output power dynamic range adjustment of 5.7 dBm, divided in 7 steps, with the efficiency changing from 25.4% to 43.7%.",
    "actual_venue": "Ieee International Conference On Electronics, Circuits And Systems"
  },
  {
    "abstract": "In this paper, we describe an experimental study of Internet topological stability and the origins of failure in Internet protocol backbones. The stability of end-to-end Internet paths is dependent both on the underlying telecommunication switching system, as well as the higher level software and hardware components specific to the Internet's packet-switched forwarding and routing architecture. Although a number of earlier studies have examined failures in the public telecommunication system, little attention has been given to the characterization of Internet stability. We provide analysis of the stability of major paths between Internet Service Providers based on the experimental instrumentation of key portions of the Internet infrastructure. We describe unexpectedly high levels of path fluctuation and an aggregate low mean time between failures for individual Internet paths. We also provide a case study of the network failures observed in a large regional Internet backbone. We characterize the type, origin, frequency and duration of these failures.",
    "actual_venue": "Ftcs"
  },
  {
    "abstract": "Influence of the filter shape on the performance of a single-wavelength combined FSK/IM scheme is investigated by simulations\n and experiments. For 156 Mbit/s FSK label and 10 Gbit/s IM payload, central wavelength misalignment of the signal and the\n optical filter can be tolerated up to 30 GHz without noticeable penalties. Results of the simulation agree very well with\n the experiments.",
    "actual_venue": "Opnetec"
  },
  {
    "abstract": "Biclustering is a data mining technique used to simultaneously partition the set of samples and the set of their attributes\n (features) into subsets (clusters). Samples and features clustered together are supposed to have a high relevance to each\n other. In this paper we provide a new mathematical programming formulation for unsupervised biclustering. The proposed model\n involves the solution of a fractional 0–1 programming problem. A linear-mixed 0–1 reformulation as well as two heuristic-based\n approaches are developed. Encouraging computational results on clustering real DNA microarray data sets are presented. In\n addition, we also discuss theoretical computational complexity issues related to biclustering.",
    "actual_venue": "Journal Of Combinatorial Optimization"
  },
  {
    "abstract": "The chemical-genetic profile can be defined as quantitative values of deletion strains' growth defects under exposure to chemicals. In yeast, the compendium of chemical-genetic profiles of genomewide deletion strains under many different chemicals has been used for identifying direct target proteins and a common mode-of-action of those chemicals. In the previous study, valuable biological information such as protein-protein and genetic interactions has not been fully utilized. In our study, we integrated this compendium and biological interactions into the comprehensive collection of,490 protein complexes of yeast for model-based prediction of a drug's target proteins and similar drugs. We assumed that those protein complexes (PCs) were functional units for yeast cell growth and regarded them as hidden factors and developed the PC-based Bayesian factor model that relates the chemical-genetic profile at the level of organism phenotypes to the hidden activities of PCs at the molecular level. The inferred PC activities provided the predictive power of a common mode-of-action of drugs as well as grouping of PCs with similar functions. In addition, our PC-based model allowed us to develop a new effective method to predict a drug's target pathway, by which we were able to highlight the target-protein, TOR1, of rapamycin. Our study is the first approach to model phenotypes of systematic deletion strains in terms of protein complexes. We believe that our PC-based approach can provide an appropriate framework for combining and modeling several types of chemical-genetic profiles including interspecies. Such efforts will contribute to predicting more precisely relevant pathways including target proteins that interact directly with bioactive compounds.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "In recent years there has been much interest in the development and the fast computation of bilinear pairings due to their practical and myriad applications in cryptography. Well known efficient examples are the Weil and Tate pairings and their variants ...",
    "actual_venue": "Selected Areas In Cryptography"
  },
  {
    "abstract": "We consider efficient methods for the recovery of block-sparse signals--i.e., sparse signals that have nonzero entries occurring in clusters--from an underdetermined system of linear equations. An uncertainty relation for block-sparse signals is derived, based on a block-coherence measure, which we introduce. We then show that a block-version of the orthogonal matching pursuit algorithm recovers block k-sparse signals in no more than k steps if the block-coherence is sufficiently small. The same condition on block-coherence is shown to guarantee successful recovery through a mixed l2/l1-optimization approach. This complements previous recovery results for the block-sparse case which relied on small block-restricted isometry constants. The significance of the results presented in this paper lies in the fact that making explicit use of block-sparsity can provably yield better reconstruction properties than treating the signal as being sparse in the conventional sense, thereby ignoring the additional structure in the problem.",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "In recent decades, a plethora of dedicated evolutionary algorithms (EAs) have been crafted to solve domain-specific complex problems more efficiently. Many advanced EAs have relied on the incorporation of domain-specific knowledge as inductive biases that is deemed to fit the problem of interest well. As such, the embedment of domain knowledge about the underlying problem within the search algorit...",
    "actual_venue": "Ieee Transactions On Evolutionary Computation"
  },
  {
    "abstract": "This paper proposes an adaptive cache coherence protocol to improve the reliability of caches against soft errors in shared-memory multi-core processors. The proposed protocol is conducted based-on a comprehensive study and analysis intended to determine the effects of cache coherence protocols on the characteristics of cache memories. The outcomes of this analysis indicate that differences in handling dirty data items play an important role to make distinction in favor of or against a cache coherence protocol. Based on the primary results, the proposed protocol tries to enhance the reliability of caches by means of sharing management. Sharing is dynamically adjusted according to the operational mode of CPU. The experimental results show that proposed protocol leads to about 16 % improvements in MTTF, with no performance degradation and with negligible bandwidth and cache energy consumption overheads compared to previous works.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "A decision support process (DSP), defined as an approach to decision support, uses a programming environment to aid the decision-maker. The DSP is discussed and a system architecture using a hierarchical planning approach to support DSP activities is proposed. These stages include problem solving, solution planning, tools integration, and model execution. A comparative study of DSP research with respect to these stages is made. Using a conceptual framework for DSP as the basis for proposing a system architecture for assisting DSP, the design and development concerns based on hierarchical planning and an attribute-based approach are outlined, and the system, called XDSP (expert decision support process), is described",
    "actual_venue": "Systems, Man and Cybernetics, IEEE Transactions  "
  },
  {
    "abstract": "Abstract A continuous-time Markov process is proposed to analyze how a group of humans solves a complex task, consisting in the search of the optimal set of decisions on a fitness landscape. Individuals change their opinions driven by two different forces: (i) the self-interest, which pushes them to increase their own fitness values, and (ii) the social interactions, which push individuals to reduce the diversity of their opinions in order to reach consensus. Results show that the performance of the group is strongly affected by the strength of social interactions and by the level of knowledge of the individuals. Increasing the strength of social interactions improves the performance of the team. However, too strong social interactions slow down the search of the optimal solution and worsen the performance of the group. In particular, we find that the threshold value of the social interaction strength, which leads to the emergence of a superior intelligence of the group, is just the critical threshold at which the consensus among the members sets in. We also prove that a moderate level of knowledge is already enough to guarantee high performance of the group in making decisions.",
    "actual_venue": "The European Physical Journal B"
  },
  {
    "abstract": "This article describes a project which aimes at reviewing perceptive works on emotion and prosodic description of affective speech. A study with a spontaneous French corpus, for which a corresponding acted version has been built, shows that native listeners perceive the difference between acted and spontaneous emotions. The results of cross-linguistic perceptual studies indicate that emotions are perceived by listeners partly on the basis of prosody only, proposing the universality of emotions like anger, and partly on the basis of the variability. The latter assumption is supported by the fact that the characterization of anger in degrees is different depending on the mother tongue of the listeners. Finally, a prosodic analysis of the emotional speech is presented, measuring F0 cues, duration parameters and intensity.",
    "actual_venue": "Speaker Classification"
  },
  {
    "abstract": "•Modeling contourlet coefficients using t Location-Scale distribution.•Study adaptation between t Location-Scale distribution and contourlet coefficients.•Designing a contourlet domain multiplicative watermark detector using LRT.•Deriving ROC of the watermark detector theoretically and analyzing its performance.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "In this paper we consider a queueing model that results from at least two apparently unrelated areas. One motivation to study a system of this type results from a test case of a computer simulation factor screening technique calledfrequency domain methodology. A second motivation comes from manufacturing, where due to cyclic scheduling of upstream machines, the arrival process to downstream machines is periodic. The model is a single server queue with FIFO service discipline and exponential interarrival and service times where the arrival and/or service rates are deterministic cyclic functions of the customer sequence number. We provide steady state results for the mean number in the system for the model with cyclic arrival and fixed service rates and for the model with fixed arrival and cyclic service rates. For the model with both cyclic arrival and service rates, upper and lower bounds are developed for the steady state mean waiting time in the system. Throughout the paper various implications and/or insights derived from the results of this study are discussed for frequency domain methodology.",
    "actual_venue": "Queueing Syst"
  },
  {
    "abstract": "In this paper, we study various policies for scheduling tasks in distributed processor queues. Their performance is studied and compared for a variety of workloads. It is our intention to find a policy that increases throughput and is fair to jobs. Simulation results indicate that the policy that schedules the shortest task in a queue, when there is not any other task that has been waiting more than some configurable period of time, yields good system performance and also provides a guarantee for fairness in individual job execution.",
    "actual_venue": "Annual Simulation Symposium"
  },
  {
    "abstract": "Hidden node collision in a contention-based medium access control protocol contributes to poor wireless network performance. This paper extended the Bianchi's study and introduces a mathematical model that can be used to calculate throughput and delay for the IEEE 802.11 distributed coordination function of a multihop wireless network infrastructure assuming the presence of hidden node collision. This research investigates three essential parameters of multi-hop wireless networks. More specifically, this paper aims to analyze the effect of hidden nodes, network size, and maximum backoff stage on the overall system throughput and packet delay. Results clearly reveal the effect of large wireless network size, maximum backoff stage, and collision probability on throughput and packet delay. On one hand, throughput does not depend on the maximum backoff stage ( m ) for a small network size (e.g., n $$=$$ = 10). On the other hand, throughput does not strongly depend on the number of nodes when the backoff stage values are high. Comparing our proposed model in case single-hop with the Bianchi model, the analysis results indicate that the throughput values in our model when the numbers of nodes are 10, 50, and 100 are 0.6031, 0.4172 and 0.3433 respectively; whereas the throughput values are respectively 0.8370, 0.8317 and 0.8255 at the same number of nodes for the Bianchi model. The difference can be attributed to several assumptions made in our proposed model that were not considered in the Bianchi model.",
    "actual_venue": "Wireless Personal Communications: An International Journal"
  },
  {
    "abstract": "Three-Dimensional Optical Network-on-Chip (3D ONoC) has recently emerged as a high-performance on-chip communication solution; however, owing to the intrinsic characteristics of photonic devices in existing 3D ONoC, the insertion loss caused by undesirable coupling among optical signals degrades network performance. Considering manufacturing defects and unpredictable noise sources that cause the failure of Optical Routers (ORs) in 3D ONoC, previous work simply abandoned the disabled OR when computing the restore path. In this paper, we propose a new OR structure that reduces insertion loss, and we present the design of a novel adaptive routing algorithm, FTRA-BL, based on the new OR structure with bidirectional waveguides, without abandoning any disabled ORs. Our FTRA-BL selects the normal waveguide in the disabled OR as the backup link so that the best macroscopic restore path can be guaranteed. Simulation results show that our method performs better than previous work in improving transmission reliability and latency.",
    "actual_venue": "Science China-Information Sciences"
  },
  {
    "abstract": "The ongoing goal of cellular services providers has been to make networks faster to enable new revenue-producing Internet-access and multimedia- and data-based broadband services, in addition to telephony. For example, carriers want to offer mobile Internet services as fast as those provided by cable- and DSL-based wireline broadband technologies. This process has taken the industry through various generations of radio-based wireless service: After first-generation (1G) analog cellular service, they have offered 2G, 2.5G, and, since 2001, 3G digital technology. As carriers upgrade their 3G offerings, they are looking perhaps five years ahead to 4G services, which would be based on the Internet Protocol and support mobile transmission rates of 100 Mbps and fixed rates of 1 Gbps. Presently the subject of extensive research, 4G would enable such currently unavailable services as mobile high-definition TV and gaming, as well as teleconferencing.",
    "actual_venue": "Ieee Computer"
  },
  {
    "abstract": "A method for reverse engineering user interfaces based on their structural and behavioral representations is presented. The interface structure is represented using an object oriented approach while interface behavior is described using Milner's process algebra (CCS). A specification language for user interfaces is designed for the multiple purposes of serving as a target language for the reverse engineering process, as a working specification language for interface redesign, and as a specification language for generating a new user interface for a specific platform. The motivations and advantages of such a representational method are discussed together with examples of user interface reverse engineering in a common business-oriented language (COBOL)/CICS environment",
    "actual_venue": "Baltimore"
  },
  {
    "abstract": "We present an adaptive algorithm for blind audio source separation (BASS) of moving sources via Independent Component Analysis (ICA) in time-domain. The method is shown to achieve good separation quality even with a short demixing filter length (L = 30). Our experiments show that the proposed adaptive algorithm can outperform the off-line version of the method (in terms of the average output SIR), even in the case in which the sources do not move, because it is capable of better adaptation to the nonstationarity of the speech.",
    "actual_venue": "Lva/Ica"
  },
  {
    "abstract": "Shape description and feature detection are fundamental problems in computer graphics and geometric modeling. Among many existing techniques, those based on geodesic distance have proven effective in providing intrinsic and discriminative shape descriptors. In this article we introduce a new intrinsic function for a three-dimensional (3D) shape and use it for shape description and geometric feature detection. Specifically, we introduce the intrinsic girth function (IGF) defined on a 2D closed surface. For a point p on the surface, the value of the IGF at p is the length of the shortest nonzero geodesic path starting and ending at p. The IGF is invariant under isometry, insensitive to mesh tessellations, and robust to surface noise. We propose a fast method for computing the IGF and discuss its applications to shape retrieval and detecting tips, tubes, and plates that are constituent parts of 3D objects.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "Many important software applications are dominated by non-trivial serial components: Amdahl's Law places a hard upper bound on possible speedup that can be achieved for these applications. In this paper, we propose an integrated software/hardware approach for accelerating hard serial bottlenecks in data structure heavy algorithms. The key idea is to overlap the processing of the main algorithmic functions and the data structure related operations. We describe the language, compiler, ISA and architectural support for such data structure co-processing (DSCP), and define a clean interface between the software and the hardware. We perform extensive simulations using the popular C++ STL container classes, as well as a detailed implementation of our approach for Dijkstra's single-source shortest path algorithm. We find potential for improvements that are well beyond what can be achieved with more conventional parallel computation methods.",
    "actual_venue": "Computer Design"
  },
  {
    "abstract": "As iris recognition moves from constrained indoor and near-infrared systems towards unconstrained on-the-move and at-a-distance systems, possibly using visible light illumination, interest in measurement of the fidelity of the acquired images and their impact on recognition performance has grown. However, the impact of the subject's physiological characteristics on the nature of the acquired images has received little attention. In this paper we catalog a selection of the most common ophthalmic disorders and investigate some of their characteristics including their prevalence and possible impact on recognition performance. The paper also includes an experimental exploration of the effect of such conditions on segmentation of the iris image.",
    "actual_venue": "Biometrics"
  },
  {
    "abstract": "A fuzzy Petri net model (FPN) is presented to represent the fuzzy production rule of a rule-based system in which a fuzzy production rule describes the fuzzy relation between two propositions. Based on the fuzzy Petri net model, an efficient algorithm is proposed to perform fuzzy reasoning automatically. It can determine whether an antecedent-consequence relationship exists from proposition d/sub s/ to proposition d/sub j/, where d/sub s/ not=d/sub j/. If the degree of truth of proposition d/sub s/ is given, then the degrees of truth of proposition d/sub j/ can be evaluated. The formal description of the model and the fuzzy reasoning algorithm are shown in detail. The upper bound of the time complexity of the fuzzy reasoning algorithm is O(nm), where n is the number of places and m is the number of transitions. Its execution time is proportional to the number of nodes in a sprouting tree generated by the algorithm only generates necessary reasoning paths from a starting place to a goal place, it can be executed very efficiently.",
    "actual_venue": "Ieee Trans Knowl Data Eng"
  },
  {
    "abstract": "In 1987 , Alavi, Boals, Chartrand, Erdös, and Oellermann conjectured that all graphs have an ascending subgraph decomposition (ASD). Though several classes of graphs have been shown to have an ASD, the conjecture remains open. In this paper, we investigate the similar problem for tournaments. In particular, using Kirkman Triple Systems, we will show that all tournaments of order 6 n + 3 have an ASD.",
    "actual_venue": "Graphs And Combinatorics"
  },
  {
    "abstract": "The web page for this conference announcesa \"Grand Challenge\" of crucial relevance to society: ensuring that the software of the future will be error-free.According to the Scope and ObjectivesIn the end, the conference should work towards the achievement of the long-standing challenge of the Verifying Compiler.I want to question whether this long-standing challenge is really relevant to the greater goal of achieving trustworthy software. Instead, I suggest that research in verification needs to support a larger effort to improve the software engineering process.",
    "actual_venue": "Vstte"
  },
  {
    "abstract": "Process mining is the automated acquisition of process models from event logs. Although many process mining techniques have been developed, most of them focus on mining models from the prospective of control flow while ignoring the structure of mined models. This directly impacts the understandability and quality of mined models. To address the problem, we have proposed a genetic programming (GP) approach to mining simplified process models. Herein, genetic programming is applied to simplify the complex structure of process models using a tree-based individual representation. In addition, the fitness function derived from process complexity metric provides a guideline for discovering low complexity process models. Finally, initial experiments are performed to evaluate the effectiveness of the method.",
    "actual_venue": "CIS"
  },
  {
    "abstract": "If the states of a finite-state Markov chain can be identified with designated subintervals of the unit interval, and if the transition probabilities are numerically specified, then the chain can be represented by a piecewise-linear difference equation. The representation is not unique: also, it is not necessarily piecewise-continuous. Certain characteristics of the representation are examined for the case of chains with absorbing states.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "There are numerous examples of sparse massive networks, in particular the Internet, WWW and online social networks. How do we model and learn these networks? In contrast to conventional learning problems, where we have many independent samples, it is often the case for these networks that we can get only one independent sample. How do we use a single snapshot today to learn a model for the network, and therefore be able to predict a similar, but larger network in the future? In the case of relatively small or moderately sized networks, it's appropriate to model the network parametrically, and attempt to learn these parameters. For massive networks, a non-parametric representation is more appropriate. In this talk, we first review the theory of graphons, developed over the last decade to describe limits of dense graphs, and the more the recent theory describing sparse graphs of unbounded average degree, including power-law graphs. We then show how to use these graphons as non-parametric models for sparse networks. Finally, we show how to get consistent estimators of these non-parametric models, and moreover how to do this in a way that protects the privacy of individuals on the network.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "Accurate description of hydrogen-bonding energies between water molecules and van der Waals interactions between guest molecules and host water cages is crucial for study of methane hydrates (MHs). Using high-level ab initio MP2 and CCSD(T) results as the reference, we carefully assessed the performance of a variety of exchangecorrelation functionals and various basis sets in describing the noncovalent interactions in MH. The functionals under investigation include the conventional GGA, meta-GGA, and hybrid functionals (PBE, PW91, TPSS, TPSSh, B3LYP, and X3LYP), long-range corrected functionals (omega B97X, omega B97, LC-omega PBE, CAM-B3LYP, and LC-TPSS), the newly developed Minnesota class functionals (M06-L, M06-HF, M06, and M06-2X), and the dispersion-corrected density functional theory (DFT) (DFT-D) methods (B97-D, omega B97X-D, PBE-TS, PBE-Grimme, and PW91-OBS). We found that the conventional functionals are not suitable for MH, notably, the widely used B3LYP functional even predicts repulsive interaction between CH4 and (H2O)6 cluster. M06-2X is the best among the M06-Class functionals. The omega B97X-D outperforms the other DFT-D methods and is recommended for accurate first-principles calculations of MH. B97-D is also acceptable as a compromise of computational cost and precision. Considering both accuracy and efficiency, B97-D, omega B97X-D, and M06-2X functional with 6-311++G(2d,2p) basis set without basis set superposition error (BSSE) correction are recommended. Though a fairly large basis set (e.g., aug-cc-pVTZ) and BSSE correction are necessary for a reliable MP2 calculation, DFT methods are less sensitive to the basis set and BSSE correction if the basis set is sufficient (e.g., 6-311++G(2d,2p)). These assessments provide useful guidance for choosing appropriate methodology of first-principles simulation of MH and related systems. (c) 2012 Wiley Periodicals, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "In the unlabeled data problem, data contains signals from various sources whose identities are not known apriori, yet the parameters of the individual sources must be estimated. To do this optimally, it is necessary to optimize the data PDF which may be modeled as a mixture density, jointly over the parameters of all the signal models. This can present a problem of enormous complexity if the number of signal classes is large. This paper describes a algorithm for jointly estimating the parameters of the various signal types, each with different parameterizations and associated sufficient statistics. In doing so, it maximizes the likelihood function of all the parameters jointly, but does so without incurring the full dimensionality of the problem. It allows lower-dimensional sufficient statistics to be utilized for each signal model, yet still achieves joint optimality. It uses an extension of the class-specific decomposition of the Bayes minimum error probability classifier.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Proceedings, Ieee International Conference"
  },
  {
    "abstract": "In this paper, we study the spatial structure of multiple-input multiple-output (MIMO) links in small urban macro-cells at 2.6 GHz. We resolve the double-directional structure of the radio channel. Also we compute common metrics used to characterize MIMO links, i.e. the structure of singular values and the resulting capacity. In a line-of-sight (LOS) link, we find that local scattering is not enough to create full-rank MIMO channels. Even behind a sector, we observe a low-rank channel. In a non-line-of-sight (NLOS) scenario, although we resolve several multi-paths with individual delay, direction of departure (DoD) and direction of arrival (DoA), the impact of local scattering is limited. Singular values indicate few more degrees of freedom for NLOS channels, but less than for a random matrix. We can model the spatial degrees of freedom in small cells better by assuming a random but small number of nearly specular paths feeding some local scattering widely spread in azimuth and elevation. Based on our results, we discuss modeling aspects and the value of large antenna arrays in mobile networks.",
    "actual_venue": "Vehicular Technology Conference"
  },
  {
    "abstract": "In this study, we develop a supply chain model with one manufacturer and one retailer, where the market demand might be subject to disruption and neither the manufacturer nor the retailer can perfectly predict whether this market disruption will occur ex ante. However, the retailer can resort to his information system to acquire more accurate disruption information than the manufacturer, and this disruption forecast information is the retailer's private information. Under this framework, we first derive the optimal outcomes of two representative categories of contract formats (i.e., the simplest but most widely used wholesale price contract, and the most sophisticated but rarely implemented menu of contracts), and then examine the impacts of the reliability of the retailer's private information about the market state (regular or disruption) on the supply chain performance. Our results demonstrate that the manufacturer's expected profit is quasi-convex in the reliability (i.e., U-shaped), and the retailer's expected profit is a piecewise (weakly) increasing function of the reliability. Specifically, there exists a threshold for the retailer's information reliability. Above the threshold, both manufacturer and retailer benefit from the improvement in the information reliability. This means that the manufacturer actually can profit from the exaggerated level of information asymmetry. Below the threshold, the improvement is still favorable to the retailer, but detrimental to the manufacturer due to increased information rents, and thus the manufacturer has no incentive to help the retailer to improve his forecast capability. At the threshold, the retailer's profit experiences a cliff-like drop, while the manufacturer's profit achieves its minimum value. The result is applicable to both wholesale price contract and menu of contracts.",
    "actual_venue": "International Transactions In Operational Research"
  },
  {
    "abstract": "In this work a previously published bioinformatics pipeline was reimplemented across various computational platforms, and the performances of its steps evaluated. The tested environments were: (I) dedicated bioinformatics-specific server (II) low-power single node (III) HPC single node (IV) virtual machine. The pipeline was tested on a use case of the analysis of a single patient to assess single-use performances, using the same configuration of the pipeline to be able to perform meaningful comparison and search the optimal environment/hybrid system configuration for biomedical analysis. Performances were evaluated in terms of execution wall time, memory usage and energy consumption per patient. Our results show that, albeit slower, low power single nodes are comparable with other environments for most of the steps, but with an energy consumption two to four times lower. These results indicate that these environments are viable candidates for bioinformatics clusters where long term efficiency is a factor.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "We present three detector/estimators (DEs) which allow multiuser detection and parameter estimation without a side channel in a dynamic asynchronous code-division multiple-access (CDMA) system in which users are entering and leaving the system. These DEs optimally detect a new user given only the chip rate and the spreading factor of the new user. Two of these DEs, the maximum-likelihood detector/estimator (MLDE) and the generalized maximum-likelihood detector/estimator (GMLDE), produce maximum-likelihood estimates of the new user's signature sequence, delay, and amplitude, which are then incorporated into a multiuser detector. The third DE, the cyclic detector/estimator (CDE), is the most computationally efficient of the three processors. This DE detects the new user by testing for cyclostationarity and then uses suboptimal schemes to estimate the new user's signature sequence, delay, and amplitude. Simulations indicate that all three DEs reliably detect a new user for an Es/σ2 (symbol-energy-to-noise ratio) of 5 dB. The MLDE and GMLDE produce signature sequence and delay estimates with probability of error less than 0.07 for an Es/σ2 of 10 dB, and the CDE produces signature sequence and delay estimates with probability of error less than 0.13 for an Es/σ2 of 15 dB",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "The classical approach in phonetics of careful observa- tion of individual utterances can, this paper contends, be usefully augmented with automatic statistical analyses of large amounts of speech. Such analyses, using methods derived from speech recognition, are shown to quantify several known phonetic phenomena, most of which re- quire syllable structure to be taken into account, and reveal some apparently new phenomena. Practical speech recognition normally ignores syllable structure. This paper presents quantitative evidence that prevocalic and postvocalic consonants behave differently. It points out some ways in which current speech recognition can be improved by taking syllable boundaries into account.",
    "actual_venue": "Interspeech"
  },
  {
    "abstract": "HEVC, which is the latest video coding standard, resulting in much higher compression efficiency than any previous standards. It is expected to take the place of the widely deployed standard H.264. The final version of HEVC has been published by ISO/IEC and ITU-T at January 2013. To speed up its adoption and application, highly efficient transcoding techniques among current deployed video standards and various versions of HEVC bitstreams are needed. Hence, an accelerated HEVC bit-rate transcoder, based on the cascaded pixel-domain framework, is proposed in this paper. Based on thorough statistical analysis, the proposed algorithm mainly utilizes information from the input video stream such as Code Unit (CU) depths, Prediction Unit PU) partitions as well as the image complexity to speedup the transcoding procedure. Experimental results demonstrate the superior performance of the proposed algorithm and its suitability for a wide range of bit-rate reduction tasks.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "The presence of a solid architectural vision is a key discriminator in the success or failure of a software project. This tutorial examines what software architecture is and what it is not. It discusses and illustrates how to describe architecture through a set of design viewpoints and views and how to express these views in the UML, in the spirit of the new IEEE Standard 1471:2000:Recommended practice for architectural description. The tutorial shows of how architectures drive the development process and how to capture architectural design patterns using the UML. It is illustrated by several widely applicable architectural patterns in different domain.",
    "actual_venue": "Orlando, Fl, Usa"
  },
  {
    "abstract": "Cooperative social interaction is critical for human social development and learning. Despite the importance of social interaction, previous neuroimaging studies lack two fundamental components of everyday face-to-face interactions: contingent responding and joint attention. In the current studies, functional MRI data were collected while participants interacted with a human experimenter face-to-face via live video feed as they engaged in simple cooperative games. In Experiment 1, participants engaged in a live interaction with the experimenter (“Live”) or watched a video of the same interaction (“Recorded”). During the “Live” interaction, as compared to the Recorded conditions, greater activation was seen in brain regions involved in social cognition and reward, including the right temporoparietal junction (rTPJ), anterior cingulate cortex (ACC), right superior temporal sulcus (rSTS), ventral striatum, and amygdala. Experiment 2 isolated joint attention, a critical component of social interaction. Participants either followed the gaze of the live experimenter to a shared target of attention (“Joint Attention”) or found the target of attention alone while the experimenter was visible but not sharing attention (“Solo Attention”). The right temporoparietal junction and right posterior STS were differentially recruited during Joint, as compared to Solo, attention. These findings suggest the rpSTS and rTPJ are key regions for both social interaction and joint attention. This method of allowing online, contingent social interactions in the scanner could open up new avenues of research in social cognitive neuroscience, both in typical and atypical populations.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Efficient mechanisms for data structuring and formatting are indispensable for managing data traffic between and within federated Cloud environments to avoid excessive bandwidth cost and to ensure portability and interoperability. This facilitates provider-agnostic communication, which is essential for interoperable inter-Cloud deployments and portable integration of service components, both with one another and with the underlying Cloud platform. The existing data interchange formats for structuring and serialising data have not yet been analysed in the context of data communication in Clouds. Thus, to address this issue, the determination of an appropriate data interchange format for Clouds is necessary. In this paper, we present a performance analysis of some selected data interchange formats to assess their efficiency in terms of their usability in realising a common messaging format for communicating data in Clouds. We first describe the characteristics of each data format for clear understanding. As a basis for the analysis, we introduce a Cloud use case scenario comprising the transmission of monitored data as messages. The communication means is achieved with a novel message bus system designed to integrate the data interchange formats. We present some evaluations of the formats and compare their compactness for supporting efficient data transmission in Clouds.",
    "actual_venue": "Utility And Cloud Computing"
  },
  {
    "abstract": "The paper surveys some relevant issues related to data management in pervasive environments. Particularly, it focuses on the compression of semantic annotations for building so called Semantic Web of Things. An approach is proposed to achieve good compression ratios, to maintain compression effectiveness and finally to query compressed data without requiring expensive decompression phases. Experimental results prove the goodness of the proposed framework.",
    "actual_venue": "Icsc"
  },
  {
    "abstract": "In 1972, Reynolds outlined a general method for eliminating functional arguments known as defunctionalization. The idea underlying defunctionalization is encoding a functional value as first-order data, and then realizing the applications of the encoded function via an apply function. Although this process is simple enough, problems arise when defunctionalization is used in a polymorphic language. In such a language, a functional argument of a higher-order function can take different type instances in different applications. As a consequence, its associated apply function can be untypable in the source language. In the paper we present a defunctionalization transformation which preserves typability. Moreover, the transformation imposes no restriction on functional arguments of recursive functions, and it handles functions as results as well as functions encapsulated in constructors. The key to this success is the use of type information in the defunctionalization transformation. Run-time characteristics are preserved by defunctionalization; hence, there is no performance improvement coming from the transformation itself. However closures need not be implemented to compile the transformed program.",
    "actual_venue": "Icfp"
  },
  {
    "abstract": "Air Quality Index (AQI) is an indicator of the rank of air pollution that is very vital for the environmental impacts to the public health. In this paper, we propose an association model between visual feature and AQI rank of lifelog data. Visual data (i.e., environmental pictures) and numerical data (i.e., environmental AQI measurements) of lifelog are utilized for the data training stage. The features of the visual data are extracted using a CNN-based method, where the latter are calculated using the standard AQI ranking. The extracted visual features and ranked AQI are combined as the input data for a deep neural network MLP (Multi-layer Perception) to study the association relationship between visual feature and AQI rank. The experimental results show that the proposed method can provide accurate predictions of good or unhealthy AQI ranks from lifelog visual data.",
    "actual_venue": "Ieee International Conference On Big Data"
  },
  {
    "abstract": "This paper presents the trends and directions of future agent design. These trends focus on advances in Agent-Oriented programming techniques and describe how agent technologies have developed with increasingly sophisticated techniques. The paper traces the origins of agent technology and describes related principles, especially those that reflect heavily on “Intelligence with Interaction”. This interaction, especially with operator-in-the-loop scenarios, is demonstrated as a critical component of modern Distributed Artificial intelligence. Unfortunately many applications fail to address this fact, creating an interruption to ongoing research. Therefore we believe the next generation of agent systems should focus on human centric interaction to achieve a level of shared- intelligence made possible with modern computational techniques.",
    "actual_venue": "KES (2)"
  },
  {
    "abstract": "The space of images can be equipped with a Riemannian metric measuring both the cost of transport of image intensities and the variation of image intensities along motion lines. The resulting metamorphosis model was introduced and analyzed in [M. I. Miller and L. Younes, Int. J. Comput. Vis., 41 (2001), pp. 61-84; A. Trouve and L. Younes, Found. Comput. Math., 5 (2005), pp. 173-198], and a variational time discretization for the geodesic interpolation was proposed in [B. Berkels, A. Effiand, and M. Rumpf, SIAM J. Imaging Sci., 8 (2015), pp. 1457-1488]. In this paper, this time discrete model is expanded and an image extrapolation via a discretization of the geometric exponential map is consistently derived for the variational time discretization. For a given weakly differentiable initial image and an initial image variation, the exponential map allows one to compute a discrete geodesic extrapolation path in the space of images. It is shown that a time step of this shooting method can be formulated in the associated deformations only. For sufficiently small time steps, local existence and uniqueness are proved using a suitable fixed point formulation and the implicit function theorem. A spatial Galerkin discretization with cubic splines on coarse meshes for the deformations and piecewise bilinear finite elements on fine meshes for the image intensities are used to derive a fully practical algorithm. Different applications underline the efficiency and stability of the proposed approach.",
    "actual_venue": "Siam Journal On Imaging Sciences"
  },
  {
    "abstract": "Using an empirical survey conducted among B2B and B2C companies in India, the paper examines the linkage between social media use, customer relationship management (CRM), social customer relationship management (SCRM) and the customer relationship performance (CRP). A framework was constructed using literature review and validated by the regression analysis. The findings show that the social media use and CRM capabilities interact positively to build SCRM capabilities which then have positive impacts on CRP. The linkages differ slightly for business to business (B2B) and business to consumers (B2C) companies. The paper also shares the challenges faced by the businesses in implementing the SCRM.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The Magnetic Resonance Imaging (MRI) signal can be made sensitive to functional parameters that provide information about tissues. In dynamic contrast enhanced (DCE) MRI these functional parameters are related to the microvasculature environment and the concentration changes that occur rapidly after the injection of a contrast agent. Typically DCE images are reconstructed individually and kinetic parameters are estimated by fitting a pharmacokinetic model to the time-enhancement response; these methods can be denoted as \"indirect\". If undersampling is present to accelerate the acquisition, techniques such as kt-FOCUSS can be employed in the reconstruction step to avoid image degradation. This paper suggests a Bayesian inference framework to estimate functional parameters directly from the measurements at high temporal resolution. The current implementation estimates pharmacokinetic parameters (related to the extended Tofts model) from undersampled (k, t)-space DCE MRI. The proposed scheme is evaluated on a simulated abdominal DCE phantom and prostate DCE data, for fully sampled, 4 and 8-fold undersampled (k, t)-space data. Direct kinetic parameters demonstrate better correspondence (up to 70% higher mutual information) to the ground truth kinetic parameters (of the simulated abdominal DCE phantom) than the ones derived from the indirect methods. For the prostate DCE data, direct kinetic parameters depict the morphology of the tumour better. To examine the impact on cancer diagnosis, a peripheral zone prostate cancer diagnostic model was employed to calculate a probability map for each method.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "This article investigates the technological development process in local firms in joint venture partnerships with foreign firms. It takes the perspective of the local firms located in a developing economy and presents a model of dynamic technology management strategies (MDTMS) of local firms for sustained technological development. Based on an analysis of six firms from the rubber products and garment accessories manufacturing in Sri Lanka, it reveals how the level of skills and competence development within the local firm, the potential for technological contribution by the foreign partner firm and the level of autonomy of the local firms are important determinants for shaping technology management strategies. It argues that these successful Sri Lankan firms adopt dynamic technology management strategies because of the evolving nature of the partnership. The proposed model has significant implications for the technology management practices of local firms in developing economies that engage in foreign partnerships.",
    "actual_venue": "International Journal Of Technology Management"
  },
  {
    "abstract": "In this paper, the problem of determining the approximate stability regions of large-scale time-delay systems (LS TDS) is solved using model approximation techniques. To achieve this, an ℋ\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub>\n-oriented approximation algorithm, referred to as TF-IRKA [1], is considered. This algorithm has been shown to be well suited for the approximation of infinite-dimensional systems into finite-dimensional ones. We show here how model reduction can be used to approximate time-delay systems with multiple delays and estimate their stability regions. Discussions regarding the adaptation of existing algorithms to the considered problem are also provided. Several numerical examples illustrate the efficiency and accuracy of the approach.",
    "actual_venue": "European Control Conference"
  },
  {
    "abstract": "Recent developments in Big Data are increasingly focusing on supporting computations in higher data velocity environments, including processing of continuous data streams in support of the discovery of valuable insights in real-time. In this work we investigate performance of streaming engines, specifically we address a problem of identifying optimal parameters that may affect the throughput (messages processed/second) and the latency (time to process a message). These parameters are also function of the parallelism property, i.e. a number of additional parallel tasks (threads) available to support parallel computation. In experimental evaluation we identify optimal cluster performance by balancing the degree of parallelism with number of nodes, which yield maximum throughput with minimum latency.",
    "actual_venue": "Adbis (Short Papers And Workshops"
  },
  {
    "abstract": "Several organizations are developing software processes twenty-four hours, seven days per week, with geographically distributed teams. This environment of software development enables to implement the Follow-the-Sun (FTS) strategy. In this study, we perform a mapping of the literature based upon electronic searching in digital libraries to identify applied practices in development environments twenty-four hours in which can be apply FTS strategy. Ours results present practices and many key aspects to FTS implementation.",
    "actual_venue": "Global Software Engineering"
  },
  {
    "abstract": "•An Au-plated Ag wire is a new method to overcome reliability problems.•The Au-coated layer can increase the oxidation resistance.•A new test method, electrical tensile test, is proposed to confirm the wires.•The Au-plated Ag wire is a robust and efficient scheme.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "There has been some discussion in the industrial control system security community of evaluation and certification. There are already at least two independent third party evaluators, and some have advocated common criteria certification of products used in critical systems. The broader IT security community has considerable experience of evaluation and certification, which we seek to summarise and share in this paper. Certification is not a silver bullet, and can very easily end up as spin rather than substance: as `security theatre' designed to reassure customers or regulators rather than a genuine risk-reduction mechanism. It can also be very expensive, and once entrenched it can impose deadweight costs on industry that are difficult to eliminate even when certification processes are widely seen as failing. We discuss a number of further issues such as perverse incentives, usability and liability and argue that the industry should proceed with great caution.",
    "actual_venue": "Mallorca"
  },
  {
    "abstract": "Automated segmentation of the esophagus in CT images is of high value to radiologists for oncological examinations of the mediastinum. It can serve as a guideline and prevent confusion with pathological tissue. However, segmentation is a challenging problem due to low contrast and versatile appearance of the esophagus. In this paper, a two step method is proposed which first finds the approximate shape using a \"detect and connect\" approach. A classifier is trained to find short segments of the esophagus which are approximated by an elliptical model. Recently developed techniques in discriminative learning and pruning of the search space enable a rapid detection of possible esophagus segments. Prior shape knowledge of the complete esophagus is modeled using a Markov chain framework, which allows efficient inferrence of the approximate shape from the detected candidate segments. In a refinement step, the surface of the detected shape is non-rigidly deformed to better fit the organ boundaries. In contrast to previously proposed methods, no user interaction is required. It was evaluated on 117 datasets and achieves a mean segmentation error of 2.28mm with less than 9s computation time.",
    "actual_venue": "Miccai"
  },
  {
    "abstract": "A power-efficient quadrature receiver employing a down-converter that uses a passive current-commutating mixer for frequency translation is presented. The architecture uses bias-current sharing between the RF and baseband stages while making the full supply voltage available to either stage. An input transconductor, realized using a differential common-source stage, converts the RF signal into a c...",
    "actual_venue": "Ieee Journal Of Solid-State Circuits"
  },
  {
    "abstract": "•Scalable method using voxelization to produce smaller and homogeneous point clouds.•Snapshots of the point cloud for 2D, two modalities RGB and geometric features.•Segmentation using deep segmentation network.•First place on Lidar benchmark Semantic8.•Experimentation on photogrammetric and RGBD data.",
    "actual_venue": "Computers And Graphics-Uk"
  },
  {
    "abstract": "Seasonal climate forecasts (SCFs) have received a lot of attention for climate risk management in agriculture. The question is, how can we use SCFs for informing decisions in agriculture? SCFs are provided in formats not so conducive for decision-making. The commonly issued tercile probabilities of most likely rainfall categories i.e., below normal (BN), near normal (NN) and above normal (AN), are not easy to translate into metrics useful for decision support. Linking SCF with crop models is one way that can produce useful information for supporting strategic and tactical decisions in crop production e.g., crop choices, management practices, insurance, etc. Here, we developed a decision support system (DSS) tool, Climate-Agriculture-Modeling and Decision Tool (CAMDT), that aims to facilitate translations of probabilistic SCFs to crop responses that can help decision makers adjust crop and water management practices that may improve outcomes given the expected climatic condition of the growing season.",
    "actual_venue": "Environmental Modelling And Software"
  },
  {
    "abstract": "•Propose the semi-supervised adversarial classification (SSAC) model without using parameter sharing.•Use learnable layers to transfer image representation from the reconstruction network to the classification network.•Use adversarial learning in the reconstruction network for performance gain.•Extend SSAC to MK-SSAC for lung nodule classification and achieved state-of-the-art performance.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "In a distributed computing environment, availability of resources required by a task is an important factor for successful completion. Especially in Grid Computing, the nodes are not completely dedicated, hence efficient utilization of computing resources is a challenge. If a task requires a certain amount of CPU, memory, network bandwidth, storage space etc., it may fail or delay execution in case any or all of the resources required are not available. In this paper, a preventive scheduling mechanism is described, which takes into consideration the resource utilization of the nodes and based on the requirement, the tasks are scheduled with a fair chance of success. A comparison of this method with First come first serve algorithm is performed for nodes at different levels of load on the resources.",
    "actual_venue": "Icacci"
  },
  {
    "abstract": "We consider a system of two reaction-diffusion-advection equations describing the one dimensional directed motion of particles with superimposed diffusion and mutual alignment. For this system we show the existence of traveling wave solutions for weak diffusion by singular perturbation techniques and provide evidence for their existence also for stronger diffusion. We discuss different types of wave fronts and their composition into more complex patterns and illustrate the emergence of these patterns from generic initial data by simulations. We also investigate the dependence of the wave velocities on the model parameters.",
    "actual_venue": "Siam Journal On Applied Mathematics"
  },
  {
    "abstract": "The numerical solution of partial differential equations frequently requires solving large and sparse linear systems. When using the Finite Element Method these systems exhibit a natural block structure that is exploited for efficiency in the 'Iterative Solver Template Library' (ISTL). Based on existing sequential preconditioned iterative solvers we present an abstract parallelisation approach which clearly separates the parallelisation aspects from the data structures and solver algorithms by imposing an abstract consistency model onto the building blocks of the iterative solver components. This allows for supporting overlapping and non-overlapping domain decompositions as well as data parallel implementations of standard linear solvers.",
    "actual_venue": "Ijcse"
  },
  {
    "abstract": "In the entertainment industry, technology advances are continuously improving and evolving the portfolio of services and distinguished features that can be experienced by the audience, whether it is attending the event on site or remotely. In this context, the SkyMedia system aims at exploring, designing, and demonstrating a novel multimedia end-to-end architecture that can provide unique immersive media experiences to audience during live events. UAV and WSN technologies play a central role as enabling technologies. UAVs capture HD and 3D videos and images from the sky, while wireless sensors form an augmentation network acquiring side-information to be fused with the main audiovisual contents. All technologies come together to form a very advanced Multimedia Service Platform able to manage in real-time HD/3D processing, and an immersive media experience demonstration is going to be performed during a public live event in order to prove the effectiveness of the system.",
    "actual_venue": "Multimedia And Expo"
  },
  {
    "abstract": "•Business processes models can be retrieved by reverse engineering.•Business process models can be mined with quality problems.•Quality problems are related to completeness, relevance or granularity.•Refactoring improve process model quality issues, but not widely used in industry.•An industrial case study demonstrates business process refactoring is feasible.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Segmentation of the object of interest is a difficult step in the analysis of digital images. Fully automatic methods sometimes fail, producing incorrect results and requiring the intervention of a human operator. This is often true in medical applications, where image segmentation is particularly difficult due to restrictions imposed by image acquisition, pathology and biological variation. In this paper we present an early review of the largely unknown territory of human–computer interaction in image segmentation. The purpose is to identify patterns in the use of interaction and to develop qualitative criteria to evaluate interactive segmentation methods. We discuss existing interactive methods with respect to the following aspects: the type of information provided by the user, how this information affects the computational part, and the purpose of interaction in the segmentation process. The discussion is based on the potential impact of each strategy on the accuracy, repeatability and interaction efficiency. Among others, these are important aspects to characterise and understand the implications of interaction to the results generated by an interactive segmentation method. This survey is focused on medical imaging, however similar patterns are expected to hold for other applications as well.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "This paper presents dsync, a file transfer system that can dynamically adapt to a wide variety of environments. While many transfer systems work well in their specialized ontext, their performance comes at the cost of generality, and they perform poorly when used elsewhere. In contrast, dsync adapts to its environment by intelligently determining which of its available resources is the best to use at any given time. The resources dsync can draw from include the sender, the local disk, and network peers. While combining these resources may appear easy, in practice it is difficult because these resources may have widely different performance or contend with each other. In particular, the paper presents a novel mechanism that enables dsync to aggressively search the receiver's local disk for useful data without interfering with concurrent network transfers. Our evaluation on several workloads in various network environments shows that dsync outperforms existing systems by a factor of 1.4 to 5 in one-to-one and one-to-many transfers.",
    "actual_venue": "Usenix Annual Technical Conference"
  },
  {
    "abstract": "In-network Processing, involving operations such as filtering, compression, and fusion is a technique widely used in wireless sensor and ad hoc networks for reducing the communication overhead. In many tactical stream-oriented applications, especially in military scenarios, both link bandwidth and node energy are critically constrained resources. For such applications, in-network processing itself imposes nonnegligible computing cost. In this work, we have developed a unified, utility-based closed-loop control framework that permits distributed convergence to both 1) the optimal level of compression performed by a forwarding node on streams, and 2) the best set of nodes where the operators of the stream processing graph should be deployed. We also show how the generalized model can be adapted to more realistic cases, where the in-network operator may be varied only in discrete steps, and where a fusion operation cannot be fractionally distributed across multiple nodes. Finally, we provide a real-time implementation of the protocol on an 802.11b network with a video application and show that the performance of the network is improved significantly in terms of the packet loss, node lifetime, and quality of video received.",
    "actual_venue": "Ieee Trans Mob Comput"
  },
  {
    "abstract": "This paper focuses on identification of the relationships between a disease and its potential risk factors using Bayesian networks in an epidemiologic study, with the emphasis on integrating medical domain knowledge and statistical data analysis. An integrated approach is developed to identify the risk factors associated with patients' occupational histories and is demonstrated using real-world data. This approach includes several steps. First, raw data are preprocessed into a format that is acceptable to the learning algorithms of Bayesian networks. Some important considerations are discussed to address the uniqueness of the data and the challenges of the learning. Second, a Bayesian network is learned from the preprocessed data set by integrating medical domain knowledge and generic learning algorithms. Third, the relationships revealed by the Bayesian network are used for risk factor analysis, including identification of a group of people who share certain common characteristics and have a relatively high probability of developing the disease, and prediction of a person's risk of developing the disease given information on his/her occupational history. Copyright (C) 2007 John Wiley & Sons, Ltd.",
    "actual_venue": "Quality And Reliability Engineering International"
  },
  {
    "abstract": "Localization (a.k.a. locationing) is a central concern for ubiquitous self-configuring sensor networks. In this paper the implementation of a distributed, least-squares-based localization algorithm is presented. Low power and energy dissipation are key requirements for sensor networks. As part of the sensor network, the localization system must also conform to these requirements. An ultralow-power and dedicated hardware implementation of the localization system is therefore presented. The cost of fixed-point implementation is also investigated. The design is implemented in a 0.13mu CMOS process. It dissipates 1.7m W of active power and 0.122J/op of active energy with a silicon area of 0.55mm(2). The mean calculated location error due to fixed-point implementation is shown to be 6%.",
    "actual_venue": "Proceedings Of Mobiquitous"
  },
  {
    "abstract": "We investigate the practice of websites selling counterfeit goods. We inspect web search results for 225 queries across 25 brands. We devise a binary classifier that predicts whether a given website is selling counterfeits by examining automatically extracted features such as WHOIS information, pricing and website content. We then apply the classifier to results collected between January and August 2014. We find that, overall, 32% of search results point to websites selling fakes. For 'complicit' search terms, such as \\\"replica rolex\\\", 39% of the search results point to fakes, compared to 20% for 'innocent' terms, such as \\\"hermes buy online\\\". Using a linear regression, we find that brands with a higher street price for fakes have higher incidence of counterfeits in search results, but that brands who take active countermeasures such as filing DMCA requests experience lower incidence of counterfeits in search results. Finally, we study how the incidence of counterfeits evolves over time, finding that the fraction of search results pointing to fakes remains remarkably stable.",
    "actual_venue": "WWW"
  },
  {
    "abstract": "In this paper, we present a visual servoing method based on a learned mapping between feature space and control space. Using a suitable recognition algorithm, we present and evaluate a complete method that simultaneously learns the appearance and control of a low-cost robotic arm. The recognition part is trained using an action precedes perception approach. The novelty of this paper, apart from the visual servoing method per se, is the combination of visual servoing with gripper recognition. We show that we can achieve high precision positioning without knowing in advance what the robotic arm looks like or how it is controlled.",
    "actual_venue": "Image Vision Comput"
  },
  {
    "abstract": "The effect of a saturation-type error nonlinearity in the weight update equation in least-mean-squares (LMS) adaptation is investigated for a white Gaussian data model. Nonlinear difference equations are derived for the eight first and second moments, which include the effect of an error function (erf) saturation-type nonlinearity on the error sequence driving the algorithm. A nonlinear difference equation for the mean norm is explicitly solved using a differential equation approximation and integration by quadratures. The steady-state second-moment weight behavior is evaluated exactly for the erf nonlinearity. Using the above results, the tradeoff between the extent of error saturation, steady-state excess mean-square error, and rate of algorithm convergence is studied. The tradeoff shows that (1) starting with a sign detector, the convergence rate is increased by nearly a factor of two for each additional bit, and (2) as the number of bits is increased further, the additional bit by very little in convergence speed, asymptotically approaching the behavior of the linear algorithm",
    "actual_venue": "Ieee Transactions On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "RNA viruses such as HIV, Influenza, impose very significant disease burden throughout the world. Identifying key protein residue determinants that affect a given viral phenotype is an important step in learning the genotype-phenotype mapping and making clinic decisions. This identification is currently done through a laborious experimental process which is arguably inefficient, incomplete, and unreliable. We describe a supervised combinatorial filtering algorithm that systematically and efficiently infers the correct set of key residue positions from all available labeled data. We demonstrate its consistency, validate it on a variety of datasets, show the superior power to conventional identification methods, and describe its use under incremental relaxation of constraints. For cases where more data is needed to fully converge to an answer, we introduce an active learning algorithm to help choose the most informative experiment from a set of unlabeled candidate strains or mutagenesis experiments, so as to minimize the expected total laboratory time or financial cost. As an example, we demonstrate the savings afforded by this algorithm in identifying the molecular determinants of fusogenicity from a previously published dataset of Feline Immunodeficiency Virus Envelope proteins.",
    "actual_venue": "Bibe"
  },
  {
    "abstract": "We consider a queueing system with multitype customers and flexible (multiskilled) servers that work in parallel. IfQ iis the queue length of typei customers, this queue incurs cost at the rate ofC i ( Q i ), whereC i (.) is increasing and convex. We analyze the system in heavy traffic (Harrison and Lopez 1999) and show that a very simple generalizedc脗µ-rule (Van Mieghem 1995) minimizes both instantaneous and cumulative queueing costs, asymptotically, over essentially all scheduling disciplines, preemptive or non-preemptive. This rule aims at myopically maximizing the rate of decrease of the instantaneous cost at all times, which translates into the following: when becoming free, serverj chooses for service a typei customer such thati ? arg max i C' i ( Q i )脗µ ij , where 脗µ ijis the average service rate of typei customers by serverj.An analogous version of the generalizedc脗µ-rule asymptotically minimizes delay costs. To this end, let the cost incurred by a typei customer be an increasing convex functionC i ( D) of its sojourn timeD. Then, serverj always chooses for service a customer for which the value ofC' i ( D) 脗µ ijis maximal, whereD andi are the customer's sojourn time and type, respectively.",
    "actual_venue": "Operations Research"
  },
  {
    "abstract": "The task of building effective representations to visualize and explore collections with moderate to large number of documents is hard. It depends on the evaluation of some distance measure among texts and also on the representation of such relationships in bidimensional spaces. In this paper we introduce an alternative approach for building visual maps of documents based on their content similarity, through reconstruction of phylogenetic trees. The tree is capable of representing relationships that allows the user to quickly recover information detected by the similarity metric. For a variety of text collections of different natures we show that we can achieve improved exploration capability and more clear visualization of relationships amongst documents.",
    "actual_venue": "Ieee Vast"
  },
  {
    "abstract": "In our proposed structure, mobile nodes are organized into nonoverlapping clusters which have adaptive variable-sizes according to their respective mobility. The mobility-based clustering (MBC) approach we are proposing uses a combination of both physical and logical partitions of the network (i.e. geographic proximity and functional relation between nodes, such as mobility pattern etc.)",
    "actual_venue": "Int Journal Of Network Management"
  },
  {
    "abstract": "This letter proposes the use of denoising in conjunction with 2-D empirical mode decomposition (2D-EMD) of hyperspectral image bands for higher classification accuracy. Initially, 2D-EMD is performed to hyperspectral image bands for decomposition into intrinsic mode functions (IMFs). Then, denoising is applied to the first IMF of each band because this IMF includes local high-spatial-frequency components. Features reconstructed as the sums of lower order IMFs are then used for classification. Support vector machine classification is used as a classification approach in this letter. Experimental results show that the proposed technique can provide a higher classification accuracy.",
    "actual_venue": "Ieee Geosci Remote Sensing Lett"
  },
  {
    "abstract": "A large number of personal digital traces is constantly generated or available online from a variety of sources, such as social media, calendars, purchase history, etc. These personal data traces are fragmented and highly heterogeneous, raising the need for an integrated view of the user's activities. Prior research in Personal Information Management focused mostly on creating a static model of the world (objects and their relationships). We argue that a dynamic world view is also helpful for making sense of collections of related personal documents, and propose a partial solution based on scripts -- a theoretically well-founded idea in AI and Cognitive Science. Scripts are stereotypical hierarchical plans for everyday activities, involving interactions between mostly social agents. We augment these with hints of the digital traces that they can leave. By connecting Personal Digital Traces through scripts, we can build an episodic view of users' digital memories, which allow users to explore related events and actions in an integrated way. The paper uses the Eating_Out script for illustration, and ends with a report on the results of a case-study of applying a prototype implementation on real user data.",
    "actual_venue": "Exploredb@Sigmod/Pods"
  },
  {
    "abstract": "Hybrid electric vehicles (HEVs) have gained much more attention recently due to declining fossil resources and greenhouse effect caused by COx and NOx emissions. Optimization of the parameters is a key element for the success of a HEV. Recently, Genetic Algorithm (GA) is widely applied in the optimization of HEV. Since it has conquered the deficiencies of the gradient-based optimization algorithms that require calculating the derivative of the objective function, GA is suitable for this non-linear optimization problem. The objective of this paper is to control the speed of Nonlinear Hybrid Electric Vehicle (HEV) by controlling the throttle position so as to get improved fuel economy, driving safety, reduced pollution and manufacturing cost. To control the speed, tuning of Proportional-Integral-Derivative (PID) controller is done using Genetic Algorithm (GA) Optimization method. The performance of the technique is evaluated by setting the objective function as mean square error (MSE) and is also compared with Ziegler-Nichols method.",
    "actual_venue": "Contemporary Computing"
  },
  {
    "abstract": "Wang et al. have proposed collision attacks for various hash functions. Their approach is to first construct a differential path, and then determine the conditions (sufficient conditions) that maintain the differential path. If a message that satisfies all sufficient conditions is found, a collision can be generated. Therefore, in order to apply the attack of Wang et al., we need techniques for constructing differential paths and for determining sufficient conditions. In this paper, we propose the “SC algorithm”, an algorithm that can automatically determine the sufficient conditions. The input of the SC algorithm is a differential path, that is, all message differentials and differentials of the chaining variables. The SC algorithm then outputs the sufficient conditions. The computation time of the SC algorithm is within few seconds. In applying the method of Wang et al. to MD5, there are 3 types of sufficient conditions: conditions for controlling the carry length when differentials appear in the chaining variables, conditions for controlling the output differentials of the Boolean function when the input variables of the function have differentials and conditions for controlling the relationship between the carry effect and left rotation operation. Sufficient conditions for SHA-1, SHA-0 and MD4 consist of only Type 1 and Type 2. Type 3 is unique to MD5. The SC algorithm can construct Type 1 and Type 2 conditions; we use the method of Liang et al. to construct Type 3 conditions. The complexity of the collision attack depends on the number of sufficient conditions needed. The SC algorithm constructs the fewest possible sufficient conditions. To check the feasibility of the SC algorithm, we apply it to the differential path of MD5 given by Wang et al. It is shown to yield 12 fewer conditions than the latest work on MD5. The SC algorithm is applicable to the MD-family and the SHA-family. This paper focuses on the sufficient conditions of MD5, but only as an example.",
    "actual_venue": "Vietcrypt"
  },
  {
    "abstract": "We examine \"religion-online,\" an underrepresented area of research in new media, communication, and geography, with a multilevel study of the online representation and (re) presentation of Protestant Christian organizations in Singapore, which has one of the highest Internet penetration rates in the world and also believers affiliated with all the major world religions. We first critically discuss and empirically examine how online technologies are employed for religious community building in novel and diverse ways. Then we investigate the role religious leaders play through their mental representations of the spatial practices and scales through which their religious communities are imagined and practiced online. We show how churches use the multimodality of the Internet to assemble multiple forms of visible data and maps to extend geographic sensibilities of sacred space and create new social practices of communication.",
    "actual_venue": "Information Society"
  },
  {
    "abstract": "We develop logarithmic approximation algorithms for extremely general formulations of multiprocessor multi-interval offline task scheduling to minimize power usage. Here each processor has an arbitrary specified power consumption to be turned on for each possible time interval, and each job has a specified list of time interval/processor pairs during which it could be scheduled. (A processor need not be in use for an entire interval it is turned on.) If there is a feasible schedule, our algorithm finds a feasible schedule with total power usage within an O(log n) factor of optimal, where n is the number of jobs.(Even in a simple setting with one processor, the problem is Set-Cover hard.) If not all jobs can be scheduled and each job has a specified value, then our algorithm finds a schedule of value at least (1-ε) Z and power usage within an O(log(1/ε)) factor of the optimal schedule of value at least Z, for any specified Z and ε 0. At the foundation of our work is a general framework for logarithmic approximation to maximizing any submodular function subject to budget constraints.",
    "actual_venue": "Spaa"
  },
  {
    "abstract": "Major compiler verification efforts, such as the CompCert project, have traditionally simplified the verification problem by restricting attention to the correctness of whole-program compilation, leaving open the question of how to verify the correctness of separate compilation. Recently, a number of sophisticated techniques have been proposed for proving more flexible, compositional notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the simple \"closed simulations\" used in verifying whole-program compilation. Applying such techniques to a compiler like CompCert, as Stewart et al. have done, involves major changes and extensions to its original verification. In this paper, we show that if we aim somewhat lower---to prove correctness of separate compilation, but only for a *single* compiler---we can drastically simplify the proof effort. Toward this end, we develop several lightweight techniques that recast the compositional verification problem in terms of whole-program compilation, thereby enabling us to largely reuse the closed-simulation proofs from existing compiler verifications. We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting its verification of whole-program compilation into a verification of separate compilation in less than two person-months. This conversion only required a small number of changes to the original proofs, and uncovered two compiler bugs along the way. The result is SepCompCert, the first verification of separate compilation for the full CompCert compiler.",
    "actual_venue": "Popl"
  },
  {
    "abstract": "Recent developments in Internet technologies have resulted in a wide range of high-speed internet choices to rural communities. In this paper, we present decision support models to select the high-speed access technologies with different characteristics under a multiple number of performance criteria (cost quality and speed). Specifically, we first construct an analytic hierarchy process (AHP) model, which provides the overall priority weights for each access technology under the multiple criteria. Next, we examine the cases of communities pooling their budget resources for additional mutual benefit. This examination leads to a couple of mixed integer programming models to determine the optimal technology selections of the pooled communities, taking the economies of scale, homogeneity/heterogeneity of communities into consideration. Throughout this paper, the main features are illustrated via numerical examples.",
    "actual_venue": "Telematics And Informatics"
  },
  {
    "abstract": "In this paper, we develop a flexible and accurate analytical model of large networks with random base station (BS) placement, in order to understand the impact of key network parameters like BS density and load on the network performance. The main goal is to understand the flow level dynamics of such a system, assuming non-saturated users and studying the congestion statistics for BSs and the per flow delay. To achieve this, we base our analysis on two main tools: (a) stochastic geometry, to understand the impact of topological randomness and coverage maps and (b) queueing theory, to model the competition between concurrent flows within the same BS. Our model is then applied the populars Radio Access Technologies (RATs), such as LTE and WIFi. Our results provide some interesting qualitative and quantitative insights about the performance of those networks.",
    "actual_venue": "Ieee Global Communications Conference"
  },
  {
    "abstract": "Using a bordering approach, and building upon an already known factorization of a principal block, we establish sucient conditions under which we can extend this factorization to the full matrix. Sim- ulations show that the approach is promising also in higher dimen- sions, provided there is no gap in the cp-rank. We expect that this property is shared by good quality approximation solutions obtained by the usual conic (semidenite) relaxation procedures in copositive programming for combinatorial optimization applications.",
    "actual_venue": "Cejor"
  }
]