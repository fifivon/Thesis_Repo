[
  {
    "abstract": "Context-aware computing is widely accepted as a promising paradigm to enable seamless computing. Several middlewares and ontology-based models for describing context information have been developed in order to support context-aware applications. However, the context variability, which refers to the possibility to infer or interpret different context information from different perspectives, has been neglected in the existing context modeling approaches. This paper presents an approach for context-aware software development based on a flexible product line based context model which significantly enhances reusability of context information by providing context variability constructs to satisfy different application needs.",
    "actual_venue": "Wi-Iat), Ieee/Wic/Acm International Conference"
  },
  {
    "abstract": "Integrating different reasoning modes in the construction of an intelligent system is one of the most interesting and challenging aspects of modern AI. Exploiting the complementarity and the synergy of different approaches is one of the main motivations that led several researchers to investigate the possibilities of building multi-modal reasoning systems, where different reasoning modalities and different knowledge representation formalisms are integrated and combined. Case-Based Reasoning (CBR) is often considered a fundamental modality in several multi-modal reasoning systems; CBR integration has been shown very useful and practical in several domains and tasks. The right way of devising a CBR integration is however very complex and a principled way of combining different modalities is needed to gain the maximum effectiveness and efficiency for a particular task. In this paper we present results (both theoretical and experimental) concerning architectures integrating CBR and Model-Based Reasoning (MBR) in the context of diagnostic problem solving. We first show that both the MBR and CBR approaches to diagnosis may suffer from computational intractability, and therefore a careful combination of the two approaches may be useful to reduce the computational cost in the average case. The most important contribution of the paper is the analysis of the different facets that may influence the entire performance of a multi-modal reasoning system, namely computational complexity, system competence in problem solving and the quality of the sets of produced solutions. We show that an opportunistic and flexible architecture able to estimate the right cooperation among modalities can exhibit a satisfactory behavior with respect to every performance aspect. An analysis of different ways of integrating CBR is performed both at the experimental and at the analytical level. On the analytical side, a cost model and a competence model able to analyze a multi-modal architecture through the analysis of its individual components are introduced and discussed. On the experimental side, a very detailed set of experiments has been carried out, showing that a flexible and opportunistic integration can provide significant advantages in the use of a multi-modal architecture.",
    "actual_venue": "Artif Intell"
  },
  {
    "abstract": "We consider the problem of providing QoS guarantees, and at the same time boosting the throughput or saving energy over a wireless link. A common solution is using a single, integrated scheduler that deals both with the QoS guarantees and the wireless link issues. Unfortunately, such an approach does not allow any of the existing high-quality schedulers for wired links to be used without modification. And it is little flexible, as a scheduler compliant with a given wireless technology may need to be modified to fit a different technology or a different solution for saving energy. To address these issues, in this paper we propose a modular architecture that basically extends the network stack by adding a special middle layer on top of the MAC. On the bottom side, this middle layer deals with the idiosyncrasies of the underlying wireless link, and possibly uses channel state information to boost performance and save energy. On the top side, the middle layer exports the abstraction of a link to which the higher layers must only pass the packets to transmit. On top of this middle layer, existing packet schedulers for wired links can be used without modification. It is then possible also to use the same packet scheduler on heterogeneous wireless technologies, by changing only the the middle layer.",
    "actual_venue": "Aina Workshops"
  },
  {
    "abstract": "Todays, the electric wheelchair has been an essential tool for handicapped people. It may be difficult, however for hand-impaired people to operate the electric wheelchair with the conventional joystick. For hand-impaired people, we have to provide other operations such as eye-tracking. Traditional eye-tracking method, however, is tiresome, because the user has to fix his or her eyes to the direction of the destination. In this paper, we propose an user interface with eye tracking extended by the augmented reality (AR) technology. In our user interface, the sight in front of the electric wheelchair is displayed on the PC screen. Furthermore, an oval is overlaid at user's view point in the sight on the screen as if a search light spots focus at the specified location. The user can decide the location at the oval as a temporary destination, to which the electric wheelchair moves. Repeating the process, the user can intuitively drive his or her electric wheelchair to the final destination without the burden of operations.",
    "actual_venue": "Universal Access In Human-Computer Interaction: Designing Novel Interactions, Pt"
  },
  {
    "abstract": "Changing structures to online news have instigated concerns that the electorate may predominantly consume soft news for entertainment purposes while neglecting public affairs information. The Internet in particular brought an increase in outlets, including unconventional low-credibility sources. A 2x2x2 within-subjects experiment (n=197) investigated whether delivery format (print vs online) and source type (high vs low credibility) shape the extent to which recipients select different types of news (public affairs news vs soft news). Participants browsed 32 news items, half of them hard news and the other half soft news, either associated with high- or low-credibility sources, and did so online or via print magazine. Results show that greater preference for online news fostered selective exposure to hard news. Greater habitual news use via social media reduced selective exposure to news from high-credibility sources.",
    "actual_venue": "New Media And Society"
  },
  {
    "abstract": "•A qualitative dimension to understanding users’ desires regarding, and expectations of, health registers.•That there is a difference between what users think health registers would be used for, and how they want them to be used.•Users wish for social contact, exchange, and networking functions, from a health register.",
    "actual_venue": "International Journal Of Medical Informatics"
  },
  {
    "abstract": "Differential fault analysis exploits faults to find secret information stored in a cryptographic device. It utilizes differential information between correct and faulty ciphertexts. We introduce new techniques to improve the previous differential fault analysis of ARIA. ARIA is a general-purpose involutional SPN (substitution permutation network) block cipher and was established as a Korean standard block cipher algorithm in 2004. While the previous method by Li et al. requires 45 faults, our method needs 13 faults to retrieve the 128-bit secret key of ARIA. If access to the decryption oracle is allowed, our method only needs 7 faults. We analyze the characteristics of the diffusion layer of ARIA in detail, which leads us to reduce the number of required faults to find the key.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "The subband adaptive filter system has been applied to the acoustic echo cancellation problem in order to overcome the problems of slow convergence due to spectrally dynamic input and high computational costs associated with a single, long adaptive filter. Research into the convergence characteristics of subband acoustic echo cancelers (AECs) have either used Gaussian white noise, USASI noise (as an approximation for speech signals), or short term speech signals. Research in the statistical modeling of speech signals using spherically invariant random processes (SIRPs) and analyses of LMS and NLMS algorithms under SIRPs has led to a better understanding of adaptive filter performance under more realistic input in the context of AEC. We present experimental results using a subband AEC under SIRP input. These results yield a better understanding of the performance in actual acoustic echo cancellation applications and highlight the benefits of subband techniques.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "In this paper, we present a texture analysis based method for diagnosing the Basal Cell Carcinoma (BCC) skin cancer using optical images taken from the suspicious skin regions. We first extracted the Run Length Matrix and Haralick texture features from the images and used a feature selection algorithm to identify the most effective feature set for the diagnosis. We then utilized a Multi-Layer Perceptron (MLP) classifier to classify the images to BCC or normal cases. Experiments showed that detecting BCC cancer based on optical images is feasible. The best sensitivity and specificity we achieved on our data set were 94% and 95%, respectively.",
    "actual_venue": "Proceedings Of Spie"
  },
  {
    "abstract": "This paper deals with the sensorless characterization of magnetic materials under nonperiodic conditions. A volt-amperometric method based on general physical principles has been adopted to do the measurements. Starting from the concept of magnetic hysteresis as a multibranch nonlinearity with nonlocal memory, we provide a description of the proposed measurement procedure, focusing on two significant topic points: 1) the determination of a well-known (within a certain interval of confidence) initial state of magnetization and 2) the effect on the final experimental results of the offset introduced by the instrumentation. Finally, we present experimental results done on a sample of soft ferrite, showing how the method is capable of measuring phenomena like accommodation or noncongruency of minor loops.",
    "actual_venue": "Instrumentation And Measurement, Ieee Transactions"
  },
  {
    "abstract": "This paper proposes a new approach to recover the relative magnitude of Gaussian curvature of the test object from four shading images using modified neural network. The method is expanded to an object with color texture using four shading images taken under the different light source directions. Neural network mapps four image irradiances on the test object onto a point on a sphere. The area value surrounded by four mapped points onto a sphere gives an approximate value of Gaussian curvature. To get more accurate Gaussian curvature, the modification neural network is introduced and learned for the synthesized 2-D basis function consisting of 2-D cosine function. It is shown that learnt NN gives better accuracy for the relative magnitude of Gaussian curvature of the test object.",
    "actual_venue": "KES (2)"
  },
  {
    "abstract": "This paper presents a unified framework for Behavior Trees (BTs), a plan representation and execution tool. The available literature lacks the consistency and mathematical rigor required for robotic and control applications. Therefore, we approach this problem in two steps: first, reviewing the most popular BT literature exposing the aforementioned issues; second, describing our unified BT framework along with equivalence notions between BTs and Controlled Hybrid Dynamical Systems (CHDSs). This paper improves on the existing state of the art as it describes BTs in a more accurate and compact way, while providing insight about their actual representation capabilities. Lastly, we demonstrate the applicability of our framework to real systems scheduling open-loop actions in a grasping mission that involves a NAO robot and our BT library.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "This paper presents a photo browsing system on mobile devices to browse and search photos efficiently by tilting action. It employs tilt dynamics and a multi-scale photo screen layout for enhancing the browsing and the search capability respectively. The implementation uses continuous inputs from an accelerometer, and a multimodal (visual, audio and vibrotactile) display coupled with the states of this model. The model is based on a simple physical model, with its characteristics shaped to enhance controllability. The multi-scale layout holds both local and global view for users to both control photos and look at the surrounding context in a single framework. The experiment on Samsung MITs PDA used seven novice users browsing from 100 photos. We compare a tilt-based interaction method with a button-based browser and an iPod wheel by a quantitative usability criteria and subjective experience. The proposed tilt dynamics improves the usability over conventional dynamics. The iPod wheel has mixed performance comparing worse on some metrics than button pushing or tilt interaction, despite its commercial popularity.",
    "actual_venue": "Mobile Hci"
  },
  {
    "abstract": "This paper presents an offshore oceanographic toolchain that can be used to generate survey missions for multi-vehicle robotic surveys for large-scale dynamic features in the coastal ocean. Our science application targets Harmful Algal Blooms (HABs) which have significant societal impact to coastal communities yet are poorly understood in terms of their ecology. Since bloom patches can be large (kms) and unpredictable in their movement, to understand their evolution, we need to be able to bring back water samples from the 'right' places and times, for lab analysis. In doing so, we target hotspots (representative of intense biogeochemical activity) for such sampling. Our approach uses remote sensing data to detect such hotspots using ocean color as a proxy, and advectively projects these patches spatio-temporally using surface current data from HF Radar stations. Experiments with satellite and Radar data sets are promising for large, coherent blooms. We show how these predictions can be used to select an appropriate lawnmower sampling trajectory for an AUV.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "Physical layer security (PLS) has become an increasingly attractive topic since it promises current and future wireless systems both reliable and secure communication, without imposing any assumptions on the computational power of the eavesdroppers. PLS benefits from the randomness property of the wireless channel, which provides better immunity and prevents different attacks. On the other hand, the multiple-input multiple-output (MIMO) system has emerged as a key technology to support high data rates and improved energy and spectral efficiency, in addition to overcoming the effect of shadowing and fading. Recently, MIMO-based PLS has been addressed in the literature due to its wide adoption and its essential role in wireless communication systems. In this paper, we provide a comprehensive overview of various MIMO-based PLS techniques that target all kinds of security services namely, key generation and distribution, data confidentiality, authentication, and availability. With this overview, readers will have a better understanding of the MIMO-based PLS techniques present in the literature, their current limitations, and challenges.",
    "actual_venue": "Wireless Networks"
  },
  {
    "abstract": "The Internet is gradually and constantly becoming a multimedia network that needs mechanisms to provide effective quality of service (QoS) requirements to users. The service curve (SC) is an efficient description of QoS and the service curve based earliest deadline first policy (SCED) is a scheduling algorithm to guarantee SCs specified by users. In SCED, deadline calculation is the core. However, not every SC has a treatable deadline calculation; currently the only known treatable SC is the concave piecewise linear SC (CPLSC). In this paper, we propose an algorithm to translate all kinds of SCs into CPLSCs. In this way, the whole Internet can have improved performance. Moreover, a modification of the deadline calculation of the original SCED is developed to obtain neat and precise results. The results combining with our proposed algorithm can make the deadline calculation smooth and the multimedia Internet possible.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "This study examines the impact of collective MMORPG play on gamers' social capital in both the virtual world and the real world. Collective MMORPG play is conceptualized as the frequency of joint gaming actions and gamers' assessment of the experience in MMORPG guilds and groups. Social capital at the individual level refers to the resources and support provided by bonding and bridging social networks; collective-level social capital refers to people's civic engagement. A two-wave online survey was conducted to collect data from 232 Chinese MMORPG players. Two structural equation models were developed to test whether collective play influences offline social capital via the mediation of online social capital; the results did not demonstrate the existence of mediation effects. Specifically, collective play positively influences gamers' online bonding social capital, online bridging social capital and online civic engagement. The effect of collective play on offline bonding and bridging social capital is not significant; the effect of online bonding/bridging social capital on offline bonding/bridging social capital is not significant either. The study finds a significantly positive impact of collective play on offline civic engagement. The effect of online civic engagement on offline civic engagement is not significant. In contrast with collective play, the time of gaming is found to negatively influence online and offline social capital. This study contributes to the knowledge of social capital because it tests the effects of new media on online and offline social capital in the Chinese culture. In addition, this study provides empirical evidence for the positive effects of online games and highlights the social experience in MMORPG play and how it influences gamers' social networks and collective participation.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "This paper focuses on parallel hash functions based on tree modes of operation for an inner Variable-Input-Length function. This inner function can be either a single-block-length (SBL) and prefix-free MD hash function, or a sponge-based hash function. We discuss the various forms of optimality that can be obtained when designing parallel hash functions based on trees where all leaves have the sam...",
    "actual_venue": "Ieee Transactions On Computers"
  },
  {
    "abstract": "Multi-hop relaying helps to improve the connectivity in wireless networks at the price of additional signaling overhead. In this paper, an ultra-low power wake-up radio is proposed to minimize signaling overhead and to build up energy-aware relaying route for low power applications. Protocol details are explained and energy consumption model is analyzed. The proposed scheme is compared with main radio routing and direct link communication. Results show that the proposed scheme brings significant improvement on connectivity. For data communication, with much lower signaling overhead, comparable energy efficiency is achieved as main radio routing.",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "Dictionary learning (DL) for sparse coding based classification has been widely researched in pattern recognition in recent years. Most of the DL approaches focused on the reconstruction performance and the discriminative capability of the learned dictionary. This paper proposes a new method for learning discriminative dictionary for sparse representation based classification, called Incoherent Fisher Discrimination Dictionary Learning (IFDDL). IFDDL combines the Fisher Discrimination Dictionary Learning (FDDL) method, which learns a structured dictionary where the class labels and the discrimination criterion are exploited, and the Incoherent Dictionary Learning (IDL) method, which learns a dictionary where the mutual incoherence between pairs of atoms is exploited. In the combination, instead of considering the incoherence between atoms in a single shared dictionary as in IDL, we propose to incorporate the incoherence between pairs of atoms within each sub-dictionary, which represent a specific object class. This aims to increase discrimination capacity between basic atoms in sub-dictionaries. The combination allows one to exploit the advantages of both methods and the discrimination capacity of the entire dictionary. Extensive experiments have been conducted on benchmark image data sets for Face recognition (ORL database, Extended Yale B database, AR database) and Digit recognition (the USPS database). The experimental results show that our proposed method outperforms most of state-of-the-art methods for sparse coding and DL based classification, meanwhile maintaining similar complexity.",
    "actual_venue": "Journal Of Information Science And Engineering"
  },
  {
    "abstract": "Efficient algorithms for the continuous representation of a discrete signal in terms of B-splines (direct B-spline transform) and for interpolative signal reconstruction (indirect B-spline transform) with an expansion factor m are described. Expressions for the z-transforms of the sampled B-spline functions are determined and a convolution property of these kernels is established. It is shown that both the direct and indirect spline transforms involve linear operators that are space invariant and are implemented efficiently by linear filtering. Fast computational algorithms based on the recursive implementations of these filters are proposed. A B-spline interpolator can also be characterized in terms of its transfer function and its global impulse response (cardinal spline of order n). The case of the cubic spline is treated in greater detail. The present approach is compared with previous methods that are reexamined from a critical point of view. It is concluded that B-spline interpolation correctly applied does not result in a loss of image resolution and that this type of interpolation can be performed in a very efficient manner.",
    "actual_venue": "Ieee Trans Pattern Anal Mach Intell"
  },
  {
    "abstract": "In this paper, the generalized linear complementarity problem over a polyhedral cone (GLCP) is reformulated as an unconstrained optimization, based on which we propose a Newton-type algorithm to solve it. Under certain conditions, we show that the algorithm converges globally and quadratically. Preliminary numerical experiments are also reported in this paper.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "In a clipped MIMO-OFDM system, the overall additive noise, including the clipping distortion, may not be white. In this paper, we develop fast ML decoding algorithms for orthogonal space-time-frequency block codes (OSTFBC) and quasi orthogonal space-time-frequency block codes (QOSTFBC) in clipped MIMO-OFDM systems by using a clipping noise model with Gaussian approximation. By using the statistics of the clipping distortions, our newly proposed fast ML decoding algorithms improve the performance for clipped MIMO-OFDM systems with OSTFBC and QOSTFBC without increasing the decoding complexity. Simulation results are presented to illustrate the improvement.",
    "actual_venue": "Globecom"
  },
  {
    "abstract": "Increasing operational and security demands changed biometrics by shifting the focus from single to multi-biometrics. Multi-biometrics are mandatory in the current context of large international biometric databases and to accommodate new emerging security demands. Our paper is a comprehensive survey on multi-biometrics, covering two important topics related to the multi-biometric field: fusion methods and security. Fusion is a core requirement in multi-biometric systems, being the method used to combine multiple biometric methods into a single system. The fusion section surveys recent multi-biometric schemes categorized from the perspective of fusion method. The security section is a comprehensive review of current issues, such as sensor spoofing, template security, and biometric encryption. New research trends and open challenges are discussed, such as soft, adaptive contextual-based biometrics. Finally, an implementation blueprint for a multi-biometric system is presented in the form of a list of questions to be answered when designing the system.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "The complexity of hardware design methodologies represents a significant difficulty for non hardware focused scientists working on CNN-based applications. An emerging generation of Electronic System Level (ESL) design tools is been developed, which allow software-hardware codesign and partitioning of complex algorithms from High Level Language (HLL) descriptions. These tools, together with High Performance Reconfigurable Computer (HPRC) systems consisting of standard microprocessors coupled with application specific FPGA chips, provide a new approach for rapid emulation and acceleration of CNN-based applications. In this article CoDeveloper, and ESL IDE from Impulse Accelerated Technologies, is analyzed. A sequential CNN architecture, suitable for FPGA implementation, proposed by the authors in a previous paper, is implemented using CoDeveloper tools and the DS1002 HPRC platform from DRC Computers. Results for a typical edge detection algorithm shown that, with a minimum development time, a 10x acceleration, when compared to the software emulation, can be obtained.",
    "actual_venue": "International Work-Conference On The Interplay Between Natural And Artificial Computation"
  },
  {
    "abstract": "In this work we present the definition of strong fuzzy subsethood measure as a unifiying concept for the different notions of fuzzy subsethood that can be found in the literature. We analyze the relations of our new concept with the definitions by Kitainik ([20]), Young ([26]) and Sinha and Dougherty ([23]) and we prove that the most relevant properties of the latter are preserved. We show also several construction methods.",
    "actual_venue": "Journal Of Multiple-Valued Logic And Soft Computing"
  },
  {
    "abstract": "Present study proposes an improved and versatile method of forecasting based on the concept fuzzy time series forecasting. The developed model has been presented in a form of simple computational algorithms. It utilizes various difference parameters being implemented on current state for forecasting the next state values to accommodate the possible vagueness in the data in a better way and making it a robust method. The developed model has been implemented on the historical student enrollments data of University of Alabama (adapted by Song and Chissom) and the obtained forecasted values have been compared with the existing methods to show its superiority. The robustness of the model has also been tested in comparison. The suitability of the developed model has also been examined in the crop production forecasting by implementing it on historical time series data of rice production of Pantnagar(Farm), India.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "In this paper the implementation of a multirate QPSK modulator is presented. The modulator architecture has been optimized to reduce the hardware complexity and maximize the carrier frequency in order to meet the requirements of deep space and satellite applications. Finally, the performance has been evaluated by implementing the modulator on a Xilinx XC2V3000 FPGA.",
    "actual_venue": "Ieee International Symposium On Circuits And Systems, Vols -, Proceedings"
  },
  {
    "abstract": "This paper compares three approaches to evolving ensembles in Genetic Programming (GP) for binary classification with unbalanced data. The first uses bagging with sampling, while the other two use Pareto-based multi-objective GP (MOGP) for the trade-off between the two (unequal) classes. In MOGP, two ways are compared to build the ensembles: using the evolved Pareto front alone, and using the whole evolved population of dominated and non-dominated individuals alike. Experiments on several benchmark (binary) unbalanced tasks find that smaller, more diverse ensembles chosen during ensemble selection perform best due to better generalisation, particularly when the combined knowledge of the whole evolved MOGP population forms the ensemble.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "Substitution in the λ-calculus is a subtle operation. In a formal description, Barendregt's variable convention isassumed to avoid variable capture, but such an assumption is notwell suited for implementation on computers. We introduce graphrepresentation and manipulation of λ-terms, in which bound andfree variables are encoded by using hyperlinks with differentattributes. A graph type called hlground is generalized to identifythe scope of a term n in substitution m[x:= n], which enablesbound variables and free variables to have suitable behaviorduring the substitution. Our representation of the λ-terms arereadable and the definition of substitution in this technique isfree from any side conditions on the freeness and freshness of variables.",
    "actual_venue": "International Symposium On Theoretical Aspects Of Software Engineering"
  },
  {
    "abstract": "We use semi-parametric methods to improve fuzzy linear regression models.We present a detailed comparison of proposed method via sumulation data.Efficiency of proposed method is demonstrated via some real-world applications. A large number of accounting studies have focused on parametric or non-parametric forms of fuzzy regression relationships between dependent and independent variables. Notably, semi-parametric partially linear model as a powerful tool to incorporate statistical parametric and non-parametric regression analyses has gained attentions in many real-life applications recently. However, fuzzy data find application in many real studies. This study is an investigation of semi-parametric partially linear model for such cases to improve the conventional fuzzy linear regression models with fuzzy inputs, fuzzy outputs, fuzzy smooth function and non-fuzzy coefficients. For this purpose, a hybrid procedure is suggested based on curve fitting methods and least absolutes deviations to estimate the fuzzy smooth function and fuzzy coefficients. The proposed method is also examined to be compared with a common fuzzy linear regression model via a simulation data set and some real fuzzy data sets. It is shown that the proposed fuzzy regression model performs more convenient and efficient results in regard to six goodness-of-fit criteria which concludes that the proposed model could be a rational substituted model of some common fuzzy regression models in many practical studies of fuzzy regression model in expert and intelligent systems.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "Epigenetic variation represents a mechanism of regulation for genes expressed in different cancer histotypes. We considered breast cancer, and investigated differential expression following treatment with the 5-Aza-2'-deoxycytidine or DAC, a demethylating agent. Several oncogenic signalling pathways altered upon DAC treatment were detected with significant enrichment, and a regulatory map integrating Transcription Factors and microRNAs was derived. The ultimate goal is deciphering the potential molecular mechanisms induced by DAC therapy in MCF7 cells.",
    "actual_venue": "Proceedings Iwbbio : International Work-Conference On Bioinformatics And Biomedical Engineering, Vols And"
  },
  {
    "abstract": "We present a simple denoising technique for geometric data represented as a semiregular mesh, based on locally adaptive Wiener filtering. The degree of denoising is controlled by a single parameter (an estimate of the relative noise level) and the time required for denoising is independent of the magnitude of the estimate. The performance of the algorithm is sufficiently fast to allow interactive local denoising.",
    "actual_venue": "Ieee Visualization"
  },
  {
    "abstract": "Echo state network contains a randomly connected hidden layer and an adaptable output layer. It can overcome the problems associated with the complex computation and local optima. But there may be ill-posed problem when large reservoir state matrix is used to calculate the output weights by least square estimation. In this study, we use L-1/2 regularization to calculate the output weights to get a sparse solution in order to solve the ill-posed problem and improve the generalized performance. In addition, an operation of iterated prediction is conducted to test the effectiveness of the proposed L1/2ESN for capturing the dynamics of the chaotic time series. Experimental results illustrate that the predictor has been designed properly. It outperforms other modified ESN models in both sparsity and accuracy.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "In earlier work, we presented translations of attack-defence trees (ADTrees) to extended asynchronous multi-agent systems. By avoiding some sequences, agent models constructed via these transformations already embed state space reductions. Here, we introduce Guarded Update Systems and their synchronisation topology, allowing us to define a new general reduction scheme that applies to tree topologies, and in particular to ADTrees. The reduction exploits the layered structure of a tree by avoiding unnecessary interleavings between nodes at different depths. We prove the soundness of this new method and present extensive experimental results, including scalable models, to demonstrate it can be effectively used alongside previously employed techniques.",
    "actual_venue": "International Conference On Engineering Of Complex Computer Systems"
  },
  {
    "abstract": "Interacting Particle Systems---exemplified by the voter model, iterative majority, and iterative k-majority processes---have found use in many disciplines including distributed systems, statistical physics, social networks, and Markov chain theory. In these processes, nodes update their \"opinion\" according to the frequency of opinions amongst their neighbors.\n\nWe propose a family of models parameterized by an update function that we call Node Dynamics: every node initially has a binary opinion. At each round a node is uniformly chosen and randomly updates its opinion with the probability distribution specified by the value of the update function applied to the frequencies of its neighbors' opinions.\n\nIn this work, we prove that the Node Dynamics converge to consensus in time Θ(n log n) in complete graphs and dense Erdös-Rényi random graphs when the update function is from a large family of \"majority-like\" functions. Our technical contribution is a general framework that upper bounds the consensus time. In contrast to previous work that relies on handcrafted potential functions, our framework systematically constructs a potential function based on the state space structure.",
    "actual_venue": "Soda : Symposium On Discrete Algorithms New Orleans Louisiana January"
  },
  {
    "abstract": "Flexible Chip Multiprocessor (CMP) systems are implemented on field programmable devices to exploit both task-level parallelism and architecture customization for parallel programs. The idea is to simultaneously allocate processor resources, map and schedule tasks to them, and to allocate one or several intertask communication resources such that the throughput or execution time is optimized. The design space of such systems is huge, requiring means to automatically optimize design parameters so as to facilitate wide and disciplined explorations. The complexity resulting from corresponding system modeling necessitates the use of optimization heuristics to cope with excessively long runtime for large problem instances. This paper provides a formal proof for the existence of optimum linear time synthesis algorithms for one of two classes of problem instances, and proceeds to present three greedy-like heuristics which exploit the structure of the synthesis problem. A comparison of results for real-time and non-real-time parallel programs is given against integer linear programming, where a synthesis strategy is proposed to achieve good results.",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "The human visual system utilizes depth information as a major cue to group together visual items constituting an object and to segregate them from items belonging to other objects in the visual scene. Depth information can be inferred from a variety of different visual cues, such as disparity, occlusions and perspective. Many of these cues provide only local and relative information about the depth of objects. For example, at occlusions, T-junctions indicate the local relative depth precedence of surface patches. However, in order to obtain a globally consistent interpretation of the depth relations between the surfaces and objects in a visual scene, a mechanism is necessary that globally propagates such local and relative information. We present a computational framework in which depth information derived from T-junctions is propagated along surface contours using local recurrent interactions between neighboring neurons. We demonstrate that within this framework a globally consistent depth sorting of overlapping surfaces can be obtained on the basis of local interactions. Unlike previous approaches in which locally restricted cell interactions could merely distinguish between two depths (figure and ground), our model can also represent several intermediate depth positions. Our approach is an extension of a previous model of recurrent V1-V2 interaction for contour processing and illusory contour formation. Based on the contour representation created by this model, a recursive scheme of local interactions subsequently achieves a globally consistent depth sorting of several overlapping surfaces. Within this framework, the induction of illusory contours by the model of recurrent V1-V2 interaction gives rise to the figure-ground segmentation of illusory figures such as a Kanizsa square.",
    "actual_venue": "Biological Cybernetics"
  },
  {
    "abstract": "In the paper we present a formal model of real-time database (RTDB) systems using Duration Calculus (DC). First, we give a formal specification of the correctness for the executions of transaction systems and the Two Phase Locking Concurrency Control Protocol (2PL-CCP). We also give a formal proof for the correctness of the 2PL-CCP using the DC proof system. Then, we present a formal description of the real-time database model by extending the model for untimed databases with state variables expressing temporal objects and with DC formulas to express their behavior. A formal description of correctness of the parallel executions of transaction systems in RTDBs is then given as the combination of the correctness for the untimed case and the time constraints for the transactions and their read data.",
    "actual_venue": "Software Engineering Conference"
  },
  {
    "abstract": "First Page of the Article",
    "actual_venue": "Ieee Micro"
  },
  {
    "abstract": "This is an era of data deluge with individuals and pervasive sensors acquiring large and ever-increasing amounts of data. Nevertheless, given the inherent redundancy, the costs related to data acquisition, transmission, and storage can be reduced if the per-datum importance is properly exploited. In this context, the present paper investigates sparse linear regression with censored data that appears naturally under diverse data collection setups. A practical censoring rule is proposed here for data reduction purposes. A sparsity-aware censored maximum-likelihood estimator is also developed, which fits well to big data applications. Building on recent advances in online convex optimization, a novel algorithm is finally proposed to enable real-time processing. The online algorithm applies even to the general censoring setup, while its simple closed-form updates enjoy provable convergence. Numerical simulations corroborate its effectiveness in estimating sparse signals from only a subset of exact observations, thus reducing the processing cost in big data applications.",
    "actual_venue": "Globalsip"
  },
  {
    "abstract": "In the past few years virtual reality based systems have been proposed and realized for many medical interventions. These simulators have the potential to provide training on a wide variety of pathologies. So far, realistic generation of anatomical variance and pathologies have not been treated as a specific issue. We report on a cellular automaton, specially developed to generate macroscopic findings fulfilling the requirements for a sophisticated simulation. The specific pathology investigated are leiomyomas protruding to different extents into the uterine cavity. The automaton presented is part of a virtual reality based hysteroscopy simulator which is currently under development.",
    "actual_venue": "Miccai"
  },
  {
    "abstract": "Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.",
    "actual_venue": "Annual Conference On Neural Information Processing Systems"
  },
  {
    "abstract": "A number of different coordination models for dynamic resource allocation are proposed. The models are based on an asynchronous and distributed approach which makes use of mobile agents to distribute the resources of the providers between the consumers. Each provider has a broker, i.e., the mobile agent, that continually visits all or a subset of the consumers, offering the resources currently available at the corresponding provider. The models are increasingly complex, starting with a rather simple static mechanism, and ending with a sophisticated solution that balance the allocations both from the consumer and the provider perspective. Finally, an evaluation of the models in a realistic Intelligent Network domain is presented.",
    "actual_venue": "Coordination"
  },
  {
    "abstract": "We explore new aspects on assistive living via smart social human-robot interaction (HRI) involving automatic recognition of multimodal gestures and speech in a natural interface, providing social features in HRI. We discuss a whole framework of resources, including datasets and tools, briefly shown in two real-life use cases for elderly subjects: a multimodal interface of an assistive robotic rollator and an assistive bathing robot. We discuss these domain specific tasks, and open source tools, which can be used to build such HRI systems, as well as indicative results. Sharing such resources can open new perspectives in assistive HRI.",
    "actual_venue": "HRI (Companion)"
  },
  {
    "abstract": "The next-generation MIPS CMOS microprocessor, the R4000, uses a technique called superpipelining to achieve a high level of performance. The authors discuss the evolution of the R4000 pipeline from the R3000 pipeline and the reasons why a superpipelined microarchitecture is chosen. First, there are no instruction issue restrictions with the R4000 superpipeline, as there would have been in a superscalar implementation. Various combinations of independent instructions, including ALU (arithmetic logic unit)/ALU and load/load can be executed without any pipeline stalls. The compiler is also simpler and instruction scheduling can be more efficient. Finally, there is no need to replicate functional units, as would be necessary in a superscalar implementation.<>",
    "actual_venue": "San Francisco, Ca, Usa"
  },
  {
    "abstract": "Permutations over $F_{2^{2k}}$ with low differential uniform, high algebraic degree and high nonlinearity are of great cryptographical importance since they can be chosen as the substitution boxes (S-boxes) for many block ciphers. A well known example is that the Advanced Encryption Standard (AES) chooses a differentially 4-uniform permutation, the multiplicative inverse function, as its S-box. In this paper, we present a new construction of differentially 4-uniformity permutations over even characteristic finite fields and obtain many new CCZ-inequivalent functions. All the functions are switching neighbors in the narrow sense of the multiplicative inverse function and have the optimal algebraic degree and high nonlinearity.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "This paper describes a technique to deliver video streams with constant perceptual quality of service (QoS) over time-varying packet erasure channels that support differentiated classes of service. During compression, the encoder estimates the distortion introduced at the decoder by each video packet both in case it is received and in case it is lost. Concealment and error propagation due to inter-frame prediction are taken into account. During transmission, an optimization algorithm assigns packets to different service classes according to the estimated distortion, the current channel status and a constraint on the desired quality at the receiver. This technique is compared with other video packet classification approaches in the specific case of a DiffServ IP network implementing the assured forwarding scheme. Network simulations show that the proposed technique delivers higher and more constant levels of perceptual QoS than traditional approaches. Moreover, the technique is characterized by reactiveness to congestion and fairness in the use of network resources.",
    "actual_venue": "Icme"
  },
  {
    "abstract": "Semantics represents a major problem area for active databases inasmuch as (i) there is no formal framework for defining the abstract semantics of active rules, and (ii) the various systems developed so far have ad-hoc operational semantics that are widely dierent from each other. This situation contributes to the diculty of predicting the run-time behavior of sets of rules: thus, ensuring the termination of a given set of rules is currently recognized as a major research issue. This situation hampers the applicability of this powerful technology in critical application areas. In this paper, we introduce a durable change semantics for active database rules; this semantics improves Starburst's deferred activation notion with concepts taken from Post- gres and Heraclitus and the semantic foundations of deductive databases. We provide a formal logic-based model for this transaction-oriented semantics, show that it is amenable to ecient implementation, and prove that it solves the non-termination problem.",
    "actual_venue": "Dood"
  },
  {
    "abstract": "Human Computer Interaction (HCI) is the methodology through which computing systems understand actions of human beings. Some important applications of HCI as reported in literature are eye gaze estimation, eye tracking, alertness and health monitoring etc. Of them major applications are based upon eye movements. However, accurate estimation of these measurements is still a challenge in research community. A particular type of eye movement i.e. saccadic eye movement has potential applications in HCI such as alertness assessment, disease diagnosis etc. Electrooculography (EOG) based measurement of saccadic movements has been reported to be an accurate method. However being a contact based method, alternative method as suggested by literature is image based approach. This paper aims to investigate the correlation between video and EOG based observation of pure horizontal directional saccades to establish a relationship between image and EOG signals.",
    "actual_venue": "Intelligent Human Computer Interaction"
  },
  {
    "abstract": "Recent research demonstrates that adversaries can inject malicious code into a peripheral's firmware during a firmware update, which can result in password leakage or even compromise of the whole host operating system. Therefore, it is desirable for a host system to be able to verify the firmware integrity of attached peripherals. Several software-based attestation techniques on embedded devices have been proposed as potentially enabling firmware verification. In this work, we propose a Software-Based Attestation technique for Peripherals that verifies the firmware integrity of a peripheral and detects malicious changes with a high probability, even in the face of recently proposed attacks. We implement and evaluate SBAP in an Apple Aluminum Keyboard and study the extent to which our scheme enhances the security properties of peripherals.",
    "actual_venue": "Trust"
  },
  {
    "abstract": "Flying Triangulation sensors enable a free-hand and motion-robust 3D data acquisition of complex shaped objects. The measurement principle is based on a multi-line light-sectioning approach and uses sophisticated algorithms for real-time registration (S. Ettl et al., Appl. Opt. 51 (2012) 281-289). As \"single-shot principle\", light sectioning enables the option to get surface data from one single camera exposure. But there is a drawback: A pixel-dense measurement is not possible because of fundamental information-theoretical reasons. By \"pixel-dense\" we understand that each pixel displays individually measured distance information, neither interpolated from its neighbour pixels nor using lateral context information. Hence, for monomodal single-shot principles, the 3D data generated from one 2D raw image display a significantly lower space-bandwidth than the camera permits. This is the price one must pay for motion robustness. Currently, our sensors project about 10 lines (each with 1000 pixels), reaching an considerable lower data efficiency than theoretically possible for a single-shot sensor. Our aim is to push Flying Triangulation to its information-theoretical limits. Therefore, the line density as well as the measurement depth needs to be significantly increased. This causes serious indexing ambiguities. On the road to a single-shot 3D movie camera, we are working on solutions to overcome the problem of false line indexing by utilizing yet unexploited information. We will present several approaches and will discuss profound information-theoretical questions about the information efficiency of 3D sensors.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "We present our work on a neuromorphic self-driving robot that employs retinomoprhic visual sensing and spike based processing. The robot senses the world through a spike-based visual system - the Asynchronous Time-based Image Sensor (ATIS) - and processes the sensory data stream using IBM's TrueNorth Neurosynaptic System. A convolutional neural network (CNN) running on the TrueNorth determines the steering direction based on what the ATIS \"sees.\" The network was trained on data from three different environments (indoor hallways, large campus sidewalks, and narrow neighborhood sidewalks) and achieved steering decision accuracies from 68% to 82% on development data from each dataset.",
    "actual_venue": "Annual Conference On Information Sciences And Systems"
  },
  {
    "abstract": "To measure the detrusor pressure for diagnosing lower urinary tract symptoms, we designed a small-area and low-power System on a Chip (SoCa). The SoC should be small and low power because it is encapsulated in tiny air-tight capsules which are simultaneously inserted in the urinary bladder and rectum for several days. Since the SoC is also required to be programmable, we designed an Application Specific Instruction set Processor (ASIP) for pressure measurement and wireless communication, and implemented almost required functions on the ASIP. The SoC was fabricated using a 0.18 mu m CMOS mixed-signal process and the chip size is 2.5 x 2.5 mm(2). Evaluation results show that the power consumption of the SoC is 93.5 mu W, and that it can operate the capsule for seven clays with a tiny battery.",
    "actual_venue": "Ieice Transactions On Electronics"
  },
  {
    "abstract": "Several structured p2p systems exist in literature and many applications have been developed on top of them. To use these applications on top of different overlays without changing their implementations, a common API was proposed in [7]. However, since that specification is meagre, current implementations of structured p2p systems have customized it reducing the portability of applications. In addition, in mobile environments, the possibility to exploit cross-layer interactions considerably improves overall performace. In fact, a crosslayer p2p system, called CrossROAD, has been recently designed to optimize structured overlays on MANETs. It directly interacts with a proactive routing protocol, and it can provide cross-layer information to upperlayer applications to further optimize their behavior. In this paper we propose a cross-layer extension of the commonAPI pointing out advantages of the cross-layer approach even at the application layer.",
    "actual_venue": "Wowmom"
  },
  {
    "abstract": "The purpose of this study is to analyze the hypothetical changes in the 2002 impact factor (IF) of the biomedical journals\n included in the Science Citation Index-Journal Citation Reports (SCI-JCR) by also taking into account cites coming from 83 non-indexed Spanish journals on different medical specialties.\n A further goal of the study is to identify the subject categories of the SCI-JCR with the largest increase in their IF, and\n to estimate the 2002 hypothetical impact factor (2002 HIF) of these 83 non-indexed Spanish journals. It is demonstrated that\n the inclusion of cites from a selection of non SCI-JCR-indexed Spanish medical journals in the SCI-JCR-indexed journals produces\n a slight increase in their 2002 IF, specially in journals edited in the USA and in the UK. More than half of the non-indexed\n Spanish journals has a higher 2002 HIF than that of the SCI-JCR-indexed journal with the lowest IF in the same subject category.",
    "actual_venue": "Scientometrics"
  },
  {
    "abstract": "Automated metering is expected to be an integral part of the modern energy grid. Automated metering entails transport of metering data from the energy consumer's premises to the data management systems of the energy provider and potentially information in the other direction. This paper describes a practical mesh networking solution based on extensions proposed to the routing protocol for low power and lossy networks (RPL) to realize automated metering communications. This solution comprises self-organizing algorithms to enable smart meters to automatically discover connectivity and recover from loss of connectivity. Results from a theoretical analysis, simulation based experiments and measurements carried out from a network deployed in our office premises are presented to show the efficacy of the proposed solution. In particular, we study the impact of these algorithms on the network discovery latency, recovery latency, and packet delivery ratio. Findings from this study demonstrate that this solution can improve scalability and performance of the network. Moreover, this solution is practical from an implementation perspective.",
    "actual_venue": "Smart Grid, Ieee Transactions"
  },
  {
    "abstract": "We discuss two combinatorial problems concerning classes of finite or countable structures of combinatorial type. We consider classes determined by a finite set of finite constraints (forbidden substructures). Questions about such classes of structures are naturally viewed as algorithmic decision problems, taking the finite set of constraints as the input. While the two problems we consider have been studied in a number of natural contexts, it remains far from clear whether they are decidable in their general form. This broad question leads to a number of more concrete problems. We discuss twelve open problems of varying levels of concreteness, and we point to the ''Hairy Ball Problem'' as a particularly concrete problem, which we give first in direct model theoretic terms, and then decoded as an explicit graph theoretic problem.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "N-version programming (NVP) and acceptance testing (AT) are techniques for ensuring reliable computation results from imperfect software. Various symmetric combinations of NVP and AT have also been suggested. We take the view that one can insert an AT at virtually any point in a suitably constructed multi-channel computation graph and that judicious placement of ATs will lead to cost-effective reliability improvement. Hence, as a general framework for the creation, representation, and analysis of combined NVP-AT schemes, we introduce MTV graphs, and their simplified data-driven version called DD-MTV graphs, composed of computation module (M), acceptance test (T), and voter (V) building blocks. Previous NVP-AT schemes, such as consensus recovery blocks, recoverable N-version blocks, and N-self-checking programs can be viewed as special cases of our general combining scheme. Results on the design and analysis of new NVP-AT schemes are presented and the reliability improvements are quantified. We show, e.g., that certain, somewhat asymmetric, combinations of M, T, and V building blocks can lead to higher reliabilities than previously proposed symmetric arrangements having comparable or higher complexities.",
    "actual_venue": "White Plains, Ny"
  },
  {
    "abstract": "Medium-voltage DC (MVDC) distribution system is considered as the promising power architecture of future shipboard power system. Fault-tolerance ability is of great importance for power system on ships. In this paper, zonal DC-DC conversion system based on modular multilevel converter (MMC) and three-phase bridge rectifier are proposed with its hierarchical fault-tolerant scheme. The topology configurations of the front-end MMC with multi-winding medium frequency transformer (MFT) under normal and post-fault circumstances are presented. The recon-figuration scheme and fault-tolerant control strategy are also proposed to ride-through sub-module (SM) level fault, phase level fault and bus level fault. Based on the hierarchical fault-tolerant scheme, redundant configuration of the MMC sub-module is further analyzed with the hierarchical reliability modelling. The effectiveness of the proposed control strategy is verified by the experimental results.",
    "actual_venue": "Ieee Transactions Industrial Informatics"
  },
  {
    "abstract": "Wireless Sensor Networks (WSN) consist in a set ofsensor nodes that collect data in the environment and send it to a Base Station that processes the final data. Some challenges may be found, such as minimizing energy consumption and maximize the network lifetime. Many protocols achieve energy savings through network clustering. This paper presents a new modeling graph using flow network to improve the routing protocols using the clustering technique, improving the routing of data between clusters-head, as well as by setting a time for a round in the clustering process. With this agnostic technique, our proposal may increase the network lifetime, balance the power consumption among the nodes and better distribute the data transmissions. Through simulations in network simulator Omnet++, Castalia, the proposal is validated and its efficiency is verified by comparing it to other protocols which work with clustering techniques.",
    "actual_venue": "Ieee International Conference On Advanced Information Networking And Applications"
  },
  {
    "abstract": "Most vertebrates are able to make detours and find shortcuts to achieve economical navigation. This ability requires the animal to keep track its direction and distance from specific locations. In rodents, direction of the animal is coded by the activity of head direction cells present in several regions of the brain, but distance information is only indirectly available, through the entorhinal cortical grid cell system. A neural system downstream from the entorhinal cortex seems to be necessary to extract the distance information from the periodic activity of grid cells. We propose that a system of such cells store the distance of the animal from important locations in the dentate gyrus region of the hippocampus and these \"distance cells\" might be identified with the dentate granule cells. A computational model is set up to study the neural mechanism of distance information decoding from the ensemble of grids cells. The proposed distance cells receive innervation from entorhinal grid cells, the connection strength between grid cells and distance cells is set by a one-shot-Iearning rule and the distance cell activity is affected by a winner-take-all mechanism. Simulation results of this model verifies that the activity of the distance cell population is able to unambiguously code the distance of the animal from important places. The proposed distance cells have a multi-peaked, patchy spatial activity pattern similar to the firing pattern of granule cells in dentate gyrus.",
    "actual_venue": "Ijcnn"
  },
  {
    "abstract": "We present an extension of the arrival theorem for the output process from a node in closed Markovian networks which we use to obtain simple representations and explicit expressions for the throughput, the distribution of the cycle time, and the joint distribution of interoutput times from a node in single class closed networks with exponential servers. Our approach uses tools from Palm calculus to obtain a recursion on the number of customers in the system. The analysis relies on a non-overtake condition and thus many of the results obtained here apply only to cyclic, single server networks. One of the surprising conclusions of our analysis is that the interoutput times that comprise the cycle time of a customer are (finitely) exchangeable, i.e., that their joint distribution is invariant under permutations.",
    "actual_venue": "Queueing Syst"
  },
  {
    "abstract": "This paper generalizes Blonder's graphical passwords to arbitrary images and solves a robustness problem that this generalization entails. The password consists of user-chosen click points in a displayed image. In order to store passwords in cryptographically hashed form, we need to prevent small uncertainties in the click points from having any effect on the password. We achieve this by introducing a robust discretization, based on multigrid discretization",
    "actual_venue": "Ieee Transactions On Information Forensics And Security"
  },
  {
    "abstract": "The object-oriented programming language Java is an ideal companion to an object-oriented database system. This paper describes our approach to provide a seamless application programmer interface. it is based on a modular architecture with components for database engines, a communications protocol and a JAVA API faciltator: The open architecture is flexible, scalable and distributed in nature.",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "A framework for the credit-apportionment process is described. Such a framework is vital in studying the credit-apportionment problem because it provides a formal basis for the problem analysis and algorithm design. The framework includes: a system-environment model, which integrates the effects of payoffs with other relevant parts of a rule-based system to model various internal and external activities of the rule-based system; and principles of usefulness, which define the inherent usefulness of rule actions and provide the semantic aspect of the credit-apportionment process. This framework formulates the credit-apportionment problem as a problem of estimating and approximating the inherent usefulness from payoffs. Two conclusions from the framework are that: the usefulness of rule action is a function of the context in which the rule activates; and the scalar-valued rule strength is not adequate for the purpose of a credit-apportionment. These conclusions led to the development of a credit-apportionment algorithm, the context-array bucket-brigade algorithm",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "The scheduling strategy of subtask decomposed (Sub-DC) is presented in this article, which includes two parts.The first part is layered transaction of DAG (DirectedAcyclic Graph), which all subtasks are divided intodifferent layers according to precedence relation; Thesecond part is task assignation, in the first, Min-Minscheduling strategy is used to assign subtasks of one layerto each computer of system; after subtasks of one layerare assigned, load balancing strategy is used todynamically adjusted system load, which makes use ofcharacter of subtasks decomposed and makes one layersubtasks are assigned better reason; then layer-by-layersubtasks are assigned using the same method. The Sub-DC not only makes subtasks of each layer be executed asquickly as possible, but also makes the application beexecuted in shorter time. The implement result shows thatthe Sub-DC is better than the level-by-level schedulingstrategy (Sub-Lev).",
    "actual_venue": "Aina Workshops"
  },
  {
    "abstract": "Anonymization is the modification of data to mask the correspondence between a person and sensitive information in the data. Several anonymization models such as k-anonymity have been intensively studied. Recently, a new model with less information loss than existing models was proposed; this is a type of non-homogeneous generalization. In this paper, we present an alternative anonymization algorithm that further reduces the information loss using optimization techniques. We also prove that a modified dataset is checked whether it satisfies the k-anonymity by a polynomial-time algorithm. Computational experiments were conducted and demonstrated the efficiency of our algorithm even on large datasets.",
    "actual_venue": "Int J Inf Sec"
  },
  {
    "abstract": "Visual object tracking is an attractive issue in the field of computer vision. Recently, correlation filters (CF) based trackers formulate the training process by solving the regression in the Fourier domain and show a great efficiency for tracking task. However, its efficiency collapsed when the distracters appear in the background. To improve the robustness of these trackers, we propose a distracter-aware tracking method via correlation filter, which utilizes the positions of target center and distracters. Since most of CF based trackers directly use the Gaussian-shaped label map as the regression target, which may lead the discriminability of tracker reduced. Differing from previous work, we first use the response map of CF model to detect the information of distracters and then utilize these information to design a distracter-aware label map as the regression target for the training process. To further promote the robustness of the tracker, we design a re-detection scheme which uses positive and negative filters to refine the tracking results when the distracters appear. The proposed method not only detect and capture the discriminative information to learn a distracter-aware label map for the filter training, but also utilize these information to train the positive and negative filters for object re-detection, which enhances the robustness of the tracker. We evaluate our proposed method on both the standard OTB2013 and OTB2015 benchmarks, the experimental results show the effectiveness and robustness of the proposed method.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "A classical while-program normal-form theorem is derived in demonic refinement algebra. In contrast to Kozen’s partial-correctness proof of the theorem in Kleene algebra with tests, the derivation in demonic refinement algebra provides a proof that the theorem holds in total correctness. A normal form for action systems is also discussed.",
    "actual_venue": "The Journal Of Logic And Algebraic Programming"
  },
  {
    "abstract": "Continuously reducing transistor sizes and aggressive low power operating modes employed by modern architectures tend to increase transient error rates. Concurrently, multicore machines are dominating the architectural spectrum today in various application domains. These two trends require a fresh look at resiliency of multithreaded applications against transient errors from a software perspective. In this paper, we propose and evaluate a new metric called the Thread Vulnerability Factor (TVF). A distinguishing characteristic of TVF is that its calculation for a given thread (which is typically one of the threads of a multithreaded application) does not depend on its code alone, but also on the codes of the threads that share resources and data with that thread. As a result, we decompose TVF of a thread into two complementary parts: local and remote. While the former captures the TVF induced by the code of the target thread, the latter represents the vulnerability impact of the threads that interact with the target thread. We quantify the local and remote TVF values for three architectural components (register file, ALUs, and caches) using a set of ten multithreaded applications from the Parsec and Splash-2 benchmark suites. Our experimental evaluation shows that TVF values tend to increase as the number of cores increases, which means the system becomes more vulnerable as the core count rises. We further discuss how TVF metric can be employed to explore performance-reliability tradeoffs in multicores. Reliability-based analysis of compiler optimizations and redundancy-based fault tolerance are also mentioned as potential usages of our TVF metric.",
    "actual_venue": "J Parallel Distrib Comput"
  },
  {
    "abstract": "In order to generate or tune fuzzy rules, Neuro-Fuzzy learning algorithms with Gaussian type membership functions based on gradient-descent method are well known. In this paper, we propose a new learning approach, the Quaternion Neuro-Fuzzy learning algorithm. This method is an extension of the conventional method to four-dimensional space by using a quaternion neural network that maps quaternion to real values. Input, antecedent membership functions and consequent singletons are quaternion, and output is real. Four-dimensional input can be better represented by quaternion than by real values. We compared it with the conventional method by several function identification problems, and revealed that the proposed method outperformed the counterpart: The number of rules was reduced to 5 from 625, the number of epochs by one fortieth, and error by one tenth in the best cases.",
    "actual_venue": "Robot, Vision And Signal Processing"
  },
  {
    "abstract": "In this paper we present two new algorithms designed to reduce the overall time required to process top-k queries. These algorithms are based on the document-at-a-time approach and modify the best baseline we found in the literature, Blockmax WAND (BMW), to take advantage of a two-tiered index, in which the first tier is a small index containing only the higher impact entries of each inverted list. This small index is used to pre-process the query before accessing a larger index in the second tier, resulting in considerable speeding up the whole process. The first algorithm we propose, named BMW-CS, achieves higher performance, but may result in small changes in the top results provided in the final ranking. The second algorithm, named BMW-t, preserves the top results and, while slower than BMW-CS, it is faster than BMW. In our experiments, BMW-CS was more than 40 times faster than BMW when computing top 10 results, and, while it does not guarantee preserving the top results, it preserved all ranking results evaluated at this level.",
    "actual_venue": "Sigir"
  },
  {
    "abstract": "The use of elliptic curves in cryptography relies on the ability to count the number of points on a given curve. Before 1999, the SEA algorithm was the only efficient method known for random curves. Then Satoh proposed a new algorithm based on the canonical p-adic lift of the curve for p ≥ 5. In an earlier paper, the authors extended Satoh's method to the case of characteristics two and three. This paper presents an implementation of the Satoh-FGH algorithm and its application to the problem of finding curves suitable for cryptography. By combining Satoh-FGH and an early-abort strategy based on SEA, we are able to find secure random curves in characteristic two in much less time than previously reported. In particular we can generate curves widely considered to be as secure as RSA-1024 in less than one minute each on a fast workstation.",
    "actual_venue": "Eurocrypt"
  },
  {
    "abstract": "When a research infrastructure is funded and implemented, new information and new publications are created. This new information is the measurable output of discovery process. In this paper, we describe the impact of infrastructure for physics experiments in terms of publications and citations. In particular, we consider the Large Hadron Collider (LHC) experiments (ATLAS, CMS, ALICE, LHCb) and compare them to the Large Electron Positron Collider (LEP) experiments (ALEPH, DELPHI, L3, OPAL) and the Tevatron experiments (CDF, D0). We provide an overview of the scientific output of these projects over time and highlight the role played by remarkable project results in the publication–citation distribution trends. The methodological and technical contributions of this work provide a starting point for the development of a theoretical model of modern scientific knowledge propagation over time.",
    "actual_venue": "Technological Forecasting And Social Change"
  },
  {
    "abstract": "This paper raises themes that are seen as some of the challenges facing the emerging practice and research field of Human Work Interaction Design. The paper has its offset in the discussions and writings that have been dominant within the IFIP Working Group on Human Work Interaction Design (name HWID) through the last two and half years since the commencement of this Working Group. The paper thus provides an introduction to the theory and empirical evidence that lie behind the combination of empirical work studies and interaction design. It also recommends key topics for future research in Human Work Interaction Design.",
    "actual_venue": "Human-Computer Interaction Symposium"
  },
  {
    "abstract": "In this paper pollen cell classification that plays an important role for many applications is achieved by using Radial Basis Function Networks (RBF). Pollen images highly contain texture information that leads us to extract two different types of texture features for classification. The first type features are; angular second moment, entropy, contrast, inverse moment and inertia of the co-occurrence Matrix (CM) obtained form each image and the second one use nine features obtained by Local Linear Transforms (LLT). RBF networks which are known as having good learning capacity are used for classification. In experimental results Bangor/Aberystwyth Pollen Image Database is used. The best classification performance it is achieved by using CM based features and it is 83%. As far as we know, this performance is better than the previous reported results on this database.",
    "actual_venue": "Proceedings Of The Iasted International Conference On Computational Intelligence"
  },
  {
    "abstract": "Any given piece of software has some number of publicly disclosed vulnerabilities at any moment, leaving the system exposed to potential attack. The author presents a method for identifying and analyzing these vulnerabilities using public data from easily accessible sources.",
    "actual_venue": "Security And Privacy, Ieee"
  },
  {
    "abstract": "ABSTRACT In this paper, a nonlinear subband decomposition scheme with per - fect reconstruction is proposed for lossless coding of multispectral images The merit of this new  scheme is to exploit efficiently   both the spatial  and the spectral  redundancies contained in  a multispec - tral  image  sequence Besides,  it  is  suitable  for  progressive  cod - ing, which constitutes a desirable feature for telebrowsing applica - tions Simulation tests performed on real scenes allow to assess the performances  of  this  new  multiresolution  coding  algorithm They demonstrate  that  the  achieved  compression  ratios  are  higher  than those obtained  with  currently used  lossless  coders",
    "actual_venue": "Geoscience and Remote Sensing, IEEE Transactions  "
  },
  {
    "abstract": "In this letter, we propose an attention network for object tracking. To construct the proposed attention network for sequential data, we combine long-short term memory (LSTM) and a residual framework into a residual LSTM (RLSTM). The LSTM, which learns temporal correlation, is used for a temporal learning of object tracking. In the proposed RLSTM method, the residual framework, which achieves the ...",
    "actual_venue": "Ieee Signal Processing Letters"
  },
  {
    "abstract": "Inspired by the philosophy of ancient Chinese Taoism, Xu's Bayesian ying-yang (BYY) learning technique performs clustering by harmonizing the training data (yang) with the solution (ying). In our previous work, the BYY learning technique was applied to a fuzzy cerebellar model articulation controller (FCMAC) to find the optimal fuzzy sets; however, this is not suitable for time series data analysis. To address this problem, we propose an incremental BYY learning technique in this paper, with the idea of sliding window and rule structure dynamic algorithms. Three contributions are made as a result of this research. First, an online expectation-maximization algorithm incorporated with the sliding window is proposed for the fuzzification phase. Second, the memory requirement is greatly reduced since the entire data set no longer needs to be obtained during the prediction process. Third, the rule structure dynamic algorithm with dynamically initializing, recruiting, and pruning rules relieves the ??curse of dimensionality?? problem that is inherent in the FCMAC. Because of these features, the experimental results of the benchmark data sets of currency exchange rates and Mackey-Glass show that the proposed model is more suitable for real-time streaming data analysis.",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "The past few years have witnessed a rapid deployment of computing infrastructures in the cloud in support of data intensive applications. The effort of the existing works is mainly focused on data reusing mechanisms without considering data processing routes, which can significantly affect the computation costs when exchanging data among the computing node in the cloud. This paper presents a genetic algorithm enhanced Automatic Data Flow Management Solution (ADFMS) that facilitates automatic routing function and a self-adjustable intermediate data management mechanism to achieve an efficient data processing structure of cloud computing. Experimental results show that ADFMS optimizes costs in managing intermediate data in the cloud.",
    "actual_venue": "Concurrency And Computation: Practice And Experience"
  },
  {
    "abstract": "Virtual communities include everything from discussion boards to massive multiplayer online role-playing games and virtual realities such as Second Life. The business world has assumed that virtual communities can be leveraged to provide access to consumers and consumer data. The benefits of this assumption have not always been realized. The purpose of this article is to understand why some business ventures into virtual communities fail and others succeed. Why do virtual communities support certain types of business activities and not others? Which firm activities are the best candidates to benefit from being positioned in virtual communities? The theories of social contracts and trust explain how firms can successfully participate in virtual communities. The theories have implications in the context of transaction-oriented, interest-oriented, relationship-oriented, and fantasy-oriented communities. The value chain provides an instructive background to understand which firm activities are candidates for being included in virtual communities. Success in virtual communities depends on an attitude of contribution, dedication of resources, building a critical mass, and matching community and business needs. Because many social technologies are in the disillusionment stage of the hype cycle, further research in the business use of virtual communities is needed to guide business practices as we move to full adoption.",
    "actual_venue": "Electronic Commerce Research And Applications"
  },
  {
    "abstract": "In this paper we discuss variability modelling for hypermedia applications. Inspired by domain engineering, we propose a domain engineering based method for hypermedia development. Since several adaptive hypermedia become more and more popular to incorporate different information views for different audience or environments, we believe that it is important to move variability capturing to modelling phases. Several established modelling views of hypermedia application are discussed from the variability point of view. We also explain modelling techniques by means of examples for the application domain view, the navigation view, the presentation view and discuss importance of the user/environment view for parametrisation of components.",
    "actual_venue": "Adbis"
  },
  {
    "abstract": "The focus of existing virtual communities is centered on a particular product or social interaction and the role of mobile devices is restricted to exchange a limited amount of contents. Herewith we envisage that the upcoming virtual communities will exploit the potential of social interaction and context information to offer personalized services to its members and mobile devices will play a significant role in this process. As a step towards this direction, in this paper we propose a business model for the mobile virtual communities in which the mobile device takes on the role of a content producer and content consumer. Though there are a number of research issues which need to be addressed to realize such virtual communities, in this paper we focus on the service requirements, architecture and open source software implementation of a technical platform for the content producer and consumer mobile devices.",
    "actual_venue": "Iscc"
  },
  {
    "abstract": "Tongue gestures are a key modality for augmentative and alternative communication in patients suffering from speech impairments and full-body paralysis. Systems for recognizing tongue gestures, however, are highly intrusive. They either rely on magnetic sensors built into dentures or artificial teeth deployed inside a patient's mouth or require contact with the skin using electromyography (EMG) sensors. Deploying sensors inside a patient's mouth can be uncomfortable for long-term use and contact-based sensors like EMG electrodes can cause skin abrasion. To address this problem, we present a novel contact-less sensor, called Tongue-n-Cheek, that captures tongue gestures using an array of micro-radars. The array of micro-radars act as proximity sensors and capture muscle movements when the patient performs the tongue gesture. Tongue-n-Cheek converts these movements into gestures using a novel signal processing algorithm. We demonstrate the efficacy of Tongue-n-Cheek and show that our system can reliably infer gestures with 95% accuracy and low latency.",
    "actual_venue": "Ipsn"
  },
  {
    "abstract": "A randomized encoding allows to express a \\\"complex\\\" computation, given by a function f and input x, by a \\\"simple to compute\\\" randomized representation f(x) whose distribution encodes f(x), while revealing nothing else regarding f and x. Existing randomized encodings, geared mostly to allow encoding with low parallel-complexity, have proven instrumental in various strong applications such as multiparty computation and parallel cryptography. This work focuses on another natural complexity measure: the time required to encode. We construct succinct randomized encodings where the time to encode a computation, given by a program Π and input x, is essentially independent of Π's time complexity, and only depends on its space complexity, as well as the size of its input, output, and description. The scheme guarantees computational privacy of (Π,x), and is based on indistinguishability obfuscation for a relatively simple circuit class, for which there exist instantiations based on polynomial hardness assumptions on multi-linear maps. We then invoke succinct randomized encodings to obtain several strong applications, including: Succinct indistinguishability obfuscation, where the obfuscated program IObf({Π}) computes the same function as Π for inputs x of apriori-bounded size. Obfuscating Π is roughly as fast as encoding the computation of Π on any such input x. Here we also require subexponentially-secure indistinguishability obfuscation for circuits. Succinct functional encryption, where a functional decryption key corresponding to Π allows decrypting Π(x) from encryptions of any plaintext x of apriori-bounded size. Key derivation is as fast as encoding the corresponding computation. Succinct reusable garbling, a stronger form of randomized encodings where any number of inputs x can be encoded separately of Π, independently of Π's time and space complexity. Publicly-verifiable 2-message delegation where verifying the result of a long computation given by Π and input x is as fast as encoding the corresponding computation. We also show how to transform any 2-message delegation scheme to an essentially non-interactive system where the verifier message is reusable. Previously, succinct randomized encodings or any of the above applications were only known based on various non-standard knowledge assumptions. At the heart of our techniques is a generic method of compressing a piecemeal garbled computation, without revealing anything about the secret randomness utilized for garbling.",
    "actual_venue": "Symposium On The Theory Of Computing"
  },
  {
    "abstract": "Service composition is an efficient way to implement a service of complex business process in heterogeneous environment. Existing service selection methods mainly utilise fitness function or constraint technique to convert multiple objectives service composition problems to single objective ones. These methods need to take effect with priori knowledge of problem&#39;s solution space. Besides, in each ...",
    "actual_venue": "Iet Software"
  },
  {
    "abstract": "Strong competition in the manufacturing industry makes efficient and effective manufacturing processes a critical success factor. However, existing warehousing and analytics approaches in manufacturing are coined by substantial shortcomings, significantly preventing comprehensive process improvement. Especially, they miss a holistic data base integrating operational and process data, e. g., from Manufacturing Execution and Enterprise Resource Planning systems. To address this challenge, we introduce the Manufacturing Warehouse, a concept for a holistic manufacturing-specific process warehouse as central part of the overall Advanced Manufacturing Analytics Platform. We define a manufacturing process meta model and deduce a universal warehouse model. In addition, we develop a procedure for its instantiation and the integration of concrete source data. Finally, we describe a first proof of concept based on a prototypical implementation.",
    "actual_venue": "Dawak"
  },
  {
    "abstract": "Hand gesture recognition using electromyography signals (EMG) has attracted increased attention due to the rise of cheaper wearable devices that can record accurate EMG data. One of the outstanding devices in this area is the Myo armband, equipped with eight EMG sensors and a nine-axis inertial measurement unit. The use of Myo armband in virtual reality, however, is very limited, because it can only recognize five pre-set gestures. In this work, we do not use these gestures, but the raw data provided by the device in order to measure the force applied to a gesture and to use Myo vibrations as a feedback system, aiming to improve the user experience. We propose two techniques designed to explore the capabilities of the Myo armband as an interaction tool for input and feedback in a VRE. The objective is to evaluate the usability of the Myo as an input and output device for selection and manipulation of 3D objects in virtual reality environments. The proposed techniques were evaluated by conducting user tests with ten users. We analyzed the usefulness, efficiency, effectiveness, learnability and satisfaction of each technique and we conclude that both techniques had high usability grades, demonstrating that Myo armband can be used to perform selection and manipulation task, and it can enrich the experience making it more realistic by using the possibility of measuring the strength applied to the gesture and the vibration feedback system.",
    "actual_venue": "Virtual, Augmented And Mixed Reality: Interaction, Navigation, Visualization, Embodiment, And Simulation, Vamr , Pt"
  },
  {
    "abstract": "When the class of monadic recursion schemes is augmented by individual constants, some of the properties change. It becomes undecidable whether “S diverges” or “S is strongly equivalent to T” for S, T schemes with individual constants. The family of value languages generated by this class of schemes is the family of recursively enumerable languages. The subclass of free schemes with constants is also investigated. It remains decidable whether “S halts” or “S diverges” for S a free scheme with individual constants, but it becomes undecidable whether “T has a strongly equivalent free scheme” for T an arbitrary scheme with individual constants.",
    "actual_venue": "Journal Of Computer And System Sciences"
  },
  {
    "abstract": "Clusters featuring the InfiniBand interconnect are continuing to scale. As an example, the Â¿RangerÂ¿ system at the Texas Advanced Computing Center (TACC) includes over 60,000 cores with nearly 4,000 InfiniBand ports. The latest Top500 list shows 30% of systems and over 50% of the top 100 are now using InfiniBand as the compute node interconnect. As these systems continue to scale, the Mean-Time-Between-Failure (MTBF) is reducing and additional resiliency must be provided to the important components of HPC systems, including the MPI library. In this paper we present a design that leverages the reliability semantics of InfiniBand, but provides a higher-level of resiliency. We are able to avoid aborting jobs in the case of network failures as well as failures on the endpoints in the InfiniBand Host Channel Adapters (HCA). We propose reliability designs for rendezvous designs using both Remote DMA (RDMA) read and write operations. We implement a prototype of our design and show that performance is near-identical to that of a non-resilient design. This shows that we can have both the performance and the network reliability needed for large-scale systems.",
    "actual_venue": "Parallel And Distributed Processing, Workshops And Phd Forum"
  },
  {
    "abstract": "Most studies tackling hysteresis identification in the technical literature follow white-box approaches, i.e. they rely on the assumption that measured data obey a specific hysteretic model. Such an assumption may be a hard requirement to handle in real applications, since hysteresis is a highly individualistic nonlinear behaviour. The present paper adopts a black-box approach based on nonlinear state-space models to identify hysteresis dynamics. This approach is shown to provide a general framework to hysteresis identification, featuring flexibility and parsimony of representation. Nonlinear model terms are constructed as a multivariate polynomial in the state variables, and parameter estimation is performed by minimising weighted least-squares cost functions. Technical issues, including the selection of the model order and the polynomial degree, are discussed, and model validation is achieved in both broadband and sine conditions. The study is carried out numerically by exploiting synthetic data generated via the Bouc–Wen equations.",
    "actual_venue": "Mechanical Systems And Signal Processing"
  },
  {
    "abstract": "Time series motifs are frequently occurring but previously unknown subsequences of a longer time series. Discovering time series motifs is a crucial task in time series data mining. In time series motif discovery algorithm, finding nearest neighbors of a subsequence is the basic operation. To make this basic operation efficient, we can make use of some advanced multidimensional index structure for time series data. In this paper, we propose two novel algorithms for discovering motifs in time series data: The first algorithm is based on \\(\\hbox {R}^{*}\\)-tree and early abandoning technique and the second algorithm makes use of a dimensionality reduction method and state-of-the-art Skyline index. We demonstrate that the effectiveness of our proposed algorithms by experimenting on real datasets from different areas. The experimental results reveal that our two proposed algorithms outperform the most popular method, random projection, in time efficiency while bring out the same accuracy.",
    "actual_venue": "Knowledge And Information Systems"
  },
  {
    "abstract": "HMM-based Level Building algorithm outperforms other methods.Performance rises when employing Grammar and sign length constraints.System runs faster by using a fast algorithm for HMM. Sign sequence segmentation and sign recognition are two main problems in continuous sign language recognition (CSLR) system. In recent years, dynamic time warping based Level Building (LB-DTW) algorithm has successfully dealt with both two challenges simultaneously. However, there still exists two crucial problems in LB-DTW: low recognition performance due to bad similarity function and offline due to high computation. In this paper, we use hidden Markov model (HMM) to calculate the similarity between the sign model and testing sequence, and a fast algorithm for computing the likelihood of HMM is proposed to reduce the computation complexity. Furthermore, grammar constraint and sign length constraint are employed to improve the recognition rate and a coarse segmentation method is proposed to provide the maximal level number. In experiments with a KINECT dataset of Chinese sign language containing 100 sentences composed of 5 signs each, the proposed method shows superior recognition performance and lower computation compared to other existing techniques.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "We present a new algorithm for solving a linear least squares problem with linear constraints. These are equality constraint equations and nonnegativity constraints on selected variables. This problem, while appearing to be quite special, is the core problem arising in the solution of the general linearly constrained linear least squares problem. The reduction process of the general problem to the core problem can be done in many ways. We discuss three such techniques.",
    "actual_venue": "Math Program"
  },
  {
    "abstract": "In this Letter an efficient recursive update algorithm for least squares support vector machines (LSSVMs) is developed. Using the previous solution and some matrix equations, the algorithm completely avoids training the LSSVM all over again whenever new training sample is available. The gain in speed using the recursive update algorithm is illustrated on four data sets from UCI repository: the Statlog Australian credit, the Pima Indians diabetes, the Wisconsin breast cancer, and the adult income data sets.",
    "actual_venue": "Neural Processing Letters"
  },
  {
    "abstract": "Meta-analytic summaries of neuroimaging studies point to at least two major functional-anatomic subdivisions within the anterior insula that contribute to the detection and processing of salient information: a dorsal region that is routinely active during attention tasks and a ventral region that is routinely active during affective experience. In two independent samples of cognitively normal human adults, we used intrinsic functional connectivity magnetic resonance imaging to demonstrate that the right dorsal and right ventral anterior insula are nodes in separable large-scale functional networks. Furthermore, stronger intrinsic connectivity within the right dorsal anterior insula network was associated with better performance on a task involving attention and processing speed whereas stronger connectivity within the right ventral anterior insula network was associated with more intense affective experience. These results support the hypothesis that the identification and manipulation of salient information is subserved by at least two brain networks anchored in the right anterior insula that exhibit distinct large-scale topography and dissociable behavioral correlates.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "One of the most active research areas in networking is quality of service (QoS). The most fundamental requirement for QoS is the ability to find a path that can provide the required network resources between two nodes. These nodes will use QoS routing to find a feasible QoS path between both of them. In this paper, we present a distributed multicast QoS routing architecture that uses probes to find a quick and scalable QoS path between a joining router and the multicast tree. Any router that receives this probe will only know its neighbours and it will create a link to the previous router from where the probe comes from. The joining router will join the multicast tree by following these links on each router until it reaches the tree. Analysis of this method shows that the convergence rate and message overhead is lower than other similar schemes.",
    "actual_venue": "Computer Communications"
  },
  {
    "abstract": "A key goal in dextrous robotic hand grasping is to balance external forces and at the same time achieve grasp stability and minimum grasping energy by choosing an appropriate set of internal grasping forces, Since it appears that there is no direct algebraic optimization approach, a recursive optimization, which is adaptive for application in a dynamic environment, is required.One key observation in this paper is that friction force limit constraints and force balancing constraints are equivalent to the positive definiteness of a certain matrix subject to linear constraints, Based on this observation, we formulate the task of grasping force optimization as an optimization problem on the smooth manifold of linearly constrained positive definite matrices for which there are known globally exponentially convergent solutions via gradient flows. There are a number of versions depending on the Riemannian metric chosen, each with its advantages, Schemes involving second derivative information for quadratic convergence are also studied.Several forms of constrained gradient flows are developed for point contact and soft-finger contact friction models, The physical meaning of the cost index used for the gradient flows is discussed in the context of grasping force optimization, A discretized version for real-time applicability is presented, Numerical examples demonstrate the simplicity, the good numerical properties, and optimality of the approach.",
    "actual_venue": "Ieee Transactions On Robotics And Automation"
  }
]