[
  {
    "abstract": "We describe a method for retrieving winds from colocated Ku- and C-band ocean wind scatterometers. The method utilizes an artificial neural network technique to optimize the weighting of the information from the two frequencies and to use the extra degrees of freedom to account for rain contamination in the measurements. A high-fidelity scatterometer simulation is used to evaluate the efficacy of the technique for retrieving hurricane force winds in the presence of heavy precipitation. Realistic hurricane wind and precipitation fields were simulated for three Atlantic hurricanes, Katrina and Rita in 2005 and Helene in 2006, using the Weather Research and Forecasting model. These fields were then input into a radar simulation previously used to evaluate the Extreme Ocean Vector Wind Mission dual-frequency scatterometer mission concept. The simulation produced high-resolution dual-frequency normalized radar cross-section (NRCS) measurements. The simulated NRCS measurements were binned into 5 x 5 km wind cells. Wind speeds in each cell were estimated using an artificial neural network technique. The method was shown to retrieve accurate winds up to 50 m/s even in intense rain.",
    "actual_venue": "Geoscience And Remote Sensing, Ieee Transactions"
  },
  {
    "abstract": "This paper details a technique, called inter-block backtracking (IBB), which improves interval solving of decomposed systems with non-linear equations over the reals. This technique, introduced in 1998 by Bliek et al., handles a system of equations previously decomposed into a set of (small) k × k sub-systems, called blocks. All solutions are obtained by combining the solutions computed in the different blocks. The approach seems particularly suitable for improving interval solving techniques. In this paper, we analyze into detail the different variants of IBB which differ in their backtracking and filtering strategies. We also introduce IBB-GBJ, a new variant based on Dechter's graph-based backjumping. An extensive comparison on a sample of eight CSPs allows us to better understand the behavior of IBB. It shows that the variants IBB-BT+ and IBB-GBJ are good compromises between simplicity and performance. Moreover, it clearly shows that limiting the scope of the filtering to the blocks is very useful. For all the tested instances, IBB gains several orders of magnitude as compared to a global solving.",
    "actual_venue": "Cocos"
  },
  {
    "abstract": "Network quality of service (NQoS) of IP networks is unpredictable and impacts the quality of networked multimedia services.\n Adaptive voice and video schemes are therefore vital for the provision of voice over IP (VoIP) services for optimised quality\n of experience (QoE). Traditional adaptation schemes based on NQoS do not take perceived quality into consideration even though\n the user is the best judge of quality. Additionally, uncertainties inherent in NQoS parameter measurements make the design\n of adaptation schemes difficult and their performance suboptimal. This paper presents a QoE-driven adaptation scheme for voice\n and video over IP to solve the optimisation problem to provide optimal QoE for networked voice and video applications. The\n adaptive VoIP architecture was implemented and tested both in NS2 and in an Open IMS Core network to allow extensive simulation\n and test-bed evaluation. Results show that the scheme was optimally responsive to available network bandwidth and congestion\n for both voice and video and optimised delivered QoE for different network conditions, and is friendly to TCP traffic.",
    "actual_venue": "Telecommunication Systems"
  },
  {
    "abstract": "In June 1992, the Internet Activities Board sought to push the Internet Engineering Task Force into a solution for the Internet&#39;s address depletion problem. Its actions provoked a management crisis that forced a restructuring of the Internet standards governance process. Although the events have been characterized as a revolt by the Internet Engineering Task Force, this article revisits the preced...",
    "actual_venue": "Ieee Annals Of The History Of Computing"
  },
  {
    "abstract": "In this paper, finite time rigidity-based formation maneuvering control of single integrator multiagent systems is considered. The target formation graph is assumed to be minimally and infinitesimally rigid, and the desired group velocity is considered to be available only to a subset of the agents. A distributed nonsmooth velocity estimator is used for each agent to estimate the desired group velocity in finite time. Using Lyapunov and input to state stability notions, a finite time distance-based formation maneuvering controller is presented and it is proved that by using the controller, agents converge to the target formation and track the desired group velocity in finite time. Furthermore, it is demonstrated that the designed controller is implementable in local coordinate frames of the agents. Simulation results are provided to show the effectiveness of the proposed control scheme.",
    "actual_venue": "Ieee Transactions On Cybernetics"
  },
  {
    "abstract": "The Real-time Specification for Java provides predictable memory and scheduling models for developing real-time systems using the Java language. However, it is silent on providing communication mechanisms suitable for distributed real-time systems. In this paper we define a synchronous and asynchronous communication component model to support different synchronous and asynchronous services and show how this model can be integrated with Java RMI in order to provide high predictability and better performance.",
    "actual_venue": "Jtres"
  },
  {
    "abstract": "This paper presents an overview of results from a phenomenographic study that investigates critical differences and similarities in the ways people experience cross-disciplinary practice in engineering contexts. Study implications are discussed regarding developmental perspectives on cross-disciplinary ways of thinking, acting, and being.",
    "actual_venue": "Icls"
  },
  {
    "abstract": "The focus of this paper is to get the optimal hierarchical mobility management solution for next generation wireless IP-based networks. Hierarchical architectures prove advantageous in minimizing signaling overhead by limiting registration signaling locally. Our ultimate goal is to find the optimum hierarchy level, which should provide best performance in terms of networks parameters like signaling overhead, handoff latency and frequency of location updates. For this purpose an n-tiered architecture has been considered and optimality test on the number of tiers has been performed through analytical and ns-2 based simulation results. From the performance evaluation it is seen that time-bandwidth product is minimum for three-tiered network architecture leading as the optimal solution.",
    "actual_venue": "Ieee Vehicular Technology Conference Fall, Vols"
  },
  {
    "abstract": "This paper examines a two-echelon supply chain with an upstream supplier (she) and a downstream manufacturer (he) transacting an intermediate product via direct bilateral contracting and futures market channels with differentiated productivities. A game model is established to examine the dual-channel supply chain operations. Analytical results reveal that downstream productivity improvement (DPI) through the bilateral interaction is necessary and sufficient for the supply chain members to trade in the bilateral channel in addition to the futures market. We show that when the price in the futures market increases, the manufacturer would purchase less from the futures market and more from the supplier, which not only increases the supplier's expected profit but also increases her risk (variance of the profit) in equilibrium. Furthermore, when the bilateral channel exhibits stronger DPI, the manufacturer obtains a higher expected profit and bears a higher risk, but the supplier enjoys a higher expected profit without incurring any additional risk.",
    "actual_venue": "International Transactions In Operational Research"
  },
  {
    "abstract": "We construct symmetric cubature formulae for the surface measure on the unit sphere, making use of a new correspondence between cubatrue formulae on the sphere and on the triangle, which states, as a special case, that fully symmetric cubature formulae for the surface measure on the unit sphere correspond to symmetric cubature formulae for the weight function (u1u2u3) 1/2, where u3 = 1 u1 u2, on the triangle.",
    "actual_venue": "Math Comput"
  },
  {
    "abstract": "We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.",
    "actual_venue": "Stoc"
  },
  {
    "abstract": "In this paper, early lumping estimation of space-time varying diffusion coefficient and source term for a nonhomogeneous linear parabolic partial differential equation (PDE) describing tokamak plasma heat transport is considered. The analysis of this PDE is achieved in a finite-dimensional framework using the cubic b-splines finite element method with the Galerkin formulation. This leads to a finite-dimensional linear time-varying state-space model with unknown parameters and inputs. The extended Kalman filter with unknown inputs without direct feedthrough (EKF-UI-WDF) is applied to simultaneously estimate the unknown parameters and inputs and an adaptive fading memory coefficient is introduced in the EKF-UI-WDF, to deal with time varying parameters. Conditions under which the direct problem is well posed and the reduced order model converges to the initial one are established. In silico and real data simulations are provided to evaluate the performances of the proposed technique.",
    "actual_venue": "Control Systems Technology, IEEE Transactions  "
  },
  {
    "abstract": "In recent years power consumption of high performance computing (HPC) clusters has become a growing problem due, e.g., to the economic cost of electricity, the emission of carbon dioxide (with negative impact on the environment), and the generation of heat (which reduces hardware reliability). In past work, we developed EnergySaving cluster, a software package that regulates the number of active nodes in an HPC facility to match the users' demands. In this paper, we extend this work by presenting a simulator for this tool that allows the evaluation and analysis of the benefits of applying different energy-saving strategies and policies, under realistic workloads, to different cluster configurations.",
    "actual_venue": "Operating Systems Review"
  },
  {
    "abstract": "In this paper we introduce a new probabilistic method which integrates building extraction with change detection in remotely sensed image pairs. A global optimization process attempts to find the optimal configuration of buildings, considering the observed data, prior knowledge, and interactions between the neighboring building parts. The accuracy is ensured by a Bayesian object model verification, meanwhile the computational cost is significantly decreased by a non-uniform stochastic object birth process, which proposes relevant objects with higher probability based on low-level image features.",
    "actual_venue": "Applications Of Computer Vision"
  },
  {
    "abstract": "Channel estimation is crucial for modern WiFi system and becomes more and more challenging with the growth of user throughput in multiple input multiple output configuration. Plenty of literature spends great efforts in improving the estimation accuracy, while the interpolation schemes are overlooked. To deal with this challenge, we exploit the super-resolution image recovery scheme to model the non-linear interpolation mechanisms without pre-assumed channel characteristics in this paper. To make it more practical, we offline generate numerical channel coefficients according to the statistical channel models to train the neural networks, and directly apply them in some practical WiFi prototype systems. As shown in this paper, the proposed super-resolution based channel estimation scheme can outperform the conventional approaches in both LOS and NLOS scenarios, which we believe can significantly change the current channel estimation method in the near future.",
    "actual_venue": "Ieee International Conference On Communications"
  },
  {
    "abstract": "In the last years, event-based communication style has been extensively studied and is considered a promising approach to develop large scale distributed systems. The historical development of event based systems has followed a line which has evolved from channel-based systems, to subject-based systems, next content-based systems and finally type-based systems which use objects as event messages. According to this historical development the next step should be usage of agents in event systems. In this paper, we propose a new model for Agent Based Distributed Event Systems, called ABDES, which combines the advantages of event-based communication and intelligent mobile agents into a flexible, extensible and fault tolerant distributed execution environment.",
    "actual_venue": "Computing And Informatics"
  },
  {
    "abstract": "This paper presents an intelligent decision support system for financial portfolio management. An adaptive business intelligence approach combines optimization, forecasting and adaptation with application specific financial information processing and quantitative investment paradigms.The methodology involves constructing a ranking of stocks by strength of a buy or sell recommendation which is inferred using an adapting forecasting model that considers a range of factors. These include company balance sheet information, market price and trading volume as well as the wider economy. The system adjusts its prediction model dynamically as market conditions change. An evolving fuzzy rule base mechanism encodes a model of relationships between model factors and a recommendation to buy, sell or hold securities.",
    "actual_venue": "Aspects Of Natural Language Processing"
  },
  {
    "abstract": "Secret sharing plays an important role in protecting confidential information from being lost, destroyed, or falling into wrong hands. Verifiable multi-secret sharing enables a dealer to share multiple secrets among a group of participants such that the deceptive behaviors of the dealer and the participants can be detected. In this paper, we analyze the security of several recently proposed verifiable multi-secret sharing schemes. We show that these schemes cannot withstand some deceptive behaviors of the dealer, and hence fails to satisfy the basic requirement of secure verifiable secret sharing schemes. After that, we present two improved verifiable multi-secret sharing schemes. Our new schemes can not only resist cheating by the dealer or participants, but also remove the use of private channels.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Support vector machine (SVM) is the state-of-the-art classification method, and the doubly regularized SVM (DrSVM) is an important extension based on the elastic net penalty. DrSVM has been successfully applied in handling variable selection while retaining (or discarding) correlated variables. However, it is challenging to solve this model. In this paper we develop an iterative @?\"2-SVM approach to implement DrSVM over high-dimensional datasets. Our approach can significantly reduce the computation complexity. Moreover, the corresponding algorithms have global convergence property. Empirical results over the simulated and real-world gene datasets are encouraging.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "This paper proposes a new strategy for making knots with a high-speed multifingered robot hand having tactile sensors. The strategy is divided into three skills: loop production, rope permutation, and rope pulling. Through these three skills, a knot can be made with a single multifingered robot hand. The dynamics of the rope permutation are analyzed in order to improve the success rate, and an effective tactile feedback control method is proposed based on the analysis. Finally, experimental results are shown.",
    "actual_venue": "Ieee/Rsj International Conference On Intelligent Robots And Systems"
  },
  {
    "abstract": "Session types are a formalism to model structured communication-based programming. A session type describes communication by specifying the type and direction of data exchanged between two parties. When session types and session primitives are added to the syntax of standard π-calculus types and terms, they give rise to additional separate syntactic categories. As a consequence, when new type features are added, there is duplication of efforts in the theory: the proofs of properties must be checked both on ordinary types and on session types. We show that session types are encodable in ordinary π types, relying on linear and variant types. Besides being an expressivity result, the encoding (i) removes the above redundancies in the syntax, and (ii) the properties of session types are derived as straightforward corollaries, exploiting the corresponding properties of ordinary π types. The robustness of the encoding is tested on a few extensions of session types, including subtyping, polymorphism and higher-order communications.",
    "actual_venue": "Inf Comput"
  },
  {
    "abstract": "This paper presents a new architecture for controlling autonomous agents in dynamic multi-agent worlds, building on previous work addressing reactive and deliberative control methods. The proposed multi-layered architecture allows a rationally bounded, goal-directed agent to reason predictively about potential conflicts by constructing causal theories which explain other agents' observed behaviors and hypothesize their intentions; at the same time it enables the agent to operate autonomously and to react promptly to changes in its real-time environment. A principal aim of this research is to understand the role different functional capabilities play in constraining an agent's behavior under varying environmental conditions. To this end, an experimental testbed has been constructed comprising a simulated multi-agent world in which a variety of agent configurations and behaviors have been investigated. A number of experimental findings are reported.",
    "actual_venue": "Ecai Workshop On Agent Theories, Architectures, And Languages"
  },
  {
    "abstract": "A VLSI feedforward neural network is presented that makes use of digital weights and analog multipliers. The network is trained in a chip-in-loop fashion with a host computer implementing the training algorithm. The chip uses a serial digital weight bus im- plemented by a long shift register to input the weights. The inputs and outputs of the network are provided directly at pins on the chip. The training algorithm used is a parallel weight perturbation technique(1). Training results are shown for a 2 input, 1 output network trained with an AND function, and for a 2 input, 2 hidden unit, 1 output network trained with an XOR function.",
    "actual_venue": "Circuits And Systems, Iscas , Ieee International Symposium"
  },
  {
    "abstract": "This letter presents a new motion compensation algorithm to process airborne interferometric repeat-pass synthetic aperture radar (SAR) data. It accommodates topography variations during SAR data processing, using an external digital elevation model. The proposed approach avoids phase artifacts, azimuth coregistration errors, and impulse response degradation, which usually appear due to the assump...",
    "actual_venue": "Ieee Geoscience And Remote Sensing Letters"
  },
  {
    "abstract": "Ant colony algorithm is an evolutionary optimization algorithm that simulates the foraging behavior of ant in nature, and it is distributed, parallel, robust and based on positive feedback. Basic principle of ant colony algorithm is introduced, and an adaptive clustering algorithm based on multi-ants parallel mechanism is constructed in this paper. The multi-ants parallel and adaptive clustering algorithm is applied to fault classification of locomotive wheel-paired bearings, and the accuracy rate of classification is 87%. Research results show the algorithm is effective on practical fault diagnosis.",
    "actual_venue": "International Symposium On Neural Networks"
  },
  {
    "abstract": "This paper reexamines the idea of developing an index of manufacturing fit with market requirements. Bozarth and Berry's (1997) recent work provides a starting point. Several conceptual and methodological issues relating to the Bozarth and Berry approach are raised. It is argued that the indifference profile concept suggested by the authors bears reexamination.",
    "actual_venue": "Decision Sciences"
  },
  {
    "abstract": "The morphology of neurons offers many insights into developmental processes and signal processing. Numerous reports have focused on metrics at the level of individual branches or whole arbors; however, no studies have attempted to quantify repeated morphological patterns within neuronal trees. We introduce a novel sequential encoding of neurite branching suitable to explore topological patterns.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "This paper introduces three hybrid methods for the generation and optimization of rules and membership functions of a fuzzy logic controller for nonlinear systems. The proposed methods overcome the deficiency of a systematic approach for optimal design of fuzzy controllers. An optimally designed fuzzy logic controller should have the least number of fuzzy variables and fuzzy rules and the best possible configuration of fuzzy rules in the rule table. The first strategy of this paper is a two-phase optimization problem: in the first phase, the number of fuzzy variables and their arrangement in the rule table are optimized by a genetic algorithm; in the second phase, the parameters of the membership functions are optimized via extended Kalman filtering. The second strategy tries to achieve the goals of the first method all in one phase by modifying the chromosome structure of the genetic algorithm. Then in the next step, a local search algorithm is utilized to improve the obtained results. The third strategy is similar to the second strategy in structure. However, along with optimizing the number of fuzzy variables and membership parameters, the number of fuzzy rules is also optimized. The first and second strategies are obliged to use every possible combination of fuzzy variables in the rule table; however, the third strategy is capable of distinguishing between useful and useless fuzzy rules in the rule table. The introduced strategies are applied to an automotive cruise control system. The results of the simulations show the effectiveness of the proposed methods and the superiority of the latter approaches over the former ones.",
    "actual_venue": "Proceedings Of The Institution Of Mechanical Engineers -Journal Of Systems And Control Engineering"
  },
  {
    "abstract": "In this paper we will present a parallel algorithm to generate the permutations of at mostk out ofn objects. The architecture consists of a linear processor array and a selector. When one single processor array is available, a parallel algorithm to generate permutations is presented which achieves the best possible speedup for any givenk. Also, this algorithm can easily be modified to generate combinations. When multiple processor arrays are available, a parallel scheme is proposed to speed up the generation by fully utilizing these processor arrays. The degree of parallelism is related to the number of available processor arrays.",
    "actual_venue": "BIT"
  },
  {
    "abstract": "Scientific applications are increasingly using cloud resources for their data analysis workflows. However, managing data effectively and efficiently over these cloud resources is challenging due to the myriad storage choices with different performance-cost trade-offs, complex application choices, complexity associated with elasticity and, failure rates. The explosion in scientific data coupled with unique characteristics of cloud environments require a more flexible and robust distributed data management solution than the ones currently in existence. This paper describes the design and implementation of FRIEDA - a Flexible Robust Intelligent Elastic Data Management framework. FRIEDA coordinates data in a transient cloud environment taking into account specific application characteristics. Additionally, we describe a range of data management strategies and show the benefit of flexible data management schemes in cloud environments. We study two distinct scientific applications from bioinformatics and image analysis to understand the effectiveness of such a framework.",
    "actual_venue": "High Performance Computing, Networking, Storage And Analysis"
  },
  {
    "abstract": "Given a document D in the form of an unordered labeled tree, we study the expressibility on D of various fragments of XPath, the core navigational language on XML documents. We give characterizations, in terms of the structure of D, for when a binary relation on its nodes is definable by an XPath expression in these fragments. Since each pair of nodes in such a relation represents a unique path in D, our results therefore capture the sets of paths in D definable in XPath. We refer to this perspective on the semantics of XPath as the \"global view.\" In contrast with this global view, there is also a \"local view\" where one is interested in the nodes to which one can navigate starting from a particular node in the document. In this view, we characterize when a set of nodes in D can be defined as the result of applying an XPath expression to a given node of D. All these definability results, both in the global and the local view, are obtained by using a robust two-step methodology, which consists of first characterizing when two nodes cannot be distinguished by an expression in the respective fragments of XPath, and then bootstrapping these characterizations to the desired results.",
    "actual_venue": "Symposium On Principles Of Database Systems"
  },
  {
    "abstract": "In a directed graph, a kernel is a subset of vertices that is both stable and absorbing. Not all digraphs have a kernel, but a theorem due to Boros and Gurvich guarantees the existence of a kernel in every clique-acyclic orientation of a perfect graph. However, an open question is the complexity status of the computation of a kernel in such a digraph. Our main contribution is to prove new polynomiality results for subfamilies of perfect graphs, among which are claw-free perfect graphs and chordal graphs. Our results are based on the design of kernel computation methods with respect to two graph operations: clique-cutset decomposition and augmentation of flat edges. We also prove that deciding the existence of a kernel – and computing it if it exists – can be done in polynomial time in any orientation of a chordal or a circular-arc graph, even not clique-acyclic.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "In this paper, we present a novel methodology for multimodal non-rigid medical image registration. The proposed approach is based on combining an optical flow technique with a pixel intensity transformation by using a local variability measure, such as statistical variance or Shannon entropy. The methodology is basically composed by three steps: first, we approximate the global deformation using a rigid registration based on a global optimization technique, called particle filtering; second, we transform both target and source images into a new intensity space where they can be compared; and third, we obtain the optical flow between them by using the Horn and Shuck algorithm in an iterative scales-space framework. After these steps, the non-rigid registration is made up by adding the resulting vector fields, computed by the rigid registration, and the optical flow. The proposed algorithm was tested using a synthetic intensity mapping and non-rigid deformation of MRI images. Preliminary results show that the methodology seems to be a good alternative for non-rigid multimodal registration, obtaining an average error of less than two pixels in the estimation of the deformation vector field.",
    "actual_venue": "Annual International Conference Of The Ieee Engineering In Medicine And Biology Society, Vols"
  },
  {
    "abstract": "We describe an algorithm to read entire databases with locking concurrency control allowing multiple readers or an exclusive writer. The algorithm runs concurrently with the normal transaction processing (on-the-fly), and locks the en- tities in the database one by one (incremental). We prove the algorithm produces consistent pictures of the database. We also show that the algorithm aborts a minimal number of updates in the class of on-the-fly, incremental, consistent algorithms. On-the-fly, incremental algorithms to read entire data- bases consistently can improve system availability and reli- ability. Most existing systems either require the transaction processing to stop, or produce potentially inconsistent re- sults. Our algorithm does not change the database physical design, so it can be adapted to existing systems by expand- ing their lock table. Finally, we extend the algorithm in a straightforward way to read entire distributed databases.",
    "actual_venue": "Algorithmica"
  },
  {
    "abstract": "Semantic web technologies are a key enabler for future information systems. The concept of Linked Data allows flexible inter-organizational collaboration and dynamic data integration. It surpasses the limitations of prevalent relational databases and proprietary access technologies. RFID, on the other hand, has been successfully applied to integrate physical artifacts into complex information systems. It allows the identification of artifacts by a unique identifier leveraging concepts such as the Internet of Things. Using the existing RFID ecosystem as universal entry point to Linked Data clouds makes it possible to connect the semantic description of the real world with the physical artifacts within this world. This provides new ways of merging and exploring real and virtual content. This paper presents an approach for using RFID as universal entry point to Linked Data clouds for task-driven and context-aware mobile support systems.",
    "actual_venue": "Rfid-Technologies And Applications"
  },
  {
    "abstract": "This small-scale study compares two groups of Year 4 (8/9 year-old) pupils either reading or playing an interactive storybook. The study considered pupils' recall of propositions, which formed the story V setting and episodes, and of micro-propositions and characters' names, and pupils' responses to inferential items derived from the 'interactive storybook'. The study indicates that, whether reading or playing, pupils' recall of the story setting was sound, but pupils who had read the interactive storybook demonstrated greater recall of the story event structure than those who had engaged in interactive picture-play. Pupils who had played the interactive storybook demonstrated significantly greater recall of micro-propositions and names.",
    "actual_venue": "Journal Of Computer Assisted Learning"
  },
  {
    "abstract": "With the popularity of group-oriented applications, secure and efficient communication among all group members has become a major issue. An efficient key management mechanism is the base and critical technology of secure group communications. A distributed group-oriented key management scheme without the participation of third parties is proposed in the paper. The scheme deploys Elliptic Curve Diffie-Hellman (ECDH) which is more lightweight compared to regular Diffie-Hellman. The approach includes group key establishment and rekeying algorithms when there are membership changes. By using a distributed architecture, the load of key management is reduced. Specifically, the scheme can be extended to hybrid architecture to provide better scalability. Consequently, the extended scheme is both fault-tolerant and efficient in terms of integrity and confidentiality. In all protocol suites, the shared group key is calculated by scalar multiplication. According to performance comparisons with other schemes, the proposed scheme dramatically reduces communication overhead and computational costs. Security analysis indicates that the proposal provides a number of desirable security properties, including group key secrecy, forward secrecy and backward secrecy.",
    "actual_venue": "Journal Of Computers"
  },
  {
    "abstract": "Security-as-a-service is an emerging area in cloud computing. Traditionally, security approaches are service provider-centric and provider-driven. In this paper, we propose a model for security-as-a-service using “crowdsourcing”. Though crowdsourcing has been used to provide specific security services like browser security, detecting phishing attacks, detecting cybersecurity threats, there has been no work which provides a unified framework to provide different types of security verification. Dispersed computing power of devices is used to perform security verifications. This is done by subscribers in a collaborative way, using their idle resources, in exchange of certain incentives. Our architecture guarantees anonymity of users who request service and the crowd who contribute in verification by using virtualization concepts and virtual machines. Moreover, we propose an approach for managing these security verification jobs, subscribers in a fault tolerant manner. To the best of our knowledge, we are the first to propose a unified security-as-a-service framework using crowdsourcing, thus introducing a new research problem. We discuss a number of applications, challenges and problems of crowdsourcing in security verification.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "IntroductionThe invention and spread of the World Wide Web made the process of paper publication significantly easier thanbefore, and added a large repository of on-line (electronic) papers to our body of knowledge. To make the process ofinformation gathering easier for scientists, Digital Libraries usually maintain Search Engines, which can be used tofind papers about a specified topic. However, the use of these Search Engines suffers from two significantdisadvantages. First, the user...",
    "actual_venue": "Webnet"
  },
  {
    "abstract": "In 2005, Lee and Chen proposed an improved one-time password authentication scheme that can prevent a stolen verifier attack and that is as efficient as the scheme of Yeh-Shen-Hwang. The current paper, however, demonstrates that Lee-Chen's scheme is still vulnerable to Denial-of-Service attacks and a simple solution is presented in order to isolate such a problem. Furthermore, we propose an efficiently optimized one-time password authentication scheme that can provide user anonymity. In addition the computational costs are lower than those of Lee-Chen's scheme.",
    "actual_venue": "Iccsa"
  },
  {
    "abstract": "Private cloud computing has been recently pushed as a promising solution to more efficiently exploit available hardware equipment and quickly reconfigure software components depending on the current load. However, private cloud computing, and in particular the Infrastructure as a Service (IaaS) model, has been primarily adopted only by large companies, at least so far. In fact, Small and Medium Enterprises (SMEs) usually have difficulties in adopting the private computing paradigm, either since they are not able to afford the cost of off-the-shelf proprietary solutions or they do not have the know-how required to adapt open-source solutions to their own requirements. The paper presents our novel framework for Easy Monitoring and Management of IaaS (EMMI) solutions, with the main objective of making easier the adoption of the private cloud paradigm. The EMMI framework is based on open-source software components, i.e., OpenStack for IaaS management, Collectd for distributed system monitoring, and Apache jk for load balancing, allowing to adopt the IaaS model easily and in a cost effective manner. The reported performance results demonstrate that the EMMI framework efficiently supports two primary features of the IaaS model, i.e., failover and scale-out, promptly identifying crucial events and autonomously taking suitable countermeasures.",
    "actual_venue": "Computers And Communication"
  },
  {
    "abstract": "The CCITT/ISO standard “The Directory” defines a major new OSI application and represents a major milestone in the development of large-scale distributed systems. The Standard provides a specification for a distributed global on-line directory. Distributed operation of the Directory is achieved through a complex combination of protocols and the knowledge each component system has of the others. This paper examines the distributed operation of the Directory. It has a threefold purpose: firstly, it gives a comprehensive tutorial on the distributed operation of the Directory; secondly, it shows the rationale behind the design; and thirdly, it provides further insights into various aspects, revealing potential traps and optimisations for implementors. Together the material caters for a range of reader, from those wanting to understand the subject for the first time through to X.500 implementors.",
    "actual_venue": "Computer Networks And Isbn Systems"
  },
  {
    "abstract": "An election procedure based on voter preference rankings is said to be monotonic if the alternative chosen by the procedure for any profile of voter preference rankings is also chosen after it is moved up in one or more of the profile's rankings. Several reasonable-sounding election procedures that are known to violate monotonicity are examined along with some new classes of non-monotonic procedures. Closely-related procedures that are monotonic are also identified. The procedural mechanisms and combinatorial structures that give rise to failures of monotonicity are analyzed in some detail.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "Motivation: Non-small-cell lung carcinoma (NSCLC) mainly consists of two subtypes: lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD). It has been reported that the genetic and epigenetic profiles vary strikingly between LUAD and LUSC in the process of tumorigenesis and development. Efficient and precise treatment can be made if subtypes can be identified correctly. Identification of discriminative expression signatures has been explored recently to aid the classification of NSCLC subtypes. Results: In this study, we designed a classification model integrating both mRNA and long noncoding RNA (lncRNA) expression data to effectively classify the subtypes of NSCLC. A gene selection algorithm, named WGRFE, was proposed to identify the most discriminative gene signatures within the recursive feature elimination (RFE) framework. GeneRank scores considering both expression level and correlation, together with the importance generated by classifiers were all taken into account to improve the selection performance. Moreover, a module-based initial filtering of the genes was performed to reduce the computation cost of RFE. We validated the proposed algorithm on The Cancer Genome Atlas (TCGA) dataset. The results demonstrate that the developed approach identified a small number of expression signatures for accurate subtype classification and particularly, we here for the first time show the potential role of LncRNA in building computational NSCLC subtype classification models.classification.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "We propose a new dynamic method for multidimensional selectivity estimation for range queries that works accurately independent of data distribution. Good estimation of selectivity is important for query optimization and physical database design. Our method employs the multilevel grid file (MLGF) for accurate estimation of multidimensional data distribution. The MLGF is a dynamic, hierarchical, balanced, multidimensional file structure that gracefully adapts to nonuniform and correlated distributions. We show that the MLGF directory naturally represents a multidimensional data distribution. We then extend it for further refinement and present the selectivity estimation method based on the MLGF. Extensive experiments have been performed to test the accuracy of selectivity estimation. The results show that estimation errors are very small independent of distributions, even with correlated and/or highly skewed ones. Finally, we analyze the cause of errors in estimation and investigate the effects of various parameters on the accuracy of estimation.",
    "actual_venue": "Vldb J"
  },
  {
    "abstract": "The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "This study focuses on the integrated guidance and autopilot (IGA) design for a path-following uninhabited aerial vehicle (UAV). A second order sliding structure which facilitates smooth bank and turn coupled motions is used for path-following flight applications. A high-order sliding mode (HOSM) differentiator is used for the estimation of the uncertain sliding surfaces. In order to apply the IGA system for a path-following UAV, a virtual target, set in a pre-specified path, is used. The potential of the proposed method is demonstrated through a UAV path-following application to a helical ascent flight under wind turbulence circumstances.",
    "actual_venue": "American Control Conference"
  },
  {
    "abstract": "This paper presents a characterization of all minimum cuts, separating a source from a sink in a network. A binary relation\n is associated with any maximum flow in this network, and minimum cuts are identified with closures for this relation. As a\n consequence, finding all minimum cuts reduces to a straightforward enumeration. Applications of this results arise in sensitivity\n and parametric analyses of networks, the vertex packing and maximum closure problems, in unconstrained pseudo-boolean optimization\n and project selection, as well as in other areas of application of minimum cuts.",
    "actual_venue": "Mathematical Programming"
  },
  {
    "abstract": "We present a new approach for how multiple users' views can be rendered in a surround virtual environment without using special multi-view hardware. It is based on the idea that different parts of the screen are often viewed by different users, so that they can be rendered from their own view point, or at least from a point closer to their view point than traditionally expected. The vast majority of 3D virtual reality systems are designed for one head-tracked user, and a number of passive viewers. Only the head tracked user gets to see the correct view of the scene, everybody else sees a distorted image. We reduce this problem by algorithmically democratizing the rendering view point among all tracked users. Researchers have proposed solutions for multiple tracked users, but most of them require major changes to the display hardware of the VR system, such as additional projectors or custom VR glasses. Our approach does not require additional hardware, except the ability to track each participating user. We propose three versions of our multi-viewer algorithm. Each of them balances image distortion and frame rate in different ways, making them more or less suitable for certain application scenarios. Our most sophisticated algorithm renders each pixel from its own, optimized camera perspective, which depends on all tracked users' head positions and orientations.",
    "actual_venue": "D User Interfaces"
  },
  {
    "abstract": "A new method of ranking based on an indicator estimating the relative position of an element determined from the preference relation on the elements to be classified is proposed. The method is then applied to the multiple criteria decision problem and is illustrated by a simple hypothetical example. Possible developments and extensions of the method to various decision environments are explored.",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "Motivation: Molecular dynamics simulations provide detailed insights into the structure and function of biomolecular systems. Thus, they complement experimental measurements by giving access to experimentally inaccessible regimes. Among the different molecular dynamics techniques, native structure-based models (SBMs) are based on energy landscape theory and the principle of minimal frustration. Typically used in protein and RNA folding simulations, they coarsegrain the biomolecular systemand/or simplify the Hamiltonian resulting in modest computational requirements while achieving high agreement with experimental data. eSBMTools streamlines running and evaluating SBM in a comprehensive package and offers high flexibility in adding experimental-or bioinformatics-derived restraints. Results: We present a software package that allows setting up, modifying and evaluating SBM for both RNA and proteins. The implemented workflows include predicting protein complexes based on bioinformatics-derived inter-protein contact information, a standardized setup of protein folding simulations based on the common PDB format, calculating reaction coordinates and evaluating the simulation by free-energy calculations with weighted histogram analysis method or by phi-values. The modules interface with the molecular dynamics simulation program GROMACS. The package is open source and written in architecture-independent Python2.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "A Knowledge Management (KM) System plays a crucial role in every industry as well as in Higher Learning Institutions. Based on the earlier research works, the authors have identified the gaps, and challenges in order to develop a comprehensive KM System framework, Hybrid Evaluation method which are helpful to assess any given KM system. The primary goal of this research paper is to propose the methodology for ranking and rating of the KM system using Multi-dimensional Metric Model, Metric Database and Weighted Arithmetic Mean (WAM) method. This paper first provides the background for KM System Technology Framework and then enlists some of the significant research works, carried out in the past two decades especially on the KM Success Factors and Models, which support and validate their success. Secondly, this paper describes and illustrates how a Multi-dimensional Metric Model and Metric Database can be formed. Finally, this paper discusses how the KM Systems are evaluated and ranked for their effectiveness through the proposed evaluation methodology.",
    "actual_venue": "International Journal Of Knowledge Management"
  },
  {
    "abstract": "Links on theWeb are unidirectional, embedded, difficult to author and maintain. With a Semantic Web architecture, COHSE (Conceptual Open Hypermedia System) aims to address these limitations by dynamically creating links on the Web. Here we present how ...",
    "actual_venue": "Smap"
  },
  {
    "abstract": "Physiological signals such as EEG and EOG have been successfully applied to detect driving fatigue in single modality. In this paper, we propose a multimodal approach by combining partial EEG and forehead EOG to enhance driving fatigue detection. We investigate the key brain area where we collect the EEG to combine with forehead EOG. Our experiment results demonstrate that the temporal EEG signals from six-channel have the best performance when combining with forehead EOG to extract shared features. Furthermore, we propose a novel multimodal fusion strategy using deep autoencoder model to learn a better shared representation. We assess our approach with other fusion strategies on 21 subjects. Our multimodal approach achieves the best performance that the average COR and RMSE are 0.85 and 0.09, respectively. The experiment results demonstrate that our multimodal approach could learn an efficient shared representation between modalities and could significantly improve the performance of detecting driving fatigue.",
    "actual_venue": "International Ieee/Embs Conference On Neural Engineering"
  },
  {
    "abstract": "The interfacing of object-oriented languages with functional languages, in general, and with Haskell, in particular, has received a considerable amount of attention. Previous work, including Lambada, a Haskell to Java bridge, showed how an object-oriented class hierarchy can be modeled using Haskell type classes, such that Java libraries can be used conveniently from Haskell. The present paper extends this previous work in two major directions. Firstly, we describe a new implementation of object-oriented style method calls and overloading in Haskell, using multi-parameter type classes and functional dependencies. This enables calling of a foreign object’s methods in a syntactically convenient, type-safe manner. Secondly, we sketch an approach to automating the generation of library bindings using compile-time meta-programming for object-oriented frameworks featuring reflection. We have evaluated the practicality of our approach by implementing a Haskell binding to the Objective-C language on the Mac OS X platform.",
    "actual_venue": "IFL"
  },
  {
    "abstract": "Research into eyes-free mobile reading devices has grown in recent years. This research has focused mainly on the image processing required by such a device, with a lower emphasis on the user interaction. In this paper, a model of a voice user interface (VUI) for a mobile reading device is presented. Three field studies with blind participants were conducted to develop and refine the model. A formal grammar is used to describe the VUI, and a stochastic Petri net was developed to model the complete user-device interaction. Evaluation and analysis of the user testing of a prototype led to empirically derived probabilities of grammar token usage for the commands that comprise the VUI.",
    "actual_venue": "Human-Machine Systems, Ieee Transactions"
  },
  {
    "abstract": "Context: Testing usually involves the interaction of the tester with the system under test. However, there are many situations in which this interaction is not feasible and so one requires a passive approach in which the system is analysed to look for failures or unexpected behaviours. The entities of a complex system usually communicate in an asynchronous manner and this complicates the testing tasks since the observed order of events need not be the same as the order in which the events were produced. In previous work, we presented a formal passive testing theory for a single user and system communicating through an asynchronous channel. We were able to check that a trace generated by the system satisfies a given property.",
    "actual_venue": "Information And Software Technology"
  },
  {
    "abstract": "A new computational approach for automated triangulation of Computer-Aided Design (CAD) surface models, applicable to various CFD (Computational Fluid Dynamics) problems of practical interest is proposed. The complex shaped product configurations are represented by a set of Non-Uniform Rational B-Splines (NURBS) surface patches. The suggested technique is based on the molecular dynamics method. The main idea of the approach is that the mesh nodes are considered as similarly charged interacting particles which move within the region to be meshed under the influence of internal (such as particle–particle interaction forces) and external forces as well as optional additional forces. Moreover, the particles experience a medium resistance due to which the system comes to equilibrium within a relatively short period of time. The proposed 3D surface mesh generation algorithm uses a parametric NURBS representation as initial definition of the domain boundary. This method first distributes the interacting nodes into optimal locations in the parametric domain of the NURBS surface patch using molecular dynamics simulation. Then, the well-shaped triangles can be created after connecting the nodes by Delaunay triangulation. Finally, the mapping from parametric space to 3D physical space is performed. Since the presented interactive algorithm allows to control the distance between a pair of nodes depending on the curvature of the NURBS surface, the method generates high quality triangular mesh. The algorithm enables to produce uniform mesh, as well as anisotropic adaptive mesh with refinement in the large gradient regions. The mesh generation approach has the abilities to preserve the representation accuracy of the input geometry model, create a close relationship between geometry modeling and grid generation process, be automated to a large degree. Some examples are considered in order to illustrate the method’s ability to generate a surface mesh for a complicated CAD model.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.",
    "actual_venue": "Ieee/Cvf Conference On Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "This paper presents a methodology for tracking a hypo- or hyper-enhanced focal liver lesion (FLL) and a healthy liver region in a video sequence of a Contrast-Enhanced Ultrasound (CEUS) examination. The outcome allows the differentiation between benign and malignant cases, by characterising FLLs of typical behaviour, according to their Time-Intensity curves. The task is challenging mainly due to intensity changes caused by contrast agents. Initially the ultrasound mask is automatically localised and then the FLL and parenchyma regions are tracked, assuming affine transformations on the image plane, employing the point-based registration technique of Lowe's scale-invariant feature transform (SIFT) keypoints detector. Finally, a quantitative evaluation of the tracking process provides a confidence measure for the characterisation decision.",
    "actual_venue": "Advances In Visual Computing, Isvc , Pt"
  },
  {
    "abstract": "Code refactoring usually generates problems in other parts of the code that had already been validated. A solution is to use an anticipated test design methodology, where unit tests are first created to each module/class/method before their modification. Thus, developers are able to ensure the correct execution of functions after their refactoring. Our work applies this strategy to the development of a set of libraries that are used in several other projects. As developers have to modify the initial implementation of these libraries, to adapt such libraries to different needs, it is important to ensure that the libraries’ functions are still properly working and verify if the efficiency of the algorithms was modified. Results show that this approach has increased the confidence of developers in modifying a library and generating several implementations of such library, so that the best implementation could be selected using the same set of unit tests.",
    "actual_venue": "Sera"
  },
  {
    "abstract": "We have used a replica exchange Monte-Carlo procedure, popularly known as Parallel Tempering, to study the problem of Coulomb explosion in homogeneous Ar and Xe dicationic clusters as well as mixed Ar-Xe dicationic clusters of varying sizes with different degrees of relative composition. All the clusters studied have two units of positive charges. The simulations reveal that in all the cases there is a cutoff size below which the clusters fragment. It is seen that for the case of pure Ar, the value is around 95 while that for Xe it is 55. For the mixed clusters with increasing Xe content, the cutoff limit for suppression of Coulomb explosion gradually decreases from 95 for a pure Ar to 55 for a pure Xe cluster. The hallmark of this study is this smooth progression. All the clusters are simulated using the reliable potential energy surface developed by Gay and Berne (Gay and Berne, Phys. Rev. Lett. 1982, 49, 194). For the hetero clusters, we have also discussed two different ways of charge distribution, that is one in which both positive charges are on two Xe atoms and the other where the two charges are at a Xe atom and at an Ar atom. The fragmentation patterns observed by us are such that single ionic ejections are the favored dissociating pattern. (c) 2017 Wiley Periodicals, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "We propose optimal VLSI layouts for butterfly networks under the multilayer 2-D grid model, the Thompson model, and the extended grid model. We show that an N-node butterfly network can be laid out with area N2/lfloorL2/2rfloorlog2 2 N+ o(N2/L2log2N), volume LN2 /lfloorL2/2rfloorlog2 2N+ o(N 2/Llog2N), and maximum wire length N/radiclfloorL2/4rfloorlog2N+o(N/LlogN) under the multilayer 2-D grid model, where only one active layer (for network nodes) is required and L wiring layers (for network links) are available, 2 les L les o(nthrootN). We also show that the proposed multilayer butterfly layouts are optimal within a factor of 1+o(1) when adjacent wiring layers have orthogonal wires (to be referred to as X-Y layouts) and the area is calculated by a slanted encompassing rectangle. The proposed layouts are the first and only optimal butterfly layouts reported in the literature thus far for L gsim 3, and match the best previous layout for L=2. We propose to use AT2L2 or 2AT2 lfloorL2/2rfloor as a new parameter for characterizing the space-time complexity for multilayer VLSI, and show that AT2L2ap 2R2for RxR butterfly networks, where R=N/log2N+o(N/logN)",
    "actual_venue": "Icpp"
  },
  {
    "abstract": "One of the most important problems when considering the design of manufacturing systems based on SOA paradigms is the integration of shop floor devices in the business processes at the enterprise level. This paper presents the design and implementation of the Customer Order Management (COM) module based on SOA architecture in the context of holonic manufacturing systems. The COM module is integrating with SOA enabled shop floor devices using industry standards. The implementation leverages a multi agent system suited for industrial applications integrated in a SOA environment capable of dynamic BPEL workflow generation and execution. The prototype consists in a SCA application for core COM module functionality and an extension for NetLogo MAS platform for SOA integration. The COM module interacts with the MES layer using real time events handled by the BPEL process implementation in the execution stage. A web based portal frontend for the COM module has been developed to allow real time tracking of customer orders, providing data about product batch execution and individual progress of each product on the production line.",
    "actual_venue": "Computers In Industry"
  },
  {
    "abstract": "We designed a vibrotactile vest with physiological monitoring that interacts with a vibroacoustic urban environment, The Humming Wall. We structured vibrotactile patterns and built a vibrotactile language to convey information and to interact towards and from the vibroacoustic environment in order to elicit sensations and encourage particular body movements. The patterns were structured to emulate calming and activating sensations and to guide or warn the vest wearer. In addition, actions such as swiping or knocking on the wall were replicated on the vest for the vest wearer, and participants could `feelu0027 (vibroacoustically) and hear their own heartbeats and breath rates at the wall. A field trial with 39 participants was conducted over a 5-week period in an urban park. Participants wearing the vest completed a set of defined tasks. We logged use and responses, videoed all activities and conducted interviews and questionnaires post-experiment. The results depicted the participantsu0027 experience, engagement and impressions while wearing the vibrotactile vest and interacting with the wall. The findings show convincing, strong and positive responses to novel interactions between the responsive vibroacoustic environment and the vibrotactile vest. We found compelling evidence to support further exploration into vibrotactile and vibroacoustic solutions for improving health and well-being. The work presented demonstrates the capacity for health and well-being solutions with multiple use cases. Additionally, this work constitutes the first field trial with a vibrotactile wearable responding to and driving vibroacoustic displays with an interactive vibroacoustic environment.",
    "actual_venue": "Universal Access In The Information Society"
  },
  {
    "abstract": "The quadratic linear ordering problem naturally generalizes various optimization problems such as bipartite crossing minimization or the betweenness problem, which includes linear arrangement. These problems have important applications, e.g., in automatic graph drawing and computational biology. We present a new polyhedral approach to the quadratic linear ordering problem that is based on a linearization of the quadratic objective function. Our main result is a reformulation of the 3-dicycle inequalities using quadratic terms. After linearization, the resulting constraints are shown to be face-inducing for the polytope corresponding to the unconstrained quadratic problem. We use this result both within a branch-and-cut algorithm and within a branch-and-bound algorithm based on semidefinite programming. Experimental results for bipartite crossing minimization show that this approach clearly outperforms other methods.",
    "actual_venue": "Informs Journal On Computing"
  },
  {
    "abstract": "This paper presents the operating principles of SLOG, a logic interpreter of equational clauses (Horn clauses where the only predicate is =). SLOG is based on an oriented form of paramodulation called superposition. Superposition is a complete inference rule for first-order logic with equality. SLOG uses only a strong restriction of superposition (innermost superposition) which is still complete for a large class of programs. Besides superposition, SLOG uses rewriting which provides eager evaluation and handling of negative knowledge. Rewriting combined with superposition improves terminability and control of equational logic programs.",
    "actual_venue": "RTA"
  },
  {
    "abstract": "In this paper, the Gibbs-Markov approach is extended to integration of observations provided by virtual sensors and organized according to a hierarchical taxonomy. The proposed extension is applied to image restoration and segmentation. A model of coupled Gibbs-Markov random fields (GMRFs) is presented, which involves performing restoration and labeling at two abstraction levels. i.e., the image (pixel) level and the region level. The maximum a posteriori (MAP) approach usually applied as an estimation criterion for single-level GMRFs is shown to be a special case of the most probable explanation (MPE) criterion, which is valid for multilevel GMRFs. A stochastic distributed optimization algorithm is used to reach the solution.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "The capability to accurately recognize users' new intentions is critical for service evolution, because software systems are designed and implemented to assist their users. In sensor-rich context-aware environments, it is possible to recognize users' intentions from what users do (actions) and their interactions with and effects on the environment (environmental contexts). However, in order to establish the computational model and implement the intention recognition mechanism, it is essential that intentions and the critical situations used to define human intentions be clearly specified using observable contexts and events. In this paper, we present a set of definitions that enable systems to capture these situations and recognize users' intentions to support service evolution. An algorithm for intention recognition and the implementation of this algorithm to simulate the process of recognizing intentions at runtime are also presented. A case study of ATM cash withdraw is used to demonstrate the intention specification process.",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "By an optimal linear code we mean that it has the highest minimum distance with a prescribed length and dimension. We construct several families of optimal linear codes over the finite field Fp by making use of down-sets generated by one maximal element of Fpn. Moreover, we show that these families of optimal linear codes are minimal and contain relative two-weight linear codes, and have applications to secret sharing schemes and wire-tap channel of type II with the coset coding scheme, respectively.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "Machine-learning methods in the form of Bayesian networks (BN), linear projection (LP) and self-organizing tree (SOT) models were used to explore association among polymorphic sites within the HVR1 and NS5a regions of the HCV genome, host demographic factors (ethnicity, gender and age) and response to the combined interferon (IFN) and ribavirin (RBV) therapy. The BN models predicted therapy outcomes, gender and ethnicity with accuracy of 90%, 90% and 88.9%, respectively. The LP and SOT models strongly confirmed associations of the HVR1 and NS5A structures with response to therapy and demographic host factors identified by BN. The data indicate host specificity of HCV evolution and suggest the application of these models to predict outcomes of IFN/RBV therapy.",
    "actual_venue": "Bibm Workshops"
  },
  {
    "abstract": "Elliptic curve cryptography (ECC) has proven its superiority, since it was proposed in the domain of Public-Key Cryptography [1]. Further, Edwards curve adds a new paradigm to ECC in terms of speed and security against exceptional point attacks. This curve has been recently extended to Binary Edwards Curves (BEC), due to efficiency of implementation in GF(2m) fields and to harvest the advantages of a unified and complete scalar point multiplication on the family of BEC. In spite of achieving the unification, it introduces more challenges to the designer to reduce the computation time and trade-off the area in efficient way. This work reports an implementation of BEC processor with an effort to better utilize the look-up table (LUT) of the FPGA. The design further implements the ternary algorithm to increase the efficiency. However, to the best of our knowledge there exists no previous implementations of BEC on FPGA platform. The proposed design has been implemented for state-of-the-art GF(2233) fields. The performance of the design has been found to compare favorably with the existing designs on standard cell ASIC libraries, in spite of being implemented on FPGA platform.",
    "actual_venue": "Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "High-level synthesis (HLS) has gained considerable traction over recent years, as it allows for faster development and verification of hardware accelerators than traditional RTL design. While HLS allows for most bugs to be caught during software verification, certain non-deterministic or data-dependent bugs still require debugging the actual hardware system during execution. Recent work has focused on techniques to allow designers to perform in-system debug of HLS circuits in the context of the original software code; however, like RTL debug, the user must still determine the root cause of a bug using small execution traces, with lengthy debug turns.\n\nIn this work, we demonstrate techniques aimed at reducing the time HLS designers spend performing in-system debug. Our approaches consist of performing data dependency analysis to guide the user in selecting which variables are observed by the debug instrumentation, as well as an associated debug overlay that allows for rapid reconfiguration of the debug logic, enabling rapid switching of variable observation between debug iterations. In addition, our overlay provides additional debug capability, such as selective function tracing and conditional buffer freeze points. We explore the area overhead of these different overlay features, showing a basic overlay with only a 1.7% increase in area overhead from the baseline debug instrumentation, while a deluxe variant offers 2×--7× improvement in trace buffer memory utilization with conditional buffer freeze support.",
    "actual_venue": "Acm Transactions On Reconfigurable Technology And Systems"
  },
  {
    "abstract": "Autonomous mapping, especially in the form of SLAM (Simultaneous Localization And Mapping), has long since been used for many indoor robotic applications and is also useful in outdoor intelligent vehicle applications such as object detection. Most existing research works on environment mapping and object detection in outdoor applications have been dedicated to single vehicle system. On the other hand, multi-vehicle cooperative perception based on inter-vehicle data sharing can bring considerable benefits in many scenarios that are challenging for a single vehicle system. In this paper, a new method for occupancy grid maps merging is proposed: an objective function based on occupancy likelihood is introduced to measure the consistency degree of maps alignment; genetic algorithm implemented in a dynamic scheme is adopted to optimize the objective function. A scheme of multi-vehicle cooperative local mapping and moving object detection using the proposed occupancy grid maps merging method is also introduced. Real-data tests are given to demonstrate the effectiveness of the introduced method.",
    "actual_venue": "Icarcv"
  },
  {
    "abstract": "The development of high-performance statistical machine translation (SMT) systems is contingent on the availability of substantial, in-domain parallel training corpora. The latter, however, are expensive to produce due to the labor-intensive nature of manual translation. We propose to alleviate this problem with a novel, semi-supervised, batch-mode active learning strategy that attempts to maximize in-domain coverage by selecting sentences, which represent a balance between domain match, translation difficulty, and batch diversity. Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outperforms the random selection baseline, but also traditional active selection techniques based on dissimilarity to existing training data.",
    "actual_venue": "Computer Speech And Language"
  },
  {
    "abstract": "We present a strong fluid-rigid coupling for Smoothed Particle Hydrodynamics (SPH) fluids and rigid bodies with particle-sampled surfaces. The approach interlinks the iterative pressure update at fluid particles with a second SPH solver that computes artificial pressure at rigid-body particles. The introduced SPH rigid-body solver models rigid-rigid contacts as artificial density deviations at rigid-body particles. The corresponding pressure is iteratively computed by solving a global formulation that is particularly useful for large numbers of rigid-rigid contacts. Compared to previous SPH coupling methods, the proposed concept stabilizes the fluid-rigid interface handling. It significantly reduces the computation times of SPH fluid simulations by enabling larger time steps. Performance gain factors of up to 58 compared to previous methods are presented. We illustrate the flexibility of the presented fluid-rigid coupling by integrating it into DFSPH, IISPH, and a recent SPH solver for highly viscous fluids. We further show its applicability to a recent SPH solver for elastic objects. Large scenarios with up to 90M particles of various interacting materials and complex contact geometries with up to 90k rigid-rigid contacts are shown. We demonstrate the competitiveness of our proposed rigid-body solver by comparing it to Bullet.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "•The time-variant fatigue reliability of welded joints is assessed.•The fatigue process is modeled as a surface crack propagation process.•Polynomial and Kriging models surrogate surface crack sizes from fatigue crack growth analyses.•The adaptive procedures are adopted to improve the reliability results.•The advantage of Kriging models and the effectiveness of adaptive procedures are demonstrated.",
    "actual_venue": "Reliability Engineering And System Safety"
  },
  {
    "abstract": "Revocation of access control on private e-healthcare records (EHRs) allows to revoke the access rights of valid users. Most existing solutions rely on a trusted third party too much to generate and update decryption keys, or require the computations of non-revoked users during the revocation, which make them impractical for some more complicated scenarios. In this paper, we propose a new revocation model, referred to as non-interactive revocable identity-based access control (NRIBAC) on EHRs. In NRIBAC, a trusted third party only needs to generate secret keys for group authorities and each group authority can generate decryption keys for the users in its domain. The NRIBAC distinguishes itself from other revocation schemes by the advantageous feature that it does not require any participation of non-revoked users in the revocation. We construct an NRIBAC scheme with short ciphertexts and decryption keys by leveraging hierarchical identity-based encryption and introducing the version information. We formally prove the security of the NRIBAC scheme and conduct thorough theoretical analysis to evaluate the performance. The results reveal that the scheme provides favorable revocation procedure without disturbing non-revoked users.",
    "actual_venue": "Information Security Practice And Experience, Ispec"
  },
  {
    "abstract": "The current study investigated the value of using immersive virtual environment technology as a tool for assessing eyewitness identification. Participants witnessed a staged crime and then examined sequential lineups within immersive virtual environments that contained 3D virtual busts of the suspect and six distractors. Participants either had unlimited viewpoints of the busts in terms of angle and distance, or a unitary view at only a single angle and distance. Furthermore, participants either were allowed to choose the angle and distance of the viewpoints they received, or were given viewpoints without choice. Results demonstrated that unlimited viewpoints improved accuracy in suspect-present lineups but not in suspect-absent lineups. Furthermore, across conditions, post-hoc measurements demonstrated that when the chosen view of the suspect during the lineup was similar to the view during the staged crime in terms of distance, accuracy improved. Finally, participants were more accurate in suspect-absent lineups than in suspect-present lineups. Implications of the findings in terms of theories of eyewitness testimony are discussed, as well as the value of using virtual lineups that elicit high levels of presence in the field. We conclude that digital avatars of higher fidelity may be necessary before actually implementing virtual lineups.",
    "actual_venue": "Presence"
  },
  {
    "abstract": "This paper describes several parallel algorithms that solve geometric problems. The algorithms are based on a vector model of computation-the scan model. The purpose of this paper is both to show how the model can be used and to formulate a set of practical algorithms. The scan model is based on a small set of operations on vectors of atomic values. It differs from the P-RAM models both in that it includes a set of scan primitives, also called parallel prefix computations, and in that it is a strictly data-parallel model. A very useful abstraction in the scan model is the segment abstraction, the subdivision of a vector into a collection of independent smaller vectors. The segment abstraction permits a clean formulation of divide-and-conquer algorithms and is used heavily in the algorithms described in this paper. Within the scan model, using the operations and routines defined, the paper describes a k-D tree algorithm requiring O(lg n) calls to the primitives for n points, a closest-pair algorithm requiring O(lg n) calls to the primitives, a line-drawing algorithm requiring O(1) calls to the primitives, a line-of-sight algorithm requiring O(1) calls to the primitives, and finally, three different convex-hull algorithms. The last convex-hull algorithm, merge-hull, utilizes a generalized binary search technique using divide-and-conquer with the segment abstraction. The paper also describes how to implement the CREW version of Cole's merge sort in O(lg n) calls to the primitives. All these algorithms should be noted for their simplicity rather than their complexity; many of them are parallel versions of known serial algorithms. Most of the algorithms discussed in this paper have been implemented on the Connection Machine, a highly parallel single instruction multiple data computer.",
    "actual_venue": "Parallel Solutions To Geometric Problems On The Scan Model Of Computation"
  },
  {
    "abstract": "Co-clustering leads to parsimony in data visualisation with a number of parameters dramatically reduced in comparison to the dimensions of the data sample. Herein, we propose a new generalized approach for nonlinear mapping by a re-parameterization of the latent block mixture model. The densities modeling the blocks are in an exponential family such that the Gaussian, Bernoulli and Poisson laws are particular cases. The inference of the parameters is derived from the block expectation–maximization algorithm with a Newton–Raphson procedure at the maximization step. Empirical experiments with textual data validate the interest of our generalized model.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Social networking tools have been enthusiastically heralded as a means to support different learning types and innovative pedagogical practices. They have also been recognized as potential tools to promote informal learning. In this paper we describe work carried out using the synergy of social web tools, learning models and innovative pedagogical practices across a Masters Degree Course. Findings suggest that the use of these tools as a means to distribute an open and flexible learning environment fosters informal interactions and such interactions are perceived by students to have a significant impact over their formal learning outcomes.",
    "actual_venue": "Ec-Tel"
  },
  {
    "abstract": "Architectural supports, e.g., user context processing and learning content management are essential for facilitating the development and proliferation of context-aware e-learning services. In this paper, we propose a context-aware e-learning infrastructure called Semantic Learning Space. It leverages the Semantic Web technologies to support semantic knowledge representation, systematic context management, interoperable content integration, expressive knowledge query, and adaptive content recommendation. The functionality encapsulated in the infrastructure handles the common, time-consuming and low-level details in learning context processing and content management. The architectural design and enabling technologies are described in detail. Finally, the prototype implementation and preliminary experimental results are presented.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Camera phones and consumer digital cameras number hundreds of millions worldwide and most of them have the ability to take video in addition to photographs. Public discussions, marketing, and academic research often emphasize the new and innovative ways in which people use their ubiquitous digital cameras, especially camera phones, in combination with the Internet. In this paper we present our qualitative study of 13 people and their picture taking habits with regular cameras and camera phones. We focus on their videography practices in the context of their general use of photo and video media. Our results contradict the general assumption that the availability of ubiquitous video technology has significantly changed people's practices in home-mode pictorial communication. The models for capturing videos are often taken from situations in which previously taking snapshot photographs was the only option. Therefore, we suggest that mobile media creation and sharing technology has only gradually changed people's snapshot photography and videography practices.",
    "actual_venue": "Nordichi"
  },
  {
    "abstract": "Stochastic approximation (SA) is a classical approach for stochastic convex optimization. Previous studies have demonstrated that the convergence rate of SA can be improved by introducing either smoothness or strong convexity condition. In this paper, we make use of smoothness and strong convexity simultaneously to boost the convergence rate. Let $lambda$ be the modulus of strong convexity, $kappa$ be the condition number, $F_*$ be the minimal risk, and $alphau003e1$ be some small constant. First, we demonstrate that, in expectation, an $O(1/[lambda T^alpha] + kappa F_*/T)$ risk bound is attainable when $T = Omega(kappa^alpha)$. Thus, when $F_*$ is small, the convergence rate could be faster than $O(1/[lambda T])$ and approaches $O(1/[lambda T^alpha])$ in the ideal case. Second, to further benefit from small risk, we show that, in expectation, an $O(1/2^{T/kappa}+F_*)$ risk bound is achievable. Thus, the excess risk reduces exponentially until reaching $O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we emphasize that our proof is constructive and each risk bound is equipped with an efficient stochastic algorithm attaining that bound.",
    "actual_venue": "Colt"
  },
  {
    "abstract": "Consistency of preferences is related to rationality, which is associated with the transitivity property. Many properties suggested to model transitivity of preferences are inappropriate for reciprocal preference relations. In this paper, a functional equation is put forward to model the ldquocardinal consistency in the strength of preferencesrdquo of reciprocal preference relations. We show that under the assumptions of continuity and monotonicity properties, the set of representable uninorm operators is characterized as the solution to this functional equation. Cardinal consistency with the conjunctive representable cross ratio uninorm is equivalent to Tanino's multiplicative transitivity property. Because any two representable uninorms are order isomorphic, we conclude that multiplicative transitivity is the most appropriate property for modeling cardinal consistency of reciprocal preference relations. Results toward the characterization of this uninorm consistency property based on a restricted set of (n-1) preference values, which can be used in practical cases to construct perfect consistent preference relations, are also presented.",
    "actual_venue": "Fuzzy Systems, Ieee Transactions"
  },
  {
    "abstract": "We first review in this paper the burstiness and aftereffect of future sampling phenomena, and propose a formal, operational criterion to characterize distributions according to these phenomena. We then introduce the Beta negative binomial distribution for text modeling, and show its relations to several models (in particular to the Laplace law of succession and to the tf-itf model used in the Divergence from Randomness framework of [2]). We finally illustrate the behavior of this distribution on text categorization and information retrieval experiments.",
    "actual_venue": "Ecir"
  },
  {
    "abstract": "Based on a recently proposed algorithmic solution technique, the inverse kinematic problem for redundant manipulators is solved. The kinematics of the manipulator is appropriately augmented to include mentioned constraints; the result is an efficient, fast, closed-loop algorithm which only makes use of the direct kinematics of the manipulator. Simulation results illustrate the tracking performance for a given trajectory in the Cartesian space, while guaranteeing a collision-free trajectory and/or not violating a mechanical joint limit.<\n<ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "actual_venue": "Ieee J Rob Autom"
  },
  {
    "abstract": "Parallel MR imaging methods like SMASH, SENSE etc. use multiple receiver coils to accelerate the imaging process by reducing the Fourier space sampling requirement. In this paper, we propose a method for selecting the optimal k-space sampling scheme for parallel MR imaging that minimizes the expected reconstruction error given coil sensitivities and desired acceleration factor. We demonstrate that higher acceleration factors are feasible by spacing the k-space non-uniformly instead of at an integer multiple of the Nyquist spacing.",
    "actual_venue": "Ieee International Symposium On Biomedical Imaging: Macro To Nano, Vols And"
  },
  {
    "abstract": "People detection is an important task for a wide range of applications in computer vision. State-of-the-art methods learn appearance based models requiring tedious collection and annotation of large data corpora. Also, obtaining data sets representing all relevant variations with sufficient accuracy for the intended application domain at hand is often a non-trivial task. Therefore this paper investigates how 3D shape models from computer graphics can be leveraged to ease training data generation. In particular we employ a rendering-based reshaping method in order to generate thousands of synthetic training samples from only a few persons and views. We evaluate our data generation method for two different people detection models. Our experiments on a challenging multi-view dataset indicate that the data from as few as eleven persons suffices to achieve good performance. When we additionally combine our synthetic training samples with real data we even outperform existing state-of-the-art methods.",
    "actual_venue": "Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "It is common to start a course on computer programming logic by teaching the algorithm concept from the point of view of natural languages, but in a schematic way. In this sense we note that the students have difficulties in understanding and implementation of the problems proposed by the teacher. The main idea of this paper is to show that the logical reasoning of computer programming students can be efficiently developed by using at the same time Turing Machine, cellular automata (Wolfram rule) and fractals theory via Problem-Based Learning (PBL). The results indicate that this approach is useful, but the teacher needs introducing, in an interdisciplinary context, the simple theory of cellular automata and the fractals before the problem implementation.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "In this paper, the focus is on accelerating XPath location steps for evaluating regular path expression with predicate parameter in particular since it is a core component of many XML processing standards such as XSLT or XQuery. We present a new indexing structure, namely Xp-tree, which is used to speed-up the evaluation of XPath. Based on accelerating a node using planar combined with the numbering scheme, we devise efficiently derivative algorithms. Our experimental results demonstrate that the proposed method outperforms previous approaches using R-tree indexing mechanism in processing XML queries.",
    "actual_venue": "Dasfaa"
  },
  {
    "abstract": "In some pattern analysis problems, there exists expert knowledge, in addition to the original data involved in the classification process. The vast majority of existing approaches simply ignore such auxiliary (privileged) knowledge. Recently a new paradigm-learning using privileged information-was introduced in the framework of SVM+. This approach is formulated for binary classification and, as typical for many kernel-based methods, can scale unfavorably with the number of training examples. While speeding up training methods and extensions of SVM+ to multiclass problems are possible, in this paper we present a more direct novel methodology for incorporating valuable privileged knowledge in the model construction phase, primarily formulated in the framework of generalized matrix learning vector quantization. This is done by changing the global metric in the input space, based on distance relations revealed by the privileged information. Hence, unlike in SVM+, any convenient classifier can be used after such metric modification, bringing more flexibility to the problem of incorporating privileged information during the training. Experiments demonstrate that the manipulation of an input space metric based on privileged data improves classification accuracy. Moreover, our methods can achieve competitive performance against the SVM+ formulations.",
    "actual_venue": "Neural Networks And Learning Systems, Ieee Transactions"
  },
  {
    "abstract": "We study real-time recovery of consecutive symbols from compressed files, in the context of grammar-based compression, see, e.g., (7). In this setti ng, a compressed text is represented as a small (a few kilobytes) dictionary (containing a set of code words), and a very long (a few megabytes) string based on symbols drawn from the dictionary . The space efficiency of this kind of compression, is comparable with standard compression methods based on the Lempel-Ziv approach (14). We show, that one can visit consecutive symbols of the original text, moving from one symbol to another in constant time and extra space. This algorithm is an improvement of the on-line linear (amortised) time algorithm presented in (7).",
    "actual_venue": "DCC"
  },
  {
    "abstract": "The detection of abnormalities in endoscopic video frames can contribute in the early and more accurate detection of pathologic conditions. In this paper we present a novel Convolutional Neural Network (CNN) architecture for automatic detection of abnormal images in endoscopic video sequences. It features multiscale representation of the endoscopic images in its structure, and peephole connections contributing in enhanced generalization with less computational requirements. An important aspect of the proposed architecture is that it enables weakly-supervised learning, using only semantically annotated images. A novel cross-dataset experimental study is performed to investigate its generalization performance on various publicly available datasets. The results validate that the proposed architecture outperforms recent approaches, with results reaching up to 90.66% in terms of the area under the receiver operating characteristic.",
    "actual_venue": "Ieee International Conference On Image Processing"
  },
  {
    "abstract": "Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 8-16 bit fixed point.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "Theories of minimization of fuzzy automata have been developed by several authors, and most of which applied methods of algebraic theory, more specifically, the equivalence relation, the quotient space and equivalence class, congruence and homomorphism, in studying fuzzy automata. In this paper, we also apply the algebraic theory in the study of lattice automata, and obtain some results similar to the ones of fuzzy automata. In this paper, concepts of refining equivalence and refining congruence are defined, the quotient lattice automaton with respect to refining congruence is formulated, the concept of lattice automaton is reviewed, the equivalence of a lattice automaton and its quotient automaton is proved, the minimal property of quotient automaton is shown, and the minimization algorithm of lattice automata is proposed. The main idea of this paper is that by putting forward the concepts of refining equivalence and refining congruence, we can derive the quotient lattice automaton with respect to refining congruence, and via showing that the quotient lattice automaton is not only equivalent to the lattice automaton but also a minimal automaton, we obtain the minimization of a lattice automaton, and thus get the minimization algorithm of lattice automata.",
    "actual_venue": "Fuzzy Information And Engineering, Proceedings"
  },
  {
    "abstract": "This paper describes a similarity measure for images to be used in image-based localization for autonomous robots with low computational resources. We propose a novel signature to be extracted from the image and to be stored in memory. The proposed signature allows, at the same time, memory saving and fast similarity calculation. The signature is based on the calculation of the 2D Haar Wavelet Transform of the gray-level image. We present experiments showing the effectiveness of the proposed image similarity measure. The used images were collected using the AIBOs ERS-7 of the RoboCup Team Araibo of the University of Tokyo on a RoboCup field, however, the proposed image similarity measure does not use any information on the structure of the environment and do not exploit the peculiar features of the RoboCup environment.",
    "actual_venue": "Ai*Ia"
  },
  {
    "abstract": "A novel blind direction-of-arrival (DOA) and polarization estimation algorithm for polarization-sensitive uniform linear array using dimension reduction multiple signal classification (MUSIC) is proposed in this paper. The proposed algorithm utilizes the signal subspace to obtain an initial estimation of DOA, then estimates more accurate DOA through a one-dimensional (1-D) local searching according to the initial estimation of DOA, and finally obtains polarization parameter estimation via the estimated polarization steering vectors. The proposed algorithm, which only requires a one-dimension local searching, can avoid the high computational cost within multi-dimensional MUSIC algorithm. The simulation results reveal that the proposed algorithm has better DOA and polarization estimation performance than both estimation of signal parameters via rotational invariance technique algorithm and trilinear decomposition algorithm. Furthermore, the proposed algorithm can be suitable for irregular array geometry, obtain automatically paired multi-dimensional parameter estimation, and avoid multi-dimensional searching. Simulation results verify the effectiveness of the proposed algorithm.",
    "actual_venue": "Multidim Syst Sign Process"
  },
  {
    "abstract": "Predictions of the polarized microwave brightness temperatures over the ocean are made using a two-scale surface bidirectional reflectance model combined with an atmospheric radiative transfer model. The reflected atmospheric radiation is found to contribute significantly to the magnitude and directional dependence of the brightness temperatures. The predicted brightness temperatures are also sens...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "This paper is concerned with a mathematical and numerical study of liquid dynamics in a horizontal capillary. We derive a two-liquids model for the prediction of capillary dynamics. This model takes into account the effects of real phenomena: like the outside flow action, or the entrapped gas inside a closed-end capillary. Moreover, the limitations of the one-dimensional model are clearly indicated. Finally, we report on several tests of interest: an academic test case that can be used to check available numerical methods, a test for decreasing values of the capillary radius, a simulation concerning a closed-end capillary, and two test cases for two liquids flow. In order to study the introduced mathematical model, our main tool, is a reliable one-step adaptive numerical approach based on a one-step one-method strategy.",
    "actual_venue": "J Comput Meth In Science And Engineering"
  },
  {
    "abstract": "The paper addresses the problem of providing performance QoS guarantees in a clustered storage system. Multiple related storage objects are grouped into logical containers called buckets, which are distributed over the servers based on the placement policies of the storage system. QoS is provided at the level of buckets. The service credited to a bucket is the aggregate of the IOs received by its objects at all the servers. The service depends on individual time-varying demands and congestion at the servers. We present a token-based, coarse-grained approach to providing IO reservations and limits to buckets. We propose pShift, a novel token allocation algorithm that works in conjunction with token-sensitive scheduling at each server to control the aggregate IOs received by each bucket on multiple servers. pShift determines the optimal token distribution based on the estimated bucket demands and server IOPS capacities. Compared to existing approaches, pShift has far smaller overhead, and can be accelerated using parallelization and approximation. Our experimental results show that pShift provides accurate QoS among the buckets with different access patterns, and handles runtime demand changes well.",
    "actual_venue": "Symposium On Mass Storage Systems And Technologies"
  },
  {
    "abstract": "A collection of articles on the statistical modelling and inference of social networks is analysed in a network fashion. The references of these articles are used to construct a citation network data set, which is almost a directed acyclic graph because only existing articles can be cited. A mixed membership stochastic block model is then applied to this data set to soft cluster the articles. The results obtained from a Gibbs sampler give us insights into the influence and the categorisation of these articles.",
    "actual_venue": "Arxiv: Applications"
  },
  {
    "abstract": "We propose low-complexity robust beamforming algorithms based on the conjugate gradient method and the worst-case optimization based criterion. Unlike the existing robust beamformers based on the worst-case optimization based criterion that use a second-order cone program, the proposed algorithms employ a joint optimization strategy based on low-complexity conjugate gradient algorithms. The proposed algorithms are termed the Robust Constrained Minimum Variance Modified Conjugate Gradient (Robust-CMV-MCG) and the Robust Constrained Constant Modulus Modified Conjugate Gradient (Robust-CCM-MCG), which has an advantage for the special case of constant modulus signals. Simulations show that the performances of the proposed algorithms are equivalent or better than that of existing robust algorithms, whereas the complexity is more than an order of magnitude lower.",
    "actual_venue": "WSA"
  },
  {
    "abstract": "With the rapid development of computer vision theory and visual display devices, High Frame Rate (HFR) and Ultra High Definition (UHD) techniques have received increasing attention from academic and industry. As they put high demands on performance and energy-efficiency, efficient customized hardware is required. In this paper, we propose an FPGA-based super-resolution system that enables real-time UHD upscaling in both high image quality and high frame rates. Our system crops each frame into blocks, measures their total variation values, and dispatches them accordingly to a neural network or an interpolation module for upscaling. We also propose a fast transposed convolution algorithm based on Winograd algorithm, which reduces the number of multiplications. Experimental results show that the proposed super-resolution system achieves superior performance in both reconstruction performance and efficiency over previous works.",
    "actual_venue": "International Conference On Field Programmable Technology"
  },
  {
    "abstract": "Detecting people remains a popular and challenging problem in computer vision. In this paper, we analyze parts-based models for person detection to determine which components of their pipeline could benefit the most if improved. We accomplish this task by studying numerous detectors formed from combinations of components performed by human subjects and machines. The parts-based model we study can be roughly broken into four components: feature detection, part detection, spatial part scoring and contextual reasoning including non-maximal suppression. Our experiments conclude that part detection is the weakest link for challenging person detection datasets. Non-maximal suppression and context can also significantly boost performance. However, the use of human or machine spatial models does not significantly or consistently affect detection accuracy.",
    "actual_venue": "Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "User Experience (UX) deals with the user's emotions and all the aspects of the end user's interaction with the system. It plays a vital role in enhancing user satisfaction by improving usability. Similarly, the effective user interface of the gaming environment can be designed by considering aspects of User Experience (UX). The primary objective of this study is to find out the influence of gender in the emotional aspects of usability in the gaming environment. In this research, we consider four emotional measures including enjoyment, ease of use, usefulness, and satisfaction. The web based gaming prototype followed by the questionnaire on Likert scale was devised to investigate the influence of gender. The 1002 participants which includes 591 (59%) males and 411 (41%) females from different academic venues were participated for experimentation. The data was analyzed through the commonly used statistical method ANOVA. It is observed that significant difference in enjoyment, ease of use, and satisfaction were found between males and females. However, no significant differences were found in usefulness between males and females. This study may pave new research directions to explore the impact of these factors in multiple different contexts.",
    "actual_venue": "Hci : Proceedings Of The International Conference On Human-Computer Interaction"
  },
  {
    "abstract": "Currently, multimedia data grow in a explosive manner, whereas the effective bandwidth is quite limited in the wireless communications, such as high definition image/video communication, satellite communication and so on. Generative adversarial network (GAN) based image compression goes beyond the traditional image encoding standards under a obviously low bitrate. However, the most existing methods do not consider the importance of different image contents, where the regions with complex and saliency structures generally are more essential to users' quality of experience (QoE). In this paper, we propose a content-aware deep perceptual image compression method (CDPIC). The proposed approach involves a saliency subnet to produce content saliency map for adaptive bit allocation. Moreover, using QoE as the ultimate goal, our approach designs different compression objectives for different image contents, where it tends to preserve more details for the regions with complex and saliency structures, and improves the perceptual quality of the background region. Our experiments show that the proposed approach outperforms the state-of-the-art methods, and produces advanced performance in preserving details and semantics.",
    "actual_venue": "International Conference On Telecommunications And Signal Processing"
  },
  {
    "abstract": "HighlightsA new method for identifying fat droplets in histological images is presented.Adjacency statistics are utilized as shape features.Fat droplets are identified with high sensitivity and specificity.Adjacency statistics greatly improve the identification of clustered fat droplets.The method can be quickly executed on standard computers. Background and objectiveThe accurate identification of fat droplets is a prerequisite for the automatic quantification of steatosis in histological images. A major challenge in this regard is the distinction between clustered fat droplets and vessels or tissue cracks. MethodsWe present a new method for the identification of fat droplets that utilizes adjacency statistics as shape features. Adjacency statistics are simple statistics on neighbor pixels. ResultsThe method accurately identified fat droplets with sensitivity and specificity values above 90%. Compared with commonly-used shape features, adjacency statistics greatly improved the sensitivity toward clustered fat droplets by 29% and the specificity by 17%. On a standard personal computer, megapixel images were processed in less than 0.05s. ConclusionsThe presented method is simple to implement and can provide the basis for the fast and accurate quantification of steatosis.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "We show that the complete bipartite graph K\"n\",\"n has a unique regular embedding in an orientable surface if and only if n is coprime to @f(n). The method, involving groups which factorise as a product of two cyclic groups, is also used to classify such embeddings when n is the square of a prime.",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "The laser backscattering from biological tissues depends on their composition and blood flow. The onset of abnormalities in\n tissues is associated with the change in composition at a specific location which may affect laser backscattering. The objective\n of this work is to study the point-to-point compositional variation of male breast tissues as this site has been prone to\n cancer development. The normalized backscattered intensity (NBI) profiles at various locations of human chest region of five\n subjects by multi-probe laser reflectometer are obtained. Based on these data the images of tissue composition, showing the\n point-to-point changes at various depths from the tissue surface, are reconstructed. The analysis of data shows that the maximum\n NBI variation is at the pectoralis major muscle and minimum variation is observed at the sternum. The optical parameters,\n based on the NBI data obtained for five human subjects, show the maximum increase in absorption (p p < 0.0001) coefficients compared to that as observed at the sternum. Also the minimum absorption and maximum scattering coefficients\n are observed at the pectoralis major muscles. The regional variations of NBI and optical parameters further support these\n findings. The variations in the NBI and optical parameters may indicate the compositional change in tissues, which could be\n used for diagnostic and therapeutic applications of laser.",
    "actual_venue": "Med Biol Engineering And Computing"
  },
  {
    "abstract": "In this work, we are developing the computer aided composition system. This system aids a person, which knows cellphone or background music of home page or software. This system is implemented with the interactive selective population climbing. We suppose ...",
    "actual_venue": "Csie"
  },
  {
    "abstract": "As XML becomes increasingly pervasive on the Web, it is necessary to provide a good solution to storing and retrieving huge amount of XML data that includes not only textual data, but also multimedia documents in XML format. Using the Sem-ODB, a multimedia database system, as the underlying repository, we show how the XML storage and retrieval issue can be tackled in a flexible and effective way. We present a prototype that effectively maps and stores XML data into a Sem-ODB through the use of a meta-schema-based approach. Our approach is flexible and effective because (1) users are given control over the resulting mapping scheme via the manipulation of the KnowledgeBase; (2) it greatly reduces the number of joins generated in the final Sem-SQL query when translating an XQuery query.",
    "actual_venue": "Ismse"
  },
  {
    "abstract": "Motivated by emerging cooperative P2P applications we study new uplink allocation algorithms for substituting the rate-based choke/unchoke algorithm of BitTorrent which was developed for non-cooperative environments. Our goal is to shorten the download times by improving the uplink utilization of nodes. We develop a new family of uplink allocation algorithms which we call BitMax, to stress the fact that they allocate to each unchoked node the maximum rate it can sustain, instead of an 1/(k + 1) equal share as done in the existing BitTorrent. BitMax computes in each interval the number of nodes to be unchoked, and the corresponding allocations, and thus does not require any empirically preset parameters like k. We demonstrate experimentally that Bit-Max can reduce significantly the download times in a typical reference scenario involving mostly ADSL nodes. We also consider scenarios involving network bottlenecks caused by filtering of P2P traffic at ISP peering points and show that BitMax retains its gains also in these cases.",
    "actual_venue": "Conext"
  },
  {
    "abstract": "In this paper, a new type of multilayer rule-based classifier is proposed and applied to image classification problems. The proposed approach is entirely data-driven and fully automatic. It is generic and can be applied to various classification and prediction problems, but in this paper we focus on image processing, in particular. The core of the classifier is a fully interpretable, understandable, self-organized set of IF…THEN… fuzzy rules based on the prototypes autonomously identified by using a one-pass type training process. The classifier can self-evolve and be updated continuously without a full retraining. Due to the prototype-based nature, it is non-parametric; its training process is non-iterative, highly parallelizable and computationally efficient. At the same time, the proposed approach is able to achieve very high classification accuracy on various benchmark datasets surpassing most of the published methods, be comparable with the human abilities. In addition, it can start classification from the first image of each class in the same way as humans do, which makes the proposed classifier suitable for real-time applications. Numerical examples of benchmark image processing demonstrate the merits of the proposed approach.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "This paper argues that the major role of assessment in learning environments should be to determine the most appropriate learning material to be presented next to each student. This can be done effectively in highly interactive technology-based learning material, provided learning and assessment are intimately combined. Several strategies for accomplishing this are presented in the paper. Finally, we discuss how we might move toward an educational system of this type.",
    "actual_venue": "Interactive Multimedia In University Education"
  },
  {
    "abstract": "To deal with the drifting issue in visual tracking, we propose an Online Transfer Boosting (OTB) algorithm that transfers knowledge from three different source domains to the target domain to improve the performance of the online classifier used in tracking-by-detection. In particular, the OTB algorithm integrates three types of knowledge by: (1) transferring prior knowledge from the first frame using semi-supervised learning; (2) transferring appearance changes from the previous frames by dynamically updating the learning factor; and (3) transferring observed sample distribution knowledge from the current frame by reweighting the training samples. Experimental results on several public video sequences demonstrated promising performance of OTB in both tracking accuracy and stability.",
    "actual_venue": "Icpr"
  },
  {
    "abstract": "In this paper, we propose a model for feature selection and indexing of online signatures based person identification. For representation of online signatures, a set of 100 global features of MCYT online signature database is considered. However, MCYT based features are high dimension features which significantly increases the response time and space requirements for signature identification process. To overcome this problem, multi cluster feature selection method is proposed to reduce the dimensionality by finding a relevant feature subset. Moreover, in some applications, where the database is supposed to be very large, the identification process typically has an unacceptably long response time. A solution to speed up the identification process is to design an indexing model prior to identification which reduces the number of candidate hypotheses to be considered during matching by the identification algorithm. Hence in this paper, Kd-tree based indexing model is designed for online signatures based person identification. The experimental results reveal that the proposed model works more efficiently both in terms of time and accuracy.",
    "actual_venue": "Hybrid Intelligent Systems"
  },
  {
    "abstract": "Glaucoma is a widespread ocular disorder leading to irreversible loss of vision. Therefore, there is a pressing need for cost-effective screening, such that preventive measures can be taken. This can be achieved with an accurate segmentation of the optic disc and cup from retinal images to obtain the cup-to-disc ratio. We describe a comprehensive solution based on applying convolutional neural networks to feature exaggerated inputs emphasizing disc pallor without blood vessel obstruction, as well as the degree of vessel kinking. The produced raw probability maps then undergo a robust refinement procedure that takes into account prior knowledge about retinal structures. Analysis of these probability maps further allows us to obtain a confidence estimate on the correctness of the segmentation, which can be used to direct the most challenging cases for manual inspection. Tests on two large real-world databases, including the publicly-available MESSIDOR collection, demonstrate the effectiveness of our proposed system.",
    "actual_venue": "Ieee International Conference On Tools With Artificial Intelligence"
  },
  {
    "abstract": "BACKGROUND: Cyanobacteria are the only known prokaryotes capable of oxygenic photosynthesis. They play significant roles in global biogeochemical cycles and carbon sequestration, and have recently been recognized as potential vehicles for production of renewable biofuels. Synechocystis sp. PCC 6803 has been extensively used as a model organism for cyanobacterial studies. DNA microarray studies in Synechocystis have shown varying degrees of transcriptome reprogramming under altered environmental conditions. However, it is not clear from published work how transcriptome reprogramming affects pre-existing networks of fine-tuned cellular processes. RESULTS: We have integrated 163 transcriptome data sets generated in response to numerous environmental and genetic perturbations in Synechocystis. Our analyses show that a large number of genes, defined as the core transcriptional response (CTR), are commonly regulated under most perturbations. The CTR contains nearly 12% of Synechocystis genes found on its chromosome. The majority of genes in the CTR are involved in photosynthesis, translation, energy metabolism and stress protection. Our results indicate that a large number of differentially regulated genes identified in most reported studies in Synechocystis under different perturbations are associated with the general stress response. We also find that a majority of genes in the CTR are coregulated with 25 regulatory genes. Some of these regulatory genes have been implicated in cellular responses to oxidative stress, suggesting that reactive oxygen species are involved in the regulation of the CTR. A Bayesian network, based on the regulation of various KEGG pathways determined from the expression patterns of their associated genes, has revealed new insights into the coordination between different cellular processes. CONCLUSION: We provide here the first integrative analysis of transcriptome data sets generated in a cyanobacterium. This compilation of data sets is a valuable resource to researchers for all cyanobacterial gene expression related queries. Importantly, our analysis provides a global description of transcriptional reprogramming under different perturbations and a basic framework to understand the strategies of cellular adaptations in Synechocystis.",
    "actual_venue": "Bmc Systems Biology"
  },
  {
    "abstract": "In service-oriented environments (e.g. Cloud Computing or Utility Computing), automated service discovery is crucial to enable self-organizing technical components that are able to discover and consume services autonomously. To this end, precise and robust service discovery algorithms are desirable. In this paper, we propose an approach combining both syntactic and semantic search to increase accuracy of discovery results. Evaluation in an SOA simulation environment shows promising improvements in contrast to traditional centralized discovery approaches, such as UDDI.",
    "actual_venue": "Apscc"
  },
  {
    "abstract": "The advancement in meaningful constraining models has resulted in increasingly useful quantitative information recovered from cardiac images. Nevertheless, single-source data used by most of these algorithms have put certain limits on the clinical completeness and relevance of the analysis results, especially for pathological cases where data fusion of multiple complementary sources is essential. As traditional image fusion strategies are typically performed at pixel level by fusing commensurate information of registered images through various mathematical operators, such approaches are not necessarily based on meaningful biological bases, particularly when the data are dissimilar in physical nature and spatiotemporal quantity. In this work, we present a physiological fusion framework for integrating information from different yet complementary sources. Using a cardiac physiome model as the central link, structural and functional data are naturally fused together for a more complete subject-specific information recovery. Experiments were performed on synthetic and real data to show the benefits and potential clinical applicability of our framework.",
    "actual_venue": "Miccai"
  },
  {
    "abstract": "Deep learning methods, with their good performance in semantic representation of different images, have been widely used for saliency detection. Recent saliency detection methods have applied deep learning to obtain high-level features and combined them with hand-crafted low-level features to estimate saliency in the images. However, it is difficult to find the relationship between high-level and low-level features, resulting in incomplete integration framework for saliency detection. In this paper, we novely propose a saliency detection model by integrating high-level and low-level features with joint probability estimation. Firstly, the high-level features from FCN-8S network are used to estimate the probability of each superpixel as foreground or background region. Secondly, low-level features are extracted from each superpixels and clustered via affinity propagation (AP) clustering. The distributions of vectors from different clusters are consequently utilized to calculate the conditional probability of each superpixel as salient object under different assumptions. Thirdly, the joint probability of each superpixel as salient object in foreground or background is computed to compose the saliency map of the whole image. To further improve the uniformity of saliency in the same object region, the structured random forest (SRF) method is used to detect the contour of the image and the saliency of superpixels in homogeneous regions are uniformly merged. The advantage of high-level features in representing semantic regions and that of low-level features in differentiating local details in the image are unified and restrained by the joint probability estimation in the proposed model. Experimental results demonstrate that the proposed method provide better saliency detection performance than the state-of-the-art methods on 5 public databases.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "As a classic high-entropy alloy system, CoCrFeNiMn is widely investigated. In the present work, we used ZrH2 powders and atomized CoCrFeNiMn powders as raw materials to prepare CoCrFeNiMnZrx (x = 0, 0.2, 0.5, 0.8, 1.0) alloys by mechanical alloying (MA), followed by spark plasma sintering (SPS). During the MA process, a small amount of Zr (x 0.5) can be completely dissolved into CoCrFeNiMn matrix, when the Zr content is above 0.5, the ZrH2 is excessive. After SPS, CoCrFeNiMn alloy is still as single face-centered cubic (FCC) solid solution, and CoCrFeNiMnZrx (x 0.2) alloys have two distinct microstructural domains, one is a single FCC phase without Zr, the other is a Zr-rich microstructure composed of FCC phase, B2 phase, Zr2Ni7, and sigma phase. The multi-phase microstructures can be attributed to the large lattice strain and negative enthalpy of mixing, caused by the addition of Zr. It is worth noting that two types of nanoprecipitates (body-centered cubic (BCC) phase and Zr2Ni7) are precipitated in the Zr-rich region. These can significantly increase the yield strength of the alloys.",
    "actual_venue": "Entropy"
  },
  {
    "abstract": "In this article, a novel voice activity detection (VAD) approach based on phoneme recognition using Gaussian Mixture Model based Hidden Markov Model (HMM/GMM) is proposed. Some sophisticated speech features such as high order statistics (HOS), harmonic structure information and Mel-frequency cepstral coefficients (MFCCs) are employed to represent each speech/non-speech segment. The main idea of this new method is regarding the non-speech as a new phoneme corresponding to the conventional phonemes in mandarin, and all of them are then trained under maximum likelihood principle with Baum-Welch algorithm using GMM/HMM model. The Viterbi decoding algorithm is finally used for searching the maximum likelihood of the observed signals. The proposed method shows a higher speech/non-speech detection accuracy over a wide range of SNR regimes compared with some existing VAD methods. We also propose a different method to demonstrate that the conventional speech enhancement method only with accurate VAD is not effective enough for automatic speech recognition (ASR) at low SNR regimes. © 2012 Bao and Zhu; licensee Springer.",
    "actual_venue": "Eurasip J Audio, Speech And Music Processing"
  },
  {
    "abstract": "A foot database comprising 3D foot shapes and footwear fitting reports of more than 300 participants is presented. It was primarily acquired to study footwear fitting, though it can also be used to analyse anatomical features of the foot. In fact, we present a technique for automatic detection of several foot anatomical landmarks, together with some empirical results.",
    "actual_venue": "Ibpria"
  },
  {
    "abstract": "A significant problem in reengineering large systems is adapting the user interface to a new environment. Often, drastic changes in the user interface are inevitable, as in migrating a text-based system to a workstation with graphical user interface capabilities. This experience report chronicles a study of user interface migration issues, examining and evaluating current tools and techniques. It also describes a case study under taken to explore the use of knowledge engineering to aid in migrating interfaces across platforms",
    "actual_venue": "Victoria, Bc"
  },
  {
    "abstract": "This paper concerns the problem of estimation of the location and intensity of reflections of a seismic wavelet. A recursive maximum a posteriori probability (MAP) algorithm is derived as an alternative to the maximum likelihood (ML) algorithm of Mendel and Kormylo. The MAP approach proposed here yields a suboptimal detector which is substantially different in details from the corresponding approximate ML detector of Mendel and Kormylo. Simulation studies are presented to show that the MAP detector performs as well as the ML detector and can yield comparable results with much less computational effort. A comparative study of both the MAP and ML detectors has been made via simulations which show some interesting differences in structure as well as performance.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "In this paper we introduce and investigate the notion of p-connectedness. As it turns out, this concept leads naturally to a unique tree representation for arbitrary graphs: the leaves of this tree are the p-connected components along with weak vertices, that is, vertices of the graph that belong to no p-connected component. We then show how to refine this decomposition to obtain a new decomposition that extends the well-known modular decomposition.",
    "actual_venue": "Siam J Discrete Math"
  },
  {
    "abstract": "Gait is a firsthand reflection of health condition. This belief has inspired recent research efforts to automate the analysis of pathological gait, in order to assist physicians in decision-making. However, most of these efforts rely on gait descriptions which are difficult to understand by humans, or on sensing technologies hardly available in ambulatory services. This paper proposes a number of semantic and normalized gait features computed from a single video acquired by a low-cost sensor. Far from being conventional spatio-temporal descriptors, features are aimed at quantifying gait impairment, such as gait asymmetry from several perspectives or falling risk. They were designed to be invariant to frame rate and image size, allowing cross-platform comparisons. Experiments were formulated in terms of two databases. A well-known general-purpose gait dataset is used to establish normal references for features, while a new database, introduced in this work, provides samples under eight different walking styles: one normal and seven impaired patterns. A number of statistical studies were carried out to prove the sensitivity of features at measuring the expected pathologies, providing enough evidence about their accuracy. Graphical Abstract Graphical abstract reflecting main contributions of the manuscript: at the top, a robust, semantic and easy-to-interpret feature set to describe impaired gait patterns; at the bottom, a new dataset consisting of video-recordings of a number of volunteers simulating different patterns of pathological gait, where features were statistically assessed.",
    "actual_venue": "Med Biol Engineering And Computing"
  },
  {
    "abstract": "This paper deals with the problem of robust absolute stability analysis for nonlinear Lur'e control systems in the presence of system parameter variations. The well known Popov criterion for absolute stability is used in order to characterize the boundary of the region of absolute stability in the parameter plane when the coefficients of the transfer function of the linear plant are polynomial functions of the uncertain parameters. For a scalar parameter, a method is given to determine the maximal interval of variation around a fixed nominal value preserving absolute stability. This result is also used to derive a technique for checking absolute stability of Lur'e systems with parameters in given planar uncertainty sets. Numerical examples showing the application of the method are reported.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "iSCSI is one of the most popular protocols among the storage area network (SAN) working on an IP network. iSCSI is a standard to encapsulate a SCSI command into a TCP/IP packet, thus we can make an access to a storage system with commoditized IP devices only. In this paper, iSCSI sequential write access for a remote backup system is focused. It is well known that iSCSI experiences performance degradation under a high latency environment. The storage access by iSCSI has multiple hierarchical protocols. Due to this complicated structure, it is very difficult to identify the performance bottleneck that causes the degradation. Therefore, system tools which can analyze the complicated layered structure are required. We have developed system tools that enable us to monitor the hierarchical protocols. As a result, we have identified the cause of the iSCSI performance degradation problem. We then fixed the problem and confirmed that one can obtain the performance close to the theoretical limit.",
    "actual_venue": "Icuimc"
  },
  {
    "abstract": "This paper reviews some works on finite automaton one-key cryptosystems and related topics such as autonomous finite automata and Latin arrays.",
    "actual_venue": "Fast Software Encryption"
  },
  {
    "abstract": "This paper describes an efficient procedure for resource-constrained project scheduling problems. It starts with a simulated annealing technique to find a base schedule, and then improves the result by a time-windowing process. Every time-window, which is part of the base schedule, is a basis for a small subproject with two sets of specific constraints. Associated subprojects with time-windows are scheduled to optimality and based on their results the base schedule is updated. The overlapping feature of time-windows makes the displacement of an activity possible within the range of the entire project. The process of creating time-windows, scheduling their associated subprojects to optimality and improving the base schedule is controlled by a feed-back based mechanism that realises a trade-off between computational effort and the improvement made. The computational results indicate that the procedure is promising and yields better solutions than several heuristic algorithms presented in the literature.",
    "actual_venue": "Or Spectrum"
  },
  {
    "abstract": "Data centers require many low-level network services to implement high-level applications. Key-Value Store (KVS) is a critical service that associates values with keys and allows machines to share these associations over a network. Mostexisting KVS systems run in software and scale out by running parallel processes on multiple microprocessor cores to increase throughput. In this paper, we take an alternate approach by implementing an ultra-low-latency KVS in Field Programmable Gate Array (FPGA) logic. As with a software-based KVS, lookup transactions are sent over Ethernet to the machine that stores the value associated with that key. We find that the implementation in logic, however, scales up to provide much higher search throughput with much lower latency and power consumption than other implementations in software. As with other KVS systems like redis, memcached, and Aerospike, high-level applications store, replace, delete, and search keys using a standard Application Programming Interface (API). Our API hashes long keys into statistically unique identifiers and maps variable-length messages into a finite set of fixed-size values. These keys and values are then formatted into a compact, binary Open Compute Storage Message (OCSM) format and transported in User Data Protocol (UDP)/Internet Protocol (IP) Ethernet frames over 10 Gigabit/second Ethernet (10GE) or faster network links. When transporting OCSM over 10 GE and by processing theperforming the key/value search in FPGA logic, a fiber-to-fiber lookup latency of under a half microsecond (0.5 μS) was achieved. Using four 10 GE interfaces, a single instance of the FPGA-accelerated search core achieves a throughput of 150 Million Searches Per Second1 (MSPS). Compared to traditional software, the FPGA KVS is 88 times faster while using 21x less power than socket I/O. Compared to software optimized with DPDK kernel bypass, the FPGA KVS was measured to process messages 10x faster while using 13x less energy.",
    "actual_venue": "Symposium On High-Performance Interconnects"
  },
  {
    "abstract": "The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16 bit fixed-point data. The scalability of Imagine's programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half and explores the scalability of the Imagine architecture in the second half.",
    "actual_venue": "Iccd"
  },
  {
    "abstract": "Recent development of hand-held plenoptic cameras has brought light field acquisition into many practical and low-cost imaging applications. We address a crucial challenge in light field data processing: dense depth estimation of 3D scenes captured by camera arrays or plenoptic cameras. We first propose a method for construction of light field scale-depth spaces, by convolving a given light field with a special kernel adapted to the light field structure. We detect local extrema in such scale-depth spaces, which indicate the regions of constant depth, and convert them to dense depth maps after solving occlusion conflicts in a consistent way across all views. Due to the multi-scale characterization of objects in proposed representations, our method provides depth estimates for both uniform and textured regions, where uniform regions with large spatial extent are captured at coarser scales and textured regions are found at finer scales. Experimental results on the HCI (Heidelberg Collaboratory for Image Processing) light field benchmark show that our method gives state of the art depth accuracy. We also show results on plenoptic images from the RAYTRIX camera and our plenoptic camera prototype.",
    "actual_venue": "Computer Vision And Pattern Recognition Workshops"
  },
  {
    "abstract": "In this paper, we study the class scheduling problem at the training center of Continental Airlines. When pilots get new assignments, they must be retrained for up to eight consecutive weeks. During that time, they are removed from the roster, and thus impose a significant cost on the airlines. We formulate the problem with the objective of minimizing the total weighted length of all classes. Solutions are obtained with a branch-and-bound algorithm and a family of heuristics based on the idea of a rolling horizon. A series of computational experiments is performed to evaluate the algorithms. The results indicate that it is possible to obtain near-optimal solutions within acceptable time limits. The algorithms have been implemented and are now in use at Continental.",
    "actual_venue": "Operations Research"
  },
  {
    "abstract": "This paper describes, in general terms, methods for interpreting the output from simulation models. Statistical methods are described for several different purposes, and related problems like design, comparison, variance reduction, sensitivity estimation, metamodeling, and optimization are mentioned. The main point is to call attention to the challenges and opportunities in using simulation models carefully and effectively.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "Globalization of semiconductor manufacturing and related activities has led to several security issues like counterfeiting, IP infringement and cloning etc. Counterfeiting not only affects the business and reputation of semiconductor vendor, it will affect the reliability of critical applications. Internet of Things (IoT's) is an emerging application area for semiconductors in which security is a prime concern. Identity or authentication of a device in the network of millions of devices in IoT's is very important. In this paper, we present Secure Split Test with functional testing capability (SSTF) scheme to mitigate the counterfeits coming out from untrusted foundries. The SSTF is suitable for low cost/low end devices. To address the identity management issue in IoT's and counterfeiting of ICs, a Physical Unclonable Function based SSTF (PUF-SSTF) is presented. PUF-SSTF is suitable for ICs targeted to use in smart phones and IoT ‘s. PUF-SSTF is a security solution to address: mitigating the counterfeit ICs coming out from untrusted foundries, identity management in IoT's and licensing the device features, which will benefit the fabless semiconductor vendors for licensing and entitlement management. The proposed SSTF and PUF-SSTF techniques are implemented in both ASIC and FPGA and security analysis is performed. The security analysis results are in par with earlier secure split test techniques like Connecticut Secure Split Test (CSST). The proposed techniques will create the comprehensive secure supply chain solution, which will benefit fabless semiconductor vendor and end-user.",
    "actual_venue": "Integration"
  },
  {
    "abstract": "In 1977, Attouch established a relationship between the Mosco epiconvergence of a sequence of convex functions and the graph convergence of the associated sequence of subdifferentials, which has been found to have many important applications in optimization. In 1995, Levy, Poliquin, and Thibault proved Attouch-type theorems for a sequence of primal-lower-nice (not necessarily convex) functions with variational behavior of \"order two.\" In this paper, using the proximal analysis technique, we provide convergence results on the associated sequence of normal cones of a Mosco-convergent sequence of general closed sets. Under the uniform subsmoothness assumption, some sharper convergence results are established. As applications, we establish Attouch-type theorems for a sequence of more general quasi-subsmooth functions with variational behavior of \"order one.\"",
    "actual_venue": "Siam Journal On Optimization"
  },
  {
    "abstract": "Context: The software's operational profile is a representation of how users use the software, in which the most commonly-used parts of the software are identified. There is evidence of a possible mismatch between the tested parts of the software and the operational profile of the software. Objective: This paper aims to present a doctoral research that investigates the use of software operational profile as a resource to improve software quality from the point of view of users. As one of the expected contributions, the research proposes a strategy in which the operational profile of the software is used to: a) evaluate and adapt a test suite based on the operational profile of the software; b) as a prioritization criterion that allows, given a set of defects, to identify the defects that have the greatest impact on the operation of the software by the users and, thus, to consider this impact in the pricing of the defects. Method: To present the research, this paper describes: a) the problem addressed and the purpose of the research; b) the results obtained by the activities carried out to evaluate the feasibility of the research (systematic mapping of the literature, systematic review of the literature and exploratory study); c) the hypotheses to be investigated; d) expected contributions; e) the current state of the research. Results: The results obtained by the activities carried out were favorable to the accomplishment and continuity of the research, evidencing the originality of the research and the proposed strategy. Conclusion: Once the feasibility of the research and the progress that has been obtained in its execution are verified, the results to be obtained are promising and, consequently, the expected contributions must be obtained.",
    "actual_venue": "Ieee Conference On Software Testing, Validation And Verification"
  },
  {
    "abstract": "Evolutionary multiobjective optimization (EMO) is an active research area in the field of evolutionary computation. EMO algorithms are designed to find a non-dominated solution set that approximates the entire Pareto front of a multiobjective optimization ...",
    "actual_venue": "SMC"
  },
  {
    "abstract": "Editors'note: Reconfigurable platforms can be very effective for lowering production costs because they allow the reuse of architectural resources across a variety of applications. In this article, the authors describe a methodology and a set of tools that allow extensive design exploration for hardware-software codesign, with the goal of improving the overall utilization of reconfigurable multimedia platforms.—Radu Marculescu, Carnegie Mellon University and Petru Eles, Linköping University",
    "actual_venue": "Ieee Design And Test Of Computers"
  },
  {
    "abstract": "High-Level Synthesis (HLS) for FPGAs is attracting popularity and is increasingly used to handle complex systems with multiple integrated components. To increase performance and efficiency, HLS flows now adopt several advanced optimization techniques. Aggressive optimizations and system level integration can cause the introduction of bugs that are only observable on-chip. Debugging support for circuits generated with HLS is receiving a considerable attention. Among the data that can be collected on chip for debugging, one of the most important is the state of the Finite State Machines (FSM) controlling the components of the circuit. However, this usually requires a large amount of memory to trace the behavior during the execution. This work proposes an approach that takes advantage of the HLS information and of the structure of the FSM to compress control flow traces and to integrate optimized components for on-chip debugging. The generated checkers analyze the FSM execution on-fly, automatically notifying when a bug is detected, localizing it and providing data about its cause. The traces are compressed using a software profiling technique, called Efficient Path Profiling (EPP), adapted for the debugging of hardware accelerators generated with HLS. With this technique, the size of the memory used to store control flow traces can be reduced up to 2 orders of magnitude, compared to state-of-the-art.",
    "actual_venue": "Acm Trans Embedded Comput Syst"
  },
  {
    "abstract": "•An effective method for removal of mixed single-point and granular impulse noise.•A new method for classification of noisy pixels based on super pixel segmentation.•A selected recursive vector median filter with adaptive window sizes.•An efficient algorithm for computing the circularity of a region.",
    "actual_venue": "Journal Of Visual Communication And Image Representation"
  },
  {
    "abstract": "Almost all networks in real world evolve over time, and analysis of these temporal changes may help in understanding or explanation of some properties or processes of a network. This paper presents GA-TVRC, a novel Relational Time Varying Classifier which uses Genetic Algorithms to extract temporal information. GA-TVRC uses Evolutionary Strategies to optimize the influence of each previous time period on classification of new nodes. A Relational Bayesian Classifier (RBC) that is proposed by Neville et.al. [3] is utilized to compute the fitness function. The performance of GA-TVRC is compared with both the RBC, which ignores the time effect and the time varying relational classifier (TVRC) that is proposed by Sharan and Neville [20]. TVRC improves the RBC by taking the time effect into account using different predetermined weights. According to the experiments on two real world datasets, GA-TVRC extracts time effect better than the previous methods and improves the classification performance by up to 5% compared to TVRC and up to 10% compared to RBC.",
    "actual_venue": "Mldm"
  },
  {
    "abstract": "Health information retrieval and YouTube can be used as powerful tools to improve user's health knowledge. However, YouTube videos must be carefully analysed in order to avoid misleading, inaccurate, obsolete and incorrect health content. We present an approach for re-ranking health videos obtained from YouTube, called Domain-based ranking. Our system automatically identifies videos coming from trusted sources (channels), such as hospitals and health organizations, and re-ranks YouTube results so that such videos are presented first in the ranking list. Video and channel metadata are used to automatically determine if a video is provided by a trusted source. The approach is tested and results show that the amount of relevant and reliable videos ranked within top-10 increase when using Domain-based ranking, compared with the original YouTube ranking.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "The recent technological overhangs have focused on the democratization of wireless networks and the miniaturization of communication devices. In this context, Ubiquitous Computing is a recent paradigm whose objective is to allow users to access data, and make information available anywhere and at any time. In other terms, Pervasive Information Systems PIS constitute an emerging class of Information Systems where Information Technology is gradually embedded in the physical environment, capable of accommodating user needs and wants when desired. PIS differ from Desktop Information Systems DIS in that they encompass a complex, dynamic environment composed of multiple artifacts instead of Personal Computers only, capable of perceiving contextual information instead of simple user input, and supporting mobility instead of stationary services. In this paper, as an initial step, the authors present PIS novel characteristics compared to traditional desktop information systems; the authors explore this domain by offering a list of challenges and concepts of ubiquitous computing that form the core elements of a pervasive environment. As a result of this work, a generic architecture for intelligent environment has been created. Based on various and related works concerning models and designs. This framework can be used to design any PIS instance.",
    "actual_venue": "Ijapuc"
  },
  {
    "abstract": "This paper outlines the grand challenges in global sustainability research and the objectives of the FP7 Future Internet PPP program within the Digital Agenda for Europe. Large user communities are generating significant amounts of valuable environmental observations at local and regional scales using the devices and services of the Future Internet. These communities' environmental observations represent a wealth of information which is currently hardly used or used only in isolation and therefore in need of integration with other information sources. Indeed, this very integration will lead to a paradigm shift from a mere Sensor Web to an Observation Web with semantically enriched content emanating from sensors, environmental simulations and citizens. The paper also describes the research challenges to realize the Observation Web and the associated environmental enablers for the Future Internet. Such an environmental enabler could for instance be an electronic sensing device, a web-service application, or even a social networking group affording or facilitating the capability of the Future Internet applications to consume, produce, and use environmental observations in cross-domain applications. The term \"envirofied\". Future Internet is coined to describe this overall target that forms a cornerstone of work in the Environmental Usage Area within the Future Internet PPP program. Relevant trends described in the paper are the usage of ubiquitous sensors (anywhere), the provision and generation of information by citizens, and the convergence of real and virtual realities to convey understanding of environmental observations. The paper addresses the technical challenges in the Environmental Usage Area and the need for designing multi-style service oriented architecture. Key topics are the mapping of requirements to capabilities, providing scalability and robustness with implementing context aware information retrieval. Another essential research topic is handling data fusion and model based computation, and the related propagation of information uncertainty. Approaches to security, standardization and harmonization, all essential for sustainable solutions, are summarized from the perspective of the Environmental Usage Area. The paper concludes with an overview of emerging, high impact applications in the environmental areas concerning land ecosystems (biodiversity), air quality (atmospheric conditions) and water ecosystems (marine asset management).",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "Extensive research has been conducted to estimate and analyze head poses for various applications. Most existing methods tend to detect facial features and locate landmarks on a face for pose estimation. However, the sensitivity to occlusion of some face parts with key features and uncontrolled illumination of face images make the facial feature detection vulnerable. In this paper, we propose a framework for pose estimation without the need of face features or landmarks detection. Specifically, we formulate the pose estimation as a linear regression applied to the pose space. This method is based on the assumption that pose space cannot be linearly approximated in the pose subspace. The experimental results strongly support this assumption. In cases where the database does not obtain various poses in the intraclass, we propose to generate those poses through a 3D reconstruction and projection method. The experiment conducted on the CMU MultiPIE and IMM Face database has shown the effectiveness of the proposed method.",
    "actual_venue": "Ijcnn"
  },
  {
    "abstract": "In order to clear regulations on electro-magnetic interference (EMI), the serial ATA (SATA) standard recommends spread spectrum clocking (SSC) to reduce the significant peak in the spectrum of system clocks. Although testing of high-speed serial interface primarily addressed using digital testers, a SSC applied waveform is not an easy task for digital resources to generate and analyze, while analog resources in mixed signal testers can easily take care of such signals. The purpose of this work is to establish methodologies to generate a SSC applied data stream by using an arbitrary waveform generator (AWG) and to analyze the SSC frequency trend by using a waveform sampler. The AWG generates 750MHz clock, align primitives and pseudo random binary sequence (PRBS) stream with SSC. The sampler analyzes the clock and the align primitives reconstructing the SSC frequency trend.",
    "actual_venue": "Itc: International Test Conference"
  },
  {
    "abstract": "In wireless sensor networks, a geographic routing requests source nodes (sensor nodes) to know location information of destination nodes (sinks) to send their data. To address this issue, many sink location service schemes have been proposed in the literature. They are designed to support only one communication mode such as unicast, anycast, manycast, or multicast communication modes. However, because an application can have one or more communication modes or a sensor network can operate several applications with each communication mode, it is necessary for a scheme to support all communication modes. Hence, we propose a general sink location service scheme for supporting all communication modes. The proposed scheme modifies the basic idea of the existing one proposed for unicast communication mode and extends it for supporting all communication modes. Simulation results show that the proposed scheme is superior to other sink location service schemes when supporting all communication modes.",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "A graph is Hilbertian if for any three vertices u , v and w , the interval l ( u , v ) contains a unique nearest vertex p from w . We show that a graph is median if and only if it is Hilbertian.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "In this letter we examine the interrelation between Noether symmetries, master symmetries and recursion operators for the Toda lattice. The topics include invariants, higher Poisson brackets and the various relations they satisfy. For the case of two degrees of freedom we prove that the Toda lattice is super-integrable.",
    "actual_venue": "Applied Mathematics Letters"
  },
  {
    "abstract": "Adaptation of parameters and operators represents one of the recent most important and promising areas of research in evolutionary computations; it is a form of designing self-configuring algorithms that acclimatize to suit the problem in hand. Here, our interests are on a recent breed of hybrid evolutionary algorithms typically known as adaptive memetic algorithms (MAs). One unique feature of adaptive MAs is the choice of local search methods or memes and recent studies have shown that this choice significantly affects the performances of problem searches. In this paper, we present a classification of memes adaptation in adaptive MAs on the basis of the mechanism used and the level of historical knowledge on the memes employed. Then the asymptotic convergence properties of the adaptive MAs considered are analyzed according to the classification. Subsequently, empirical studies on representatives of adaptive MAs for different type-level meme adaptations using continuous benchmark problems indicate that global-level adaptive MAs exhibit better search performances. Finally we conclude with some promising research directions in the area.",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "E-commerce sites usually leverage taxonomies for better organizing products. The fine-grained categories, regarding the leaf categories in taxonomies, are defined by the most descriptive and specific words of products. Fine-grained product categorization remains challenging, due to blurred concepts of fine grained categories (i.e. multiple equivalent or synonymous categories), instable category vocabulary (i.e. the emerging new products and the evolving language habits), and lack of labelled data. To address these issues, we proposes a novel Neural Product Categorization model---NPC to identify fine-grained categories from the product content. NPC is equipped with a character-level convolutional embedding layer to learn the compositional word representations, and a spiral residual layer to extract the word context annotations capturing complex long range dependencies and structural information. To perform categorization beyond predefined categories, NPC categorizes a product by jointly recognizing categories from the product content and predicting categories from predefined category vocabularies. Furthermore, to avoid extensive human labors, NPC is able to adapt to weak labels, generated by mining the search logs, where the customers' behaviors naturally connect products with categories. Extensive experiments performed on a real e-commerce platform datasets illustrate the effectiveness of the proposed models.",
    "actual_venue": "Cikm Proceedings Of The Second International Conference On Information And Knowledge Management"
  },
  {
    "abstract": "Natural language processing techniques are dependent upon punctuation to work well. When their input is taken from speech recognition, it is necessary to reconstruct the punctuation; in particular sentence boundaries. We define a range of features from low level acoustics to those with high level lexical semantics, including deep and recurrent models; these in turn are representative of a broad range of approaches used by previous authors for punctuation prediction. We combine the features using a gradient boosting machine that is also capable of indicating the relative importance of each feature. In an empirical study, we show that features from different semantic levels are in fact complementary, that combining statistical and deep learning methods yields better prediction results, and that generalization across different speaking styles is difficult to achieve without adaptation. Our best model achieves an F-Measure of 82.8 on a challenging broadcast news dataset.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "There is a growing interest in designing systems for sharing experience through bodily interaction. To explore this design space, we built a probe system we named the Lega. In our 2-month-long research design process, we noted that the users’ attention was set on their own reflective experience, rather than attending to the person(s) with which they were sharing their experience. To explain these findings, we present an inductive analysis of the data through a phenomenological lens to pinpoint what causes such behavior. Our analysis extends our understanding of how to design for social embodied interaction, pointing to how we need to embrace the tension between self-reflection and shared experience, making inward listening and social expression visible acts, accessible to social construction and understanding. It entails experiencing our embodied self as others experience us in order to build a dialogue.",
    "actual_venue": "Acm Trans Comput-Hum Interact"
  },
  {
    "abstract": "The paper presents a parallel implementation of a variant of quantum inspired genetic algorithm (QIGA) for the problem of community structure detection in complex networks using NVIDIA® Compute Unified Device Architecture (CUDA®) technology. The paper explores feasibility of the approach in the domain of complex networks. The approach does not require any knowledge of the number of communities beforehand and works well for both directed and undirected networks. Experiments on benchmark networks show that the method is able to successfully reveal community structure with high modularity.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "This paper describes the design and implementation of a data mining system called SNODMAL Stream based novel class detection for malware for malware detection. SNODMAL extends our data mining system called SNOD Stream-based Novel Class Detection for detecting malware. SNOD is a powerful system as it can detect novel classes. We also describe the design of SNODMAL++ which is an extended version of SNODMAL.",
    "actual_venue": "J Integrated Design And Process Science"
  },
  {
    "abstract": "This paper is a contribution to probabilistic data mining and pattern recognition. A DTW-based statistical model is proposed to explore the subspace structures of speaker feature space for feature evaluation, dimension reduction and inter-class information discovery in pattern space. We demonstrate its usefulness in isolated digits speaker identification, and the performance of the statistical model is compared with standard DTW recognition rate in the experiment. We argue that the probability model can be taken as data mining tools.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "This paper applies score and feature normalization techniques to parts-based Gaussian mixture model (GMM) face authentication. In particular, we propose to utilize techniques that are well established in state-of-the-art speaker authentication, and apply them to the face authentication task. For score normalization, T-, Z- and ZT-norm techniques are evaluated. For feature normalization, we propose a generalization of feature warping to 2D images, which is applied to discrete cosine transform (DCT) features prior to modeling. Evaluation is performed on a range of challenging databases relevant to forensics and security, including surveillance and access control scenarios. The normalization techniques are shown to generalize well to the face authentication task, resulting in relative improvements in half total error rate (HTER) of between 17% and 62%.",
    "actual_venue": "Ieee Transactions On Information Forensics And Security"
  },
  {
    "abstract": "Purpose - The purpose of this paper is to use systemic thinking to explain and predict the cost of logistics outsourcing, and to devise policies to minimize the cost of risk. Design/methodology/approach - A method of system dynamics is adopted to capture the dynamic interaction of logistics outsourcing systems and to analyze the impact of some factors in the system on policy decisions over a long-term horizon. Findings - This paper illustrates the internal mechanism of the logistics outsourcing cost of risk systems by virtue of system dynamic principles, to develop a system dynamics model, and to give a quite detailed description of how the model could work. Practical implications - The results of the simulation analysis provide useful information for logistics outsourcing risk managers. Originality/value - This paper contributes to the discussion on the use of system dynamics for studying logistics outsourcing cost of risk.",
    "actual_venue": "Kybernetes"
  },
  {
    "abstract": "A distinguishing characteristic of a software-defined network is separation of the network's control plane from its data plane. Especially when the granularity of control is an individual network flow, such separation entails frequent communications between these two planes. This communication pattern demands the same level of resilience from the control plane as that from the data plane, and thus calls into question the conventional out-of-band control network design as used in many existing SDN systems. Peregrine is an Ethernet-based software-defined network that was originally designed as the internal network of a container computer, and unifies storage access, inter-server communication, and network control into a single network comprising only commodity off-the-shelf Ethernet switches. To fully utilize all available physical network links, Peregrine treats the physical network as an explicitly routed mesh and equalizes the loads of its links using a global load-balancing routing algorithm running on a centralized controller. The in-band control architecture of Peregrine leads to two issues: (1) how to evolve a Peregrine network from its initial bootstrapping mode to the explicit routing mode at run time, and (2) how to support fast fail-over for physical failures that break both the control and data plane. This paper describes how Peregrine addresses these two issues, and shows its effectiveness with performance measurements collected from a fully operational test-bed.",
    "actual_venue": "Systor"
  },
  {
    "abstract": "Groups of collaborative agents within organizations need to create group awareness in order to act as a single entity. The notion of collective belief, which has been used extensively to cope with group awareness, is not appropriate in organized settings where group members accept that certain states hold based on shared practices, even if some members of the group do not believe that these states hold. This paper distinguishes between individual beliefs and group acceptances and introduces state recognition recipes that drive groups within organizations to create common awareness.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "We address the model checking problem of omega-regular linear-time properties for shared memory concurrent programs modeled as multi-pushdown systems. We consider here boolean programs with a finite number of threads and recursive procedures. It is well-known that the model checking problem is undecidable for this class of programs. In this paper, we investigate the decidability and the complexity of this problem under the assumption of scope-boundedness defined recently by La Torre and Napoli in [24]. A computation is scope-bounded if each pair of call and return events of a procedure executed by some thread must be separated by a bounded number of context-switches of that thread. The concept of scope-bounding generalizes the one of context-bounding [31] since it allows an unbounded number of context switches. Moreover, while context-bounding is adequate for reasoning about safety properties, scope-bounding is more suitable for reasoning about liveness properties that must be checked over infinite computations. It has been shown in [24] that the reachability problem for multi-pushdown systems under scope-bounding is PSPACE-complete. We prove in this paper that model-checking linear-time properties under scope-bounding is also decidable and is EXPTIME-complete.",
    "actual_venue": "Atva"
  },
  {
    "abstract": "In this paper we study linear-programming based approaches to the maximum matching problem in the semi-streaming model. In this model edges are presented sequentially, possibly in an adversarial order, and we are only allowed to use a small space. The allowed space is near linear in the number of vertices (and sublinear in the number of edges) of the input graph. The semi-streaming model is relevant in the context of processing of very large graphs. In recent years, there have been several new and exciting results in the semi-streaming model. However broad techniques such as linear programming have not been adapted to this model. In this paper we present several techniques to adapt and optimize linear-programming based approaches in the semi-streaming model. We use the maximum matching problem as a foil to demonstrate the effectiveness of adapting such tools in this model. As a consequence we improve almost all previous results on the semi-streaming maximum matching problem. We also prove new results on interesting variants.",
    "actual_venue": "International Colloquium On Automata Languages And Programming"
  },
  {
    "abstract": "The management policy of an M/G/1 queue with a single removable and non-reliable server is considered. The decision-maker can turn the single server on at any arrival epoch or off at any service completion. It is assumed that the server breaks down according to a Poisson process and the repair time has a general distribution. Arrivals form a Poisson process and service times are generally distributed. In this paper, we consider a practical problem applying such a model. We use the analytic results of the queueing model and apply an efficient Matlab program to calculate the optimal threshold of management policy and some system characteristics, Analytical results for sensitivity analysis are obtained. We carry out extensive numerical computations for illustration purposes. An application example is presented to display how the Matlab program could be used. The research is useful to the analyst for making reliable decisions to manage the referred queueing system.",
    "actual_venue": "Computers And Industrial Engineering"
  },
  {
    "abstract": "Two RNases, Dicer and Argonaute, are at the heart of the RNA interference (RNAi) molecular machinery responsible for gene silencing. Both RNases contain multiple domains, most of which have been characterized or have functions that can be predicted based on sequence comparisons. However, Dicers of higher eukaryotes contain the domain known as DUF283 which at present has no assigned role. Using sensitive profile-profile comparisons, we detected a divergent double-stranded RNA-binding domain coinciding with the DUF283 of Dicer. This finding has potential implications regarding the mechanistic role of Dicer in RNAi.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "For wheel-based humanoid robot, the dynamic stability is also an important question just like for biped robot. This paper unites the new recursive Newton-Euler method for dynamic modeling and the ZMP concept and then obtains the ZMP model of the robot. The merit of this model is that it is a function of the position, velocity and acceleration of joints in joint space and easier to realize realtime control due to less calculation. Calculation and simulation show that the waist motion takes the main effect on the dynamic stability of the robot. Using the compensation of motion of the waist, the robot can realize dynamic motion. This paper also presents the concepts of the valid stable region and stable degree for the wheel-based robot.",
    "actual_venue": "Robio"
  },
  {
    "abstract": "This paper presents a computational model addressing behavioral learning and planning with a fully neural approach. The prefrontal functionality that is modeled is the ability to schedule elementary action schemes to reach behavioral goals. The use of robust context detection is discussed, as well as relations to biological views of the prefrontal cortex.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "We transfer here basic univariate Lyapunov inequalities to the multivariate setting of a shell by using the polar method.",
    "actual_venue": "Applied Mathematics Letters"
  },
  {
    "abstract": "Summary  The term \"individual motor car traffic\" underlines the independent, \"individual\" aspect of mobility. Cars allow their users\n to decide about a journey's modalities independently of the local circumstances. Competitive electric cars must also ensure\n this. Besides, they have to match for conventional cars in other purchase relevant features such as total costs of ownership,\n convenience or image.",
    "actual_venue": "Elektrotechnik Und Informationstechnik"
  },
  {
    "abstract": "This paper reports the result of a comparison between reduced instruction set computing and the transport triggered architecture. Because of the simplicity and efficiency of the transport triggered architecture, its processor requires less execution cycles compared to the OpenRisc processor. This paper also presents a case study about designing an Architecture Definition File for a transport triggered architecture-based design tool, and it depicts how the Architecture Definition File structures are responsible for implementing high-speed design. In a custom Architecture Definition File, a new function unit is designed to improve processor performance, and it shows that the cycle count required to implement the Cyclic Redundancy Check algorithm drops to 7 executions from 5031.",
    "actual_venue": "Ieice Electronics Express"
  },
  {
    "abstract": "Abstract   The accurate characterization of the diffusion process in tissue using diffusion MRI is greatly challenged by the presence of artefacts. Subject motion causes not only spatial misalignments between diffusion weighted images, but often also slicewise signal intensity errors. Voxelwise robust model estimation is commonly used to exclude intensity errors as outliers. Slicewise outliers, however, become distributed over multiple adjacent slices after image registration and transformation. This challenges outlier detection with voxelwise procedures due to partial volume effects. Detecting the outlier slices before any transformations are applied to diffusion weighted images is therefore required. In this work, we present i) an automated tool coined SOLID for slicewise outlier detection prior to geometrical image transformation, and ii) a framework to naturally interpret data uncertainty information from SOLID and include it as such in model estimators. SOLID uses a straightforward intensity metric, is independent of the choice of the diffusion MRI model, and can handle datasets with a few or irregularly distributed gradient directions. The SOLID-informed estimation framework prevents the need to completely reject diffusion weighted images or individual voxel measurements by downweighting measurements with their degree of uncertainty, thereby supporting convergence and well-conditioning of iterative estimation algorithms. In comprehensive simulation experiments, SOLID detects outliers with a high sensitivity and specificity, and can achieve higher or at least similar sensitivity and specificity compared to other tools that are based on more complex and time-consuming procedures for the scenarios investigated. SOLID was further validated on data from 54 neonatal subjects which were visually inspected for outlier slices with the interactive tool developed as part of this study, showing its potential to quickly highlight problematic volumes and slices in large population studies. The informed model estimation framework was evaluated both in simulations and in vivo human data.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Most data mining systems follow a data flow and toolbox paradigm. While this modular approach delivers ultimate flexibility, it gives the user almost no guidance on the issue of choosing an efficient combination of algorithms in the current problem context. In the field of Software Engineering the Pattern Based development process has empirically proven its high potential. Patterns provide a broad and generic framework for the solution process in its entirety and are based on equally broad characteristics of the problem. Details of the individual steps are filled in at later stages. Basic research on pattern based thinking has provided us with a list of generally applicable and proven patterns. User interaction in a pattern based approach to data mining will be divided into two steps: (1) choosing a pattern from a generic list based an a handful of characteristics of the problem and later (2) filling in data mining algorithms for the subtasks.",
    "actual_venue": "Studies In Classification Data Analysis And Knowledge Organization"
  },
  {
    "abstract": "Peptide-Mass Fingerprinting (PMF) encompasses a number of techniques for protein characterization which have as a first step the cleaving of target proteins by chemical or enzymatic reagents. Software systems exist which perform similar analyses. However, this is the first study which examines theoretically the effectiveness of the particular reagents for PMF. In this study, the task of PMF was to identify every sequence in a non-redundant protein database, digestion with theoretical proteases. experiments, some conclusions are drawn about the characteristics of better reagents and the experimental conditions which are more likely to be useful for PMF. The need for strongly non-redundant databases is also highlighted.",
    "actual_venue": "Ismb"
  },
  {
    "abstract": "This study examines 9,861 public firms to investigate the current adoption status of two popular social media platforms (FacebookTM and TwitterTM) and their application in corporate disclosure. The investigation is based on the framework defined by Meek, Roberts, and Gray (1995) and on variations in platform, industry, firm size, time, and intensity (i.e., accounts owned, messages released, and user interaction). The results show that 49 percent of the firms have adopted one platform, and 30 percent have adopted both platforms. However, the numbers of new adopters on both platforms have been decreasing continuously since 2010, even though more than half of the firms are yet to adopt either platform. The results also show that 7.06 percent and 3.45 percent of Facebook and Twitter messages, respectively, are related to corporate disclosures. On average, users respond more quickly to disclosures released on Twitter (13 minutes) than on Facebook (25 minutes), whereas disclosures on Facebook have longer user engagement (427 minutes) than those on Twitter (10 minutes).",
    "actual_venue": "Journal Of Information Systems"
  },
  {
    "abstract": "Successful Enterprise Resource Planning (ERP) implementation depends upon various factors known as critical success factors (CSFs). This study developed a system dynamics model of ERP implementation based on CSFs to discuss ERP implementation complexities, which identifies the effect of CSF interrelations on different aspects of ERP project failure. Based on the model hypothesis, CSF interrelations include many causal loop dependencies. Some of these causal loops are called mortal loops, because they may cause the failure of risk reduction efforts to a more severe failure in effect of lack of system thinking on CSFs interrelations. This study discusses how system thinking works as a leverage point for overcoming ERP implementation challenges.",
    "actual_venue": "Systems"
  },
  {
    "abstract": "OTIS-based architectures appear to have the potential to be an interesting option for future generations of multiprocessing systems. In this paper, we propose a new adaptive unicast routing algorithm and four software-based (unicast-based) broadcast algorithms for the wormhole switched OTIS hypercube. We then present an empirical performance evaluation of these algorithms in OTIS-hypercube for different topologies, message length and traffic loads.",
    "actual_venue": "Ispa"
  },
  {
    "abstract": "An image watermarking scheme in the 2D DCT domain is proposed by exploring the advantages of using Zernike moments. Zernike transform has been used in image processing applications such as image recognition, authentication, protection, etc. Here, we propose to use the Zernike moments of the DCT transform to provide an efficient watermarking method. Particularly, the novelty of the proposed approach relies on the method for selection of features that will enable both preserving the image quality and robustness to attacks. Also, a criterion for selection of image blocks suitable for watermarking is given. It is based on the  -norm of Zernike moments. The efficiency of the proposed watermarking algorithm is proved on several examples considering different types of attacks (compression, noise, filtering, geometrical attacks).",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "An efficient three-dimensional unstructured Euler solver is parallelized on a CRAY Y-MP C90 shared-memory computer and on an Intel Touchstone Delta distributed-memory computer. This paper relates the experiences gained and describes the software tools and hardware used in this study. Performance comparisons between the two differing architectures are made.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "With increasing application complexity and improvements in process technology, Chip MultiProcessors (CMPs) with tens to hundreds of cores on a chip are becoming a reality. Networks-on-Chip (NoCs) have emerged as scalable communication fabrics that can support high bandwidths for these massively parallel multicore systems. However, traditional electrical NoC implementations still need to overcome the challenges of high data transfer latencies and large power consumption. On-chip photonic interconnects with high performance-per-watt characteristics have recently been proposed as an alternative to address these challenges for intra-chip communication. In this article, we explore using low-cost photonic interconnects on a chip to enhance traditional electrical NoCs. Our proposed hybrid photonic ring-mesh NoC (METEOR) utilizes a configurable photonic ring waveguide coupled to a traditional 2D electrical mesh NoC. Experimental results indicate a strong motivation to consider the proposed architecture for future CMPs, as it can provide about 5× reduction in power consumption and improved throughput and access latencies, compared to traditional electrical 2D mesh and torus NoC architectures. Compared to other previously proposed hybrid photonic NoC fabrics such as the hybrid photonic torus, Corona, and Firefly, our proposed fabric is also shown to have lower photonic area overhead, power consumption, and energy-delay product, while maintaining competitive throughput and latency.",
    "actual_venue": "Acm Trans Embedded Comput Syst"
  },
  {
    "abstract": "Unlike for a single-class queueing network, diffusion approximations for a multiclass queueing network may not exist under the heavy traffic condition. In this paper, we derive a necessary and sufficient condition for the existence of the diffusion approximation for a four-class two-station multiclass queueing network (known as Kumar-Seidman network) under a priority service discipline.",
    "actual_venue": "Oper Res Lett"
  },
  {
    "abstract": "The use of association rule mining techniques in diverse contexts and domains has resulted in the creation of numerous interestingness measures. This, in turn, has motivated researchers to come up with various classification schemes for these measures. One popular approach to classify the objective measures is to assess the set of mathematical properties they satisfy in order to help practitioners select the right measure for a given problem. In this research, we discuss the insufficiency of the existing properties in the literature to capture certain behaviors of interestingness measures. This motivates us to adopt an approach where a measure is described by how it varies if there is a unit change in the frequency count $$(f_{11},f_{10},f_{01},f_{00})$$, at different preexisting states of the counts. This rate of change analysis is formally defined as the first partial derivative of the measure with respect to the various frequency counts. We use this analysis to define two novel properties, unit-null asymptotic invariance (UNAI) and unit-null zero rate (UNZR). UNAI looks at the asymptotic effect of adding frequency patterns, while UNZR looks at the initial effect of adding frequency patterns when they do not preexist in the dataset. We present a comprehensive analysis of 50 interestingness measures and classify them in accordance with the two properties. We also present multiple empirical studies, involving both synthetic and real-world datasets, which are used to cluster various measures according to the rule ranking patterns of the measures. The study concludes with the observation that classification of measures using the empirical clusters shares significant similarities to the classification of measures done through the properties presented in this research.",
    "actual_venue": "Knowledge And Information Systems"
  },
  {
    "abstract": "Human action recognition is a very active research topic in computer vision and pattern recognition. Recently, it has shown a great potential for human action recognition using the three-dimensional (3D) depth data captured by the emerging RGB-D sensors. Several features and/or algorithms have been proposed for depth-based action recognition. A question is raised: Can we find some complementary features and combine them to improve the accuracy significantly for depth-based action recognition&quest; To address the question and have a better understanding of the problem, we study the fusion of different features for depth-based action recognition. Although data fusion has shown great success in other areas, it has not been well studied yet on 3D action recognition. Some issues need to be addressed, for example, whether the fusion is helpful or not for depth-based action recognition, and how to do the fusion properly. In this article, we study different fusion schemes comprehensively, using diverse features for action characterization in depth videos. Two different levels of fusion schemes are investigated, that is, feature level and decision level. Various methods are explored at each fusion level. Four different features are considered to characterize the depth action patterns from different aspects. The experiments are conducted on four challenging depth action databases, in order to evaluate and find the best fusion methods generally. Our experimental results show that the four different features investigated in the article can complement each other, and appropriate fusion methods can improve the recognition accuracies significantly over each individual feature. More importantly, our fusion-based action recognition outperforms the state-of-the-art approaches on these challenging databases.",
    "actual_venue": "Acm Transactions On Intelligent Systems And Technology"
  },
  {
    "abstract": "In order to address the high transmission bandwidth requirement of an Internet-of-Video-Things (IoVT), an object-based on-line video summarization algorithm is proposed to summarize the captured video information at the sensor nodes before being transmitted to the server. It is composed of two stages: intra-view and inter-view stages. In the intra-view stage, human object detector is employed with the proposed human object descriptor. In the inter-view stage, an on-line clustering algorithm with a two-layer K-nearest-neighbor model is also proposed for object clustering. Experimental results show that significant improvement can be achieved when compared with state-of-the-art works.",
    "actual_venue": "Ieee International Symposium On Circuits And Systems"
  },
  {
    "abstract": "Australian education has been traditionally divided between the university sector and the vocational training sector. In Australia, the movement or \"articulation\" of students between these sectors has interested governments and professional bodies concerned with the associated structural and workforce issues. Such articulation includes movement from vocational training into professional education, from professional education to vocational training, and what has been called a \"swirling\" combination of movement back and forth between the sectors. The authors have considered why students who already hold a university degree and wish to pursue a career in librarianship/information management choose to undertake a vocational sector qualification in preference to a postgraduate professional qualification in Library and Information Studies (LIS). This article will provide historical context and report the results of research undertaken to investigate the underlying motivation of these students. The methodology included a survey of current students undertaking Vocational Education and Training (VE) in LIS paraprofessional qualifications at Australian educational institutions, and an analysis of statistical data from the VE sector. The study will contribute to a broader understanding of student motivations, career choices, and understanding of the concept of lifelong learning. It may also lead to a re-evaluation of how entry into the LIS industry should be managed.",
    "actual_venue": "Library Trends"
  },
  {
    "abstract": "Chaetoceros is a dominant genus of marine planktonic diatoms with worldwide distribution. Due to the difficulty of extracting setae from Chaetoceros images, automatic segmentation of Chaetoceros is still a challenging task. In this paper, we address this difficult task by regarding the whole segmentation process as unsupervised pixel-wise classification without human participation. First, we automatically produce positive (object) and negative (background) samples for follow-up training, by combining the advantages of two image processing algorithms: Grayscale Surface Direction Angle Model (GSDAM) for extracting setae information and Canny for detecting cell edges from low-contrast and strong-noisy microscopic images. Second, we develop pixel-wise training by using the produced samples in the training process of Deep Convolutional Neural Network (DCNN). At last, the trained DCNN is used to label other pixels into object and background for final segmentation. We compare our method with eight mainstream segmentation approaches: Otsu’s thresholding, Canny, Watershed, Mean Shift, gPb-owt-ucm, Normalized Cut, Efficient Graph-based method and GSDAM. To objectively evaluate segmentation results, we apply six well-known evaluation indexes. Experimental results on a new Chaetoceros image dataset with human labelled ground truth show that our method outperforms the eight mainstream segmentation methods in terms of both quantitative and qualitative evaluation.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "The visibility of outdoor images captured in inclement weather is often degraded due to the presence of haze, fog, sandstorms, and so on. Poor visibility caused by atmospheric phenomena in turn causes failure in computer vision applications, such as outdoor object recognition systems, obstacle detection systems, video surveillance systems, and intelligent transportation systems. In order to solve this problem, visibility restoration (VR) techniques have been developed and play an important role in many computer vision applications that operate in various weather conditions. However, removing haze from a single image with a complex structure and color distortion is a difficult task for VR techniques. This paper proposes a novel VR method that uses a combination of three major modules: 1) a depth estimation (DE) module; 2) a color analysis (CA) module; and 3) a VR module. The proposed DE module takes advantage of the median filter technique and adopts our adaptive gamma correction technique. By doing so, halo effects can be avoided in images with complex structures, and effective transmission map estimation can be achieved. The proposed CA module is based on the gray world assumption and analyzes the color characteristics of the input hazy image. Subsequently, the VR module uses the adjusted transmission map and the color-correlated information to repair the color distortion in variable scenes captured during inclement weather conditions. The experimental results demonstrate that our proposed method provides superior haze removal in comparison with the previous state-of-the-art method through qualitative and quantitative evaluations of different scenes captured during various weather conditions.",
    "actual_venue": "Ieee Trans Circuits Syst Video Techn"
  },
  {
    "abstract": "The nadir transmittance of sand samples was monitored as the illumination direction moved from 0° to 70°. Measurements were made for high- and low-density samples, at two depths and under both air-dry and saturated conditions. Transmittance decreased monotonically, but slowly, with increasing illumination angle at all wavelengths. A peak in transmittance appeared only at the 0° (zenith) illuminati...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "A fundamental challenge for any process-aware information system is to ensure compliance of modeled and executed business processes with imposed compliance rules stemming from guidelines, standards and laws. Such compliance rules usually refer to multiple process perspectives including control flow, time, resources, data, and interactions with business partners. On one hand, compliance rules should be comprehensible for domain experts who must define and apply them. On the other, they should have a precise semantics such that they can be automatically processed. In this context, providing a visual compliance rule language seems promising as it allows hiding formal details and offers an intuitive way of modeling. So far, visual compliance rule languages have focused on the control flow perspective, but lack adequate support for the other perspectives. To remedy this drawback, this paper provides an approach that extends visual compliance rule languages with the ability to consider data, time, resources, and partner interactions when modeling business process compliance rules. Overall, this extension will foster business process compliance support in practice.",
    "actual_venue": "Conceptual Modeling, Er"
  },
  {
    "abstract": "This paper focuses on Timely dataflow programming model for processing streams of data. We propose a technique to define CPU resource allocation (i.e., CPU capping) with the goal to improve response time latency in such type of applications with different quality of service (QoS) level, as they are concurrently running in a shared multi-core computing system with unknown and volatile demand. The proposed solution predicts the expected performance of the underlying platform using an online approach based on queuing theory and adjusts the corrections required in CPU allocation to achieve the most optimized performance. The experimental results confirms that measured performance of the proposed model is highly accurate while it takes into account the percentiles on the QoS metrics. The theoretical model used for elastic allocation of CPU share in the target platform takes advantage of design principals in model predictive control theory and dynamic programming to solve an optimization problem. While the prediction module in the proposed algorithm tries to predict the temporal changes in the arrival rate of each data flow, the optimization module uses a system model to estimate the interference among collocated applications by continuously monitoring the available CPU utilization in individual nodes along with the number of outstanding messages in every intermediate buffer of all TDF applications. The optimization module eventually performs a cost-benefit analysis to mitigate the total amount of QoS violation incidents by assigning the limited CPU shares among collocated applications. The proposed algorithm is robust (i.e., its worst-case output is guaranteed for arbitrarily volatile incoming demand coming from different data streams), and if the demand volatility is not large, the output is optimal, too. Its implementation is done using the TDF framework in Rust for distributed and shared memory architectures. The experimental results show that the proposed algorithm reduces the average and p99 latency of delay-sensitive applications by 21% and 31.8%, respectively, while can reduce the amount of QoS violation incidents by 98% on average.",
    "actual_venue": "Ieee International Symposium On Network Computing And Applications"
  },
  {
    "abstract": "This paper presents the performance of pyramidal horn antennas operating at X-band (8-12 GHz) that have been metallized with low cost conductive paint. The pyramidal horns are based on a WR-90 waveguide standard gain horn (SGH) designed for 15 dBi gain that have been manufactured by fused deposition modeling 3-D printing. They are then metalized using conductive copper paint. The antenna gain and radiation patterns are compared for antennas with three different paint thicknesses. A full-wave electromagnetic simulator, ANSYS HFSS, is used to simulate the horns and compare with measurement. We observe that the thickness of the conductive paint impacts the antenna 3dB beamwidth and gain.",
    "actual_venue": "Ieee Radio And Wireless Symposium"
  },
  {
    "abstract": "In this paper sensitivity analysis of the Charnes-Cooper-Rhodes (CCR) model in Data Envelopment Analysis (DEA) is studied for the case of perturbation of all outputs and of all inputs of an ecient Decision Making Unit (DMU). Using an approximate inverse of the perturbed optimal basis matrix, an approximate preservation of eciency for an ecient DMU under these perturbations is considered. Sucient conditions for an ecient DMU to preserve its eciency are obtained in that case. An illustrative example is provided.",
    "actual_venue": "Jors"
  },
  {
    "abstract": "A popular choice for anonymous Internet communication, the Tor network uses entry, relay, and exit nodes to hide the traffic's origin. However, an investigation that involved running real applications and website requests through Tor revealed numerous agglomerations of exiting traffic that an attacker could exploit.",
    "actual_venue": "Ieee Computer"
  },
  {
    "abstract": "Many problems of practical import can be formulated mathematically as problems in signal identification, where the signal is modeled as a finite number of discrete sinusoidal components which are additively combined and corrupted by an additive zero mean white noise source. It is desired to identify the parameters which characterize the signal; that is, the power (or amplitude), phase, and frequency of each of the sinusoids, and the noise power. A recently developed technique which shows much promise in its ability to estimate parameters of such a signal is Pisarenko's Harmonic Decomposition (PHD). Given such a signal, PHD will estimate the number of sinusoids present, the powers of each of the sinusoids, and the power of the noise. What is missing from a complete characterization of the signal is an estimate of the phases of the sinusoids. The goal of our effort has been to extend the PHD to yield phase information and thus to give the complete characterization of the signal.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Ieee International Conference Icassp"
  },
  {
    "abstract": "We evaluate the impact of forward error correction (FEC) coding on the end-to-end video quality of video communications over burst-loss channels. We formulate the probability of a given packet loss pattern and the associated video distortion such that we can compute the expected distortion for the video signal after FEC recovery at the receiver. Using our analytical model, we compute the expected video quality in the case of transmission over different Markov channels, both in the presence or absence of FEC. These results match with high accuracy actual simulation-obtained data thereby validating the proposed analysis. © 2011 IEEE.",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "Learning persistence in a cyber-learning environment is not only an index determining the success or failure of individual learners but also a source of important information to establish the management direction of educational programs in an organization. Accordingly, learners need to be motivated to continue to grow in order to ensure both qualitative improvement and quantitative growth of cyber learning. However, previous research on successful and continuous learning has considered the factors relevant to learning persistence independently from satisfaction and only investigated the correlation and prediction, rather than examining the comprehensive causal relationships. Accordingly, the current study established self-regulated learning as an exogenous variable, and learning flow, satisfaction, and learning persistence as endogenous variables. We investigated the structural causal relationships among these variables by using structural equation model (SEM). We collected data from 594 students in W Cyber University and conducted surveys regarding self-regulated learning, flow, satisfaction, and learning persistence. In the study results, the self-regulated learning ability of cyber-university students directly affected learning flow (beta=.420), learning flow directly affected satisfaction, and learning flow (beta=.464) and satisfaction (beta=.354) directly affected learning persistence. The SEM results showed that learning flow intermediated between self-regulated learning ability and satisfaction and between self-regulated learning ability and learning persistence. In addition, satisfaction intermediated between learning flow and learning persistence. In the concluding remarks, we suggest the necessary strategies for planning and managing a successful learning process for effective cyber education.",
    "actual_venue": "Interactive Learning Environments"
  },
  {
    "abstract": "XML has recently made inroads into the data-oriented world of machine-to-machine interaction, most promi- nently in the form of SOAP and Web services. In this paper we present a new programming interface that views XML documents as sequences of events, and into which awareness of data typing is built in. These two features are intended to make the interface attractive for data-oriented applications that do not use XML internally. Our experi- ence in using this interface in a mobile middleware system indicates that this sequentiality combined with type aware- ness and properly-layered higher-level interfaces provides a very natural way of handling XML for the data-oriented world.",
    "actual_venue": "Euroimsa"
  },
  {
    "abstract": "Decentralised rate-based flow controller design in multi-bottleneck data-communication networks is considered. An H problem is formulated to find decentralised controllers which can be implemented locally at the bottleneck nodes. A suboptimal solution to this problem is found and the implementation of the decentralised controllers is presented. The controllers are robust to time-varying uncertain multiple time-delays in different channels. They also satisfy tracking and weighted fairness requirements. Lower bounds on the actual stability margins are derived and their relation to the design parameters is analysed. A number of simulations are also included to illustrate the time-domain performance of the proposed controllers.",
    "actual_venue": "International Journal Of Control"
  },
  {
    "abstract": "A new stochastic switched linear model is established to describe the Zigbee-based wireless networked control system WNCS with both network-induced delay and packet dropout. The network-induced delay can be less or longer than one sampling period. A sufficient condition is presented for the exponentially mean square stability of the closed-loop WNCS, and corresponding state feedback controller is designed by using the augmenting technique and multi-Lyapunov approach. Then, combined with carrier sense multiple access with collision avoidance CSMA-CA algorithm, a method is given to choose proper parameter values. Finally, a numerical example is provided to demonstrate the effectiveness of the proposed method.",
    "actual_venue": "Int J Systems Science"
  },
  {
    "abstract": "Visual impairment is one of the disabilities that could injure people congenitally or adventitiously. The number of individuals who suffer from visual impairment is increasing rapidly. Therefore, there is an urgent need to develop assistive technologies that aid blind people. Our proposed system aims to provide an assistive technology for blind and visually impaired individuals by exploiting the power of smart phones, which are existing, popular, and often used by blind individuals. Our proposed system provides a novel navigation system that operates on smart phones and provides a route to any destination in a building from the user's current position.",
    "actual_venue": "Iiwas"
  },
  {
    "abstract": "There are 5 groups of order 20. This paper reports on the search for binary self-dual codes of length 40, cocyclic over any one of the first four groups, using cocyclic Hadamard matrices and the [I, A] construction. The fifth group is not investigated here. A total of 28 classes of extremal cocyclic self-dual codes were found—27 of these are doubly-even and one singly-even. The majority of these classes arise from the dihedral-cocyclic Hadamard matrices. There is also a class of dihedral-cocyclic Hadamard matrices which gives a large collection of [40, 20] codes with only one codeword of length 4.",
    "actual_venue": "Des Codes Cryptography"
  },
  {
    "abstract": "Web applications can be classified as hybrids between hypermedia and information systems. They have a relatively simple distributed architecture from the user viewpoint, but a complex dynamic architecture from the designer viewpoint. They need to respond to operation by an unlimited number of heterogeneously skilled users, address security and privacy concerns, access heterogeneous, up-to-date information sources, and exhibit dynamic behaviors that involve such processes as code transferring. Common system development methods can model some of these aspects, but none of them is sufficient to specify the large spectrum of Web application concepts and requirements. This paper introduces OPM/Web, an extension to the Object-Process Methodology (OPM) that satisfies the functional, structural and behavioral Web-based information system requirements. The main extensions of OPM/Web are adding properties of links to express requirements, such as those related to encryption; extending the zooming and unfolding facilities to increase modularity; cleanly separating declarations and instances of code to model code transferring; and adding global data integrity and control constraints to express dependence or temporal relations among (physically) separate modules. We present a case study that helps evaluate OPM/Web and compare it to an extension of the Unified Modeling Language (UML) for the Web application domain.",
    "actual_venue": "Ann Software Eng"
  },
  {
    "abstract": "This paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging",
    "actual_venue": "Arxiv: Computation And Language"
  },
  {
    "abstract": "This article presents an overview of several known approaches to causal discovery. It is organized by relating the different fundamental assumptions that the methods depend on. The goal is to indicate that for a large variety of different settings the assumptions necessary and sufficient for causal discovery are now well understood.",
    "actual_venue": "J Data Science And Analytics"
  },
  {
    "abstract": "One of the unanticipated consequences of the Internet age is a pervasive loss of context. Information is often filtered, sampled, repackaged, condensed, or altered to suit any number of purposes. Over time, the entropy of these processes causes information to lose its essential validity. This column argues the needs, applications, and challenges of providing greater access to data provenance in information systems.",
    "actual_venue": "Security And Privacy, Ieee"
  },
  {
    "abstract": "We propose distributed algorithms for real-time monitoring and admission control that allow base stations in heterogeneous wireless cellular networks to dynamically serve mobile users under the constraint of: 1) accommodating all active transmissions in a single shared channel; and 2) guaranteeing a minimum signal-to-interference-plus-noise ratio (SINR) to each served user. In particular, we develop distributed techniques for iterative real-time computation of the spectral radius of an unknown network matrix (often the Perron root of the matrix) that indicates the time-varying limits of power control stability, i.e., the limits of network capacity. Solely locally available information is used as algorithmic input. By drawing a formal analogy with the Google PageRank algorithm, the computations are shown analytically to be exponentially fast and sufficiently accurate for optimal (error-free) stability detection. Numerical simulations of an existing office building demonstrate the applicability of the proposed algorithms to actual UMTS W-CDMA systems characterized by discrete power control with limited step-size.",
    "actual_venue": "Networking, IEEE/ACM Transactions  "
  },
  {
    "abstract": "A video processing architecture based on FPGA for real-time embedded vision systems is proposed in this paper. Recently, embedded vision systems are used in various applications. On the other hand, the throughput required to the system has been increasing as the high-definition vision replaces the current vision systems. Since more complex vision algorithms become available, higher performance and better expandability are requested accordingly. As a solution for this challenging situation, FPGA is now drawing more attention as an embedded vision system platform. In addition, high-level synthesis design is preferred instead of traditional low-level design based on hardware description languages with lower productivity. In this study, we implemented a video processing pipelined architecture which can offer flexibility for interchange processing modules. Each module was implemented using Verilog HDL and Vivado HLS for its evaluation.",
    "actual_venue": "Aina Workshops"
  },
  {
    "abstract": "One solution to the timing closure problem is to perform infrequent operations in more than one cycle. Despite simplicity of the solution statement, it is not easily considered because it requires changes in RTL, which, in turn, exacerbates the verification problem. We offer a timing closure solution guaranteed to preserve functional correctness of designs expressed using atomic actions or rules. We exploit the fact that the semantics of atomic actions are untimed, that is, the time to execute an action is not specified. The current hardware synthesis technique from atomic actions assumes that each rule takes one clock cycle to complete its computation. Consequently, the rule with the longest combinational path determines the clock cycle of the entire design, often leading to needlessly slow circuits.\n\nWe present a synthesis procedure for a system where the combinational circuits embodied in a rule can take multiple cycles without changing the semantics of the original design. We also present preliminary results based on an experimental compiler which uses the Bluespec (BSV) compiler front end and generates Verilog. The results show that the clock speed and the performance of circuits can be improved substantially by allowing slow paths to complete over multiple cycles. Our technique is orthogonal to solutions based on multiple clock domains.",
    "actual_venue": "San Jose, Ca"
  },
  {
    "abstract": "Tensor Product (TP) transformation based modeling and control can be useful in biomedical engineering, since complex nonlinear control tasks can be handled easier with it. Moreover, the modeling approach can handle the Linear Parameter Varying (LPV) models and produces a tensor based system description, which can be used during Linear Matrix Inequality (LMI) based controller design. The TP property makes the usability of the method beneficial as LMI connected techniques allows using the Lyapunov theorems. The aim of the current work is to demonstrate the usability of TP models in biomedical applications, i.e. diabetes modeling. The core model, the minimal model is investigated and simulation results are presented under Matlab.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "In this paper, an experiment that is oriented to discriminate between different oil slicks using polarimetric SAR image was introduced. Dark patches which often appear in SAR images such as biogenic slicks, atmospheric front, and crude oil with different chemical composition were identified according to the result of Eigenvector-Eigenvalue based Incoherent Target Decomposition. The experiment demonstrated that polarimetric SAR can be of great help in classifying of oil spills.",
    "actual_venue": "Igarss"
  },
  {
    "abstract": "We present a simulation approach that provides a relatively risk-free and cost-effective environment to examine the decision space for both bid takers and bid makers in web-based dynamic price setting processes. The applicability of the simulation platform is demonstrated for Yankee auctions in particular. We focus on the optimization of bid takers' revenue, as well as on examining the welfare implications of a range of consumer-bidding strategies--some observed, some hypothetical. While these progressive open discriminatory multiunit auctions with discrete bid increments are made feasible by Internet technologies, little is known about their structural characteristics, or their allocative efficiency. The multiunit and discrete nature of these mechanisms renders the traditional analytic framework of gametheory intractable (Nautz and Wolfstetter 1997). The simulation is based on theoretical revenue generating properties of these auctions. We use empirical data from real online auctions to instantiate the simulation's parameters. For example, the bidding strategies of the bidders are specified based on three broad bidding strategies observed in real online auctions. The validity of the simulation model is established and subsequently the simulation model is configured to change the values of key control factors, such as the bid increment. Our analysis indicates that the auctioneers are, most of the time, far away from the optimal choice of bid increment, resulting in substantial losses in a market with already tight margins. The simulation tool provides a test bed forjointly exploring the combinatorial space of design choices made by the auctioneer's and the bidding strategies adopted by the bidders. For instance, a multinomial logit model reveals that endogenous factors, such as the bid increment and the absolute magnitude of the auction have a statistically significant impact on consumer-bidding strategies. This endogeniety is subsequently modeled into the simulation to investigate whether the effects are significant enough to alter the optimal bid increments or auctioneer revenues. Additionally, we investigate hybrid-bidding strategies, derived as a combination of three broad strategies, such as jump bidding and strategic-at-margin (SAM) bidding. We find that hybrid strategies have the potential of significantly altering bidders' likelihood of winning, as well as their surplus.",
    "actual_venue": "Information Systems Research"
  },
  {
    "abstract": "As the prices of commodity workstations go down, clusters of workstationshave started to emerge as a viable economic solution for scalable computing.Recent advances in networking technology have made it possible to obtainhigh-bandwidth connections between applications. However, the interconnectlatency between workstation nodes in a cluster remains a serious concern andcan prove to be the limiting factor in workstation performance. In thispaper, we present the CNI orcluster network interface that achieves the twingoals of low latency and high bandwidth. In addition, CNI efficientlysupports multiple programming paradigms for programming generality. This isdone by functionally coupling the network interface more closely to the CPUwithout violating the constraints of a standard workstation architecture,CNI results in performance gains for applications, substantially reducingcommunication overhead and delay.",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "Linear codes with complementary duals (abbreviated LCD) are linear codes whose intersection with their dual are trivial. When they are binary, they play an important role in armoring implementations against side-channel attacks and fault injection attacks. Non-binary LCD codes in characteristic 2 can be transformed into binary LCD codes by expansion. In this paper, we introduce a general construction of LCD codes from any linear codes. Further, we show that any linear code over $mathbb F_{q} (qu003e3)$ is equivalent to an Euclidean LCD code and any linear code over $mathbb F_{q^2} (qu003e2)$ is equivalent to a Hermitian LCD code. Consequently an $[n,k,d]$-linear Euclidean LCD code over $mathbb F_q$ with $qu003e3$ exists if there is an $[n,k,d]$-linear code over $mathbb F_q$ and an $[n,k,d]$-linear Hermitian LCD code over $mathbb F_{q^2}$ with $qu003e2$ exists if there is an $[n,k,d]$-linear code over $mathbb F_{q^2}$. Hence, when $qu003e3$ (resp.$qu003e2$) $q$-ary Euclidean (resp. $q^2$-ary Hermitian) LCD codes possess the same asymptotical bound as $q$-ary linear codes (resp. $q^2$-ary linear codes). Finally, we present an approach of constructing LCD codes by extending linear codes.",
    "actual_venue": "Arxiv: Information Theory"
  },
  {
    "abstract": "We reveal some drawbacks in Ghodosi's scheme and present an improved RSA-based access control scheme for hierurchical groups. The proposed scheme enjoys the following properties: The size of an individualýs share sequence is bounded by a small constant times the size of the RSA modulus corresponding to that group. The delegation of information flow from higher-level groups to lower-level groups is completely non-interactive. doesnýt require an on-line trusted authority to upgrade the system as soon as communication happened. The proposed scheme is resistant to conspiracy",
    "actual_venue": "Aina"
  },
  {
    "abstract": "Due to the growing dependence of information society on Information and Communication Technologies, the need to protect information is getting more and more important for enterprises. In this context, Information Security Management Systems (ISMSs), have arisen for supporting the processes and systems for effectively managing information security. The fact of having these systems available has become more and more vital for the evolution of Small and Medium-Sized Enterprises (SMEs), but however, this type of enterprises have special characteristics which make it difficult for them the correct deployment of ISMSs. In this article, we show the methodology that we have created for the development, implementation and maintenance of ISMSs, adapted for the needs and resources available for SMEs. This approach is being directly applied to real case studies and thus, we are obtaining a constant improvement in its application.",
    "actual_venue": "J Ucs"
  },
  {
    "abstract": "The study of biological pathways is key to a large number of systems analyses. However, many relevant tools consider a limited number of pathway sources, missing out on many genes and gene-to-gene connections. Simply pooling several pathways sources would result in redundancy and the lack of systematic pathway interrelations. To address this, we exercised a combination of hierarchical clustering and nearest neighbor graph representation, with judiciously selected cutoff values, thereby consolidating 3215 human pathways from 12 sources into a set of 1073 SuperPaths. Our unification algorithm finds a balance between reducing redundancy and optimizing the level of pathway-related informativeness for individual genes. We show a substantial enhancement of the SuperPaths' capacity to infer gene-to-gene relationships when compared with individual pathway sources, separately or taken together. Further, we demonstrate that the chosen 12 sources entail nearly exhaustive gene coverage. The computed SuperPaths are presented in a new online database, PathCards, showing each SuperPath, its constituent network of pathways, and its contained genes. This provides researchers with a rich, searchable systems analysis resource. Database URL: http://pathcards.genecards.org/",
    "actual_venue": "Data Base"
  },
  {
    "abstract": "Public administrations are developing several mechanisms to increase the reception level or the impact of the information that they communicate (i.e public policy-making or public service delivery) using for this, marketing techniques. Therefore, we may consider the concept of marketing in the public sector as the process of identifying the needs of a target audience to try to meet them, taking into account at the same time the objectives of the organization and the society. This paper aims to address this reality, regarding to the new opportunities resulting in the use of the Internet. Therefore, the purpose of this paper is to analyze how the public administrations are using Internet and, above all, the tools associated with social networks, to carry out the marketing process of their policies and public services. This study shows several experiencies to integrate the citizenry within a more collaborative information process.",
    "actual_venue": "Wims"
  },
  {
    "abstract": "The new usability agenda is driving empirical and experimental studies into a growing range of quality criteria such as engagement, user experience, and aesthetics. Some see this as a positive move to theorise about the nature of good design qualities, and to objectively test such hypotheses on the new usability theme. However, others (e.g. [1], [2]) have argued for interpretation-based inquiry into user engagement and experience on the grounds that such phenomena can only be understood by investigations into contexts of use which defy quantitative approaches. Many in the design community would agree with them and go further to argue that quality in design is a matter of creativity and can not be measured or theorised per se; instead, research should focus on understanding and improving the process of design. This panel will debate the tensions between these positions and explore possible common ground between them as a contribution towards the research agenda that is being debated in the DIS conference series.",
    "actual_venue": "Conference On Designing Interactive Systems"
  },
  {
    "abstract": "The research on community question answering (CQA) has been paid increasing attention in recent years. In CQA, to reduce the number of unanswered questions and the time for askers to wait, it is very necessary to identify relevant experts or best answers for these questions. Generally, the experts' answers are more likely to be the best answers. Existing studies considered that user expertise is reflected by the voting scores of both answers and questions. However, voting scores of questions are not really related to user expertise. In this paper, we proposed a new probabilistic model to depict users' expertise based on answers and their descriptive ability based on questions. To exploit social information in CQA, the link analysis is also considered. Extensive experiments on the large Stack Overflow dataset demonstrate that our methods can achieve comparable or even better performance than the state-of-the-art models.",
    "actual_venue": "Asonam"
  },
  {
    "abstract": "The past two decades have seen a growing interest in methods for anonymous communication on the Internet, both from the academic community and the general public. Several system designs have been proposed in the literature, of which a number have been implemented and are used by diverse groups, such as journalists, human rights workers, the military, and ordinary citizens, to protect their identities on the Internet. In this work, we survey the previous research done to design, develop, and deploy systems for enabling private and anonymous communication on the Internet. We identify and describe the major concepts and technologies in the field, including mixes and mix networks, onion routing, and Dining Cryptographers networks. We will also review powerful traffic analysis attacks that have motivated improvements and variations on many of these anonymity protocols made since their introduction. Finally, we will summarize some of the major open problems in anonymous communication research and discuss possible directions for future work in the field.",
    "actual_venue": "Acm Comput Surv"
  },
  {
    "abstract": "This paper focuses on the Turing patterns in the general Gierer-Meinhardt model of morphogenesis. The stability analysis of the equilibrium for the associated ODE system is carried out and the stability conditions are obtained. Furthermore, we perform a detailed Hopf bifurcation analysis for this system. The results show that the equilibrium undergoes a supercritical Hopf bifurcation in certain parameter range and the bifurcated limit cycle is stable. With added diffusions, we then show that both the stable equilibrium and the Hopf periodic solution experience Turing instability with unequal spatial diffusions and obtain the instability conditions. Numerical simulations are given to illustrate the theoretical analysis, which show that the Turing patterns are of either spot or stripe type.",
    "actual_venue": "International Journal Of Bifurcation And Chaos"
  },
  {
    "abstract": "We demonstrate how several programming language concepts and methods can be used economically toobtain an improved solution to a difficult algorithmic problem. The problem is to compile a subset RCS ofRelational Calculus defined by Willard (1978) in a novel way so that efficient run-time query performanceis guaranteed. Willard gives an algorithm to compile each query q belonging to RCS so that it executes inO(n logdn+o) steps and O(n) space, where n and o are respectively the input and ...",
    "actual_venue": "Algorithmic Languages And Calculi"
  },
  {
    "abstract": "Linear regression models are used to quantitatively predict drug resistance, the phenotype, from the HIV-1 viral genotype. As new antiretroviral drugs become available, new resistance pathways emerge and the number of resistance associated mutations continues to increase. To accurately identify which drug options are left, the main goal of the modeling has been to maximize predictivity and not interpretability. However, we originally selected linear regression as the preferred method for its transparency as opposed to other techniques such as neural networks. Here, we apply a method to lower the complexity of these phenotype prediction models using a 3-fold cross-validated selection of mutations.Compared to standard stepwise regression we were able to reduce the number of mutations in the reverse transcriptase (RT) inhibitor models as well as the number of interaction terms accounting for synergistic and antagonistic effects. This reduction in complexity was most significant for the non-nucleoside reverse transcriptase inhibitor (NNRTI) models, while maintaining prediction accuracy and retaining virtually all known resistance associated mutations as first order terms in the models. Furthermore, for etravirine (ETR) a better performance was seen on two years of unseen data. By analyzing the phenotype prediction models we identified a list of forty novel NNRTI mutations, putatively associated with resistance. The resistance association of novel variants at known NNRTI resistance positions: 100, 101, 181, 190, 221 and of mutations at positions not previously linked with NNRTI resistance: 102, 139, 219, 241, 376 and 382 was confirmed by phenotyping site-directed mutants.We successfully identified and validated novel NNRTI resistance associated mutations by developing parsimonious resistance prediction models in which repeated cross-validation within the stepwise regression was applied. Our model selection technique is computationally feasible for large data sets and provides an approach to the continued identification of resistance-causing mutations.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "Today, there is an increasing need for fault tolerance capabilities in integrated circuits used in critical applications such as aircraft control. The classical way to achieve fault tolerance in a logic block is to triplicate it and to implement a majority voting block on the outputs (Triple Modular Redundancy, or TMR). The Single Independent Decoder (SID) architecture was defined in order to achieve with a lower hardware overhead the tolerance of faults in the circuit control part (the Finite State Machine), and more precisely in the sequencing logic (next-state logic and state register). A dedicated synthesis tool (ASYL-SdF) has been developed and the results obtained on a large set of examples in terms of silicon area and dependability evaluation have shown its efficiency, especially compared with the TMR implementation of the sequencing logic (TMR Seq)",
    "actual_venue": "Edandtc"
  },
  {
    "abstract": "This paper extends a previously proposed algorithm for generating unstructured meshes in three-dimensional and in two- dimensional domains to generate surface meshes. A surface mesh is generated in parametric space and mapped to Cartesian space. Finite elements may be stretched on parametric space, but they present a good-quality shape on the 3D surface. The algorithm uses a metric map defined by Tristano et al. to obtain correct distances and stretches. A background quadtree structure is used to store local surface metrics and to develop local guidelines for node location in an advancing-front meshing strategy.",
    "actual_venue": "IMR"
  },
  {
    "abstract": "We present a web-based authoring tool to create interactive multimedia contents for English learning. Target users of this authoring tool are English teachers and content designer, therefore, ease of use and simplicity are the fundamental issues. Furthermore, the authoring tool and multimedia database are integrated in the context of Intelligent Web-based Interactive Language Learning (IWiLL) system. Special language tools to access corpus, manipulate multimedia elements, and create collaborative learning sessions are designed in the system. These resources and tools on hand yield the potential to create rich and deep interactive multimedia contents.",
    "actual_venue": "Icalt"
  },
  {
    "abstract": "To take advantage of the interdisciplinary experience of our colleagues, we decided several years ago to add heuristic evaluation to our expert analysis method. Although heuristic evaluation is a cost-effective method for evaluating interfaces, we found that the recommended prioritization strategy--ranking the problems according to severity--has several limitations. Specifically, it does not address how much it will cost the developers to fix the problems, nor does it adequately capture the distinction between high-level (global) and low-level (specific, screen-level) problems. To address these limitations, we developed a method which retains the richness of heuristic evaluation, but communicates the results in such a way that project managers, developers, and designers can form a clear and immediately executable plan for addressing the problems. Our method integrates user research, heuristic evaluation, affinity diagramming, cost-benefit charts, and recommendations into a report that others can use to plan both short and long-term improvements.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "Clustering methods have been increasingly applied over gene expression datasets. Different results are obtained when different clustering methods are applied over the same dataset as well as when the same set of genes is clustered in different microarray datasets. Most approaches cluster genes' profiles from only one dataset, either by a single method or an ensemble of methods; we propose using the binarization of consensus partition matrix (Bi-CoPaM) method to analyze comprehensively the results of clustering the same set of genes by different clustering methods and from different datasets. A tunable consensus result is generated and can be tightened or widened to control the assignment of the doubtful genes that have been assigned to different clusters in different individual results. We apply this over a subset of 384 yeast genes by using four clustering methods and five microarray datasets. The results demonstrate the power of Bi-CoPaM in fusing many different individual results in a tunable consensus result and that such comprehensive analysis can overcome many of the defects in any of the individual datasets or clustering methods.",
    "actual_venue": "Machine Learning For Signal Processing"
  },
  {
    "abstract": "This paper deals with speed control of a squirrel cage six-phase induction machine (SC6PIM) in phase opening occurrences. A decoupled model of induction machine with unbalanced structure is given to analyze the phase opening operation of the SC6PIM. In order to reduce torque oscillations created in phase opening events, a fault-tolerant operation method by using indirect rotor field oriented control (IRFOC) technique is proposed and used to control the speed of the SC6PEVI in each faulty operation. The proposed controller will be renewed in each fault situation of machine regarding the number and type of missed phases. Furthermore, in this paper, switching between controllers in some different cases is investigated and real time initial conditions setting for controllers is proposed to minimize the transient problems in switching instant. To evaluate the performance of proposed method, the experimental results are obtained on a small SC6PIM rated at 90 W.",
    "actual_venue": "Iecon"
  },
  {
    "abstract": "The paper presents an approach to an engineering of “gareki”, which generally means destroyed or disordered structures in Japanese. Gareki has never been a subject for both natural sciences and engineering. Therefore we have had less scientific knowledge about gareki, while search and rescue activities in earthquake disasters need it much. The first part of the paper discusses what to be studied about gareki with respect to search and rescue activities. To share knowledge about gareki, the concept of digital gareki archives is explained and four approaches for gareki data acquisition are discussed. As a prototype of digital gareki archives, a computer software system named Virtual Gareki Field (VGF) which generates digital gareki data based on collapsing process simulation has been developed and models and tools related to VGF are explained.",
    "actual_venue": "Safety, Security, And Rescue Robotics"
  },
  {
    "abstract": "Prefetching aids in reducing the increasing processor-memory latency gap in a multiprocessor system. In a Tiled Chip MultiProcessor system (TCMP), the cache block reuse pattern of applications vary across the cores. Applications that have a working set larger than the L1 cache size cannot be accommodated in the limited L1 cache space. Such applications suffer from cache space constraints. Prefetching cache blocks of heavy application may cause thrashing by evicting useful cache lines from L1 cache thereby demanding frequent cache block replacements. On the other hand, light applications due to less cache block demands may under-utilize the available L1 cache space. This paper proposes Near Vicinity Prefetching (NVP) where some prefetch blocks are placed in the L1 caches of adjacent tiles running light applications. The prefetching framework works simultaneously with other L1 caches in different tiles to reduce the cache block access time. During cache hits in the neighboring L1 cache, the network transaction required to bring that block reduces from an average of 'n' hops to just one. NVP helps in better cache space utilization and reduced network transactions at minimal hardware. Experimental analysis on a 64-core TCMP with SPEC CPU 2006 benchmark mixes shows that the average memory access time is reduced by 4.42% using the proposed NVP technique.",
    "actual_venue": "International Conference On Vlsi Design And International Conference On Embedded Systems"
  },
  {
    "abstract": "We have extended the parallel computer algebra system Distributed Maple by fault tolerance mechanisms such that computations are not any more limited by the meantime between failures. This is complicated by the fact that task arguments and results may embed task handles and that the system's scheduling layer has only a little information about the computing layer. Nevertheless, the mostly functional parallel programming model makes it possible with relatively simple means.",
    "actual_venue": "Euro-Par"
  },
  {
    "abstract": "•An efficient method of content-targeted online video advertising is proposed in this paper.•Shot segmentation method based on block histogram.•Adaptive key frame extraction based on motion information.•Feature extraction and similarity measurement.•Shot clustering and scene boundary detection.",
    "actual_venue": "Journal Of Visual Communication And Image Representation"
  },
  {
    "abstract": "This paper describes the world's first real-time, international transmission of 4K digital cinema and 4K Super High Definition (SHD) digital video at iGrid 2005, hosted at the California Institute of Telecommunications and Information Technology (Calit2) at the University of California, San Diego. Nearly six hours of live and pre-recorded 4K motion picture and audio content was streamed to iGrid in San Diego from the Research Institute for Digital Media and Content (DMC) at Keio University in Tokyo.To implement this demonstration, several new technologies were introduced, including a prototype high-performance 4K compressed multicasting system called \"JPEG 2000 Flexcast\", and \"Soundscape\", a practical scheme for synchronizing audio and video transmitted from different locations over IP networks.These iGrid 2005 demonstrations proved that it is now feasible to implement networked professional audio/video applications - production, post-production and distribution - even at 4K quality over IP networks up to 15,000 km long. The demonstrations also showed the new 4K motion picture technology being introduced for digital cinema can be usefully applied to other network applications such as remote telepresence, distance learning and scientific visualization.",
    "actual_venue": "Future Generation Comp Syst"
  },
  {
    "abstract": "In [CHECK END OF SENTENCE], the authors proposed a framework for automated clustering and visualization of biological data sets named AUTO-HDS. This letter is intended to complement that framework by showing that it is possible to get rid of a user-defined parameter in a way that the clustering stage can be implemented more accurately while having reduced computational complexity",
    "actual_venue": "Ieee/Acm Trans Comput Biology Bioinform"
  },
  {
    "abstract": "This paper proposes a fully decentralized peer-to-peer overlay structure\nGeoP2P, to facilitate geographic location based search and retrieval of\ninformation. Certain limitations of centralized geographic indexes favor\npeer-to-peer organization of the information, which, in addition to avoiding\nperformance bottleneck, allows autonomy over local information. Peer-to-peer\nsystems for geographic or multidimensional range queries built on existing DHTs\nsuffer from the inaccuracy in linearization of the multidimensional space.\nOther overlay structures that are based on hierarchical partitioning of the\nsearch space are not scalable because they use special super-peers to represent\nthe nodes in the hierarchy. GeoP2P partitions the search space hierarchically,\nmaintains the overlay structure and performs the routing without the need of\nany super-peers. Although similar fully-decentralized overlays have been\npreviously proposed, they lack the ability to dynamically grow and retract the\npartition hierarchy when the number of peers change. GeoP2P provides such\nadaptive features with minimum perturbation of the system state. Such\nadaptation makes both the routing delay and the state size of each peer\nlogarithmic to the total number of peers, irrespective of the size of the\nmultidimensional space. Our analysis also reveals that the overlay structure\nand the routing algorithm are generic and independent of several aspects of the\npartitioning hierarchy, such as the geometric shape of the zones or the\ndimensionality of the search space.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "We discuss the design of a high-performance field programmable gate array (FPGA) architecture that efficiently prototypes asynchronous (clockless) logic. In this FPGA architecture, low-level application logic is described using asynchronous dataflow functions that obey a token-based compute model. We implement these dataflow functions using finely pipelined asynchronous circuits that achieve high computation rates. This asynchronous dataflow FPGA architecture maintains most of the performance benefits of a custom asynchronous design, while also providing postfabrication logic reconfigurability. We report results for two asynchronous dataflow FPGA designs that operate at up to 400 MHz in a typical TSMC 0.25 /spl mu/m CMOS process.",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "In this paper we promote the idea of analyzing the character of a conversation that the user is engaged in to help determine her receptivity to mobile notifications. Given that most previous mobile notification management systems incorporate only general awareness of speech or just consider the mere presence of the user's voice, we argue for the use of speaker tracking to completely capture presence as well as dynamics and other characteristics of personal conversations in order to assess personal interruptibility.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "Profile based sequence search methods are widely used to obtain homologous proteins with the usage of stringent statistical measures. These simple searches, albeit their high sensitivity, cannot solely be relied, while searching for protein families with high functional diversity and sharing structural similarity with other families. Myosins are motor proteins that drive cellular mobility and associated functions in eukaryotes. These motors utilize the chemical energy released by ATP hydrolysis to bring about conformational changes leading to a motor function. The major feature of the protein is a highly conserved head domain which is an ATPase followed by a variable tail that binds to different cargoes. Motor domain is an ATPase that evolved from P-loop containing NTPase ancestral protein. A number of other protein families are believed to be related through divergent evolution of an ancestral P-loop NTP binding motif, hence sequence searches for the members of one super family results in cross talks with another. We developed a strategic protocol for effective sequence-based searches of such families. This protocol employs position-specific Iterative Blast followed by a three way validation: 1. Text search scripts 2. clustering using neighbour joining method 3. domain architecture definitions and applied in Myosins from five model genomes as standard. This protocol can be followed for genome scan of similar protein families with sequence wise diverse members and sharing common ancestral structural motifs with other families.",
    "actual_venue": "Data Mining Workshop"
  },
  {
    "abstract": "Image super resolution has gained a lot of attention due to its applications in different fields of image processing. It is used to produce high-resolution images from low-resolution input. Because of the excellent learning capability of convolution neural networks, these networks are able to learn complex spatial structures for image super-resolution. In this paper, two different architectures have been proposed for image super resolution. The first architecture is Dual Subpixel Layer Convolution Neural Network (DSL-CNN), which stacks two subpixel CNN architectures to enhance model depth for better representational capability. Two stages provide an effective upscaling factor of 4. In the second architecture, named as Residue based Dual Subpixel Layer Convolution Neural Network (RDSL-CNN), two-stage residual learning has been introduced which effectively sustains the high frequency details and provides superior results than the previous state-of-the-art methods. The performance of the two architectures has been evaluated on various image datasets, and compared with other state-of-the-art methods.",
    "actual_venue": "Fundamenta Informaticae"
  },
  {
    "abstract": "Visibility is one of the major items of meteorological observation. Its accuracy is very important to air, sea and highways and transport. A method of visibility calculation based on image analysis and learning is introduced in this paper. First, visibility image is effectively represented by contrast based vision features. Then, a Supported Vector Regression (SVR) based learning system is constructed between image features and the target visibility. Consequently, visibility can be measured directly from a single inputting image with this learning system. The method makes use of the existing video cameras to calculate visibility in real time. Specific experiments show that this method has the characteristic of low cost, fast calculation, and convenience. Moreover, our proposed technology can be used anywhere to measure visibility.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Ambient information collected by wireless sensor networks (WSNs) (e.g. space, environment or physiological data) is made available to end-user applications via gateways. In situations where infrastructure-based networks are not available, these gateways and end-user applications are usually implemented as distributed entities forming P2P overlays. Enabling the interconnection of these application and gateway entities faces crucial challenges due to the nature of infrastructure-less networks such as MANETs and the heterogeneity of protocols, middleware, etc. that are employed by these entities. This paper proposes an architecture that will interconnect overlay gateways for WSNs with end-user applications in MANETs. The architecture relies on interconnector nodes that do not belong to either of the two overlays. A motivating scenario is presented, requirements are derived, and the proposed interconnection architecture is described. The prototype is also briefly discussed along with the performance evaluation.",
    "actual_venue": "International Conference On Protocol Engineering And International Conference On New Technologies Of Distributed Systems"
  },
  {
    "abstract": "Conflict is one of the most important phenomena of social life, but it is still largely neglected by the computing community. This work proposes an approach that detects common conversational social signals (loudness, overlapping speech, etc.) and predicts the conflict level perceived by human observers in continuous, non-categorical terms. The proposed regression approach is fully Bayesian and it adopts automatic relevance determination to identify the social signals that influence most the outcome of the prediction. The experiments are performed over the SSPNet Conflict Corpus, a publicly available collection of 1,430 clips extracted from televised political debates (roughly 12 hours of material for 138 subjects in total). The results show that it is possible to achieve a correlation close to 0.8 between actual and predicted conflict perception.",
    "actual_venue": "Affective Computing, IEEE Transactions  "
  },
  {
    "abstract": "In the pervasive computing environment consisting of peers (a generic name for clients, servers or agents), context -awareness plays an important role to offer personalized services for various applications, e.g., medical services, robotics, travel planning, security monitoring, and multi-player gaming. Accordingly, context management turns out to be an important issue in manipulating, acquiring information and reacting to the situation. In this paper, we describe a contract-based workflow paradigm to support long and short duration transactions in a context-aware pervasive computing environment. This paradigm provides for software contract that captures mutual obligations using program constructs such as \"require [else]\" for precondition and \"ensure [then]\" for post condition, assertions, invariants needed in the pervasive computing environment. Such program constructs are essential to deal with the uncertain nature of connectivity of pervasive devices and networks, and the trial-error (subjunctive) nature of the processes and the programs used in E-commerce, robotics and multi-player gaming.",
    "actual_venue": "Kes Journal"
  },
  {
    "abstract": "Considering the feature of remote sensing images, we put forward a remote sensing image classification algorithm based on Hopfield neural network. First, the function and principle of Hopfield neural network is described in this paper. Then based on the common model of Hopfield neural network, the image classification algorithm using Hopfield neural network is realized and experimental results show that its precision is superior to that of the conventional maximum likelihood classification algorithm.",
    "actual_venue": "Isnn"
  },
  {
    "abstract": "Electrostatic quantum dots fabricated in nanometer-scale CMOS are believed to be the best candidates for wide implementation of quantum bits (qubits) in practical quantum computers. In this work, we look at a modelling of qubits from the perspective of circuit- and system-level engineering. We show that in the case of a finite double-well potential, a closed-form solution can be found and it displays transition oscillations. Such oscillations between two levels are known as occupancy oscillations and they correspond to an electron transferring between these levels. We propose an equivalent electrical circuit to model them.",
    "actual_venue": "Ieee International Conference On Electronics, Circuits And Systems"
  },
  {
    "abstract": "Let (Z,=) be a chain and let (Z',=') be its dual chain. Then the length l(Z) of (Z,=) is the least upper bound of all cardinal numbers which can be order-embedded into (Z,=) or (Z',='). In particular, a chain is said to be short if its length is not greater than the smallest infinite cardinal. In this paper we shall prove that the cardinality |Z| of a chain (Z,=) cannot be smaller than l(Z) and not greater than 2l(Z). The inequality |Z|=2l(Z) is an immediate consequence of a general theorem which combines the structure of a chain with its length. In case of a short chain it follows that its structure may be rather complicated but that its cardinality cannot be greater than the cardinality of the real line.",
    "actual_venue": "Order"
  },
  {
    "abstract": "Monitoring of residences and businesses can be effectively performed using machine learning algorithms. As sensors and devices used for monitoring become more complex, having humans process the information to detect intrusions would be expensive and difficult to scale. We propose an automated home/business monitoring system which resides on edge servers performing online learning on streaming data coming from homes and businesses in the neighborhood. The edge servers run Open-NetVM, a Network Function Virtualization (NFV) platform, and host multiple machine learning applications instantiated on demand. This enables us to serve a set of customers in the neighborhood on a timely basis, permitting customization and learning of the behavior of each home. We combine the results of the multiple classifiers, with each classifier examining a distinct feature related to a distinct sensor, to finally infer whether the entry is a normal one or an intrusion. Our results show that our system is able to classify intrusions better than basing the decision on a single classifier, thus reducing false alarms. We have also shown that our system can effectively scale and monitor thousands of homes.",
    "actual_venue": "Ieee International Conference On Network Protocols"
  },
  {
    "abstract": "Ambiguous figure-ground images, mostly represented as binary images, are fascinating as they present viewers a visual phenomena of perceiving multiple interpretations from a single image. In one possible interpretation, the white region is seen as a foreground figure while the black region is treated as shapeless background. Such perception can reverse instantly at any moment. In this paper, we investigate the theory behind this ambiguous perception and present an automatic algorithm to generate such images. We model the problem as a binary image composition using two object contours and approach it through a three-stage pipeline. The algorithm first performs a partial shape matching to find a good partial contour matching between objects. This matching is based on a content-aware shape matching metric, which captures features of ambiguous figure-ground images. Then we combine matched contours into a compound contour using an adaptive contour deformation, followed by computing an optimal cropping window and image binarization for the compound contour that maximize the completeness of object contours in the final composition. We have tested our system using a wide range of input objects and generated a large number of convincing examples with or without user guidance. The efficiency of our system and quality of results are verified through an extensive experimental study.",
    "actual_venue": "Ieee Trans Vis Comput Graph"
  },
  {
    "abstract": "Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques.",
    "actual_venue": "Ieee International Symposium On Software Reliability Engineering"
  },
  {
    "abstract": "Mesh texture smoothing (MTS) seeks to smooth out the detailed appearances in noise-free surfaces, while preserving the intrinsic geometric properties. However, the intricacy of mesh texture patterns commonly results in side-effects, including remnant textures, improperly-smoothed structures and distorted shapes. We propose a new joint low-rank matrix recovery (J-LRMR) model to the problem of MTS with the properties of texture smoothing, structure preservation, and shape fitting. J-LRMR benefits from our three key observations: (1) local surface patches sharing approximate geometric properties always exist within an original surface (OS); (2) the OS comprises three geometric properties, i.e., details, structures, and the smooth-varying shape, where the structures construct the step-edge surface (SS), but more importantly the later two components contribute to forming the underlying surface (US); (3) the US normals can be defined as signals over both the OS and SS. By exploring the three observations, we first collect similar patches together by the local iterative closest point technique on the OS, and concurrently optimize the OS to produce the SS by ℓ0 normal minimization. Second, we pack the normals on the similar patches of both the OS and SS into the patch-group matrix (PGM) for rank analysis. The rank of sub-PGM from the OS is high due to suffering from details (textures), while the rank of sub-PGM from the SS is low but does not have a smooth shape. Third, we formulate a structure-guided shape-preserving optimization framework by non-local low-rank matrix recovery to produce the normals of US containing both the structures and the smooth-varying shape. Finally, we update the vertex positions of OS to fit the recovered normals of US by Poisson reconstruction. We have verified our method on a variety of surfaces with abundant details, and compared it with several state-of-the-art methods. Both visual and quantitative comparisons show that our method can better remove mesh textures with a minimal disturbance of the underlying surface.",
    "actual_venue": "Computer-Aided Design"
  },
  {
    "abstract": "The increasing use of biometrics has given rise to new privacy concerns. Biometric encryption systems have been proposed in order to alleviate such concerns: rather than comparing the biometric data directly, a key is derived from these data and subsequently knowledge of this key is proved. One specific application of biometric encryption is the use of biometric sketches: in this case biometric template data are protected with biometric encryption. We address the question whether one can undermine a user's privacy given access to biometrically encrypted documents, and more in particular, we examine if an attacker can determine whether two documents were encrypted using the same biometric. This is a particular concern for biometric sketches that are deployed in multiple locations: in one scenario the same biometric sketch is deployed everywhere; in a second scenario the same biometric data is protected with two different biometric sketches. We present attacks on template protection schemes that can be described as fuzzy sketches based on error-correcting codes. We demonstrate how to link and reverse protected templates produced by code-offset and bit-permutation sketches.",
    "actual_venue": "Ieee Symposium On Security And Privacy"
  },
  {
    "abstract": "Hop Count based distance estimation is an important element for localization of devices in mobile ad hoc networks. Deriving distance estimates from hop counts is prone to error, especially in networks with low density. This paper shows that mobility can affect the accuracy of hop count based distance estimation. Two types of error are defined to describe and analyze the source of underestimation and overestimation of distances in a mobile ad hoc network. Different movement patterns are examined to get an insight of their impact on the hop counts and the estimated distances accordingly. Our experiments and analysis indicate that mobility can have a positive effect on the accuracy of distance estimates which results from a combination of asynchronous computation of hop counts and mobility of the nodes. At the same time, this positive effect can turn into a negative one with increasing mobility. Therefore, we determine characteristics, such as direction, speed, and similarity in movements of neighbors which are responsible for the disparity in the influences of the investigated mobility patterns. A study of these properties is presented and their individual effect is explained in detail. The difference between mobility and density induced error is discussed and their individual adverse effect is weighted against each other. In addition, we introduce a modified algorithm to determine hop counts which is designed to mitigate the effect of mobility. Two indicators are presented to identify and characterize the mobility of devices in a decentralized way.",
    "actual_venue": "Ad Hoc Networks"
  },
  {
    "abstract": "In this paper, we propose a normalized difference vector (NDV) for texture representation. Compared to local-binary-pattern-based descriptors, the proposed NDV takes full advantage of the local difference, and the size can be extended flexibly to cover a large local region. We further employ the bag-of-words model to integrate the local descriptors into a global feature representation of an image....",
    "actual_venue": "Ieee Transactions On Multimedia"
  },
  {
    "abstract": "By adopting the Graphical Kernel System (GKS), groups who manipulate pixelated images can take advantage of device independent graphics without giving up the functions which have traditionally been hardware dependent. Most of these functions, including image I/O, zoom, pan, lookup table manipulation, and cursor reading, are supported within GKS; several other functions, such as the use of multiple image planes and multiple look up tables, are accomodated by the GKS Escape and the Generalized Drawing Primitive (GDP). Because GKS has powerful inquire capabilities, it's possible to tightly customize applications code to a particular hardware display device. The inquire functions also can be used by an applications program to determine the hardware display size and thus avoid resampling of a pixelated image. GKS thoroughly separates applications programs from device- dependencies; however, device drivers must still be written. The time and effort of writing device drivers can be largely eliminated by using a single programmable device driver, which is tailored to each device by a GRAPHCAP configuration file, in much the same way as TERMCAP is used by Berkeley Unix implementations.",
    "actual_venue": "Siggraph"
  },
  {
    "abstract": "In this research, we propose a novel parallelizable architecture for the optimization of various sound synthesis parameters. The architecture employs genetic algorithms to match the parameters of different sound synthesizer topologies to target sounds. The fitness function is evaluated in parallel to decrease its convergence time. Based on the proposed architecture, we have implemented a framework using the SuperCollider audio synthesis and programming environment and conducted several experiments. The results of the experiments have shown that the framework can be utilized for accurate estimation of the sound synthesis parameters at promising speeds.",
    "actual_venue": "Evoapplications"
  },
  {
    "abstract": "This paper describes a model that explicitly represents the declarative and behavioral knowledge of a goal-driven agent. The developed declarative and the behavioral models allow an agent to reason about itself, other agents, and the environment. The declarative model specifies data and services (capabilities) assigned to an agent while the behavioral model specifies the execution model of an agent (defined as states, transitions between states, and events affecting the transitions). An Extended Statechart (ESC) is used as the execution model. To maintain these models, a self-contained computational module called the Perspective Modeler is proposed and incorporated into the Sensible Agent Architecture. With the Perspective Modeler, a Sensible Agent has the capability to model itself, other agents, and the environment to generate more desirable behaviors. The Perspective Modeler is implemented as a CORBA© object, as demonstrated successfully at the AAAI''98 Intelligent Systems Demonstration session held in Madison, WI.",
    "actual_venue": "Agents"
  },
  {
    "abstract": "Probe-level data from Affymetrix GeneChips can be summarized in many ways to produce probe-set level gene expression measures (GEMs). Disturbingly, the different approaches not only generate quite different measures but they could also yield very different analysis results. Here, we explore the question of how much the analysis results really do differ, first at the gene level, then at the biological process level. We demonstrate that, even though the gene level results may not necessarily match each other particularly well, as long as there is reasonably strong differentiation between the groups in the data, the various GEMs do in fact produce results that are similar to one another at the biological process level. Not only that the results are biologically relevant. As the extent of differentiation drops, the degree of concurrence weakens, although the biological relevance of findings at the biological process level may yet remain.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "Features of the diffusion-time dependence of the diffusion-weighted magnetic resonance imaging (MRI) signal provide a new contrast that could be altered by numerous biological processes and pathologies in tissue at microscopic length scales. An anomalous diffusion model, based on the theory of Brownian motion in fractal and disordered media, is used to characterize the temporal scaling (TS) characteristics of diffusion-related quantities, such as moments of the displacement and zero-displacement probabilities, in excised rat hippocampus specimens. To reduce the effect of noise in magnitude-valued MRI data, a novel numerical procedure was employed to yield accurate estimation of these quantities even when the signal falls below the noise floor. The power-law dependencies characterize the TS behavior in all regions of the rat hippocampus, providing unique information about its microscopic architecture. The relationship between the TS characteristics and diffusion anisotropy is investigated by examining the anisotropy of TS, and conversely, the TS of anisotropy. The findings suggest the robustness of the technique as well as the reproducibility of estimates. TS characteristics of the diffusion-weighted signals could be used as a new and useful marker of tissue microstructure.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "•Handheld devices, social networks and projections formed an information ecology.•Combine physical and digital tools to support collaborative design activities.•Understand interactions among learners, tasks and tools via distributed cognition.•Use DICOT to build a detailed account of activities in the information ecology.•Reveal practical insights on the effectiveness of the information ecology.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "Obtaining the optimal extrusion process parameters by integration of optimization techniques was crucial and continuous engineering task in which it attempted to minimize the tool load. The tool load should be minimized as higher extrusion forces required greater capacity and energy. It may lead to increase the chance of part defects, die wear and die breakage. Besides, optimization may help to save the time and cost of producing the final product, in addition to produce better formability of work material and better quality of the finishing product. In this regard, this study aimed to determine the optimal extrusion process parameters. The minimization of punch load was the main concern, in such a way that the structurally sound product at minimum load can be achieved. Minimization of punch load during the extrusion process was first formulated as a nonlinear programming model using response surface methodology in this study. The established extrusion force model was then taken as the fitness function. Subsequently, the analytical approach and metaheuristic algorithms, specifically the particle swarm optimization, cuckoo search algorithm (CSA) and flower pollination algorithm, were applied to optimize the extrusion process parameters. Performance assessment demonstrated the promising results of all presented techniques in minimizing the tool loading. The CSA, however, gave more persistent optimization results, which was validated through statistical analysis.",
    "actual_venue": "Neural Computing And Applications"
  },
  {
    "abstract": "The usual model for (Poissonian) linear birth–death processes is extended to multiple birth–death processes with fractional birth probabilities in the form λi(Δt)α+o((Δt)α, 0<α<1. The probability generating function for the time dependent population size is provided by a fractional partial differential equation. The solution of the latter is obtained and comparison with the usual model is made. The probability of ultimate extinction is obtained. One considers the special case of fractional Poissonian processes with individual arrivals only, and then one outlines basic results for continuous processes defined by fractional Poissonian noises. The key is the Taylor’s series of fractional order f(x+h)=Eα(hαDxα)f(x), where Eα(·) is the Mittag–Leffler function, and Dxα is the modified Riemann–Liouville fractional derivative, as previously introduced by the author.",
    "actual_venue": "Journal Of The Franklin Institute"
  },
  {
    "abstract": "With the introduction of the automotive functional safety standard ISO 26262, several challenges related to the representation of dependability information has emerged. This paper addresses how safety requirements can be formalized; which is mandatory for high-integrity requirements. Particular focus is given to asymmetric failures. Such a failure can be caused by a communication fault, and implies that data in a distributed system will be inconsistent among system outputs or within the system (incorrect, corrupt or omitted, etc.). We investigate along two lines; 1) The EAST-ADL automotive architecture description language is extended with a capability to represent asymmetric faults and failures. 2) The Compute-Distribute Results (CDR) pattern is introduced to assist reasoning about distributed systems, in particular potential inconsistencies. We show how this can support architectural decisions regarding selection of communication topology and communication technology for a given distributed system. A brake-by-wire application and FlexRay bus are analysed to illustrate the concepts.",
    "actual_venue": "Safecomp"
  },
  {
    "abstract": "In today's fast paced military operational environment, vast amounts of information must be sorted out and fused not only to allow commanders to make situation assessments, but also to support the generation of hypotheses about enemy force disposition and enemy intent. Current information fusion technology has the following two limitations. First, current approaches do not consider the battlefield context as a first class entity. In contrast, we consider situational context in terms of terrain analysis and inference. Second, there are no integrated and implemented models of the high-level fusion process. This paper describes the HiLIFE (High-Level Information Fusion Environment) computational framework for seamless integration of high levels of fusion (levels 2, 3 and 4). The crucial components of HiLIFE that we present in this paper are: (1) multi-sensor fusion algorithms and their performance results that operate in heterogeneous sensor networks to determine not only single targets but also force aggregates, (2) computational approaches for terrain-based analysis and inference that automatically combine low-level terrain features (such as forested areas, rivers, etc.) and additional information, such as weather, and transforms them into high-level militarily relevant abstractions, such as NO-GO, SLOW-GO areas, avenues of approach, and engagement areas, (3) a model for inferring adversary intent by mapping sensor readings of opponent forces to possible opponent goals and actions, and (4) sensor management for positioning intelligence collection assets for further data acquisition. The HiLIFE framework closes the loop on information fusion by specifying how the different components can computationally work together in a coherent system. Furthermore, the framework is inspired by a military process, the Intelligence Preparation of the Battlefield, that grounds the framework in practice. HiLIFE is integrated with a distributed military simulation system, OTBSAF, and the RETSINA multi-agent infrastructure to provide agile and sophisticated reasoning. In addition, the paper presents validation results of the automated terrain analysis that were obtained through experiments using military intelligence Subject Matter Experts (SMEs).",
    "actual_venue": "Information Fusion"
  },
  {
    "abstract": "In this paper, a complete methodology is detailed for the design and analysis of robust gain-scheduled fighter aircraft flight control laws. The proposed approach consists of three steps. A set of locally robust controllers is first generated using LMI optimization. The global controller is then obtained by a standard linear interpolation and transformed into LFT format. Finally, a validation step is achieved using both LTI and LTVan alysis techniques, for which specific methods and tools have been developed.",
    "actual_venue": "European Journal Of Control"
  },
  {
    "abstract": "Computer anxiety, as defined and operationalized in the human-computer studies literature, has been synonymous with negative thoughts and attitudes about the use of computers. This approach, together with correlational analyses that have formed the mainstay of research on computer anxiety, invokes two important points. First, it can be argued that computer anxiety, by definition, implies an attitude that is indicative of an extremity of thoughts and dispositions. Second, if one were to reject the strictly clinical definition of computer anxiety and adopt the more traditional measurements as well as the attendant statistical analyses based on the full sample, there is the clear possibility of dilution of statistically significant relations by observations in the middle range. This paper adopts Weil and Rosen's [Weil, M. M., & Rosen, L. D. (1995). The psychological impact of technology from a global perspective: a study of technological sophistication and computer anxiety in university students from twenty-three countries. Computers in Human Behavior, 11(1), 95-133] definition of computer anxiety in terms of anxiety about interactions with computers and negative global attitudes, and ''negative cognitions'' or ''self-critical internal dialogs''. Using data from a study of 242 graduate and undergraduate students at a small private university in Western New York, the sample is segmented into high and low computer anxiety groups. These groups are then tested for significant differences in individual characteristics, including the Big Five personality dimensions, computer experience, math and verbal skills, and cognitive orientation. It is found that three of the personality dimensions (Neuroticism, Openness, and Agreeableness), one aspect of cognitive orientation (Flexibility), and verbal skills show statistically significant differences between the two groups. The evidence with respect to math skills and computer experience is mixed. Interpretation of results is presented. Limitations, delimitations, and potential directions for future research are discussed.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "This article describes properties, design and implementation of PATOP (Performance Analysis TOol for Parallel systems). It was developed as part of the TOPSYS project at Technical University of Munich. Its purpose is to make the efficiency of parallel programs visible, in order to find performance bottlenecks (performance debugging). PATOP was implemented on the IPSC/2 Hypercube. It supports measurements on the system's hardware structure (whole system, processor nodes, communication links) and on objects of the programming model (Task, Mailbox, Semaphore). Measurements can be started and evaluated during program execution (on-line). User interaction is menu driven and measurement results are displayed graphically.",
    "actual_venue": "Conpar"
  },
  {
    "abstract": "Recent technology advances in information technology and other engineering fields provide new opportunities for research and practices in precision agriculture. Using these technologies, field operators can collect voluminous data from a heterogeneous network of devices that provides real-time and multiple-factor measurement of field conditions with much finer granularity. A major challenge in precision agriculture today is how to analyze these data efficiently and use them effectively to improve farming decisions. We propose an extensible and integrated software architecture for data analysis and visualization in precision agriculture, with three distinctive features: (a) a meta-data-model-based data importation component capable of importing data in various formats from a variety of devices in different settings; (b) a data-flow-driven data processing subsystem in which a user can define his/her own data processing workflows and add custom-defined data processing operators for a specific application; (c) an overall architecture design following a client-server model that supports a variety of client devices, including mobile devices such as the Apple iPad. We implemented the software architecture in an open-source decision support tool for precision agriculture. The tool has been successfully used in a USDA-sponsored project on canopy management for specialty crops.",
    "actual_venue": "IRI"
  },
  {
    "abstract": "We develop numerical schemes for solving the isothermal compressible and incompressible equations of fluctuating hydrodynamics on a grid with staggered momenta. We develop a second-order accurate spatial discretization of the diffusive, advective, and stochastic fluxes that satisfies a discrete fluctuation-dissipation balance and construct temporal discretizations that are at least second-order accurate in time deterministically and in a weak sense. Specifically, the methods reproduce the correct equilibrium covariances of the fluctuating fields to the third (compressible) and second (incompressible) orders in the time step, as we verify numerically. We apply our techniques to model recent experimental measurements of giant fluctuations in diffusively mixing fluids in a microgravity environment [A. Vailati et al., Nat. Comm., 2 (2011), 290]. Numerical results for the static spectrum of nonequilibrium concentration fluctuations are in excellent agreement between the compressible and incompressible simulations and in good agreement with experimental results for all measured wavenumbers.",
    "actual_venue": "Multiscale Modeling And Simulation"
  },
  {
    "abstract": "In this paper we consider the problem of learning hidden independent cascade social networks using exact value injection queries. These queries involve activating and suppressing agents in the target net- work. We develop an algorithm that optimally learns an arbitrary social network of size n using O(n2) queries, matching the information theo- retic lower bound we prove for this problem. We also consider the case when the target social network forms a tree and show that the learn- ing problem takes (nlog(n)) queries. We also give an approximation algorithm for finding an influential set of nodes in the network, without resorting to learning its structure. Finally, we discuss some limitations of our approach, and limitations of path-based methods, when non-exact value injection queries are used.",
    "actual_venue": "Theor Comput Sci"
  },
  {
    "abstract": "Polycrystalline thin films of Zn 1− x Co x O with different cobalt (Co) content were grown on indium tin oxide (ITO) substrates by cathodic electrodeposition technique and subsequently annealed in air at 400 °C. The effect of annealing in their structural, optical and chemical properties has been characterized by X-ray diffraction (XRD), energy-dispersive spectroscopy (EDS), X-ray photoelectron spectroscopy (XPS), Raman scattering and optical spectroscopy. Our measurements indicate that moderate annealing increases the crystal quality of the films. The films are highly transparent in the visible range and evidence an increase of the band gap and of the intensity of three typical Co absorption bands in the visible with the amount of Co. Thermal annealing produces an increase of the intensity of the Co 2+ -related absorption bands revealing that higher amount of Co atoms are occupying Zn sites.",
    "actual_venue": "Microelectronics Journal"
  },
  {
    "abstract": "Novel energy-efficient modulation technique and circuits for transceivers operating in body sensor networks (BSNs) are described and investigated. They are obtained as a result of the development of differential binary phase shift keying (DBPSK) and named alternating quadratures DBPSK (AQ-DBPSK). While keeping the noise immunity and simplicity of synchronization and realization of DBPSK, AQ-DBPSK ...",
    "actual_venue": "Ieee Journal Of Emerging And Selected Topics In Circuits And Systems"
  },
  {
    "abstract": "This paper presents a novel method of analyzing morphosemantic patterns in language to the detect cyberbullying, or frequently appearing harmful messages and entries that aim to humiliate other users. The morphosemantic patterns represent a novel concept, with the assumption that analyzed elements can be perceived as a combination of morphological information, such as parts of speech, and semantic information, such as semantic roles, categories, etc. The patterns are further automatically extracted from the data containing harmful entries (cyberbullying) and non-harmful entries found on the informal websites of Japanese high schools. These website data were prepared and standardized by the Human Rights Center in Mie Prefecture, Japan. The patterns extracted in this way are further applied to a document classification task using the provided data in 10-fold cross-validation. The results indicate that morphosemantic sentence representation can be considered useful in the task of detecting the deceptive and provocative language used in cyberbullying.",
    "actual_venue": "Journal Of Advanced Computational Intelligence And Intelligent Informatics"
  },
  {
    "abstract": "Metallization multilayers on the back side of a power device were focused in this study. Si wafers coated with high melting point metals were exposed at 300 degrees C for 300 h to investigate diffusion condition of the metallization layer. We developed and examined the thermal stability oldie bonding material (Au paste) including sub-micrometer-sized Au particles. Auger electron spectroscopy was applied to observe the atomic composition of the multilayers in depth direction after the high temperature aging. Surface morphology was observed using optical microscope and scanning electron microscope. While atomic composition on Ti/Au changed drastically after the high temperature aging, other multilayers maintained their metallization composition. However, the surface morphology was slightly changed on Ti/Ru/Au, W/Au, and Ta/Au. Bond strength on the Ti/Pt/Au kept over 40 MPa with unified bonding layer after exposing at 300 degrees C for 1000 h. (C) 2011 Elsevier Ltd. All rights reserved.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Without Abstract",
    "actual_venue": "Disco"
  },
  {
    "abstract": "Network Management applications using distributed software agents require secure techniques for agent distribution and communication. Although there are several popular mobile agent environments written in the Java programming language that contend to offer these functions, these packages are generally heavy weight and do not provide a framework for authenticating agent responsibilities within agent systems. This paper describes an agent distribution and communication framework for Java-based software agents. The framework has several key features: 1) a secure mechanism for dispatching agents to selected nodes from secured agent repositories. 2) an operating environment for each agent that depends upon its role-based authenticated security level. 3) object-based, peer-to-peer and peer-to-many communication. 4) provision for encrypted communication. 5) a low overhead distributed naming directory of agents populating the network. 6) proactive agent monitoring for secure operations. 7) tools for reflective monitoring of agent contents and communication. 8) a lightweight environment that is easy to use while keeping the agents small in size.",
    "actual_venue": "Integrated Network Management : Distributed Management For The Networked Millennium"
  },
  {
    "abstract": "We state a set of criteria that has guided the development of a metric system for measuring the quality of a largescale software product. This metric system uses the flow of information within the system as an index of system interconnectivity. Based on this observed interconnectivity, a variety of software metrics can be defined. The types of software quality features that can be measured by this approach are summarized. The data-flow analysis techniques used to establish the paths of information flow are explained and illustrated. Finally, a means of integrating various metrics and models into a comprehensive software development environment is discussed. This possible integration is explained in terms of the Gandalf system currently under development at Carnegie-Mellon University.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Memristive devices with a simple structure are not only very small but also very versatile, which makes them an ideal candidate used for the next generation computing system in the post-Si era. The working mechanism of the devices and a family of nanodevices built based on this working mechanism are introduced first followed by some proposed applications of these novel devices. The promises and challenges of these devices are then discussed, together with the significant progresses made recently in dealing with these challenges.",
    "actual_venue": "Jetc"
  },
  {
    "abstract": "In a \"managed network\" where the location / point of attachment of network elements (NE) are entirely planned and the NEs are tightly controlled by an element management system, the roll-out of new NEs or changes to the NE HW and SW cause considerable overhead. The overhead is due in large part due to the concurrent processes at the network operator, NE factory and roll-out field service which require extensive interaction. An autoconfiguration method is proposed which is able to decouple the individual roll-out processes, i.e., changes in the NE configuration and association of NE with certain locations can be changed at any time in the roll-out process without incurring additional overhead. This is achieved by an autoconfiguration support system under control of the operator where all data relevant to a to-be-rolled-out NE is stored. When the NE is booting it can retrieve the required configuration and SW which is needed to fulfil the required network function at its point of attachment to the network. Finally it is shown that in a managed network it is advantageous to use the geo-location of a network element as its key identifier in network planning and rollout, rather than a hardware identifier as usually employed in DHCP-based autoconfiguration. This is the case because in a \"managed network\" it is important that autoconfiguration support is provided for a certain network function at a certain location, rather than autoconfiguration of a certain hardware which is attached at an arbitrary location.",
    "actual_venue": "Integrated Network Management"
  },
  {
    "abstract": "Cloud computing enables elastic resource provisioning on demand and removes the boundaries of resources' physical locations. The number of cloud-based services is on the rise due to the growing interest from both providers and consumers. These services are characterized by a large number of features or properties, which makes the automatic service selection and deployment challenging. This paper proposes QuRAM Recommender, a cloud infrastructure service recommender framework based on case-based reasoning (CBR) that supports effective service selection. QuRAM Recommender supports decision making that accommodates the customer's preferences and feedback. We show the feasibility of our approach through a prototype implementation that elaborates on the main features of our system. The experimental results suggest that case-based reasoning is a viable option for recommending cloud services that best fit the customer's requirements.",
    "actual_venue": "Iccac"
  },
  {
    "abstract": "This paper presents power-control strategies of a grid-connected hybrid generation system with versatile power transfer. The hybrid system is the combination of photovoltaic (PV) array, wind turbine, and battery storage via a common dc bus. Versatile power transfer was defined as multimodes of operation, including normal operation without use of battery, power dispatching, and power averaging, which enables grid- or user-friendly operation. A supervisory control regulates power generation of the individual components so as to enable the hybrid system to operate in the proposed modes of operation. The concept and principle of the hybrid system and its control were described. A simple technique using a low-pass filter was introduced for power averaging. A modified hysteresis-control strategy was applied in the battery converter. Modeling and simulations were based on an electromagnetic-transient-analysis program. A 30-kW hybrid inverter and its control system were developed. The simulation and experimental results were presented to evaluate the dynamic performance of the hybrid system under the proposed modes of operation.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Existing packet combining schemes, implemented either by hardware or software, developed for hot-spot congestion-control in multiprocessors are not applicable for use in telecommunication systems. The authors propose a hot-spot prevention scheme which exploits the structural characteristics of buffered delta networks to regulate the traffic. This scheme prevents the tree saturation by controlling the flow of the packets into the network and ensures that the hot-spot traffic can be supported with little degradation to the throughput of the uniform traffic, even when more than one hot-spot exists. The scheme retains all the desirable properties, including packet order preserving, of buffered delta networks, and is shown by simulations to be highly effective under either light or heavy hot-spot traffic. Since no a priori knowledge of the traffic patterns is required, the proposed scheme can be applied equally well to buffered delta networks in multiprocessors and telecommunication systems",
    "actual_venue": "Bal Harbour, Fl"
  },
  {
    "abstract": "State-of-the-art theta-subsumption engines like Django (C) and Resumer2 (Java) are implemented in imperative languages. Since theta-subsumption is inherently a logic problem, in this paper we explore how to e ffi ciently implement it in Prolog. theta-subsumption is an important problem in computational logic and particularly relevant to the Inductive Logic Programming (ILP) community as it is at the core of the hypotheses coverage test which is often the bottleneck of an ILP system. Also, since most of those systems are implemented in Prolog, they can immediately take advantage of a Prolog based theta-subsumption engine. We present a relatively simple (approximate to 1000 lines in Prolog) but e ffi cient and general theta-subsumption engine, Subsumer. Crucial to Subsumer's performance is the dynamic and recursive decomposition of a clause in sets of independent components. Also important are ideas borrowed from constraint programming that empower Subsumer to e ffi ciently work on clauses with up to several thousand literals and several dozen distinct variables. Using the notoriously challenging Phase Transition dataset we show that, cputime wise, Subsumer clearly outperforms the Django subsumption engine and is competitive with the more sophisticated, state-of-the-art, Resumer2. Furthermore, Subsumer's memory requirements are only a small fraction of those engines and it can handle arbitrary Prolog clauses whereas Django and Resumer2 can only handle Datalog clauses.",
    "actual_venue": "Leibniz International Proceedings In Informatics"
  },
  {
    "abstract": "Linda is an elegant parallel and distributed programming model. It is based on a shared associative memory, structured in tuples. We show in this paper that this model suffers from the false matching phenomenon. We explain under which conditions this problem occurs, we examine the solutions already proposed to solve it, and we show why they are not sufficient. In this framework, our goal is to propose an extension to the Linda model in order to eliminate the false matching phenomenon. This model驴Linda with bound types or B-Linda驴suitable for modern programming paradigms, adds an extended-type notion into the basic Linda model. It is first introduced in an informal manner, then we present an implementation of it. Some formal aspects are specified in the appendix: The definition of the model's elements and operational semantics.",
    "actual_venue": "Parallel and Distributed Systems, IEEE Transactions  "
  },
  {
    "abstract": "Biometric person identity authentication is gaining more and more attention. The authentication task performed by an expert is a binary classification problem: reject or accept identity claim. Combining experts, each based on a different modality (speech, face, fingerprint, etc.), increases the performance and robustness of identity authentication systems. In this context, a key issue is the fusion of the different experts for taking a final decision (i.e., accept or reject identity claim). We propose to evaluate different binary classification schemes (support vector machine, multilayer perceptron, C4.5 decision tree, Fisher's linear discriminant, Bayesian classifier) to carry on the fusion. The experimental results show that support vector machines and Bayesian classifier achieve almost the same performances, and both outperform the other evaluated classifiers.",
    "actual_venue": "Ieee Transactions On Neural Networks"
  },
  {
    "abstract": "Despite the portability and platform independence of Java programs, their performance depends on the threading mechanisms of the host operating system. In this paper, we measure the performance of Java threads for two different multithreading implementations, Linux Thread and Green Thread, using PersonalJava (TM) on a Linux-based platform. The experimental results show the relative strengths and weaknesses of the two threading mechanisms with respect to synchronization overhead, I/O efficiency, and thread control.",
    "actual_venue": "Inf Process Lett"
  },
  {
    "abstract": "An accurately evaluate about the zone number and position of the gas zone is put forward in this paper. It provides the reliable basis for developing natural gas through synthetically analyzing the result of carrying on wavelet de-noising and wavelet package denoising disposal simultaneously to the density porosity curve and neutron porosity curve.",
    "actual_venue": "WAA"
  },
  {
    "abstract": "Recent developments in ad-hoc wireless networks, single-chip embedded systems and the wide-spread availability of Internet in homes has made it possible to remotely monitor status of external data sources and home appliances using small, independent wireless devices. This paper presents an open-architecture and an implementation of such a system called the IPnfopods System. This architecture is based on a Zigbee-based controller. The architecture allows multiple family members to simultaneously monitor their home appliances as well as external Internet resources using cheap, stand-alone hand-held mobile wireless devices. The proposed architecture can be easily integrated with existing smart-home systems. The architecture is implemented and demonstrated in the context of an existing Java-based smarthome architecture.",
    "actual_venue": "Ieee Trans Consumer Electronics"
  },
  {
    "abstract": "The polymorphic type assignment system F2 is the type assignment counterpart of Girard's and Reynolds' (1972) system F. Though introduced in the early seventies, both the type inference and the type checking problems for F2 remained open for a long time. Recently, an undecidability result was announced. Consequently, it is considerably interesting to find decidable restrictions of the system. We show a bounded type inference and a bounded type checking algorithm, both based on the study of the relationship between the typability of a term and the typability of terms that “properly” η-reduce to it",
    "actual_venue": "Paris"
  },
  {
    "abstract": "Outlier detection is a well studied problem in various fields. The unique challenges of wireless sensor networks make this problem especially challenging. Sensors can detect outliers for a plethora of reasons and these reasons need to be inferred in real time. Here, we present a new communication technique to find outliers in a wireless sensor network. Communication is minimized through controlling sensor when sensors are allowed to communicate. At the same time, minimal assumptions are made about the nature of the data set as to minimize the loss of generality in the architecture.",
    "actual_venue": "Mobile Data Management"
  },
  {
    "abstract": "A new parametric inversion technique to locate cylindrical structures has been developed for borehole radar cross-hole measurements. The technique calculates prediction errors, focusing on curve shapes of first-arrival times, and explicitly uses known parameters on a target. Two schemes to compare the measured first-arrival time curves with the calculated ones are proposed. One is by taking the er...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "This paper describes the active, object-oriented database system SAMOS being developed as a research prototype. Its main approach is the integration of rules in the sense of active database systems into a general object-oriented data model. Our effort is also focussed on integrating the rule system with transaction processing in a meaningful way.",
    "actual_venue": "Dbpl3 Proceedings Of The Third International Workshop On Database Programming Languages : Bulk Types And Persistent Data: Bulk Types And Persistent Data"
  },
  {
    "abstract": "Concurrent constraint programming is a simple but powerful frame- work for computation based on four basic computational ideas: concurrency (mul- tiple agents are simultaneously active), communication (they interact via the mono- tonic accumulation of constraints on shared variables), coordination (the presence or absence of information can guard evolution of an agent), and localization (each agent has access to only a finite, though dynamically varying, number o f variables, and can create new variables on the fly). Unlike other foundational models of con- currency such as CCS, CSP, Petri nets and the ~r-calculns, such flexibility is al- ready made available within the context of determinate computation. This allows the development of a rich and tractable theory of concurrent processes within the context of which additional computational notion such as indeterminacy, reactiv- ity, instantaneous interrupts and continuous (dense-time) autonomous evolution have been developed. We survey the development of some of these extensions and the relationships be- tween their semantic models.",
    "actual_venue": "Concur"
  },
  {
    "abstract": "Some applications utilizing Grid computing infrastructure require the simultaneous allocation of resources, such as compute servers, networks, memory, disk storage and other specialized resources. Collaborative working and visualization is one example of such applications. In this c ontext, Quality of Service (QoS) is related to Grid services, and not just to the network connecting these services. With the emerging interest in service-oriented Grids, resources may be advertised and traded as services based on a Service Level Agreement (SLA). Such a SLA must include both general and technical specifications, including pricing po licy and properties of the resources required to execute the service - to ensure QoS requirements are satisfied. A QoS adap tation algorithm is presented to enable the dynamic adjustment of behavior of an application based on changes in the pre-defined SLA. The approach is particularly useful if workload or network traffic changes in unpredictable ways during an active session. The proposed QoS adaptation scheme is used to compensate for QoS degradation and optimize resource utilization, by increasing the number of requests managed over a particular time.",
    "actual_venue": "Middleware Workshops"
  },
  {
    "abstract": "Reverse Nearest Neighbor (RNN) queries are of particular interest in a wide range of applications such as decision support systems, profile based marketing, data streaming, document databases, and bioinformatics. The earlier approaches to solve this problem mostly deal with two dimensional data. However most of the above applications inherently involve high dimensions and high dimensional RNN problem is still unexplored. In this paper, we propose an approximate solution to answer RNN queries in high dimensions. Our approach is based on the strong correlation in practice between k-NN and RNN. It works in two phases. In the first phase the k-NN of a query point is found and in the next phase they are further analyzed using a novel type of query Boolean Range Query (BRQ). Experimental results show that BRQ is much more efficient than both NN and range queries, and can be effectively used to answer RNN queries. Performance is further improved by running multiple BRQ simultaneously. The proposed approach can also be used to answer other variants of RNN queries such as RNN of order k, bichromatic RNN, and Matching Query which has many applications of its own. Our technique can efficiently answer NN, RNN, and its variants with approximately same number of I/O as running a NN query.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "This is the third of three papers which develop various fundamental aspects of the theory of ditopological texture spaces and relate this work with the theory of fuzzy topological spaces.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "The growing popularity of wearable devices is leading to new ways to interact with the environment, with other smart devices, and with other people. Wearables equipped with an array of sensors are able to capture the owner’s physiological and behavioural traits, thus are well suited for biometric authentication to control other devices or access digital services. However, wearable biometrics have substantial differences from traditional biometrics for computer systems, such as fingerprints, eye features, or voice. In this article, we discuss these differences and analyse how researchers are approaching the wearable biometrics field. We review and provide a categorization of wearable sensors useful for capturing biometric signals. We analyse the computational cost of the different signal processing techniques, an important practical factor in constrained devices such as wearables. Finally, we review and classify the most recent proposals in the field of wearable biometrics in terms of the structure of the biometric system proposed, their experimental setup, and their results. We also present a critique of experimental issues such as evaluation and feasibility aspects, and offer some final thoughts on research directions that need attention in future work.",
    "actual_venue": "Acm Comput Surv"
  },
  {
    "abstract": "Preclinical applications of resting-state functional magnetic resonance imaging (rsfMRI) offer the possibility to non-invasively probe whole-brain network dynamics and to investigate the determinants of altered network signatures observed in human studies. Mouse rsfMRI has been increasingly adopted by numerous laboratories worldwide. Here we describe a multi-centre comparison of 17 mouse rsfMRI datasets via a common image processing and analysis pipeline. Despite prominent cross-laboratory differences in equipment and imaging procedures, we report the reproducible identification of several large-scale resting-state networks (RSN), including a mouse default-mode network, in the majority of datasets. A combination of factors was associated with enhanced reproducibility in functional connectivity parameter estimation, including animal handling procedures and equipment performance. RSN spatial specificity was enhanced in datasets acquired at higher field strength, with cryoprobes, in ventilated animals, and under medetomidine-isoflurane combination sedation. Our work describes a set of representative RSNs in the mouse brain and highlights key experimental parameters that can critically guide the design and analysis of future rodent rsfMRI investigations.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "In this paper we present the initial results of a national trend analysis - an approach that allows collecting and investigating location- and time-specific acceptance factors from user-generated content. For this purpose, an annotation scheme is adapted that is originally developed for sentiment analysis and opinion detection. By applying this annotation scheme, German Web comments of a newspaper and a news-site are quantitatively and qualitatively analyzed. The analysis focuses on the investigation of acceptance drivers of deep geothermal energy. Thereby, it is assumed that the public opinion - positive, negative or neutral - is location-and time-dependent. In contradiction, media often draw an adulterated picture of citizen opinions. Our initial assumption of opposed opinions on deep geothermal energy in public and media was confirmed by the conducted trend analysis.",
    "actual_venue": "Professional Communication Conference"
  },
  {
    "abstract": "The term innovation resonates broadly in cyberspace, books and journals. A careful analysis of the vast open-source information indicates that the engineering literature on underlying science of innovation is limited. Innovations in any domain can be enhanced by principles and insights from different disciplines. However, the process of identifying the linkages between the diverse disciplines and the target domain is not well understood. The innovation process and conditions triggering innovation set the stage for economic progress. This paper contributes to better understanding of the process of innovation by introducing basic innovation models. The ideas outlined provide a roadmap for areas of future study, as innovation science can provide a pathway for industries to be able to successfully compete in the global market.",
    "actual_venue": "Journal Of Computer Applications In Technology"
  },
  {
    "abstract": "We discussed previously the need for a risk analysis tool which could account simultaneously for the variety of hazards produced by the explosion of a space launch vehicle. We particularly argued that by analyzing separately the risks posed by the pressure of the explosion blast, the dispersion of toxic gases and the fragments cloud, current practices fail to account for the interdependencies between those hazards, potentially miscalculating the overall risk on the public and the surrounding infrastructure. In this work we set two objectives to expand on our claim. The first is to propose a methodology based on Distribution Envelope Determination (DEnv) to address the above need. The second on the other hand focuses on describing the ongoing software development activities aimed at implementing this methodology as an ArcGIS platform component.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "Autoencoders have emerged as a useful framework for unsupervised learning of internal representations, and a wide variety of apparently conceptually disparate regularization techniques have been proposed to generate useful features. Here we extend existing denoising autoencoders to additionally inject noise before the nonlinearity, and at the hidden unit activations. We show that a wide variety of previous methods, including denoising, contractive, and sparse autoencoders, as well as dropout can be interpreted using this framework. This noise injection framework reaps practical benefits by providing a unified strategy to develop new internal representations by designing the nature of the injected noise. We show that noisy autoencoders outperform denoising autoencoders at the very task of denoising, and are competitive with other single-layer techniques on MNIST, and CIFAR-10. We also show that types of noise other than dropout improve performance in a deep network through sparsifying, decorrelating, and spreading information across representations.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "Honeypot Internet of Things (IoT) (HIoTPOT) keep a secret eye on IoT devices and analyzes the various recent threats which are dangerous to IoT devices. In this paper, implementation of a research honeypot is presented which is used to learn the recent tactics and ethics used by black hat community to attack on IoT devices. As IoT is open and easy for accessing, all the intruders are highly attracted towards IoT. Recently Telnet based attacks are very famous on IoT devices to get easy access and attack on other devices. To reduce these kinds of threats, it is necessary to know in details about intruder, therefore the aim of this research work is to implement novel based secret eye server known as HIoTPOT which will make the IoT environment more safe and secure.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "Data management can now be outsourced to cloud service providers like Amazon Web Services or IBM SmartCloud. This calls for encrypted data-representation schemes that also give way to efficient query processing. State-of-the-art approaches are overly expensive for exact-match queries in the worst case, or they do not ensure privacy if an adversary knows the data distribution. In this paper, we propose a new privacy approach without these shortcomings. It makes use of encryption, obfuscated indices, and data fragmentation. To speed up query processing, we propose three novel data-transformation and query-execution schemes. For two schemes, we prove that an adversary capable of solving any polynomial problem cannot determine if any attribute values appear together in a tuple. Thus, with our schemes, sensitive data is not linked to personally identifiable information. To evaluate our third scheme, we propose a measure that quantifies the risk of disclosure. We evaluate our approach on real-world folksonomy data. Our evaluation shows that its average response time of exact-match queries with 15 million tuples is under one second on a conventional desktop PC.",
    "actual_venue": "World Wide Web"
  },
  {
    "abstract": "The ability to perform long, accurate molecular dynamics (MD) simulations involving proteins and other biological macro-molecules could in principle provide answers to some of the most important currently outstanding questions in the fields of biology, chemistry and medicine. A wide range of biologically interesting phenomena, however, occur over time scales on the order of a millisecond--about three orders of magnitude beyond the duration of the longest current MD simulations.\n\nIn this paper, we describe a massively parallel machine called Anton, which should be capable of executing millisecond-scale classical MD simulations of such biomolecular systems. The machine, which is scheduled for completion by the end of 2008, is based on 512 identical MD-specific ASICs that interact in a tightly coupled manner using a specialized high-speed communication network. Anton has been designed to use both novel parallel algorithms and special-purpose logic to dramatically accelerate those calculations that dominate the time required for a typical MD simulation. The remainder of the simulation algorithm is executed by a programmable portion of each chip that achieves a substantial degree of parallelism while preserving the flexibility necessary to accommodate anticipated advances in physical models and simulation methods.",
    "actual_venue": "Special Interest Group On Computer Architecture"
  },
  {
    "abstract": "Being cleaner and climate friendly, wind energy has been increasingly utilized to meet the ever-growing global energy demands. In the State of Nebraska, USA, a wide gap exists between wind resource and actual energy production, and it is imperative to expand the wind energy development. Because of the formidable costs associated with wind energy development, the locations for new wind turbines need to be carefully selected to provide the greatest benefit for a given investment. Geographic Information Systems (GIS) have been widely used to identify the suitable wind farm locations. In this study, a GIS-based multi-criteria approach was developed to identify the areas that are best suited to wind energy development in Northeast Nebraska, USA. Seven criteria were adopted in this method, including distance to roads, closeness to transmission lines, population density, wind potential, land use, distance to cities, slope and exclusionary areas. The suitability of wind farm development was modeled by a weighted overlay of geospatial layers corresponding to these criteria. The results indicate that the model is capable of identifying locations highly suited for wind farm development. The approach could help identify suitable wind farm locations in other areas with a similar geographic background.",
    "actual_venue": "Isprs International Journal Of Geo-Information"
  },
  {
    "abstract": "Today&#39;s businesses increasingly rely on cloud computing, which brings both great opportunities and challenges. One of the critical challenges is resiliency: disruptions due to failures (either accidental or because of disasters or attacks) may entail significant revenue losses (e.g., US$ 25.5 billion in 2010 for North America). Such failures may originate at any of the major components in a cloud ...",
    "actual_venue": "Ieee Communications Surveys And Tutorials"
  },
  {
    "abstract": "Metadata creation is a crucial aspect of the ingest of digital materials into digital libraries. Metadata needed to document and manage digital materials are extensive and manual creation of them expensive. The Digital Curation Centre (DCC) has undertaken research to automate this process for some classes of digital material. We have segmented the problem and this paper discusses results in genre classification as a first step toward automating metadata extraction from documents. Here we propose a classification method built on looking at the documents from five directions; as an object exhibiting a specific visual format, as a linear layout of strings with characteristic grammar, as an object with stylo-metric signatures, as an object with intended meaning and purpose, and as an object linked to previously classified objects and other external sources. The results of some experiments in relation to the first two directions are described here; they are meant to be indicative of the promise underlying this multi-facetted approach.",
    "actual_venue": "Ecdl"
  },
  {
    "abstract": "Robust and optimized agent behavior can be achieved by allowing for learning mechanisms within the underlying adaptive control strategies. Therefore, a classic feedback loop concept is used that chooses the best action for an observed situation â and learns the success by analyzing the achieved performance. This typically reflects only the local scope of an agent and neglects the existence of other agents with impact on the reward calculation. However, there are significant mutual influences among agents population. For instance, the success of a Smart Cameraâs control strategy depends (in terms of person detection or 3D-reconstruction) largely on the current strategy performed by its spatially neighbors. In this paper, we compare two concepts to consider such influences within the adaptive control strategy: Distributed W-Learning and Q-Learning in combination with mutual influence detection. We demonstrate that the performance can be improved significantly, if taking detected influences into account.",
    "actual_venue": "Icaart"
  },
  {
    "abstract": "This paper generalizes the migrativity of triangular norms (t-norms for short). Precisely, we find those continuous t-norms T which satisfy T(@l\"1x,@l\"2y)=@l\"3T(x,y), where @l\"i@?(0,1],i=1,2,3 are given. It is worth pointing out that this paper provides a new characterization of the product t-norm.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "Malformations of the cerebral cortex are recognized as a common cause of developmental delay, neurological deficits, mental retardation and epilepsy. Currently, the diagnosis of cerebral cortical malformations is based on a subjective interpretation of neuroimaging characteristics of the cerebral gray matter and underlying white matter. There is no automated system for aiding the observer in making the diagnosis of a cortical malformation. In this paper a fuzzy rule-based system is proposed as a solution for this problem. The system collects the available expert knowledge about cortical malformations and assists the medical observer in arriving at a correct diagnosis. Moreover, the system allows the study of the influence of the various factors that take part in the decision. The evaluation of the system has been carried out by comparing the automated diagnostic algorithm with known case examples of various malformations due to abnormal cortical organization. An exhaustive evaluation of the system by comparison with published cases and a ROC analysis is presented in the paper.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "First Page of the Article",
    "actual_venue": "Ieee Trans Circuits Syst Video Techn"
  },
  {
    "abstract": "This paper proposes a novel face representation and recognition method based on local Gabor textons. Textons, defined as a vocabulary of local characteristic features, are a good description of the perceptually distinguishable micro-structures on objects. In this paper, we incorporate the advantages of Gabor feature and textons strategy together to form Gabor textons. And for the specificity of face images, we propose local Gabor textons (LGT) to portray faces more precisely and eficiently. The local Gabor textons histogram sequence is then utilized for face representation and a weighted histogram sequence matching mechanism is introduced for face recognition. Preliminary experiments on FERET database show promising results of the proposed method.",
    "actual_venue": "ICB"
  },
  {
    "abstract": "We report on work concerning the use of object-oriented analysis and design (OAD) methods in the development of artificial intelligence (AI) software applications, in which we compare such techniques to software development methods more commonly used in AI, in particular CommonKADS. As a contribution to clarifying the role of OAD methods in AI, in this paper we compare the analysis models of the object-oriented methods and the CommonKADS high-level expertise model. In particular, we study the correspondences between generic tasks, methods and ontologies in methodologies such as CommonKADS and analysis patterns in object-oriented analysis. Our aim in carrying out this study is to explore to what extent, in areas of AI where the object-oriented paradigm may be the most adequate way of conceiving applications, an analysis level 'pattern language' could play the role of the libraries of generic knowledge models in the more commonly used AI software development methods. As a case study we use the decision task - its importance arising from its status as the basic task of the intelligent agent - and the associated heuristic multi-attribute decision method, for which we derive a corresponding decision pattern described in the unified modelling language, a de facto standard in OAD.",
    "actual_venue": "Expert Systems"
  },
  {
    "abstract": "In many situations a statistical database contains multiple summary tables, which report summary statistics on the same summary variable for the same population of individuals or objects using different classification criteria (\"homogeneous\" summary tables). Existing query languages consider only those queries which may aggregate data stored in a single summary table. When a statistical database contains homogeneous summary tables, such query languages do not allow an integrated view of data, whereas statisticians are inclined to view and query a collection of homogeneous summary tables as if they were actually a single higher-dimensional summary table. This legitimizes the search for a universal-scheme solution to the problem of data integration is such statistical databases. It is shown that such a solution can be found if the database tables contain additive summary data. Accordingly, queries are grouped into three classes: queries that can be evaluated to single values (evaluable queries); queries that can be evaluated to value ranges (answerable queries); and queries whose values remain unknown (unanswerable queries). The membership of a given query to one of these three classes is not an intrinsic property of the query, but depends on both the type of the summary variable and the dependencies that are assumed in the universal scheme by the database designer. On the basis of such information, linear-time procedures for recognizing and answering answerable and evaluable queries are developed.",
    "actual_venue": "Acm Trans Database Syst"
  },
  {
    "abstract": "Despite the growing interest for component-based systems, few works tackle the question of the trust we can bring into a component. This paper presents a method and a tool for building trustable OO components. It is particularly adapted to a design-by-contract approach, where the specification is systematically derived into executable assertions (invariant properties, pre/postconditions of methods). A component is seen as an organic set composed of a specification, a given implementation and its embedded test cases.We propose an adaptation of mutation analysis to the OO paradigm that checks the consistency between specification/implementation and tests. Faulty programs, called 驴mutants驴, are generated by systematic fault injection in the implementation. The quality of tests is related to the mutation score, i.e. the proportion of faulty programs it detects. The main contribution of this is to show how a similar idea can be used in the same context to address the problem of effective test optimization. To map the genetic analogy to the test optimization problem, we consider mutant programs to be detected as the initial preys population and test cases as the predators population.The test selection consists of mutating the 驴predator驴 test cases and crossing them over in order to improve their ability to kill the prey population. The feasibility of components validation using such a 驴Darwinian驴 model and its usefulness for test optimization are studied.",
    "actual_venue": "Issre"
  },
  {
    "abstract": "The conductivity values of cancerous tissues in the breast are significantly higher than those of surrounding normal tissues. Breast imaging using MREIT (Magnetic Resonance Electrical Impedance Tomography) may provide a new noninvasive way of detecting breast cancer in its early stage. In breast MREIT, the conductivity image quality highly depends on the amount of injected currents assuming a certain signal-to-noise ratio (SNR) of an MRI scanner. The injected current should not produce any significant adverse effect especially on the nerve conduction system of the heart and still distinguish a small cancerous anomaly inside the breast. In this paper, we present results of experimental and numerical simulation studies of breast MREIT. From breast phantom experiments, we evaluated practical amounts of noise in measured magnetic flux density data. We built a realistic three-dimensional model of the human breast connected to a simplified model of the chest including the heart. We performed numerical simulations of various scenarios in breast MREIT including different amplitudes of injected currents and predicted SNRs of MR images related with imaging parameters. Simulation results are promising to show that we may detect a cancerous anomaly in the breast while restricting the maximal current density inside the heart below a level of nerve excitation.",
    "actual_venue": "Annual International Conference Of The Ieee Engineering In Medicine And Biology Society, Vols"
  },
  {
    "abstract": "The compression of still images by means of the discrete wavelet transform (DWT), adopted in the JPEG-2000 and MPEG-4 standards, is becoming more and more widespread because it yields better performance than other compression methods, such as discrete cosine transform. The demand of efficient architectures for 2-D DWT coding and decoding for a variety of different applications and embedded systems is rapidly increasing. This paper presents the implementation of a 2-D DWT decoder for Mallat-tree decomposition, suitable for low power applications, such as portable devices. The decoder design has been synthesized and validated in 0.35-μm CMOS technology. The architecture is scalable according to the desired maximum image size, the maximum DWT kernel length, and arithmetic accuracy, and it is programmable at run-time to process different image sizes and use different DWT kernels.",
    "actual_venue": "Circuits and Systems for Video Technology, IEEE Transactions  "
  },
  {
    "abstract": "We discuss a theoqy of heurtitic strategies and tactics for making arguments in a domain guided by rules where the primary task is to use previous cases to argue for a particular interpretation of a rule in a new fact situation. Since rules frequently have various problematic aspects, such as unstated exceptions or prerequisites, or use terms that are not clearly defined, the actual interpretation is a matter of debate, and the arguer must use interpretations from precedent cases to form his argument. The argument strategies and tactics actually adopted depend on the arguer’s point of view and the complexwn of his case in light of the rules and the precedents. The tactics, called “moves” here, are ultimately expressed in a small set of generic “argument primitives, ” such as analogizing and distinguishing. We discuss how these argument strategies, moves, and primitives are used by our mixed paradigm system, CABARET (Roland & Skalak, 1990). We illustrate with examples from an area of U.S. Federal income tax law.",
    "actual_venue": "Icail"
  },
  {
    "abstract": "AbstractWe solve the following geometric problem, which arises in several{\\mbox three-dimensional} applications in computational geometry:For which arrangements of two lines and two spheres in ${\\Bbb R}^3$are there infinitely many lines simultaneously transversal to the two lines and tangent to the two spheres? We also treat a generalization of this problem to projective quadrics.Replacing the spheres in ${\\bf R}^3$ by quadrics in projective space${\\Bbb P}^3$, and fixing the lines and one general quadric, we give the following complete geometric description of the set of (second)quadrics for which the two lines and two quadrics have infinitely manytransversals and tangents:in the nine-dimensional projective space ${\\Bbb P}^9$ of quadrics, this is a curve of degree 24 consisting of 12 plane conics,a remarkably reducible variety.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "Central utility services are increasingly networked systems that use an interconnection of sensors and programmable logic controllers, and feed data to servers and human-machine interfaces. These systems are connected to the Internet so that they can be accessed remotely, and the network in these plants is structured according to the SCADA model. Although the physical systems themselves are generally designed with high degrees of safety in mind, and designers of computer systems are well advised to incorporate computer security principles, a combined framework for supervisory control of the physical and cyber architectures in these systems is still lacking. Often absent are provisions to defend against external and internal attacks, and even operator errors that might bypass currently standalone security measures to cause undesirable consequences. In this paper we examine a prototypical instance of SCADA network in the distribution network that handles central cooling and heating for a set of buildings. The electrical loads are networked through programmable logic controllers (PLCs), electrical meters, and networks that deliver data to and from servers that are part of a SCADA system, which has grown in size and complexity over many years.",
    "actual_venue": "Apsipa"
  },
  {
    "abstract": "Economic activities in and around online gaming in China are often correlated in the West with practices of gold farming, or selling in-game currency to players for real money in online games. What can we learn about online gaming in China and about online gaming and online sociality more broadly when we look at economic and other \"pragmatic\" practices through which online gaming becomes meaningful to players? In this paper, we present findings from an ethnographic study of online gaming in China's urban Internet cafes to discuss implications for game design, and HCI design more broadly. Considering the ties between socio-economic practices, development of trust and culturally situated imaginings of self-hood and otherness, brings to the fore how online gaming in and of itself constitutes the means for practical achievements in day-to-day management of guanxi (social connection).",
    "actual_venue": "Interact"
  },
  {
    "abstract": "CT perfusion imaging is used for the follow-up of abdominal tumors. A specificity of our work is that patients are breathing freely during image acquisition (5 minutes). We propose an automatic 3D image registration to compensate for respiratory motion. The registration is computed in two main steps: global translation in the z-direction and 3D multiresolution blockmatching. Within this algorithm, the choice of similarity measure largely determines the algorithm robustness in presence of intensity shifts due to contrast diffusion. We exploit a modified entropy-based similarity measure to improve the quality of registration. We also propose two relevant criteria allowing to quantify the registration quality: one based on the gradients of difference images and one based on the smoothness of enhanced-intensity curves.",
    "actual_venue": "Abdominal Imaging"
  },
  {
    "abstract": "Conveying uncertainty in information artifacts is difficult; the challenge only grows as the demand for mass communication through multiple channels expands. In particular, as natural hazards increase with changing global conditions, including hurricanes which threaten coastal areas, we need better means of communicating uncertainty around risks that empower people to make good decisions. We examine how people share and respond to a range of visual representations of risk from authoritative sources during hurricane events. Because these images are now shared widely on social media platforms, Twitter provides the means to study them on a large scale as close to in vivo as possible. Using mixed methods, this study analyzes diffusion of and reactions to forecast and other risk imagery during the highly damaging 2017 Atlantic hurricane season to describe the collective response to visual representations of risk.",
    "actual_venue": "CHI"
  },
  {
    "abstract": "Intelligent Transportation Systems (ITS) are a class of quickly evolving modern safety-critical embedded systems. Dealing with their growing complexity demands a high-level formal modeling language along with adequate verification techniques. STeC has recently been introduced as a process algebra that deals natively with both spatial and temporal properties. Even though STeC has the right expressive power, it does not provide a direct tooled support for verification. We propose to encode STeC specifications as Timed Automata to provide such a support and we illustrate our transformation strategy on a simple example.",
    "actual_venue": "Tase"
  },
  {
    "abstract": "For $S \\subseteq \\mathbb{Z}^+$ and $k$ and $r$ fixed positive integers, denote by $f(S,k;r)$ the least positive integer $n$ (if it exists) such that within every $r$-coloring of $\\{1,2,\\dots,n\\}$ there must be a monochromatic sequence $\\{x_{1},x_{2},\\dots,x_{k}\\}$ with $x_{i}-x_{i-1} \\in S$ for $2 \\leq i \\leq k$. We consider the existence of $f(S,k;r)$ for various choices of $S$, as well as upper and lower bounds on this function. In particular, we show that this function exists for all $k$ if $S$ is an odd translate of the set of primes and $r=2$.",
    "actual_venue": "Siam J Discrete Math"
  },
  {
    "abstract": "In this paper we introduce the problem of planning for perception of a target position. Given a sensing target, the robot has to move to a goal position from where the target can be perceived. Our algorithm minimizes the overall path cost as a function of both motion and perception costs, given an initial robot position and a sensing target. We contribute a heuristic search method, PA*, that efficiently searches for an optimal path. We prove the proposed heuristic is admissible, and introduce a new goal state stopping condition.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "A promising direction in the analysis of gene expression focuses on the changes in expression of specific predefined sets of genes that are known in advance to be related (e.g., genes coding for proteins involved in cellular pathways or complexes). Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation. In this article, we present a new method of this kind that operates by quantifying the level of 'activity' of each pathway in different samples. The activity levels, which are derived from singular value decompositions, form the basis for statistical comparisons and other applications.We demonstrate our approach using expression data from a study of type 2 diabetes and another of the influence of cigarette smoke on gene expression in airway epithelia. A number of interesting pathways are identified in comparisons between smokers and non-smokers including ones related to nicotine metabolism, mucus production, and glutathione metabolism. A comparison with results from the related approach, 'gene-set enrichment analysis', is also provided.Our method offers a flexible basis for identifying differentially expressed pathways from gene expression data. The results of a pathway-based analysis can be complementary to those obtained from one more focused on individual genes. A web program PLAGE (Pathway Level Analysis of Gene Expression) for performing the kinds of analyses described here is accessible at http://dulci.biostat.edu/pathways.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "We present a novel method to generate 3D scenes that allow the same activities as real environments captured through noisy and incomplete 3D scans. As robust object detection and instance retrieval from low-quality depth data is challenging, our algorithm aims to model semantically-correct rather than geometrically-accurate object arrangements. Our core contribution is a new scene synthesis technique which, conditioned on a coarse geometric scene representation, models functionally similar scenes using prior knowledge learned from a scene database. The key insight underlying our scene synthesis approach is that many real-world environments are structured to facilitate specific human activities, such as sleeping or eating. We represent scene functionalities through virtual agents that associate object arrangements with the activities for which they are typically used. When modeling a scene, we first identify the activities supported by a scanned environment. We then determine semantically-plausible arrangements of virtual objects -- retrieved from a shape database -- constrained by the observed scene geometry. For a given 3D scan, our algorithm produces a variety of synthesized scenes which support the activities of the captured real environments. In a perceptual evaluation study, we demonstrate that our results are judged to be visually appealing and functionally comparable to manually designed scenes.",
    "actual_venue": "Acm Transactions On Graphics"
  },
  {
    "abstract": "In this paper, we discuss how to build user friendly user interfaces to the smart world. We present the REACHeS architecture for controlling Internet services through physical user interfaces, using a mobile terminal and icons placed in the environment. An icon advertises a service that can be started by touching the icon with a mobile terminal. This service activation configures the mobile terminal as a remote control for the service. We have implemented this architecture and designed an icon set. The physical user interface is based on RFID technology: the terminals are equipped with RFID readers and RFID tags are placed under the icons. We present the first prototype applications and the first usability tests that we have carried out.",
    "actual_venue": "UIC"
  },
  {
    "abstract": "A dynamical system is proposed for generalized eigen- decomposition problem. The stable points of the dynamical system are proved to be the eigenvectors corresponding to the largest generalized eigenvalue.",
    "actual_venue": "Isnn"
  },
  {
    "abstract": "Polarimetric synthetic aperture radar satellite and ground-based Ku- and X-band scatterometer measurements are used to explore the scattering mechanism for ice in shallow Arctic lakes, wherein strong radiometric responses are seen for floating ice, and low returns are evident where the ice has grounded. Scatterometer measurements confirm that high backscatter is from the ice/water interface, where...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "The control of flexible arms with friction in the joints is studied. A method to identify the dynamics of a flexible arm from its frequency response (which is strongly distorted by Coulomb's friction) is proposed. A robust control scheme that minimizes the effects of this friction is presented. The scheme consists of two nested feedback loops: an inner loop to control the motor position and an outer loop to control the tip position. It is shown that a proper design of the inner loop eliminates the effects of friction while controlling the tip position and significantly simplifies the design of the outer loop. The proposed scheme is applied to a class of lightweight flexible arms, and the experiments show that the control scheme results in a simple controller. As a result, the computations are minimized and, thus, high sampling rates may be used",
    "actual_venue": "Robotics and Automation, IEEE Transactions  "
  },
  {
    "abstract": "R1 is a rule based program used by Digital Equipment Corporation's manufacturing organization fc configuring VAX 11 systems. Since its inception. R1 has had the capability of producing a functional system by fleshing out an order consisting only of a CPU. some primary memory, and some devices; until recently, however, it was not capable of accepting as part of its input a set of ad hoc (customer specific) constraints Left to itself, R1 was capable of producing reasonable configurations. But it was incapable of modifying its decisions on the basis of information that others could provide This paper describes how rules were added to R1 to take advantage of such information R1 can now accept as input commands that specify how particular components are to be configured Whenever one of the commands becomes relevant, these rules take control, extend the configuration in the direction indicated by the command, and then step aside, allowing Rt's ordinary-case configuration rules to regain control.",
    "actual_venue": "Ijcai"
  },
  {
    "abstract": "Requirements elicitation and management has become ever more important as products become more complex and time to market is shortened and the definitions of product lines has significantly increased project complexity. Outsourcing has added a new dimension to requirements management, exacerbating problems associated with transitioning from analysis to design. This half day tutorial will provide an introduction to product line requirements engineering from the perspective of project and product management: how it impacts project managers, quality assurance personnel, requirements analysts, developers and testers. Topics covered will include product line requirements, feature modeling, CMMI compliant requirements management and requirements analysis processes (both UML and text based). Business analysts who are interested in using UML for modeling will also find the course interesting. No formal knowledge of programming is required. developers and testers.",
    "actual_venue": "Splc"
  },
  {
    "abstract": "3D IC is a promising technology to meet the demands of high throughput, high scalability, and low power consumption for future generation integrated circuits. One way to implement the 3D IC is to interconnect layers of two-dimensional (2D) IC with Through-Silicon Via (TSV), which shortens the signal lengths. Unfortunately, while TSVs are bundled together as a cluster, the crosstalk coupling noise may lead to transmission errors. As a result, the working frequency of TSVs has to be lowered to avoid the errors, leading to narrower bandwidth that TSVs can provide. In this paper, we first derive the crosstalk noise model from the perspective of 3D chip and then propose ShieldUS, a runtime data-to-TSVs remapping strategy. With ShieldUS, the transition patterns of data over TSVs are observed at runtime, and relatively stable bits will be mapped to the TSVs which act as shields to protect the other bits which have more fluctuations. We evaluate the performance of ShieldUS with address lines from real benchmark traces and data lines of different similarities. The results show that ShieldUS is accurate and flexible. We further study dynamic shielding and our design of Interval Equilibration Unit (IEU) can intelligently select suitable parameters for dynamic shielding, which makes dynamic shielding practical and does not need to predefine parameters. This also improves the practicability of ShieldUS.",
    "actual_venue": "Asp-Dac"
  },
  {
    "abstract": "Smart City solutions are currently based on multiple architectures, standards and platforms, which have led to a highly fragmented landscape. In order to allow cities to share data across systems and coordinate processes across domains, it is essential to break these silos. A way to achieve the purpose is sensor virtualization, discovery and data restitution. In this paper, a federation of FIT IoT-LAB within OpenIoT is presented. OpenIoT is a middleware that enables the collection of data streams from multiple heterogeneous geographically dispersed data sources, as well as their semantic unification and streaming with a cloud infrastructure. Future Internet of Things IoT-LAB (FIT IoT-LAB) provides a very large scale infrastructure facility suitable for testing small wireless sensor devices and heterogeneous communicating objects. The integration proposed represents a way to reduce the gap existing in the Internet of Things (IoT) fragmentation, and, moreover, allows users to develop smart city applications by interacting directly with sensors at different layers. We illustrate it trough a basic temperature monitoring application to show its efficiency.",
    "actual_venue": "Secon Workshops"
  },
  {
    "abstract": "Presents the President's message for this issue of the publication.",
    "actual_venue": "Ieee Communications Magazine"
  },
  {
    "abstract": "Future computer historians may speak of the technology advances developed for the US Department of Energy&#39;s Accelerated Strategic Computing Initiative the way today&#39;s historians speak about technology that came out of DARPA in the 1970s and 80s, with awe. DARPA projects produced innovation that led to the Internet and Unix, among other things. ASCI has already produced the first teraflops computer...",
    "actual_venue": "Ieee Concurrency"
  },
  {
    "abstract": "Security-aware embedded systems are widespread nowadays and many applications, such as payment, pay-TV and automotive applications rely on them. These devices are usually very resource constrained but at the same time likely to operate in a hostile environment. Thus, the implementation of low-cost protection mechanisms against physical attacks is vital for their market relevance. An appealing choice, to counteract a large family of physical attacks with one mechanism, seem to be protocol-level countermeasures. At last year's Africacrypt, a fresh re-keying scheme has been presented which combines the advantages of re-keying with those of classical countermeasures such as masking and hiding. The contribution of this paper is threefold: most importantly, the original fresh re-keying scheme was limited to one low-cost party (e.g. an RFID tag) in a two party communication scenario. In this paper we extend the scheme to n low-cost parties and show that the scheme is still secure. Second, one unanswered question in the original paper was the susceptibility of the scheme to algebraic SPA attacks. Therefore, we analyze this property of the scheme. Finally, we implemented the scheme on a common 8-bit microcontroller to show its efficiency in software.",
    "actual_venue": "Cardis"
  },
  {
    "abstract": "Detecting local events (e.g., protests, accidents) in real-time is an important task needed by a wide spectrum of real-world applications. In recent years, with the proliferation of social media platforms, we can access massive geo-tagged social messages, which can serve as a precious resource for timely local event detection. However, existing local event detection methods either suffer from unsatisfactory performances or need intensive annotations. These limitations make existing methods impractical for large-scale applications. Through the analysis of real-world datasets, we found that the informativeness level of social media users, which is neglected by existing work, plays a highly critical role in distilling event-related information from noisy social media contexts. Motivated by this finding, we propose an unsupervised framework, named LEDetect, to estimate the informativeness level of social media users and leverage the power of highly informative users for local event detection. Experiments on a large-scale real-world dataset show that the proposed LEDetect model can improve the performance of event detection compared with the state-of-the-art unsupervised approach. Also, we use case studies to show that the events discovered by the proposed model are of high quality and the extracted highly informative users are reasonable.",
    "actual_venue": "Asonam : International Conference On Advances In Social Networks Analysis And Mining Barcelona Spain August"
  },
  {
    "abstract": "Multiple object visual tracking of real time detected objects using a low-power embedded solution is shown. The proposal is implemented on a NVIDIA Jetson TX2 development kit demonstrating the feasibility of deep learning techniques for IoT and mobile edge computing applications.",
    "actual_venue": "Ieee International Symposium On Circuits And Systems"
  },
  {
    "abstract": "In this paper, we address the problem of survivable multicast traffic grooming in WDM bidirectional ring networks. The rapid growth of multicast applications such as video conferencing, distance learning, and online auction, has initiated the need for cost-effective solutions to realize multicasting in WDM optical networks. Many of these applications, being time critical and delay sensitive, demand robust and fault-tolerant means of data communication. The end user traffic demands in metro environment are in fractional bandwidth as compared to the wavelength channel capacity. Providing survivability at connection level is resource intensive. Hence cost-effective solutions that require minimum resources for realizing survivable multicasting are in great demand. In order to realize multicast traffic grooming in bidirectional ring networks, we propose a node architecture based on Bidirectional Add Drop Multiplexers (BADM) to support bidirectional add/drop functionality along with traffic duplication at each node. We also propose two traffic grooming algorithms, namely Survivable Grooming with Maximum Overlap of Sessions (SGMOS) and Survivable Grooming with Rerouting of Sessions (SGRS). Extensive simulation studies reveal that the proposed algorithms consume minimum resources measured in terms of BADM grooming ports, backup cost, and wavelengths.",
    "actual_venue": "Optical Switching And Networking"
  },
  {
    "abstract": "An EMCCD-based dual modular x-ray imager was recently designed and developed from the component level, providing a high dynamic range of 53 dB and an effective pixel size of 26 mu m for angiography and fluoroscopy. The unique 2x1 array design efficiently increased the clinical field of view, and also can be readily expanded to an MxN array implementation. Due to the alignment mismatches between the EMCCD sensors and the fiber optic tapers in each module, the output images or video sequences result in a misaligned 2048x1024 digital display if uncorrected. In this paper, we present a method for correcting display registration using a custom-designed two layer printed circuit board. This board was designed with grid lines to serve as the calibration pattern, and provides an accurate reference and sufficient contrast to enable proper display registration. Results show an accurate and fine stitching of the two outputs from the two modules.",
    "actual_venue": "Annual International Conference Of The Ieee Engineering In Medicine And Biology Society, Vols"
  },
  {
    "abstract": "A methodology for clustering data in which a distance metric or similarity function is not used is described. Instead, clusterings are optimized based on their intended function: the accurate prediction of properties of the data. The resulting clustering methodology is applicable, without further ad hoc assumptions or transformations of the data, (1) when features are heterogeneous (both discrete and continuous) and not combinable, (2) where some data points have missing feature values, and (3) where some features are irrelevant, i.e. have large variance but little correlation with other features. Further, it provides an integral measure of the quality of the resulting clustering. A clustering program, RIFFLE, has been implemented in line with this approach, and experiments with synthetic and real data show that the clustering is, in many respects, superior to traditional methods.",
    "actual_venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions  "
  },
  {
    "abstract": "The growth of service sector in recent years has led to renewed research interests in the design and management of service systems. Decision support systems (DSS) play an important role in supporting this endeavor, through management of organizational resources such as models and data, thus forming the \"back stage\" of service systems. In this article, we identify the requirements for semantically annotating decision models and propose a model representation scheme, termed Semantically Annotated Structure Modeling Markup Language (SA-SMML) that extends Structure Modeling Markup Language (SMML) by incorporating mechanisms for linking semantic models such as ontologies that represent problem domain knowledge concepts. This model representation format is also amenable to a scalable Service-Oriented Architecture (SOA) for managing models in distributed environments. The proposed model representation technique leverages recent advances in the areas of semantic web, and semantic web services. Along with design considerations, we demonstrate the utility of this representation format with an illustrative usage scenarios with a particular emphasis on model discovery and composition in a distributed environment.",
    "actual_venue": "Inf Syst E-Business Management"
  },
  {
    "abstract": "Under the cloud trend of enterprises, how do traditional businesses get on the cloud becomes a worth pondering question. To help those traditional businesses that have no experience to dispel the clouds and see the sun as soon as possible, we are planning to choose one corporation with rich experience to take them into the cloud market. The quintessence of dual probabilistic linguistic term sets (DPLTSs) is that it uses the combination of several linguistic terms and their proportions to reveal decision information by opposite angles. This paper proposes the dual probabilistic multiplicative linguistic preference relations (DPMLPRs) based upon the dual probabilistic multiplicative linguistic term sets (DPMLTSs). Then, it defines the comparable degree between the DPMLPRs and studies the consensus of the group DPMLPR. Moreover, it probes the expanding grey relational analysis (EGRA) under the proposed comparable degree between the DPMLTSs. After that, one example of choosing the experienced cloud cooperative partner is simulated under the dual probabilistic linguistic circumstance. Besides, the comparative analysis is performed by considering the similarity among the EGRA, TODIM, and VIKOR.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "In this work, we introduce an efficient attack for removing, the watermark with minimum distortion from quantization-based watermarking schemes. The attack is based on analyzing the quantization error to extract the key parameters of the encoder. The removal of the watermark is done by perturbing enough components of the quantized watermarked signal by half of the estimated quantization step. Simulation experiments show the high effectiveness of the proposed algorithm in hacking known quantization-based schemes for watermarking.",
    "actual_venue": "Isspa"
  },
  {
    "abstract": "Motivated by the recent development of energy harvesting communications, and the trend of multimedia contents caching and push at the access edge and user terminals, this paper considers how to design an effective push mechanism of energy harvesting powered small-cell base stations (SBSs) in heterogeneous networks. The problem is formulated as a Markov decision process by optimizing the push policy based on the battery energy, user request and content popularity state to maximize the service capability of SBSs. We extensively analyze the problem and propose an effective policy iteration algorithm to find the optimal policy. According to the numerical results, we find that the optimal policy reveals a state dependent threshold based structure. Besides, more than 50% performance gain is achieved by the optimal push policy compared with the non-push policy.",
    "actual_venue": "Ieee International Conference On Communications"
  },
  {
    "abstract": "NTP is a well-known and widely used clock synchronization mechanism for PC cluster environments. However, like other software-based clock synchronization algorithms, its precision depends on the estimated accuracy of the network latency for synchronization messages. This paper presents a low-cost internal clock synchronization mechanism that uses a simple TTL signal distributor to support remote clock reading to obtain the time drift necessary to execute local clock adjustments. The objective is to improve precision, thus eliminating the need to estimate the network latency as in usual methods.",
    "actual_venue": "Euro-Par"
  },
  {
    "abstract": "We use extended answer set programming (ASP), a logic programming paradigm which allows for the defeat of conflicting rules, to check satisfiability of computation tree logic (CTL) temporal formulas via an intuitive translation. This translation, to the best of our knowledge the first of its kind for CTL, allows CTL reasoning with existing answer set solvers. Furthermore, we demonstrate how preferred ASP, where rules are ordered according to preference for satisfaction, can be used for synthesizing synchronization skeletons of processes in a concurrent program from a temporal specification. We argue that preferred ASP is put to good use since a preference order can be used to make explicit some of the decisions tableau algorithms make, e.g. declaratively specifying a preference for maximal concurrency makes synthesis more transparent and thus less error-prone.",
    "actual_venue": "Ictcs"
  },
  {
    "abstract": "The exact and general formulation of optimal control for biped robots based on a numerical representation of the motion equation is proposed. We can solve exactly the minimum energy consumption trajectories for a biped running motion. Through the numerical study of a five link planar biped robot, it is found that big peak power and torque is required for the knee joints but its consumption power is small and the main work is done by the hip joints.",
    "actual_venue": "Robotics And Automation Proceedings Ieee International Conference"
  },
  {
    "abstract": "In this paper, we focus on subcarrier and power allocation for layered multicast streams in OFDMA cellular networks, where the multicast stream is composed of a basic layer and an enhancement layer. Our goal is to maximize the system total throughput with a total power constraint and a minimum rate requirement. A low-complexity allocation algorithm is proposed, which combined the suitability-based subcarrier allocation (SSA) with the traditional water filling (TWF) for the base layer and an advanced water filling (AWF) for the enhancement layer. Besides, we present a throughput-based user selection (TUS) algorithm which selects a proper set of users to serve when the minimum rate requirement can not be satisfied. Simulation results show that our proposed algorithm can improve the system throughput and outage probability.",
    "actual_venue": "ICC"
  },
  {
    "abstract": "Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Probabilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.",
    "actual_venue": "ACL"
  },
  {
    "abstract": "Memetic algorithms are popular approaches to improve pure evolutionary methods. But were and when in the system the local search should be applied and does it really speed up evolutionary search is a still an open question. In this paper we investigate the influence of the memetic extensions on globally induced regression and model trees. These evolutionary induced trees in contrast to the typical top-down approaches globally search for the best tree structure, tests at internal nodes and models at the leaves. Specialized genetic operators together with local greedy search extensions allow to the efficient tree evolution. Fitness function is based on the Bayesian information criterion and mitigate the over-fitting problem. The proposed method is experimentally validated on synthetical and real-life datasets and preliminary results show that to some extent memetic approach successfully improve evolutionary induction.",
    "actual_venue": "Icaisc"
  },
  {
    "abstract": "RÉSUMÉ. Le problème de l'isomorphisme de graphes consiste à prouver que deux graphes don- nés ont la même structure. Ce problème peut très facilement être modélisé en un problème de satisfaction de contraintes puis être résolu par un solveur de contraintes. Toutefois, sur ce type de problèmes, la programmation par contraintes est bien moins efficace que les algorithmes dédiés qui sont capables de tirer partie de la sémantique globale du problème. Nous introdui- sons dans cet article une nouvelle contrainte globale dédiée au problème de l'isomorphisme de graphes. Nous définissons ensuite l'algorithme de filtrage associé à cette contrainte. Celui-ci exploite les arêtes du graphe de façon globale afin de réduire le domaine des variables. Nous montrons aussi que cette contrainte globale est décomposable en un ensemble de \"contraintes de distance\" propageant mieux les réductions des domaines que les \"contraintes d'arêtes\" ha- bituellement utilisées pour ce problème. ABSTRACT. The graph isomorphism problem consists in deciding if two given graphs have an identical structure. This problem can be modeled as a constraint satisfaction problem in a very straightforward way, so that one can use constraint programming to solve it. However, constraint programming is a generic tool that may be less efficient than dedicated algorithms which can take advantage of the global semantic of the original problem. Hence, we introduce in this paper a new global constraint dedicated to graph isomorphism problems, and we define an associated filtering algorithm that exploits all edges of the graphs in a global way to narrow variable domains. We then show how this global constraint can be decomposed into a set of \"distance\" constraints which propagate more domain reductions than \"edge\" constraints that are usually generated for this problem.",
    "actual_venue": "Jfplc"
  },
  {
    "abstract": "Describes a program used for identifying the executable software configuration installed on a computer. The application, Auditor's Aid allows an auditor to automatically identify what programs (and versions) reside on a given system. The approach discussed generates a checksum and uses this value as a primary key (not to be confused with an encryption key) into a configuration table. The prototype described operates on an IBM PC platform",
    "actual_venue": "Tucson, Az"
  },
  {
    "abstract": "There is a multicore platform that is currently concentrating an enormous attention due to its tremendous potential in terms of sustained performance: the NVIDIA Tesla boards. These cards intended for general-purpose computing on graphic processing units (GPGPUs) are used as data-parallel computing devices. They are based on the Computed Unified Device Architecture (CUDA) which is common to the latest NVIDIA GPUs. The bottom line is a multicore platform which provides an enormous potential performance benefit driven by a non-traditional programming model. In this paper we try to provide some insight into the peculiarities of CUDA in order to target scientific computing by means of a specific example. In particular, we show that the parallelization of the two-dimensional fast wavelet transform for the NVIDIA Tesla C870 achieves a speedup of 20.8 for an image size of 8192x8192, when compared with the fastest host-only version implementation using OpenMP and including the data transfers between main memory and device memory.",
    "actual_venue": "PDP"
  },
  {
    "abstract": "This paper presents an experimental study on modeling machine emotion elicitation in a socially intelligent service, the typing tutor. The aim of the study is to evaluate the extent to which the machine emotion elicitation can influence the affective state (valence and arousal) of the learner during a tutoring session. The tutor provides continuous real-time emotion elicitation via graphically rendered emoticons, as an emotional feedback to learner's performance. Good performance is rewarded by the positive emoticon, based on the notion of positive reinforcement. Facial emotion recognition software is used to analyze the affective state of the learner for later evaluation. Experimental results show the correlation between the positive emoticon and the learner's affective state is significant for all 13 (100%) test participants on the arousal dimension and for 9 (69%) test participants on both affective dimensions. The results also confirm our hypothesis and show that the machine emotion elicitation is significant for 11 (85%) of 13 test participants. We conclude that the machine emotion elicitation with simple graphical emoticons has a promising potential for the future development of the tutor.",
    "actual_venue": "Computers"
  },
  {
    "abstract": "A thesis is put forward that there is a paradigm shift underway in how (information) systems, and the software which supports them, are developed. The shift is away from a craft-based structure in which user requirements are specified and custom solutions developed, to a market-generated, product-based approach in which users themselves select and arrange meaningful-to-them components as solutions to their requirements. There are several drivers to this shift. First, there are the numerous problems inherent in the methods now being used (expensive, time-consuming, myopic, inflexible). Second, there is a software technology objects which enables such a shift to take place. This paper examines these drivers, the ensuing development paradigm shift, and some of its many implications.",
    "actual_venue": "Database"
  },
  {
    "abstract": "A new generation of \"behavior-aware\" delay tolerant networks is emerging in what may define future mobile social networks. With the introduction of novel behavior-aware protocols, services and architectures, there is a pressing need to understand and realistically model mobile users behavioral characteristics, their similarity and clustering. Such models are essential for the analysis, performance evaluation, and simulation of future DTNs. This paper addresses issues related to mobile user similarity, its definition, analysis and modeling. To define similarity, we adopt a behavioral-profile based on users location preferences using their on-line association matrix and its SVD, then calculate the behavioral distance to capture user similarity. This measures the difference of the major spatio-temporal behavioral trends and can be used to cluster users into similarity groups or communities. We then analyze and contrast similarity distributions of mobile user populations in two settings: (i) based on real measurements from four major campuses with over ten thousand users for a month, and (ii) based on existing mobility models, including random direction and time-varying community models. Our results show a rich set of similar communities in real mobile societies with distinct behavioral clusters of users. This is true for all the traces studied, with the trend being consistent over time. Surprisingly, however, we find that the existing mobility models do not explicitly capture similarity and result in homogeneous users that are all similar to each other. Thus the richness and diversity of user behavioral patterns is not captured to any degree in the existing models. These findings strongly suggest that similarity should be explicitly captured in future mobility models, which motivates the need to re-visit mobility modeling to incorporate accurate behavioral models in the future.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "Let p be a prime number, Z/(p^e) the integer residue ring, e=2. For a sequence a@? over Z/(p^e), there is a unique decomposition a@?=a@?\"0+a@?\"1@?p+...+a@?\"e\"-\"1@?p^e^-^1, where a@?\"i be the sequence over {0,1,...,p-1}. Let f(x)@?Z/(p^e)[x] be a primitive polynomial of degree n, a@? and b@? be sequences generated by f(x) over Z/(p^e), such that a@?0@?(modp^e^-^1). This paper shows that the distribution of zero in the sequence a@?\"e\"-\"1=(a\"e\"-\"1(t))\"t\"=\"0 contains all information of the original sequence a@?, that is, if a\"e\"-\"1(t)=0 if and only if b\"e\"-\"1(t)=0 for all t=0, then a@?=b@?. Here we mainly consider the case of p=3 and the techniques used in this paper are very different from those we used for the case of p=5 in our paper [X.Y. Zhu, W.F. Qi, Uniqueness of the distribution of zeroes of primitive level sequences over Z/(p^e), Finite Fields Appl. 11 (1) (2005) 30-44].",
    "actual_venue": "Finite Fields And Their Applications"
  },
  {
    "abstract": "Model-based design is a promising technique to improve the quality of software and the efficiency of the software development process. We are investigating how to efficiently model embedded software and its environment to verify the requirements for the system controlled by the software. The software environment consists of mechanical, electrical and other parts; modelling it involves learning how these parts work, deciding what is relevant to model and how to model it. It is not possible to fully automate these steps. There are general guidelines, but given that every modelling problem differs, much is left to the modeller's own preference, background and experience. Still, when the next generation of a system is designed, the new system will have common elements with its previous version. Therefore, lessons learned from the current model could inform future models. We propose a framework for identifying the non-formal elements of knowledge, insights and a model itself, which can support modelling of the next system generation. We will present the application of our framework on an action research case - modelling mechanical parts of a paper-inserting machine.",
    "actual_venue": "Expert Systems"
  },
  {
    "abstract": "A simple derivation for the time derivative of a large class of statistics associated with the size of a queue is presented. This method is based on a discrete time approximation in conjunction with conditional expectation and is suitable for any queueing system with multiple interconnected and controlled queues such that the overall dynamics can be modeled as a continuous-time Markov chain. The general procedure is specialized to obtain the Jackson network time-varying k th moment of system size (queue+server) as well as several special cases. Results are also obtained for a network of queues with Erlangian servers. Applications of these results to closure approximations for Jackson networks and to the approximation of statistics of controlled queues are also given.",
    "actual_venue": "Perform Eval"
  },
  {
    "abstract": "An adaptive power loading algorithm with uniform (nonadaptive) bit allocation is proposed for constant data rate OFDM systems in this letter. This algorithm aims to minimize the transmit power while guaranteeing the target mean bit error rate (BER). The power loading is based on the unequal-BER (UBER) strategy which permits unequal mean BERs on different subcarriers. The closed-form expressions for optimal BER and power distributions are derived. Simulation results indicate the superiority of the proposed algorithm.",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "Wireless sensor networks consist of sensor nodes that are deployed in a large area and collect information from a sensor field. Since the nodes have very limited energy resources, the energy consuming operations such as data collection, transmission and reception must be kept to a minimum. Low Energy Adaptive Clustering Hierarchy (LEACH) is a cluster based communication protocol where cluster-heads (CH) are used to collect data from the cluster nodes and transmit it to the remote base station. In this paper we propose two extensions to LEACH. Firstly, nodes are evenly distributed during the cluster formation process, this is accomplished by merging multiple overlapping clusters. Secondly, instead of each CH directly transmitting data to remote base station, it will do so via a CH closer to the base station. This reduces transmission energy of cluster heads. The combination of above extensions increases the data gathering at base station to 60% for the same amount of sensor nodes energy used in LEACH.",
    "actual_venue": "GPC"
  },
  {
    "abstract": "Stochastic Petri nets have been used to analyze the performance and reliability of complex systems comprising concurrency and synchronization. Various extensions have been proposed in literature in order to broaden their field of application to an increasingly larger range of real situations. In this paper we extend the class of Markov regenerative stochastic Petri nets ∗ (MRSPNs ∗ ), removing the restriction that at most one generally distributed timed transition can be enabled in any marking. This new class of Petri nets, which we call concurrent generalized Petri nets (CGPNs), allows simultaneous enabling of immediate, exponentially and generally distributed timed transitions, under the hypothesis that the latter are all enabled at the same instant. The stochastic process underlying a CGPN is shown to be still an MRGP. We evaluate the kernel distribution of the underlying MRGP and define the steps required to generate it automatically. The methodology described is used to assess the behavior of a system in both steady-state and transient functioning conditions.",
    "actual_venue": "Perform Eval"
  },
  {
    "abstract": "Covering rough sets are an important extension of Pawlak rough sets. This paper studies the relations arising from coverings and their topological structures. Every covering induces a reflexive and transitive relation. We represent the approximate pairs proposed by Ma (2015) 20 with different combinations of a relation and its inverse. Based on this representation, we give the relationship among approximate pairs. We also consider the topological structures induced by these lower approximations and establish the relationship among these topologies. The results show that the approximate pairs can be precisely characterized by a relation and its inverse. Propose a new method to represent the approximate pairs.Characterize the approximate pairs with a relation and its inverse.Study the topological structures induced by the approximate pairs.",
    "actual_venue": "Int J Approx Reasoning"
  },
  {
    "abstract": "Participatory user interface design with adolescent users on the autism spectrum presents a number of unique challenges and opportunities. Through our work developing a system to help autistic adolescents learn to recognize facial expressions, we have learned valuable lessons about software and hardware design issues for this population. These lessons may also be helpful in assimilating iterative user input to customize technology for other populations with special needs.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "Although transaction scheduling and concurrency control issues that arise in real-time databases have been studied in detail, insufficient attention has been paid to issues that arise when real-time transactions access data with temporal validity. Such transactions must not only meet their dead- lines but also read and use data that correctly reflects the environment. In this paper, we discuss the issues involved in the design of a real-time active database which maintains data temporal consistency. The concept of data-deadline is introduced and time cognizant transaction scheduling algorithms which exploit the semantics of data and trans- actions based on data-deadline are proposed.",
    "actual_venue": "Rtdb"
  },
  {
    "abstract": "User authentication is necessary for proving and verifying the claimed identity of users in a distributed environment. Three factors such as user's knowledge, belongings, and biometric traits are usually considered for the purpose. A sort of multi-factor authentication may combine those factors in the way that a user provides the requested multi-factors separately, for improving the accuracy and security of authentication. However, such a combination of distinct factors should require each different human-computer interfaces. In this short paper, rather we introduce our on-going work to associate knowledge-based authentication with biometrics for requiring less interfaces and examine the benefits expected from it in a conceptual level.",
    "actual_venue": "HCI (5)"
  },
  {
    "abstract": "The purpose of a Network Intrusion Detection System (NIDS) is to monitor network traffic such to detect malicious usages of network facilities. NIDSs can also be part of the affected network facilities and be the subject of attacks aiming at degrading their detection capabilities. The present paper investigates such vulnerabilities in a recent consensus-based NIDS proposal [1]. This system uses an average consensus algorithm to share information among the NIDS modules and to develop coordinated responses to network intrusions. It is known however that consensus algorithms are not resilient to compromised nodes sharing falsified information, i.e. they can be the target of Byzantine attacks. Our work proposes two different strategies aiming at identifying compromised NIDS modules sharing falsified information. Also, a simple approach is proposed to isolate compromised modules, returning the NIDS into a non-compromised state. Validations of the defense strategies are provided through several simulations of Distributed Denial of Service attacks using the NSL-KDD data set. The efficiency of the proposed methods at identifying compromised NIDS nodes and maintaining the accuracy of the NIDS is compared. The computational cost for protecting the consensus-based NIDS against Byzantine attacks is evaluated. Finally we analyze the behavior of the consensus-based NIDS once a compromised module has been isolated.",
    "actual_venue": "Informatica-Journal Of Computing And Informatics"
  },
  {
    "abstract": "Intersection matrices help identify the common graphical structure of two or more objects. They arise naturally in a variety of settings. Several examples of their use in a computer algebra environment are given. These include: simplifying an expression involving array products, automating cumulant calculations, determining the behaviour of an expected value operator and identifying model hierarchy in a factorial experiment. The emphasis is placed on the graphical structure, and the symmetry of arrays help reduce the complexity of the graphical problem.",
    "actual_venue": "Statistics And Computing"
  },
  {
    "abstract": "In agent-based markets, adapting to the behavior of other agents is often necessary for success. When it is riot possible to directly model individual competitors, an agent may instead model and adapt to the market conditions that result from competitor behavior. Such an agent could still benefit from reasoning about specific competitor strategies by considering how various combinations of these strategies would impact the conditions being modeled. We present an application of such an approach to a specific prediction problem faced by the agent TacTex-06 in the Trading Agent Competition's Supply Chain Management scenario (TAC SCM).",
    "actual_venue": "Agent-Mediated Electronic Commerce And Trading Agent Design And Analysis"
  },
  {
    "abstract": "Research has shown that the functionalities of proteins are largely influenced by their three dimensional (3D) shapes. This observation is especially relevant in drug design, where the knowledge of the 3D structure of a protein enables pharmacologists to select the best binding proteins when aiming to moderate functions. However, a relatively small number of 3D shapes are known. In contrast, amino acid sequences may be acquired through very efficient automated, high throughput experimental methods and the amino acid sequences of a vast number of proteins have therefore been identified. It follows that it is important to address this knowledge gap. To this end, this paper introduces an approach to predict the 3D shapes of proteins, utilizing feed-forward artificial neural networks. Our novel solution allows one to learn the representations of the 3D shape associated with a protein by starting directly from its amino acid sequence descriptors. Once a neural network is trained, our search engine enables one to retrieve the closest known 3D shape associated with an unknown, so-called query protein. We evaluate the performance of our approach against the Protein Data Bank (PDB), by considering proteins from a diverse set of families. Our results indicate that our system is able to accurately find the most similar protein structures for a wide variety of protein 3D shapes and diverse protein family sizes.",
    "actual_venue": "Neural Networks"
  },
  {
    "abstract": "This paper presents an integrated artificial neural network-computer simulation (ANNSim) for optimization of G/G/K queue systems. The ANNSim is a computer program capable of improving its performance by referring to production constraints, system's limitations and desired targets. It is a goal oriented, flexible and integrated approach and produces the optimum solution by utilizing Multi Layer Perceptron (MLP) neural networks. The properties and modules of the prescribed intelligent ANNSim are: (1) parametric modeling, (2) flexibility module, (3) integrated modeling, (4) knowledge-base module, (5) integrated database and (6) learning module. The integrated ANNSim is applied to 30 distinct tandem G/G/K queue systems. Furthermore, its superiority over conventional simulation approach is shown in two dimensions which are average run time and maximum number of required iterations (scenarios).",
    "actual_venue": "Mathematics And Computers In Simulation"
  },
  {
    "abstract": "We give a characterization of NPDAs with reversal-bounded counters (NPCMs) in terms of context-free grammars with monotonic counters. We show that the grammar characterization can be used to give simple proofs of previously known results such as the semilinearity of the Parikh map of any language accepted by an NPCM. We prove a Chomsky–Schutzenberger-like theorem: A language L is accepted by an NPCM if and only if there is a k≥1 and an alphabet Σ containing at least k distinguished symbols, p1,...,pk, such that L=h(D∩Ek(R)) for some homomorphism h, Dyck language D⊆Σ⁎, and regular set R⊆Σ⁎, where Ek(R)={w|w∈R,|w|p1=⋯=|w|pk}. We also give characterizations of other machine models, such as visibly pushdown automata with reversal-bounded counters (VPCMs). We then investigate the complexity of some decision problems concerning these grammatical models. Finally, we introduce other grammatical models equivalent to NPCM.",
    "actual_venue": "Theoretical Computer Science"
  },
  {
    "abstract": "During the transition to packet-switched on-chip networks we lose the relative timing and ordering of requests, which are essential for shared memory coherency and the communication of spikes in hardware-based artificial neural networks. We present a bufferless network architecture that enforces a time-based sharing of multi-hop single-cycle paths, providing guaranteed services at low cost. We guarantee ordered delivery of requests, fixed network latency, and jitter-free neural spikes. In a 64-node network, we achieve a 84% lower latency and 7.5x higher throughput than SCORPIO. Full-system 36-core simulations show a 9% lower runtime than SCORPIO, with 39% lower power and 36% lower area.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "In this paper, we introduce a new formalism for mesh geometry prediction. We derive a class of smooth linear predictors from a simple approach based on the Taylor expansion of the mesh geometry function. We use this method as a generic way to compute weights for various linear predictors used for mesh compression and compare them with those of existing methods. We show that our scheme is actually equivalent to the Modified Butterfly subdivision scheme used for wavelet mesh compression. We also build new efficient predictors that can be used for connectivity-driven compression in place of other schemes like Average/Dual Parallelogram Prediction and High Degree Polygon Prediction. The new predictors use the same neighbourhood, but do not make any assumption on mesh anisotropy. In the case of Average Parallelogram Prediction, our new weights improve compression rates from 3% to 18% on our test meshes. For Dual Parallelogram Prediction, our weights are equivalent to those of the previous Freelence approach, that outperforms traditional schemes by 16% on average. Our method effectively shows that these weights are optimal for the class of smooth meshes. Modifying existing schemes to make use of our method is free because only the prediction weights have to be modified in the code.",
    "actual_venue": "Computer Graphics Forum"
  },
  {
    "abstract": "A subject of interest in classical and quantum mechanics is the development of the appropriate treatment of the time variable. In this paper we introduce a method of choosing the initial time eigensurface and how this method can be used to generate time-energy coordinates and, consequently, time-energy representations for classical and quantum systems.",
    "actual_venue": "Entropy"
  },
  {
    "abstract": "Flexible network configuration in software-defined networks makes it possible to dynamically restore flows. To this end, network devices carry out flow operations (i.e., adding or removing flow-entries to/from the flow-tables) to re-route the disrupted flows. Current flow restoration techniques do not consider the number of operations, and hence, are inefficient in disaster scenarios. We aim to minimize the number of operations in such cases and formulate integer programs to find a path: 1) with the lowest path cost requiring up to a given number of operations; 2) requiring the fewest possible operations; and 3) with a Dijkstra-like path cost requiring minimum operations. We study the tradeoff between path cost and the number of operations and prove that the second and third problems are polynomial-time solvable. We propose optimal/suboptimal algorithms with Dijkstra-like complexity that find nearly-optimal solutions. The simulation results show that our methods reduce the number of operations up to 50%, and the best performance is achieved when the number of failed links is small.",
    "actual_venue": "Ieee Trans Network And Service Management"
  },
  {
    "abstract": "It is a well-established fact that wireless sensor networks (WSNs) are very power constraint networks, but besides this, they are inherently more fault-prone than any other type of wireless network and their protocol design is very application specific. Major reasons for the faults are the unpredictable wireless communication channel, battery depletion, as well as fragility and mobility of the nodes. Furthermore, as traditional protocol design methods have proved inadequate, the cross-layer design (CLD) approach, which allows for interactions between different layers, providing more flexible and energy-efficient functionality, has emerged as a viable solution for WSNs. In this study we define a fault tolerance management module suitable to the requirements, limitations, and specifics of WSNs, encompassing methods for fault detection, fault prevention, fault management, and recovery. The suggested solution is in line with the CLD approach, which is an important factor in increasing the network performance. Through simulations the functionality of the network is evaluated, based on packet loss, delay, and energy consumption, and is compared with a similar solution not including fault management. The results achieved support the idea that the introduction of a unified approach to fault management improves the network performance as a whole.",
    "actual_venue": "Journal Of Zhejiang University: Science"
  },
  {
    "abstract": "Load balancing requirements in parallel image analysis are considered and results on theperformance of parallel implementations of two image feature extraction tasks on theConnection Machine and the iPSC/2 hypercube are reported and discussed. A loadredistribution algorithm, which makes use of parallel prefix operations and one-to-onepermutations among the processors, is described and has been used. The expectedimprovement in performance resulting from load balancing has been determinedanalytically and is compared to actual performance results obtained from the aboveimplementations. The analytical results demonstrate the specific dependence of theexpected improvement in performance on the computational and communicationrequirements of each task, characteristic machine parameters, a characterization of priorload distribution in terms of parameters which can be computed dynamically at the startof task execution, and the overhead incurred by load redistribution.",
    "actual_venue": "Ieee Trans Parallel Distrib Syst"
  },
  {
    "abstract": "Often during the requirements engineering (RE) process, the value of a requirement is assessed, e.g., in requirement prioritisation, release planning, and trade-off analysis. In order to support these activities, this research evaluates Goal Oriented Requirements Engineering (GORE) methods for the description of a requirement's value. Specifically, we investigate the goal-to-goal contribution relationship for its ability to demonstrate the value of a requirement, and propose that it is enriched with concepts such as correlation, confidence, and utility.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "In this paper, a video watermarking technique is proposed to hide copyright information in H.264 motion vectors. In this method, watermark is embedded in the motion vectors, and the region of motion vector is restricted to hide watermarking information. In order to obtain the good watermarked video quality, intra mode can be used to encode those macroblocks badly affected by restricting the region of the chosen motion vectors in inter mode. The watermark can be easily retrieved by the encoding mode of the macroblocks or the region of its motion vectors. The experimental results demonstrate that the video quality is almost the same as that of the original. The increase of bit rate is less than 1%. The proposed method keeps the excellences of hiding watermark by using the region of motion vectors. The capacity of watermark can be increased by choosing more macroblocks to the watermarks.",
    "actual_venue": "Csse"
  },
  {
    "abstract": "BackgroundInformation about drug–drug interactions (DDIs) is crucial for computational applications such as pharmacovigilance and drug repurposing. However, existing sources of DDIs have the problems of low coverage, low accuracy and low agreement. One common type of DDIs is related to the mechanism of drug metabolism: a DDI relation may be caused by different interactions (e.g., substrate, inhibit) between drugs and enzymes in the drug metabolism process. Thus, information from drug enzyme interactions (DEIs) serves as important supportive evidence for DDIs. Further, potential DDIs present implicitly could be detected by inference and reasoning based on DEIs.",
    "actual_venue": "J Biomedical Semantics"
  },
  {
    "abstract": "Interoperability in the public sector can be improved by the use of open standards. Nonetheless, the openness of standards in government policies is debatable. This paper introduces the Dutch government policy on open standards, and will introduce a multi-dimensional view (and model) on openness rather than a one-dimensional strict definition. Applicability of the multi-dimensional model is tested in a case study, which demonstrates that this model has value for standardization organizations active in the government domain. In future cases the model helps in understanding how government-related standardization organizations can influence openness in a situation-specific way and the model therefore narrows the gap between open standards policy and practice.",
    "actual_venue": "Ifip Advances In Information And Communication Technology"
  },
  {
    "abstract": "Numerical evidence is presented which strongly suggests that \"Jacobi's last geometric statement\"-that the conjugate locus from a point has exactly four cusps and the corresponding cut locus consists of only one topological segment-holds for compact real analytic Liouville surfaces diffeomorphic to S-2 if the Gaussian curvature is everywhere positive and has exactly six critical points, these being two saddles, two global minima, and two global maxima (as is the case for an ellipsoid). Our experiments suggest that this is a sufficient rather than a necessary condition. Furthermore, for compact real analytic Liouville surfaces diffeomorphic to S-2 upon which the Gaussian curvature can be negative but has exactly six critical points, these being two saddles, two global minima, and two global maxima, it appears that the cut locus is always a subarc of a line given by x(1) = const or x(2) = const, where (x(1),x(2)) are canonical coordinates with respect to which the metric has the form (f(1)(x(1)) + f(2)(x(2)))(dx(1)(2) + dx(2)(2)). In the case of an ellipsoid, these curves are lines of curvature.",
    "actual_venue": "Mathematics Of Computation"
  },
  {
    "abstract": "The effectiveness of a test data adequacy criterion for a given program and specification is the probability that a test set satisfying the criterion will expose a fault. Experiments were performed to compare the effectiveness of the mutation testing and all-uses test data adequacy criteria at various coverage levels, for randomly generated test sets. Large numbers of test sets were generated and executed, and for each, the proportion of mutants killed or def-use associations covered was measured. This data was used to estimate and compare the effectiveness of the criteria. The results were mixed: at the highest coverage levels considered, mutation was more effective than all-uses for five of the nine subjects, all-uses was more effective than mutation for two subjects, and there was no clear winner for two subjects. However, mutation testing was much more expensive than all-uses. The relationship between coverage and effectiveness for fixed-sized test sets was also explored and was found to be nonlinear and, in many cases, nonmonotonic.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "We formalize some basic properties of Fourier series in the logic of ACL2(r), which is a variant of ACL2 that supports reasoning about the real and complex numbers by way of non-standard analysis. More specifically, we extend a framework for formally evaluating definite integrals of real-valued, continuous functions using the Second Fundamental Theorem of Calculus. Our extended framework is also applied to functions containing free arguments. Using this framework, we are able to prove the orthogonality relationships between trigonometric functions, which are the essential properties in Fourier series analysis. The sum rule for definite integrals of indexed sums is also formalized by applying the extended framework along with the First Fundamental Theorem of Calculus and the sum rule for differentiation. The Fourier coefficient formulas of periodic functions are then formalized from the orthogonality relations and the sum rule for integration. Consequently, the uniqueness of Fourier sums is a straightforward corollary.We also present our formalization of the sum rule for definite integrals of infinite series in ACL2(r). Part of this task is to prove the Dini Uniform Convergence Theorem and the continuity of a limit function under certain conditions. A key technique in our proofs of these theorems is to apply the overspill principle from non-standard analysis.",
    "actual_venue": "Electronic Proceedings In Theoretical Computer Science"
  },
  {
    "abstract": "This paper presents a visual tracking system that is capable or running real time on-board a small UAV (Unmanned Aerial Vehicle). The tracking system is computationally efficient and invariant to lighting changes and rotation of the object or the camera. Detection and tracking is autonomously carried out on the payload computer and there are two different methods for creation of the image patches. The first method starts detecting and tracking using a stored image patch created prior to flight with previous flight data. The second method allows the operator on the ground to select the interest object for the UAV to track. The tracking system is capable of re-detecting the object of interest in the events of tracking failure. Performance of the tracking system was verified both in the lab and during actual flights of the UAV. Results show that the system can run on-board and track a diverse set of objects in real time.",
    "actual_venue": "Arxiv: Robotics"
  },
  {
    "abstract": "Storage and memory systems for modern data analytics are heavily layered, managing shared persistent data, cached data, and non- shared execution data in separate systems such as distributed file system like HDFS, in-memory file system like Alluxio and computation framework like Spark. Such layering introduces significant performance and management costs for copying data across layers redundantly and deciding proper resource allocation for all layers. In this paper we propose a single system called Pangea that can manage all data---both intermediate and long-lived data, and their buffer/caching, data placement optimization, and failure recovery---all in one monolithic storage system, without any layering. We present a detailed performance evaluation of Pangea and show that its performance compares favorably with several widely used layered systems such as Spark.",
    "actual_venue": "Proceedings Of The Vldb Endowment"
  },
  {
    "abstract": "In this paper we propose X-DART, a new Learning to Rank algorithm focusing on the training of robust and compact ranking models. Motivated from the observation that the last trees of MART models impact the prediction of only a few instances of the training set, we borrow from the DART algorithm the dropout strategy consisting in temporarily dropping some of the trees from the ensemble while new weak learners are trained. However, differently from this algorithm we drop permanently these trees on the basis of smart choices driven by accuracy measured on the validation set. Experiments conducted on publicly available datasets shows that X-DART outperforms DART in training models providing the same effectiveness by employing up to 40% less trees.",
    "actual_venue": "Sigir"
  },
  {
    "abstract": "Follow the promising Web 2.0 paradigm, the telecommunications world also wants to implement the Telco 2.0 vision by inviting its users to actively participate in the creating and sharing of services accessible using handheld devices. The EU-IST research project OPUCE (Open Platform for User-Centric Service Creation and Execution) aims at providing end users with an innovative platform which allows an easy creation and delivery of personalized communication and information services. This paper introduces a novel visual semantic service browser built on top of the OPUCE service repository which enables intuitive visualized service exploring and discovery while requires no technical semantic Web knowledge from the user.",
    "actual_venue": "Aina"
  },
  {
    "abstract": "This paper constructs a logic of soft constraints where the set of degrees of preference forms a partially ordered set. When the partially ordered set is a distributive lattice, this reduces to the idempotent semiring-based CSP approach, and the lattice operations can be used to define a sound and complete proof theory. For the general case, it is shown how sound and complete deduction can be performed by using a particular embedding of a partially ordered set in a distributive lattice.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "Diabetes has become a leading cause of death worldwide. Although there is no cure for diabetes, blood glucose monitoring combined with appropriate medication can enhance treatment efficiency, alleviate the symptoms, as well as diminish the complications. For point-of-care purposes, continuous glucose monitoring (CGM) devices are considered to be the best candidates for diabetes therapy. This review focuses on current growth areas of CGM technologies, specifically focusing on subcutaneous implantable electrochemical glucose sensors. The superiority of CGM systems is introduced firstly, and then the strategies for fabrication of minimally-invasive and non-invasive CGM biosensors are discussed, respectively. Finally, we briefly outline the current status and future perspective for CGM systems.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "Several fairly large sets of programming rules have been developed recently. It is natural to ask whether the process of developing such rule bases may converge. Having developed sets of rules for specific programming tasks and domains, will they be helpful when other tasks and domains are considered? While it is too early to give definitive answers, experience with the rules of the PECOS system has been positive. Both during the process of developing the rule set and while developing rules for another domain, the existence of already codified rules proved very helpful.",
    "actual_venue": "Acm Trans Program Lang Syst"
  },
  {
    "abstract": "In this article, I present a practical configuration-space computation algorithm for pairs of curved planar parts, based on the general algorithm developed by Bajaj and me. The general algorithm advances the theoretical understanding of configuration-space computation, but is too slow and fragile for some applications. The new algorithm solves these problems by restricting the analysis to parts bounded by line segments and circular arcs, whereas the general algorithm handles rational parametric curves. The trade-off is worthwhile, because the restricted class handles most robotics and mechanical engineering applications. The algorithm reduces run time by a factor of 60 on nine representative engineering pairs, and by a factor of 9 on two human-knee pairs. It also handles common special pairs by specialized methods. A survey of 2,500 mechanisms shows that these methods cover 90% of pairs and yield an additional factor of 10 reduction in average run time. The theme of this article is that application requirements, as well as intrinsic theoretical interest, should drive configuration-space research.",
    "actual_venue": "International Journal Of Robotic Research"
  },
  {
    "abstract": "In this paper, restorations for both voltage and frequency in the droop-controlled inverter-based islanded microgrid (MG) are addressed. A distributed finite-time control approach is used in the voltage restoration which enables the voltages at all the distributed generations (DGs) to converge to the reference value in finite time, and thus, the voltage and frequency control design can be separated. Then, a consensus-based distributed frequency control is proposed for frequency restoration, subject to certain control input constraints. Our control strategies are implemented on the local DGs, and thus, no central controller is required in contrast to existing control schemes proposed so far. By allowing these controllers to communicate with their neighboring controllers, the proposed control strategy can restore both voltage and frequency to their respective reference values while having accurate real power sharing, under a sufficient local stability condition established. An islanded MG test system consisting of four DGs is built in MATLAB to illustrate our design approach, and the results validate our proposed control strategy.",
    "actual_venue": "Industrial Electronics, Ieee Transactions"
  },
  {
    "abstract": "We introduce the notion of monadic pseudo BCI-algebras and study some related properties. Then, we introduce monadic filters and monadic congruences of monadic pseudo BCI-algebras and discuss the relations between them. We proved that there is a one-to-one correspondence between the set of closed m-congruence relations and the set of normal closed m-filters in a monadic pseudo BCI-algebra. Moreover, we introduce a notion of strong residuated mappings and study the relation between monadic operators and strong residuated mappings in pseudo BCI-algebras. Let A be a pseudo BCI-algebra and \\(f:A\\rightarrow A\\) be a mapping, we obtain that \\((f, f^+)\\) is a monadic operator on A if and only if f is a strong residuated mapping on A where \\(f^+\\) is the residual of f. Also we exhibit an axiom system of monadic pseudo BCI-logic, which enrich the language of pseudo BCI-logics. Based on the monadic pseudo BCI-algebras, we prove the completeness and soundness of the monadic pseudo BCI-logic propositional system. Finally, using provable formula set, normal subset and monadic subset in the set of all formulas of a monadic pseudo BCI-logic \\(\\mathcal {L}\\), we characterize filters, normal filters and monadic normal filters in a monadic pseudo BCI-algebra, respectively.",
    "actual_venue": "Soft Comput"
  },
  {
    "abstract": "The interaction of hexamminecobalt(III), Co(NH3)(6)(3+) with 160 and 3000-8000 bp length calf thymus DNA has been investigated by circular dichroism, acoustic and densimetric techniques. The acoustic titration curves of 160 bp DNA revealed three stages of interaction: (i) Co(NH3)(6)(3+) binding up to the molar ratio [Co(NH3)(6)(3+)]/[P] = 0.25, prior to DNA condensation; (ii) a condensation process between [Co(NH3)(6)(3+)]/[P] = 0.25 and 0.30; and (iii) precipitation after [Co(NH3)(6)(3+)]/[P] = 0.3. In the case of 3000-8000 bp DNA only two processes were observed: (i) binding up to [Co(NH3)(6)(3+)]/[P] = 0.3; and (ii) precipitation after this point. In agreement with earlier observations, long DNA aggregates without changes in its B-form circular dichroism spectrum, while short DNA demonstrates a positive B --> Psi transition after [Co(NH3)(6)(3+)]/[P] = 0.25, From ultrasonic and densimetric measurements the effects of Co(NH3)(6)(3+) binding on volume and compressibility have been obtained. The binding of Co(NH3)(6)(3+) to both short and long DNA is characterized by similar changes in volume and compressibility calculated per mole Co(NH3)(6)(3+): DeltaV= 9 cm(3) mol(-1) and Delta kappa = 33 x 10(-4) cm(3) mol(-1) bar(-1). The positive sign of the parameters indicates dehydration, i.e, water release from Co(NH3)(6)(3+) and the atomic groups of DNA, This extent of water displacement would be consistent with the formation of two direct, hydrogen bonded contacts between the cation and the phosphates of DNA.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "The edge revised Szeged index S z e ź ( G ) is defined as S z e ź ( G ) = ź e = u v ź E ( m u ( e ) + m 0 ( e ) / 2 ) ( m v ( e ) + m 0 ( e ) / 2 ) , where m u ( e ) and m v ( e ) are, respectively, the number of edges of G lying closer to vertex u than to vertex v and the number of edges of G lying closer to vertex v than to vertex u , and m 0 ( e ) is the number of edges equidistant to u and v . In this paper, we give an upper bound of the edge revised Szeged index for a connected bicyclic graphs with size m ź 5 , that is, S z e ź ( G ) ź { ( m 3 - 4 m + 16 ) / 4 , ifź m źisźoddź, ( m 3 - 4 m + 18 ) / 4 , ifź m źisźeven with equality if and only if G is the graph obtained from the cycle C m - 2 by duplicating a single vertex.",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "We proposed a Traffic Analysis Resistant Network (TARN) that randomizes IP addresses in a fashion similar to Frequency Hop Spread Spectrum (FHSS), allowing users to blend into background traffic. IP hopping alone is not enough. TARN may still be susceptible to side-channel analysis. To remove the vulnerabilities, we introduce a SDX-based solution. In this work, we describe the design and implementation of TARN and experimental environment used to test TARN.",
    "actual_venue": "Ieee International Conference On Network Protocols"
  },
  {
    "abstract": "Industry is increasingly demanding IT support for large engineering processes, i.e., process structures consisting of hundreds up to thousands of processes. Developing a car, for example, requires the coordination of development processes for hundreds of components. Each of these development processes itself comprises a number of interdependent processes for designing, testing, and releasing the respective component. Typically, the resulting process structure becomes very large and is characterized by a strong relation with the assembly of the product. Such process structures are denoted as data-driven. On the one hand, the strong linkage between data and processes can be utilized for automatically creating process structures. On the other hand, it is useful for (dynamically) adapting process structures at a high level of abstraction. This paper presents new techniques for (dynamically) adapting data-driven process structures. We discuss fundamental correctness criteria needed for (automatically) detecting and disallowing dynamic changes which would lead to an inconsistent runtime situation. Altogether, our COREPRO approach provides a new paradigm for changing data-driven process structures at runtime reducing costs of change significantly.",
    "actual_venue": "Ca()Se"
  },
  {
    "abstract": "On the one hand, probabilistic model checkers such as PRISM have been successfully employed to verify models of probabilistic systems. However, they are not suitable for checking properties such as uncaught exceptions of the actual code of the system. On the other hand, model checkers such as Java PathFinder (JPF) have been used with success to verify actual code of systems. However, they do not take into account the probabilities associated with the probabilistic choices of the systems. In this paper, we bridge the gap by extending JPF so that it takes those probabilities into account. We introduce a method to express a probabilistic choice in Java so that JPF can easily extract the probabilities of the alternatives of the probabilistic choice. By default, JPF traverses the state space using a depth-first search or a breadth-first search. We have implemented in JPF several search strategies which use the probabilities associated with the alternatives of probabilistic choices. To address the state explosion problem, we keep track of the amount of progress made by JPF.",
    "actual_venue": "Qest"
  },
  {
    "abstract": "Objectives: One of the tasks, of information management is systematic planning of a Hospital Information System (HIS). However, the description and the analysis of the current state of a HIS typically create high costs and are not well supported. The aim of this paper is therefore to report about the specification of a reference model for the domain layer of a Hospital Information System. Methods: We developed a reference model for the domain layer of a Hospital Information System based oil the requirements index for information processing in hospitals for describing the enterprise functions, and based oil the object types from the Health Level 7 Reference Information Model (HL7-RIM)for describing the entity types. Result: The developed reference model is a comprehensive hierarchic model of the enterprise functions of hospital information systems. The central enterprise function \"patient treatment\" for example is described with 35 enterprise functions and 38 entity types oil a three-level hierarchy. Discussion: Reference models provide a kind of modelling patterns that call easily be used and adapted to a respective Information System. The availability of reference models should therefore provide a highly valuable contribution to keep the costs for modelling Hospital Information Systems low. We will start to evaluate the reference model by using it in the description of the information systems of a University Clinic of the Tiroler Landeskrankenanstalten GmbH (TILAK), Austria. If this pre-test is positive, it is planned to extend the use of the reference model to the overall Hospital Information System of the TILAK.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "For indoor positioning scenarios, the advantage of IMU (Inertial Measurement Unit) based method lies on its high positioning information refresh rate and the strong ability of anti-interference. However, due to the procedure of recursive integral, the cumulative positioning error of IMU positioning grows over time and will diverge in the long run. Due to the high time resolution, the accuracy of ultrasonic positioning is extremely high, and the measured location information is independent of each other. But limited by the physical properties, the updating frequency of ultrasound positioning is low as compared to IMU positioning and it is easy to produce wild value. To achieve the complementary advantages of the above mentioned methods, we propose a fusion localization approach which is based on IMU and augmented with ultrasonic sensors. The EKF (Extended Kalman Filter) is adapted for position estimation and the innovation filter is added into EKF in order to remove the wild value of measurement. The IMU location data is corrected with the fused error information, and so as to achieve the fusion positioning. The simulation shows that compared with the single measurement positioning, our fusion method can ensure not only the positioning accuracy but also the stability and anti-interference of the localization.",
    "actual_venue": "International Conference On Big Data Computing And Communications"
  },
  {
    "abstract": "In this paper, the problem of verifying integral input-to-state stability (iISS) and input-to-state stability (ISS) of interconnected stochastic systems is addressed. The aim is at extending the deterministic iISS small-gain framework to stochastic systems by including ISS as a special case. This paper highlights fundamental differences between the stochastic and deterministic cases. Dealing with nonlinearities in interconnection gives rise to a unique issue in the presence of stochastic noises. The key is to cope effectively with the gradient of Lyapunov functions of subsystems in composing a Lyapunov function of the entire system. It is demonstrated that the influence of the gradient appears considerably different in two types of robustness notions. This paper also clarifies the influence of the gradient on the establishment of a trajectory estimate of iISS which is weaker than ISS.",
    "actual_venue": "American Control Conference"
  },
  {
    "abstract": "In this paper we cast the long-standing debate in transformational grammar over movement derivations vs. well-formedness conditions on chains in a new light. We claim that the argument itself is misguided: a transformational grammar should have both a derivational and representational interpretation, connected by soundness and completeness results. Second, we argue that the proper form of the representational interpretation is as an axiomatization of T-markers rather than P-markers (e.g., LF structures). Little of significance in representationalist approaches is lost by this move, however. Antecedent goverment conditions on chains, for example, can still be easily stated in terms of the kinds of T-markers that we propose. Many well-known problems for representational approaches are avoided, however. In addition, from a derivational perspective, global constraints on derivations become simple structural well-formedness conditions on T-markers. We offer a rigorous formalization of a simple system of T-markers and show its equivalence to a simple minimalist derivation system.",
    "actual_venue": "Lacl"
  },
  {
    "abstract": "Video scene detection is the task of dividing a video into semantic sections. To perform this fundamental task, we propose a novel and effective method for temporal grouping of scenes using an arbitrary set of features computed from the video. We formulate the task of video scene detection as a generic optimization problem to optimally group shots into scenes, and propose an efficient procedure for solving the optimization problem based on a novel dynamic programming scheme. This unique formulation directly results in a temporally consistent segmentation, and has the advantage of being parameter-free, making it applicable across various domains. We provide detailed experimental results, showing that our algorithm outperforms current state-of-the-art methods. To assess the comprehensiveness of this method even further, we present experimental results testing different types of modalities and their applicability in this formulation.",
    "actual_venue": "International Journal Of Semantic Computing"
  },
  {
    "abstract": "Given a text string of lengthn and a pattern string of lengthm over ab-letter alphabet, thek differences approximate string matching problem asks for all locations in the text where the pattern occurs with at mostk differences (substitutions, insertions, deletions). We treatk not as a constant but as a fraction ofm (not necessarily constant-fraction). Previous algorithms require at leastO(kn) time (or exponential space). We give an algorithm that is sublinear time0((n/m)k log\n \n b\n \n m) when the text is random andk is bounded by the threshold m/(logb\n m + O(1)). In particular, whenk=o(m/logb\n m) the expected running time iso(n). In the worst case our algorithm is O(kn), but is still an improvement in that it is practical and uses0(m) space compared with0(n) or0(m\n 2). We define three problems motivated by molecular biology and describe efficient algorithms based on our techniques: (1)\n approximate substring matching, (2) approximate-overlap detection, and (3) approximate codon matching. Respectively, applications\n to biology are local similarity search, sequence assembly, and DNA-protein matching.",
    "actual_venue": "Algorithmica"
  },
  {
    "abstract": "The bribery problem in elections has received a considerable amount of attention. In this paper, we initiate the study of a related, but new problem, the protection problem, namely protecting elections from bribery. In this problem, there is a defender who is given a defense budget and can use the budget to award some of the voters such that they cannot be bribed anymore. This naturally leads to the following bi-level decision problem: Is it possible for the defender with a given defense budget to protect an election from being manipulated by the attacker with a given attack budget for bribing voters? We characterize the computational complexity of the protection problem. We show that it is in general significantly harder than the bribery problem. However, the protection problem can be solved, under certain circumstances, in polynomial time.",
    "actual_venue": "Proceedings Of The International Conference On Autonomous Agents And Multiagent Systems"
  },
  {
    "abstract": "In this paper, a new method of low frequency multiple-point equalization for room acoustics is presented. This approach is theoretically based on Common Acoustical Pole (CAP) modeling of the room transfer functions developed previously by Haneda et al. [1]. With the hybrid approach, the multiple-point equalizer is made up with cascaded notch filters which coefficients are directly determined from the CAP model. This new method reduces the complexity of the equalizing structure in addition to an automatisation of notch filters design which should be very attractive to many applications such as home theater systems by intelligently controlling room modes.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "We introduce a generalized method for constructing subquadratic complexity multipliers for even characteristic field extensions. The construction is obtained by recursively extending short convolution algorithms and nesting them. To obtain the short convolution algorithms, the Winograd short convolution algorithm is reintroduced and analyzed in the context of polynomial multiplication. We present a recursive construction technique that extends any d point multiplier into an n=d/sup k/ point multiplier with area that is subquadratic and delay that is logarithmic in the bit-length n. We present a thorough analysis that establishes the exact space and time complexities of these multipliers. Using the recursive construction method, we obtain six new constructions, among which one turns out to be identical to the Karatsuba multiplier. All six algorithms have subquadratic space complexities and two of the algorithms have significantly better time complexities than the Karatsuba algorithm.",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "Deep learning models are widely used in object detection area, including combination of multiple non-linear data transformations. The objective is receiving brief and concise information for feature representations. Due to the high volume of processing data, object detection in videos has been faced with big challenges, such as mass calculation. To increase the object detection precision in videos, a hybrid method is proposed, in this paper. Some modifications are applied to auto encoder neural networks, for the compact and discriminative learning of object features. Furthermore, for object classification, firstly extracted features are transferred to a convolutional neural network, and after feature convolution with input pictures, they will be classified. The proposed method has two main advantages over other unsupervised feature learning techniques. Firstly, as it will be shown, features are detected with a much higher precision. Secondly, in the proposed method, the outcome is compact and additional unnecessary information is removed; while the existing unsupervised feature learning models mainly learn repeated and redundant information of the features. Experimental evaluation shows that precision of feature detection improved by 1.5% in average in compare with the state-of-the-art methods.",
    "actual_venue": "Signal Processing: Algorithms, Architectures, Arrangements, And Applications"
  },
  {
    "abstract": "Lane detection is a significant component of driver assistance systems. Highway-based lane departure warning solutions are in the market since the mid-1990s. However, improving and generalizing vision-based lane detection remains to be a challenging task until recently. Among various lane detection methods developed, strong lane models, based on the global assumption of lane shape, have shown robustness in detection results, but are lack of flexibility to various shapes of lane. On the contrary, weak lane models will be adaptable to different shapes, as well as to maintain robustness. Using a typical weak lane model, particle filtering of lane boundary points has been proved to be a robust way to localize lanes. Positions of boundary points are directly used as the tracked states in the current research. This paper introduces a new weak lane model with this particle filter-based approach. This new model parameterizes the relationship between points of left and right lane boundaries, and can be used to detect all types of lanes. Furthermore, a modified version of an Euclidean distance transform is applied on an edge map to provide information for boundary point detection. In comparison to an edge map, properties of this distance transform support improved lane detection, including a novel initialization and tracking method. This paper fully explains how the application of this distance transform greatly facilitates lane detection and tracking. Two lane tracking methods are also discussed while focusing on efficiency and robustness, respectively. Finally, the paper reports about experiments on lane detection and tracking, and comparisons with other methods.",
    "actual_venue": "Mach Vis Appl"
  },
  {
    "abstract": "Summary: The reliable assessment of the quality of protein structural models is fundamental to the progress of structural bioinformatics. The ModFOLD server provides access to two accurate techniques for the global and local prediction of the quality of 3D models of proteins. Firstly ModFOLD, which is a fast Model Quality Assessment Program (MQAP) used for the global assessment of either single or multiple models. Secondly ModFOLDclust, which is a more intensive method that carries out clustering of multiple models and provides per-residue local quality assessment. Availability: http://www.biocentre.rdg.ac.uk/bioinformatics/ModFOLD/ Contact: l.j.mcguffin@reading.ac.uk",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "In this paper, a criterion for nonlinear stability analysis of microwave oscillator has been devised. The circuit envelope method has been used for analyzing the perturbed circuit. The proposed approach is evaluated by analyzing the nonlinear stability of a practical FET oscillator.",
    "actual_venue": "Ieice Transactions On Electronics"
  },
  {
    "abstract": "Time-Triggered Ethernet (TTEthernet) is an SAE standard of a real-time Ethernet extension, which supports real-time requirements, fault isolation and mixed criticality applications. TTEthernet supports different communication mechanisms ranging from best-effort messaging with a high channel utilization to predictable real-time messaging based on a time-triggered communication schedule. This paper presents a simulation framework for TTEthernet-based systems, which supports the analysis and validation of TTEthernet-based applications at early development stages. We introduce generic model building blocks (e.g., TTEthernet switches, TTEthernet end systems, fault injectors), which can be instantiated, configured and extended to model distributed embedded applications. In particular, these building blocks can be configured to support application-specific time-triggered schedules and communication topologies. The fault injector allows to evaluate the reliability in the presence of messages failures with given failure modes and failure rates. We demonstrate the simulation environment in an example scenario with two TTEthernet switches, multiple end systems and injected faults.",
    "actual_venue": "Industrial Informatics"
  },
  {
    "abstract": "Many string manipulations can be performed efficiently on suffix trees. In this paper a CRCW parallel RAM algorithm is presented\n that constructs the suffix tree associated with a string ofn symbols inO(logn) time withn processors. The algorithm requires Θ(n\n 2) space. However, the space needed can be reduced toO(n\n 1+ɛ) for any 0< ɛ ≤1, with a corresponding slow-down proportional to 1/ɛ. Efficient parallel procedures are also given for some\n string problems that can be solved with suffix trees.",
    "actual_venue": "Algorithmica"
  },
  {
    "abstract": "This paper deals with a path planning problem in the dynamic and cluttered environments. The presence of moving obstacles and kinodynamic constraints of the robot increases the complexity of path planning problem. We model the environment and motion of dynamic obstacles in 3D time-space. We propose the utilization of the arrival time field for examining the most promising area in those obstacles-occupied 3D time-space for approaching the goal. The arrival time field is used for guiding the expansion of a randomized tree search in a favorable way, considering kinodynamic constraints of the robot. The quality and the optimality of the path are taken into account by performing heuristic methods on the randomized tree. Simulation results are also provided to prove the feasibility, possibility, and effectiveness of our algorithm.",
    "actual_venue": "International Conference On Robotics And Automation"
  },
  {
    "abstract": "This work is concerned with the analysis, based on MATLAB simulation, of 1st and 2nd order Multi-level envelope delta sigma modulators (EDSMs) that are used for high efficiency and linearity wireless mobile transmitter architectures with an Orthogonal Frequency Division Modulation (OFDM) signal and long term evolution (LTE) signal. The analysis is based on measuring the linearity and the efficiency of the three-level EDSM. It was shown that, at different input signal power levels of the three-level EDSM, 1st order three-level EDSM is able to achieve a performance that is close to the performance of the 2nd order counterpart. While 1st order DSM requires less circuitry, it is recommended to employ 1st order three-level EDSM instead of using 2nd order three-level EDSM. 1st order three-level EDSM has a signal to noise distortion ratio (SNDR) and coding efficiency of 58 dB and 77%, respectively. While 2nd order three-level EDSM showed SNDR of 61.5 dB and coding efficiency of 76.4%. Also, with a long term evolution (LTE) signal, the 1st order EDSM has better performance, in terms of SNDR and CE, than that of the 2nd order EDSM circuit.",
    "actual_venue": "J Electrical And Computer Engineering"
  },
  {
    "abstract": "BACKGROUND: The enormous throughput and low cost of second-generation sequencing platforms now allow research and clinical geneticists to routinely perform single experiments that identify tens of thousands to millions of variant sites. Existing methods to annotate variant sites using information from publicly available databases via web browsers are too slow to be useful for the large sequencing datasets being routinely generated by geneticists. Because sequence annotation of variant sites is required before functional characterization can proceed, the lack of a high-throughput pipeline to efficiently annotate variant sites can act as a significant bottleneck in genetics research. RESULTS: SeqAnt (Sequence Annotator) is an open source web service and software package that rapidly annotates DNA sequence variants and identifies recessive or compound heterozygous loci in human, mouse, fly, and worm genome sequencing experiments. Variants are characterized with respect to their functional type, frequency, and evolutionary conservation. Annotated variants can be viewed on a web browser, downloaded in a tab-delimited text file, or directly uploaded in a BED format to the UCSC genome browser. To demonstrate the speed of SeqAnt, we annotated a series of publicly available datasets that ranged in size from 37 to 3,439,107 variant sites. The total time to completely annotate these data completely ranged from 0.17 seconds to 28 minutes 49.8 seconds. CONCLUSION: SeqAnt is an open source web service and software package that overcomes a critical bottleneck facing research and clinical geneticists using second-generation sequencing platforms. SeqAnt will prove especially useful for those investigators who lack dedicated bioinformatics personnel or infrastructure in their laboratories.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "In this paper, we propose a trajectory control method for a redundant flexible space manipulator with slewing and deployable links on a space platform. This method consists manipulator tip position feedback with a transposed Jacobian matrix and local vibration control. We newly derive the Jacobian matrix for a flexible manipulator considering link deformation with the assumed modes method. Simulation results show the effectiveness of this method. We use dynamics consisting of an order N algorithm for N bodies, based on the non-recursive Lagrangian approach, to simulate the dynamics of an orbiting manipulator with an arbitrary number of slewing and deployable flexible links.",
    "actual_venue": "Advanced Robotics"
  },
  {
    "abstract": "It is shown that any member of the Frank copula family fulfills a family of functional inequalities which arises as a family of sufficient conditions that allow to characterize the transitivity of reciprocal relations expressing the winning probabilities among random variables artificially coupled by a same copula. The inequalities are parameterized by a single integer parameter. The first of these inequalities expresses symmetry of the copula, while the second one expresses an upper bound on its volume. Subsequent inequalities are increasingly restrictive.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "As business intelligence becomes increasingly essential for organizations and as it evolves from strategic to operational, the complexity of Extract-Transform-Load (ETL) processes grows. In consequence, ETL engagements have become very time consuming, labor intensive, and costly. At the same time, additional requirements besides functionality and performance need to be considered in the design of ETL processes. In particular, the design quality needs to be determined by an intricate combination of different metrics like reliability, maintenance, scalability, and others. Unfortunately, there are no methodologies, modeling languages or tools to support ETL design in a systematic, formal way for achieving these quality requirements. The current practice handles them with ad-hoc approaches only based on designers' experience. This results in either poor designs that do not meet the quality objectives or costly engagements that require several iterations to meet them. A fundamental shift that uses automation in the ETL design task is the only way to reduce the cost of these engagements while obtaining optimal designs. Towards this goal, we present a novel approach to ETL design that incorporates a suite of quality metrics, termed QoX, at all stages of the design process. We discuss the challenges and tradeoffs among QoX metrics and illustrate their impact on alternative designs.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "Let d=3. In PG(d(d+3)/2,2), there are four known non-isomorphic d-dimensional dual hyperovals by now. These are Huybrechts' dual hyperoval by Huybrechts (2002) [4], Buratti-Del Fra's dual hyperoval by Buratti and Del Fra (2003) [1], Del Fra and Yoshiara (2005) [3], Veronesean dual hyperoval by Thas and van Maldeghem (2004) [9], Yoshiara (2004) [12] and the dual hyperoval, which is a deformation of Veronesean dual hyperoval by Taniguchi (2009) [6]. In this paper, using a generator @s of the Galois group Gal(GF(2^d^m)/GF(2)) for some m=3, we construct a d-dimensional dual hyperoval T\\\"@s in PG(3d,2), which is a quotient of the dual hyperoval of [6]. Moreover, for generators @s,@t@?Gal(GF(2^d^m)/GF(2)), if T\\\"@s and T\\\"@t are isomorphic, then we show that @s=@t or @s=@t^-^1 on GF(2^d). Hence, we see that there are many non-isomorphic quotients in PG(3d,2) for the dual hyperoval of [6] if d is large.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "This paper studies the power minimization problem for the MIMO broadcast channel. The optimal solution involves interference-balancing (IB) and iterative convex optimization procedures. In this paper, the zero-forcing (ZF) problem is considered, with dirty paper coding (DPC), resulting in a simple non-iterative implementation using the block diagonal geometric mean decomposition (BD-GMD). Subchannel selection is applied in order to approach the performance of the optimal IB solution. Optimal and near-optimal solutions are provided to find the encoding order and subchannel selection for each user. The advantages of the methods proposed are their non-iterative nature and much reduced computational complexity. Simulations run on both uncorrelated and correlated channels show that a transmit power close to the optimal IB solution can be reached.",
    "actual_venue": "Calgary, Bc"
  },
  {
    "abstract": "Computer systems are taking an important role in the field of medicine with the introduction of electronic biomedical devices in diagnosis and treatment, where medical records obtained from the devices are documented and processed as reference to medical panels. The capability of latest computed tomography technology in generating three-dimensional patient model for graphical representation is very informative for surgeons to plan and make decision before the operation. This initiates the idea of implementing the model into computer-generated simulator for surgical education and training. This paper presents the development of virtual reality cataract surgery simulator. Three-dimensional eye model and surgical instruments are generated as the virtual surgical environment. The system is equipped with a pair of haptic devices to provide actual sensation. The results show that the simulator is capable of providing interactive training on the main procedures of cataract surgery. It has the potential to be incorporated as part of the curriculum of medical program when the proposed future work in the end of the paper is completed.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "This paper presents a design solution that enables the use of powerful multi-bit error-correcting code (ECC) to realize L2 cache defect tolerance at minimal latency and silicon area cost. This work is motivated by the observation that the continuous CMOS Technology scaling may result in an increasing level defect density and make conventional cache memory defect tolerance strategies inadequate. The basic idea is to complement conventional L2 cache core with two separate fully associative caches, one stores multi-bit ECC check bits for realizing area-efficient selective multi-bit ECC protection and another one stores the most recently decoded multi-bit ECC code word to minimize the impact of explicit multi-bit ECC decoding on L2 cache access latency. Its effectiveness has been demonstrated using SimpleScalar and Cacti tools. At the defect density of 0.5%, this design approach can maintain almost the same instruction per cycle (IPC) performance over a wide spectrum of benchmarks compared with ideal L2 cache without defects, while only incurring less than 2.5% of silicon area overhead.",
    "actual_venue": "DFT"
  },
  {
    "abstract": "Many data sets, from different and seemingly unrelated marketing domains, all involve paths---records of consumers' movements in a spatial configuration. Path data contain valuable information for marketing researchers because they describe how consumers interact with their environment and make dynamic choices. As data collection technologies improve and researchers continue to ask deeper questions about consumers' motivations and behaviors, path data sets will become more common and will play a more central role in marketing research. To guide future research in this area, we review the previous literature, propose a formal definition of a path (in a marketing context), and derive a unifying framework that allows us to classify different kinds of paths. We identify and discuss two primary dimensions (characteristics of the spatial configuration and the agent) as well as six underlying subdimensions. Based on this framework, we cover a range of important operational issues that should be taken into account as researchers begin to build formal models of path-related phenomena. We close with a brief look into the future of path-based models, and a call for researchers to address some of these emerging issues.",
    "actual_venue": "Marketing Science"
  },
  {
    "abstract": "In real-world law enforcement encounters, a seemingly docile situation can turn violent in a matter of seconds. Being able to identify the signs of imminent aggression or violence may be extremely important in keeping safe those in harm's way. In the current study university student observers were shown an array of 12 facial expressions that included target images for two types of assault - premeditated assault and a loss of impulse control assault - and some distractors. The target images were created based on consultation with individuals who had experienced incidences of physical assault, aggression, and/or violence. The observers were split into two groups - those who had experience with physical assault and those who did not. The group with experience selected the target images as those preceding assault; the group with no experience did not. These findings provide continuing evidence that reliable facial signs of imminent aggression may exist, and if so that people in harm's way can be trained to identify those signs before they happen.",
    "actual_venue": "Ieee International Conference On Intelligence And Security Informatics: Big Data, Emergent Threats, And Decision-Making In Security Informatics"
  },
  {
    "abstract": "In what circumstances might privacy concerns about new communication tools like instant messaging help predict the degree to which people feel comfortable communicating via these new communication tools? The current study examined whether topic intimacy and perceived privacy predict levels of comfort with disclosure, and whether these associations are moderated by overall levels of trust and frequency of technology use. Participants reported on the degree to which they would feel comfortable discussing each of 32 topics (e.g., ''times when I felt that I was in love'') using 10 different communication tools. Topic and tool interacted, such that the privacy of the communication tool was related to disclosure comfort only for intimate topics. Privacy concerns were more important to less frequent technology users, and topic intimacy mattered most to participants with low levels of trust. Results are discussed in terms of implications for extending models of disclosure to the selection of new communication tools.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "This paper addresses the problem of scheduling hard and non-hard real-time sets of tasks that share the processor. The notions of singularity and k-schedulability are introduced and methods based on them are proposed. The execution of hard tasks is postponed in such a way that hard deadlines are not missed but slack time is advanced to execute non-hard tasks. In a first application, two singularity methods are used to schedule mixed systems with hard deterministic sets and stochastic non-hard sets. They are compared to methods proposed by other authors (servers, slack stealing), background and M/M/1. The metric is the average response time in servicing non-hard tasks and the proposed methods show a good relative performance. In a second application, the previous methods, combined with two heuristics, are used for the on-line scheduling of real-time mandatory/reward-based optional systems with or without depreciation of the reward with time. The objective is to meet the mandatory time-constraints and maximize the reward accrued over the hyperperiod. To the best of the authors' knowledge, these are the only on-line methods proposed to address the problem and outperform Best Incremental Return, often used as a yardstick.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Collective communication libraries are widely developed and used in scientific community to support parallel and Grid programming. On the other side they often lack in Mobile Agents systems even if message passing is always supported to grant communication ability to the agents. Collective communication primitives can help to develop agents based parallel application. They can also benefit social ability and interactions of collaborative agents. Here we present a collective communication service implemented in the Jade agent platform. Furthermore we propose its exploitation to interface transparently heterogeneous executions instances of a scientific parallel application that runs in a distributed environment.",
    "actual_venue": "Ispa"
  },
  {
    "abstract": "The spatial separation of the transmitter and the receiver in bistatic synthetic aperture radar (SAR) enables a variety of data acquisition geometries to achieve benefits like the increased information content of bistatic SAR data. In the case of hybrid bistatic SAR constellations where the transmitter is spaceborne and the receiver is onboard an aircraft, one has to deal with a huge discrepancy between platform velocities. This paper presents bistatic spaceborne/airborne SAR experiments, where the radar satellite TerraSAR-X is used as a transmitter and the airborne SAR sensor Phased Array Multifunctional Imaging Radar (PAMIR) of the Fraunhofer Institute for High Frequency Physics and Radar Techniques (FHR) is used as a receiver. Both sensors are equipped with phased-array antennas, which offer the possibility of beam steering and could be used for the first time for the \"double sliding spotlight mode.\" In this mode, the space-and airborne sensors operate with different sliding factors (ratio between footprint and platform velocity). The performance of two different experiments is analyzed, and the novel double sliding spotlight mode is presented. This paper describes the experimental setups, the synchronization system, and the data acquisition. The image results were processed by a modified backprojection algorithm and a frequency-domain algorithm. The analysis of the final bistatic images comprises the spatial resolution and the scattering behavior of selected objects. Parts of the bistatic SAR images are compared with the corresponding monostatic images of PAMIR and TerraSAR-X. It will be shown that hybrid bistatic SAR is a worthwhile and helpful addition to current monostatic SAR.",
    "actual_venue": "Ieee Trans Geoscience And Remote Sensing"
  },
  {
    "abstract": "This paper presents a workflow designed to quantitatively characterize the 3D structural attributes of macroscopic tissue specimens acquired at a micron level resolution using light microscopy. The specific application is a study of the morphological change in a mouse placenta induced by knocking out the retinoblastoma gene.This workflow includes four major components: (i) serial section image acquisition, (ii) image preprocessing, (iii) image analysis involving 2D pair-wise registration, 2D segmentation and 3D reconstruction, and (iv) visualization and quantification of phenotyping parameters. Several new algorithms have been developed within each workflow component. The results confirm the hypotheses that (i) the volume of labyrinth tissue decreases in mutant mice with the retinoblastoma (Rb) gene knockout and (ii) there is more interdigitation at the surface between the labyrinth and spongiotrophoblast tissues in mutant placenta. Additional confidence stem from agreement in the 3D visualization and the quantitative results generated.The source code is available upon request.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "Due to the limits of battery capacity of mobile devices, how to select cloud services to invoke in order to reduce energy consumption in mobile environments is becoming a critical issue. This paper addresses the problem of mobile service selection for composition in terms of energy consumption. It formally models this problem and constructs energy consumption computation models. Energy consumption...",
    "actual_venue": "Ieee Transactions On Automation Science And Engineering"
  },
  {
    "abstract": "Acoustic Event Classification (AEC) has become a significant task for machines to perceive the surrounding auditory scene. However, extracting effective representations that capture the underlying characteristics of the acoustic events is still challenging. Previous methods mainly focused on designing the audio features in a u0027hand-craftedu0027 manner. Interestingly, data-learnt features have been recently reported to show better performance. Up to now, these were only considered on the frame-level. In this paper, we propose an unsupervised learning framework to learn a vector representation of an audio sequence for AEC. This framework consists of a Recurrent Neural Network (RNN) encoder and a RNN decoder, which respectively transforms the variable-length audio sequence into a fixed-length vector and reconstructs the input sequence on the generated vector. After training the encoder-decoder, we feed the audio sequences to the encoder and then take the learnt vectors as the audio sequence representations. Compared with previous methods, the proposed method can not only deal with the problem of arbitrary-lengths of audio streams, but also learn the salient information of the sequence. Extensive evaluation on a large-size acoustic event database is performed, and the empirical results demonstrate that the learnt audio sequence representation yields a significant performance improvement by a large margin compared with other state-of-the-art hand-crafted sequence features for AEC.",
    "actual_venue": "Arxiv: Sound"
  },
  {
    "abstract": "The painting process is an important part of the entire automobile manufacturing system. Changing color in the painting process is expensive because of the wasted paint and solvent during color change. By intelligently selecting cars toward downstream operations at the places where conveyors converge or diverge, we can reduce the number of such color changes without additional hardware investment. Discrete Event Simulation is a tool of choice in analyzing these issues in order to develop an effective and efficient selection algorithm to ensure the system throughput. The concepts and methods presented here are also applicable to other discrete event manufacturing processes where setup reduction is pursued.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "In the face of ongoing environmental concerns, there is a strong push worldwide to improve efficiencies and curtail consumption of energy. The European Union (EU) has risen to these challenges by announcing ambitious 20-20-20 targets. These targets aim to cut greenhouse gas emissions by 20% (in comparison to 1990 levels), increase the contribution of renewables by 20% of the total energy produced, and achieve 20% improvement in energy efficiency (reduce consumption by 20%), all by the year 2020.",
    "actual_venue": "Technology and Society Magazine, IEEE  "
  },
  {
    "abstract": "Graphs are attached to the n-dimensional space Z2rn where Z2r is the ring with 2r elements using an analogue of Euclidean distance. The graphs are shown to be non-Ramanujan for r = 4. Comparisons are made with Euclidean graphs attached to Zprn for p an odd prime. The percentage of non-zero eigenvalues of the adjacency operator attached to these finite Euclidean graphs is shown to tend to zero as n tends to infinity.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "Analysis of functional magnetic resonance imaging (fMRI) data from multiple subjects is at the heart of many medical imaging studies, and recently, the approaches based on dictionary learning (DL) are noted as promising solutions to the problem. However, the DL-based methods for fMRI analysis proposed to date do not naturally extend to multi-subject analysis. In this paper, we propose a DL algorithm for multi-subject fMRI data analysis which is derived using a hybrid (temporal and spatial) concatenation scheme. It differs from existing DL methods in both its sparse coding and dictionary update stages. It has the advantage of learning a dictionary common to all subjects as well as a set of subject-specific dictionaries, as a result, it is able to generate both group-level spatial activation maps as well as group-level temporal dynamics, which are particularly attractive for task-based fMRI studies. In addition, by simultaneously learning multiple sub-specific dictionaries, it also provides us with unique sub-specific features as well. Performance of the proposed DL method is illustrated using simulated and real fMRI datasets. The results show that it can successfully extract common as well as sub-specific latent components.",
    "actual_venue": "Digital Signal Processing"
  },
  {
    "abstract": "A number of variations of block predictor-corrector methods have been proposed for the parallel solution of ordinary differential equations, but they have usually been viewed as deficient because of their relatively small stability intervals. This paper proposes a simple idea for extending the stability intervals: with the use of suitable linear combinations of the predicted and corrected values, the stability intervals can be expanded considerably. Several existing methods are modified in this manner. The resulting new methods and the existing methods are compared using a collection of standard test problems.",
    "actual_venue": "J Parallel Distrib Comput"
  },
  {
    "abstract": "Without Abstract",
    "actual_venue": "Eurosam"
  },
  {
    "abstract": "This paper deals with control algorithms development for output stabilization of the parametrically uncertain linear plant of unknown relative degree. Simple adaptive control paradigm lies in the basis of proposed approach. Controller was supplemented with the switching-based scheme of substitution of multiharmonic external disturbance frequencies' estimations that improves controller performance without its significant complication. Stability analysis of the closed-loop hybrid system was conducted and corresponding conditions on switching parameters were stated.",
    "actual_venue": "Ifac Proceedings Volumes"
  },
  {
    "abstract": "We present an automatic method which combines logical proof search and rippling heuristics to prove specifications. The key idea is to instantiate meta-variables in the proof with a simultaneous match based on rippling/reverse rippling heuristic. Underlying our rippling strategy is the rippling distance strategy which introduces a new powerful approach to rippling, as it avoids termination problems of other rippling strategies. Moreover, we are able to synthesize conditional substitutions for meta-variables in the proof. The strength of our approach is illustrated by discussing the specification of the integer square root and automatically synthesizing the corresponding algorithm. The described procedure has been integrated as a tactic into the NUPRL system but it can be combined with other proof methods as well.",
    "actual_venue": "Fundam Inform"
  },
  {
    "abstract": "In the recent past, a relevant effort has been devoted to the definition of process modeling languages (PMLs). The resulting languages and environments -although technically successful-did not receive much attention from industry. On the contrary, researchers and practitioners have recently started experimenting with the usage of UML as a PML. Being so popular and widely used, UML has an important competitive advantage compared to any specialized PML. However, it has also a main limitation. While most PMLs are executable by some process engine, UML was conceived as a non-executable, semi-formal language. The work described here aims at assessing the possibility of employing a subset of UML as an executable PML. The article proposes a formalization of the semantics of the UML subset and presents the translation of UML process models into code, which can be enacted in the OPSS process-centered environment. The paper also presents a case study to validate the approach. We expect that process modeling by means of UML is easier and available to a larger community of software process managers. Moreover, process enactment makes the process more efficient, reliable, predictable and controllable, as widely shown by previous research.",
    "actual_venue": "Orlando, Fl, Usa"
  },
  {
    "abstract": "The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS, 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.",
    "actual_venue": "Journal Of Consciousness Studies"
  },
  {
    "abstract": "Recent work in text analysis has suggested that data on words that frequently occur together reveal important information about text content. Co-occurrence relations can serve two main purposes in language processing. First, the statistics of co-occurrence have been shown to produce accurate results in syntactic analysis. Second, the way that words appear together can help in assigning thematic roles in semantic interpretation. This paper discusses a method for collecting co-occurrence data, acquiring lexical relations from the data, and applying these relations to semantic analysis.",
    "actual_venue": "Coling"
  },
  {
    "abstract": "Project management relies on testing personnel's expertise to ensure software quality. However, contractual and management issues also determine a project's quality. Such issues might even control testing itself.",
    "actual_venue": "Ieee Software"
  },
  {
    "abstract": "Pessimistic, optimistic and risk neutral decision criteria were proposed for order decision when demand information was updated to a fuzzy random variable. To a supply chain coordinated by a buyback contract, a method to formulate feasible set for adjusted parameters of buyback contract was put forward in the condition of demand information updating. The study shows that, to coordinate the supply chain, buyback contract has to make real time response to information updating.",
    "actual_venue": "Ieee International Conference On Fuzzy Systems"
  },
  {
    "abstract": "Virtual reality (VR) is being increasingly used in medical education. However, investigations into its effectiveness in this field have yielded mixed results, indicating that no general conclusions can be drawn in this regard. Therefore, the suitability of a system to teach a given task has to be investigated on a case-by-case basis. In this paper, we focus on teaching clinically oriented surgical ear anatomy, as it is important to provide a well-rounded view of ear anatomy as a foundation for understanding clinical surgery. Although there are some VR systems that teach basic ear anatomy, for example, identification of structures and spatial relationships, to our knowledge, those that deliver a complete clinical understanding of ear anatomy do not yet exist. As such, we discuss the design and development of a 3D interactive VR tutor with integrated haptic capability to teach clinically oriented surgical anatomy of the ear and establish its effectiveness through a user study.",
    "actual_venue": "Ieee International Symposium On Computer-Based Medical Systems"
  },
  {
    "abstract": "We consider the orthogonal range aggregation problem. The dataset S consists of N axis-parallel rectangles in R2, each of which is associated with an integer weight. Given an axis-parallel rectangle Q and an aggregate function F, a query reports the aggregated result of the weights of the rectangles in S intersecting Q. The goal is to preprocess S into a structure such that all queries can be answered efficiently. We present indexing schemes to solve the problem in external memory when F = max (hence, min) and F = sum (hence, count and average), respectively. Our schemes have linear or near-linear space, and answer a query in O(logBN) or O(logB2/BN) I/Os, where B is the disk block size.",
    "actual_venue": "Pods"
  },
  {
    "abstract": "Recent work has revealed the sensing theory of human respiration outside the First Fresnel Zone (FFZ) using commodity Wi-Fi devices. However, there is still no theoretical model to guide human respiration detection when the subject locates in the FFZ. In our work [10], we propose a diffraction-based sensing model to investigate how to effectively sense human respiration in FFZ. We present this demo system to show human respiration sensing performance varies based on different human locations and postures. By deploying the respiration detection system using COTS Wi-Fi devices, we can observe that the respiration sensing results match the theoretical model well.",
    "actual_venue": "Ubicomp : The Acm International Joint Conference On Pervasive And Ubiquitous Computing Singapore Singapore October"
  },
  {
    "abstract": "A magnetic amplifier (Magamp) postregulator using a time-sharing control strategy is proposed when using a multioutput LLC series resonant converter. The operating principles of the proposed converter and the Magamp postregulator are analyzed in detail. Furthermore, the design criterion for the Magamp core is presented, and the dead-time problem of the Magamp postregulator is analyzed. Compared with a flyback converter, the dead-time impact in the LLC structure is much better for two main reasons. The first one is that there is no reverse-recovery resetting effect for the Magamp postregulator in an LLC topology, and the second is that, when the same dead time is applied in both converters, the effect is better by using an LLC topology than a flyback topology. Finally, an experimental prototype of 310-V input and 24-V/3-A and 18-V/2-A outputs was built and tested to verify the zero-voltage-switching (zero-current-switching) operation and the high cross-regulating ratio. The experimental results prove that the dead-time impact over a Magamp in an LLC converter is much better than that when using a flyback converter, since a high cross-regulating ratio was easily achieved without using complicate resetting methods or large dead load.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Some Khmu' dialects have phonologically distinctive voice registers. Auditory observations have claimed a stable distinction between clear voice and high pitch for Register 1 and breathy voice and low pitch for Register 2 in the Khmu' Rawk dialect of northern Thailand. Word pairs distinguished only by register were recorded by 25 native speakers. Acoustic analysis yielded F0 and overall amplitude contours, frequencies of F1 and F2 in quasi-steady states of the vowels, relative intensities of higher harmonics to that of the first harmonic, and vowel durations. When circumstances caused early attention to perception testing, the words of only 8 speakers had been analyzed for properties other than amplitude and F0. Since the only significant factor that had emerged by then was F0 contour, the synthetic stimuli were made with just a series of seven contours. The labeling by 32 native speakers yielded two categories, demonstrating the sufficiency of F0 as an acoustic cue. The completed acoustic analysis showed a significant effect of one of the harmonic ratios for the women only, suggesting a conservative bias. The language has been shifting toward tonality and may have reached it. Copyright (c) 2007 S. Karger AG, Basel.",
    "actual_venue": "Phonetica"
  },
  {
    "abstract": "A new extended state space recursive least squares (ESSRLS) algorithm is proposed for state estimation of nonlinear systems. It is based on state space recursive least squares (SSRLS) approach and uses first order linearization of the system. It inherits the capability of obtaining state estimate without knowledge of process and measurement noise covariance matrices (Q and R respectively). The proposed approach is considered to provide new design option for scenarios where noise statistics and system dynamics vary. ESSRLS is initialized using delayed recursion method and a forgetting factor λ is employed to optimize the performance. The selection of λ can be problem specific as shown through experimental validations. However a value closer to and less than unity is generally recommended. Theoretical bases are validated by applying this algorithm to problems of tracking a non-conservative oscillator, a damped system with amplitude death and a signal modeled by mixture of Gaussian kernels. Simulation results show an MSE performance gain of 20 dB and 23 dB over extended Kalman filter (EKF) and unscented Kalman filter (UKF) while tracking van der Pol oscillator without knowledge about noise variances. The computational complexity of ESSRLS falls within that of EKF and UKF.",
    "actual_venue": "Digital Signal Processing"
  },
  {
    "abstract": "Company growth in a global setting causes challenges in the adaptation and maintenance of an organization's methods. In this paper, we will analyze incremental method evolution in software product management in a global environment. We validate a method increment approach, based on method engineering principles, by applying it to a retrospective case study conducted at a large ERP vendor. The results show that the method increment types cover all increments that were found in the case study. Also, we identified the following lessons learned for company growth in a global software product management context: method increment drivers, such as the change of business strategy, vary during evolution; a shared infrastructure is critical for rollout; small increments facilitate gradual process improvement; and global involvement is critical. We then claim that method increments enable software companies to accommodate evolutionary adaptations of development process in agreement with the overall company expansion.",
    "actual_venue": "Information And Software Technology"
  },
  {
    "abstract": "spaceDisplaced: Investigating Presence Through Mediated Participatory Environments is an interdisciplinary telepresence performance that linked four physical spaces. We conducted a participatory telepresence performance to explore how the experience of presence in separate spaces is influenced by the scale, function, sonic potential and accessibility of the space that each performer inhabits. The performance served as a form of experience modeling, a design methodology put forth by Schiphorst and Andersen (2004) that uses somatic, theater, and dance practices to model and structure experiences that can inform interaction design. In this paper we describe our exploration of personal, social and environmental forms of presence in the performance. We demonstrate how our findings led to new insights on how to stage the experience of presence. We apply these findings in Presence in a Box: Crossing Liminal Spaces, an interactive public performance where participants can transfer the experience of presence between small and large spaces.",
    "actual_venue": "Moco"
  },
  {
    "abstract": "After the greatest financial debacle since the great depression, the need for accurate and systematic assessment of loan granting decisions has never been more important than now. The paper compares the classification accuracy rates of six models: logistic regression (LR), neural network (NN), radial basis function neural network (RBFNN), support vector machine (SVM), k-Nearest Neighbor (kNN), and decision tree (DT) for loan granting decisions. We build models and test their classification accuracy rates on five very versatile data sets drawn from different loan granting decision contexts. The results from computer simulation constitute a fertile ground for interpretation.",
    "actual_venue": "Icaisc"
  },
  {
    "abstract": "The differences between connectionism and symbolicism in artificial intelligence (AI) are illustrated on several aspects in details firstly; then after conceptually decision factors of connectionism are proposed, the commonalities between connectionism and symbolicism are tested to make sure, by some quite typical logic mathematics operation examples such as “parity”; At last, neuron structures are expanded by modifying neuron weights and thresholds in artificial neural networks through adopting high dimensional space geometry cognition, which give more overall development space, and embodied further both commonalities.",
    "actual_venue": "Isnn"
  },
  {
    "abstract": "Our framework is the synthesis of multispectral images (MS) at higher spatial resolution, which should be as close as possible to those that would have been acquired by the corresponding sensors if they had this high resolution. This synthesis is performed with the help of a high spatial but low spectral resolution image: the panchromatic (Pan) image. The fusion of the Pan and MS images is classic...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "The central autonomic network (CAN) has been described in animal models but has been difficult to elucidate in humans. Potential confounds include physiological noise artifacts affecting brainstem neuroimaging data, and difficulty in deriving non-invasive continuous assessments of autonomic modulation. We have developed and implemented a new method which relates cardiac-gated fMRI timeseries with continuous-time heart rate variability (HRV) to estimate central autonomic processing. As many autonomic structures of interest are in brain regions strongly affected by cardiogenic pulsatility, we chose to cardiac-gate our fMRI acquisition to increase sensitivity. Cardiac-gating introduces T1-variability, which was corrected by transforming fMRI data to a fixed TR using a previously published method [Guimaraes, A.R., Melcher, J.R., et al., 1998. Imaging subcortical auditory activity in humans. Hum. Brain Mapp. 6(1), 33–41]. The electrocardiogram was analyzed with a novel point process adaptive-filter algorithm for computation of the high-frequency (HF) index, reflecting the time-varying dynamics of efferent cardiovagal modulation. Central command of cardiovagal outflow was inferred by using the resample HF timeseries as a regressor to the fMRI data. A grip task was used to perturb the autonomic nervous system. Our combined HRV-fMRI approach demonstrated HF correlation with fMRI activity in the hypothalamus, cerebellum, parabrachial nucleus/locus ceruleus, periaqueductal gray, amygdala, hippocampus, thalamus, and dorsomedial/dorsolateral prefrontal, posterior insular, and middle temporal cortices. While some regions consistent with central cardiovagal control in animal models gave corroborative evidence for our methodology, other mostly higher cortical or limbic-related brain regions may be unique to humans. Our approach should be optimized and applied to study the human brain correlates of autonomic modulation for various stimuli in both physiological and pathological states.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Dynamic supply current (IDD) analysis has emerged as an effective way for defect oriented testing of analog circuits. In this paper, we propose using wavelet decomposition of IDD for fault detection in analog circuits. Wavelet transform has the property of resolving events in both time and frequency domain simultaneously unlike Fourier expansion which localizes a signal in terms of frequency only. Wavelet transform also has better sub-banding property and it can be easily adapted to current waveforms from different circuits. These make wavelet a more suitable candidate for fault detection in analog circuits than pure time-domain or pure frequency-domain methods. We have shown that for equivalent number of spectral components, sensitivity of wavelet based fault detection is much higher than Fourier or time-domain analysis for both catastrophic and parametric faults. Simulation results on benchmark circuits show that wavelet based method is on average 25 times more sensitive than DFT (Discrete Fourier Transform) for parametric faults and can be considered as a promising alternative for analog fault detection amidst measurement hardware noise and process variation.",
    "actual_venue": "Vlsi Test Symposium"
  },
  {
    "abstract": "Content-based image retrieval (CBIR) is an important and widely studied topic since it can have significant impact on multimedia information retrieval. Recently, support vector machine (SVM) has been applied to the problem of CBIR. The SVM-based method has been compared with other methods such as neural network (NN) and logistic regression, and has shown good results. Genetic algorithm (GA) has been increasingly applied in conjunction with other AI techniques. However, few studies have dealt with the combining GA and SVM, though there is a great potential for useful applications in this area. This paper focuses on simultaneously optimizing the parameters and feature subset selection for SVM without degrading the SVM classification accuracy by combining GA for CBIR. In this study, we show that the proposed approach outperforms the image classification problem for CBIR. Compared with NN and pure SVM, the proposed approach significantly improves the classification accuracy and has fewer input features for SVM.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "Since tree-structured data such as XML files are widely used for data representation and exchange on the Internet, discovering frequent tree patterns over tree-structured data streams becomes an interesting issue. In his paper, we propose an online algorithm to continuously discover the current set of frequent tree patterns from the data stream. A novel and efficient technique is introduced to incrementally generate all candidate tree patterns without duplicates. Moreover, a framework for counting the approximate frequencies of the candidate tree patterns is presented. Combining these techniques, the proposed approach is able to compute frequent tree patterns with guarantees of completeness and accuracy.",
    "actual_venue": "Siam Proceedings Series"
  },
  {
    "abstract": "This paper discusses practical issues in the generation of test data. Implementation requirements are derived from these issues, and a procedural test interface is described. A language to support that interface is presented, as well as methodologies to constrain an instance of the language to a particular tester.",
    "actual_venue": "ITC"
  },
  {
    "abstract": "Power and flexibility are important constraints in the design of new chips. The efficiency extracted from a design is increasingly becoming a dominant question, and several techniques and technological advances can be used to optimize efficiency in its energy and functionality domains. These two characteristics are critical in digital communication systems that must work accordingly with multiple communication standards at different power, throughput and latency requirements. In this work, we focus on the physical layer Forward Error Correcting (FEC) system, due to the tight throughput and latency constraints they are required to meet, and develop specialized processing engines for Low-Density Parity-Check (LDPC) codes decoding, a class of widely standardized codes. The engines were developed for execution on Field-Programmable Gate Array (FPGA) devices by exploring dataflow and wide-pipeline design approaches, and have the design flexibility to target different LDPC codes, since they were implemented using recent High-Level Synthesis (HLS) tools. The generated engines and architectures allow achieving highly efficient decoders with decoding throughputs ranging from 16 Mbit/s to 1.2 Gbit/s at energy efficiencies of 42 to 908 Mbit/Joule/iteration, while the achieved clock frequencies of operation vary from 80 to 300 MHz. Furthermore, our bandwidth analysis shows that workload boundaries do not impose limitations on a system bus.",
    "actual_venue": "Application-Specific Systems, Architectures, And Processors"
  },
  {
    "abstract": "In this paper we show that visual landmark generation and redetection is possible with a single feature per frame. The approach is based on the assumption that highly discriminative regions are easily redetectable in subsequent frames as well as in frames visited from different viewpoints. We investigate which feature detectors fit for this purpose and under which conditions the discriminability applies. The approach is tested in a topological localization scenario in which the best feature is tracked over several frames to build landmarks. We show that we can represent a large environment with a few salient landmarks and that a large percentage of these landmarks is robustly redetectable from different viewpoints.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "This paper proposes an automated method for the segmentation of peripheral blood smear images, depicting the differential count of white blood cells. It also proposes a reversible fragile watermarking technique, in which the watermark-related information is embedded in the Region of Non-Interest (RONI) of the image. Image authenticity and integrity, is maintained by embedding the Input message along with the Hash Message Authentication Codes (HMAC) of the image and the message itself.",
    "actual_venue": "Bio-Inspired Computing: Theories And Applications"
  },
  {
    "abstract": "Information extraction applications that extract structured event and entity information from unstructured text can leverage knowledge of clinical report structure to improve performance. The Subjective, Objective, Assessment, Plan (SOAP) framework, used to structure progress notes to facilitate problem-specific, clinical decision making by physicians, is one example of a well-known, canonical structure in the medical domain. Although its applicability to structuring data is understood, its contribution to information extraction tasks has not yet been determined. The first step to evaluating the SOAP framework's usefulness for clinical information extraction is to apply the model to clinical narratives and develop an automated SOAP classifier that classifies sentences from clinical reports. In this quantitative study, we applied the SOAP framework to sentences from emergency department reports, and trained and evaluated SOAP classifiers built with various linguistic features. We found the SOAP framework can be applied manually to emergency department reports with high agreement (Cohen's kappa coefficients over 0.70). Using a variety of features, we found classifiers for each SOAP class can be created with moderate to outstanding performance with F(1) scores of 93.9 (subjective), 94.5 (objective), 75.7 (assessment), and 77.0 (plan). We look forward to expanding the framework and applying the SOAP classification to clinical information extraction tasks.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "The web services stack of standards is designed to supportthe reuse and the interoperation of software componentson the web. A critical step in the process of developingapplications based on the service oriented architecture isthe service discovery. This paper shows how servicecomposition can be used as a technique to support servicediscovery. The paper discusses the current state ofresearch in this area and introduces a semantic matchingalgorithm that exploits the possibility to compose multipleservices in order to satisfy a service request.",
    "actual_venue": "Icws"
  },
  {
    "abstract": "The didactic focus on algorithm visualization (AV) in CS education has been shifting to the construction of AV by students. Based on the principles of Constructivism and Constructionism and our observation, we propose approaches to constructing algorithm visualizations that may have been overlooked in the current practice of AV design, implementation and deployment.",
    "actual_venue": "Icalt"
  },
  {
    "abstract": "Resolvers are used for both absolute angular and linear position sensing. A robust amplitude to phase converter is described for the determination of position from the sine and cosine output signals of the resolver. The scheme is based on a linearization technique for converting the co-sinusoids into a nearly perfect triangular signal from which the angle is determined using simple linear equation. Even though the amplitudes of the resolver output signals are proportional to that of the resolver excitation signal, the proposed amplitude to phase technique is robust to fluctuation and drift of the amplitude and frequency of the excitation signal. The theoretical absolute error of non-linearity of the converter is 0.0082 degree over the full 360 degree range. The converter may be implemented numerically or electronically. In this work, details are given for an implementation using basic analogue electronic circuitry The theory of operation and experimental results are given.",
    "actual_venue": "Proceedings Ieee International Conference On Industrial Informatics"
  },
  {
    "abstract": "We study the bidding strategies of vertically differentiated firms that bid for sponsored search advertisement positions for a keyword at a search engine. We explicitly model how consumers navigate and click on sponsored links based on their knowledge and beliefs about firm qualities. Our model yields several interesting insights; a main counterintuitive result we focus on is the “position paradox.” The paradox is that a superior firm may bid lower than an inferior firm and obtain a position below it, yet it still obtains more clicks than the inferior firm. Under a pay-per-impression mechanism, the inferior firm wants to be at the top where more consumers click on its link, whereas the superior firm is better off by placing its link at a lower position because it pays a smaller advertising fee, but some consumers will still reach it in search of the higher-quality firm. Under a pay-per-click mechanism, the inferior firm has an even stronger incentive to be at the top because now it only has to pay for the consumers who do not know the firms' reputations and, therefore, can bid more aggressively. Interestingly, as the quality premium for the superior firm increases, and/or if more consumers know the identity of the superior firm, the incentive for the inferior firm to be at the top may increase. Contrary to conventional belief, we find that the search engine may have the incentive to overweight the inferior firm's bid and strategically create the position paradox to increase overall clicks by consumers. To validate our model, we analyze a data set from a popular Korean search engine firm and find that (i) a large proportion of auction outcomes in the data show the position paradox, and (ii) sharp predictions from our model are validated in the data.",
    "actual_venue": "Marketing Science"
  },
  {
    "abstract": "Aspect is a static analysis technique for detecting bugs in imperative programs, consisting of an annotation language and a checking tool. Like a type declaration, an Aspect annotation of a procedure is a kind of declarative, partial specification that can be checked efficiently in a modular fashion. But instead of constraining the types of arguments and results, Aspect specifications assert dependences that should hold between inputs and outputs. The checker uses a simple dependence analysis to check code against annotations and can find bugs automatically that are not detectable by other static means, especially errors of omission, which are common, but resistant to type checking. This article explains the basic scheme and shows how it is elaborated to handle data abstraction and aliasing.",
    "actual_venue": "Acm Trans Softw Eng Methodol"
  },
  {
    "abstract": "Standard pulsewidth modulation (PWM) techniques assume a ripple-free DC-link voltage at the inverter input terminals and ideal switches. Most techniques proposed in the literature to compensate the nonideal characteristics require additional and complex circuitry. This paper proposes and analyzes a simple method of generating PWM switching patterns which ensures a high-quality output voltage and inherently compensates for a nonideal DC bus and switching delays. The method is based on maintaining a sinusoidal volt-second distribution at the inverter output by sensing the output voltage and generating the gating pattern online. The principles of operation are explained, and a design procedure is presented. Simulation results illustrating the features of the proposed modulator are verified experimentally on a 3 kVA prototype unit",
    "actual_venue": "Ieee Trans Industrial Electronics"
  },
  {
    "abstract": "The broadband millimeter-wave dielectric properties of chlorobenzene, cyclohexane, 10% formalin, and 1,4-dioxane are presented for the first time in this paper. A variable-temperature variable-thickness interferometer was assembled to perform dispersive Fourier transform spectroscopy on liquids at millimeter and submillimeter waves. Using the two-thickness method, the refractive index, absorption ...",
    "actual_venue": "Ieee Transactions On Instrumentation And Measurement"
  },
  {
    "abstract": "When multiple items are auctioned sequentially, the ordering of auctions plays an important role in the total revenue collected by the auctioneer. This is true especially with budget constrained bidders and the presence of complementarities among items. It is difficult to develop efficient algorithms for finding an optimal sequence of items. However, when historical data are available, it is possible to learn a model in order to predict the outcome of a given sequence. In this work, we show how to construct such a model, and provide methods that finds a good sequence for a new set of items given the learned model. We develop an auction simulator and design several experiment settings to test the performance of the proposed methods.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "Usability is a critical measure of a system's success. This study focuses on evaluating the usability of Agent99 Trainer, a Web-based multimedia training system for deception detection training. Common measures and evaluation methods for system usability are reviewed. After briefly introducing the design of Agent99 Trainer, this paper focuses on presenting the evaluation methods used in the study and the results. Both quantitative and qualitative data were collected using questionnaires for testing the following system usability attributes: learnability, usefulness, ease of use, effectiveness, and user satisfaction. The results of this evaluation indicate that Agent99 Trainer is a well-designed system with good usability. Several features that make Agent99 Trainer a useful and valuable tool are discussed, and lessons learned from this evaluation for future system improvement are summarized.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "Personalized recommendation needs powerful Web Intelligence (WI) technologies to manage, analyze and employ various business data on the Web for e-business intelligence. This paper presents a novel recommendation framework on the Web, which is based on a multilevel customer model comprising three submodels, namely, the customer shopping model (CSM), the customer preference model (CPM), and the customer consumption model (CCM). These models capture a customer's information from different aspects. After preprocessing of raw data, we first build the CSM based on Bayesian networks by mining from customer shopping transactions, and then find the CPM by analyzing customer shopping history. Furthermore, the customer purchasing power can be formalized as a linear CCM. By combining the CSM with the present customer shopping action, a recommendation algorithm based on Bayesian probability inference is used to generate an individual recommendation set of commodities. A personalized filter including customization of the CPM and orientation of the CCM is also used to realize a more personalized recommendation. Experimental evaluation on real world data shows that the proposed approach can achieve personalized commodities recommendation efficiently and effectively.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "This personal account describes various experiences of a 1950s graduate student who worked at the University of Illinois's Digital Computer Laboratory. Anecdotes recount some of the era's rigorous academic requirements and early work assignments performed for the IBM 701 and Illiac computers.",
    "actual_venue": "Annals Of The History Of Computing, Ieee"
  },
  {
    "abstract": "Since finding the nearest point in a lattice for multi-input multi-output (MIMO) channels is NP-hard, simplified algorithms such as sphere decoder (SD) have been proposed. List sphere decoder (LSD), which is a modified version of SD, allows soft information to be extracted for channel decoding and iterative detection/decoding. In this paper, recently proposed efficient methods for reducing the computational complexity of SD and LSD with depth-first tree searching are summarized. Numerous simulations have been carried out and comparison has been made based on the average number of processing cycles. We also present two efficient schemes which can decrease hardware complexity without significant performance degradation, restricted list updating in LSD and restricted node storing at each tree level",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "This paper discusses the results of applied research on the eco-driving domain based on a huge data set produced from a fleet of Lisbon's public transportation buses for a three-year period. This data set is based on events automatically extracted from the control area network bus and enriched with GPS coordinates, weather conditions, and road information. We apply online analytical processing (OLAP) and knowledge discovery (KD) techniques to deal with the high volume of this data set and to determine the major factors that influence the average fuel consumption, and then classify the drivers involved according to their driving efficiency. Consequently, we identify the most appropriate driving practices and styles. Our findings show that introducing simple practices, such as optimal clutch, engine rotation, and engine running in idle, can reduce fuel consumption on average from 3 to 5l/100 km, meaning a saving of 30 l per bus on one day. These findings have been strongly considered in the drivers' training sessions.",
    "actual_venue": "Intelligent Transportation Systems, IEEE Transactions  "
  },
  {
    "abstract": "Pragmatic clinical trials (PCTs) are research investigations embedded in health care settings designed to increase the efficiency of research and its relevance to clinical practice. The Health Care Systems Research Collaboratory, initiated by the National Institutes of Health Common Fund in 2010, is a pioneering cooperative aimed at identifying and overcoming operational challenges to pragmatic research. Drawing from our experience, we present 4 broad categories of informatics-related challenges: (1) using clinical data for research, (2) integrating data from heterogeneous systems, (3) using electronic health records to support intervention delivery or health system change, and (4) assessing and improving data capture to define study populations and outcomes. These challenges impact the validity, reliability, and integrity of PCTs. Achieving the full potential of PCTs and a learning health system will require meaningful partnerships between health system leadership and operations, and federally driven standards and policies to ensure that future electronic health record systems have the flexibility to support research.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "We present Permutation Routing as a method for increased robustness in IP networks with traditional hop-by-hop forwarding. Permutation Routing treats routers involved in traffic forwarding as a sequence of resources, and creates permutations of these resources that give several forwarding options. We introduce Permutation Routing as a concept, and use it to create routings where we seek to maximize single link fault coverage. Analogous to the IETF standardized Loop-Free Alternate (LFA), Permutation Routing can easily be implemented for OSPF or IS-IS networks to augment existing ECMP forwarding with additional loop-free forwarding entries for improved load balancing or fault tolerance. Our evaluations show that Permutation Routing can increase single link fault coverage by up to 28% compared to LFA in inferred network topologies.",
    "actual_venue": "Networking"
  },
  {
    "abstract": "Multiple organization indices have been used to predict the outcome of stepwise catheter ablation in long-standing persistent atrial fibrillation (AF), however with limited success. Our study aims at developing innovative organization indices from baseline ECG (i.e. during the procedure, before ablation) in order to identify the site of AF termination by catheter ablation. Seventeen consecutive male patients (age 60 +/- 5 years, AF duration 7 +/- 5 years) underwent a stepwise catheter ablation. Chest lead V6 was placed in the back (V6b). QRST cancelation was performed from chest leads V1 to V6b. Using an innovative adaptive harmonic frequency tracking, two measures of AF organization were computed to quantify the harmonics components of ECG activity: (1) the adaptive phase difference variance (APD) between the AF harmonic components as a measure of AF regularity, and (2) and adaptive organization index (AOI) evaluating the cyclicity of the AF oscillations. Both adaptive indices were compared to indices computed using a time-invariant approach: (1) ECG AF cycle length (AFCL), (2) the spectrum based organization index (CH), and (3) the time-invariant phase difference TIPD. Long-standing persistent AF was terminated into sinus rhythm or atrial tachycardia in 13/17 patients during stepwise ablation, 11 during left atrium ablation (left terminated patients - LT), 2 during the right atrium ablation (right terminated patients RT), and 4 were non terminated (NT) and required electrical cardioversion. Our findings showed that LT patients were best separated from RT/NT before ablation by the duration of sustained AF and by AOI on chest lead V1 and APD from the dorsal lead V6b as compared to ECG AFCL, OI and TIPD, respectively. Our results suggest that adaptive measures of AF organization computed before ablation perform better than time-invariant based indices for identifying patients whose AF will terminate during ablation within the left atrium. These findings are indicative of a higher baseline organization in these patients that could be used to select candidates for the termination of AF by stepwise catheter ablation. (C) 2013 Elsevier Ltd. All rights reserved.",
    "actual_venue": "Biomedical Signal Processing And Control"
  },
  {
    "abstract": "Understanding search intents behind queries is of vital importance for improving search performance or designing better evaluation metrics. Although there exist many efforts in Web search user intent taxonomies and investigating how users' interaction behaviors vary with the intent types, only a few of them have been made specifically for the image search scenario. Different from previous works which investigate image search user behavior and task characteristics based on either lab studies or large scale log analysis, we conducted a field study which lasts one month and involves 2,040 search queries from 555 search tasks. By this means, we collected relatively large amount of practical search behavior data with extensive first-tier annotation from users. With this data set, we investigate how various image search intents affect users' search behavior, and try to adopt different signals to predict search satisfaction under the certain intent. Meanwhile, external assessors were also employed to categorize each search task using four orthogonal intent taxonomies. Based on the hypothesis that behavior is dependent of task type, we analyze user search behavior on the field study data, examining characteristics of the session, click and mouse patterns. We also link the search satisfaction prediction to image search intent, which shows that different types of signals play different roles in satisfaction prediction as intent varies. Our findings indicate the importance of considering search intent in user behavior analysis and satisfaction prediction in image search.",
    "actual_venue": "Wsdm"
  },
  {
    "abstract": "We propose a method for reducing out-of-focus blur caused by projector projection. In this method, we estimate the Point-Spread-Function (PSF) of the out-of-focus blur in the image projected onto the screen by comparing the screen image captured by a camera with the original image projected by the projector. According to the estimated PSF, the projected image is pre-corrected, so that the screen image can be deblurred. Experimental results show that our method can reduce out-of-focus projection blur.",
    "actual_venue": "Minneapolis, Mn"
  },
  {
    "abstract": "In the Motive project [1], the Industrial IT Laboratory (INIT) of Helsinki University of Technology (HUT) is studying the use of digital TV as a learning environment for University courses. Funded by the European Social Fund, the project has used itsý last years to focus on user-interaction in the digital TV and flexible content production. Practical experience of the learning environment in use has been with the course Local Demands for Global Enterprising where HUT students have had a chance to study a module in an experimental digital TV environment in autumn 2002. That experiment continued with a new version of the course, which started on January 12th, 2004 in digital TV cable networks.",
    "actual_venue": "Icalt"
  },
  {
    "abstract": "A modular robotic system consists of standardized joint and link units that can be assembled into various kinematic configurations for different types of tasks. For the control and simulation of such a system, manual derivation of the kinematic and dynamic models, as well as the error model for kinematic calibration, require tremendous effort, because the models constantly change as the robot geometry is altered after module reconfiguration. This paper presents a framework to facilitate the model-generation procedure for the control and simulation of the modular robot system. A graph technique, termed kinematic graphs and realized through assembly incidence matrices (AIM), is introduced to represent the module-assembly sequence and robot geometry. The kinematics and dynamics are formulated based on a focal representation of the theory of Lie groups and Lie algebras The automatic model-generation procedure starts with a given assembly graph of the modular robot. Kinematic, dynamic, and error models of the robot are then established, based on the local representations and iterative graph-traversing algorithms This approach can be applied to a modular robot with both serial and branch-type geometries, and arbitrary degrees of freedom. Furthermore, the AIM of the robot naturally leads to solving the task-oriented optimal configuration problem in modular robots. There is no need to maintain a huge library of robot models, and the footprint of the overall software system can be reduced.",
    "actual_venue": "International Journal Of Robotic Research"
  },
  {
    "abstract": "AbstractHuge numbers of documents are being generated on the Web, especially for news articles and social media. How to effectively organize these evolving documents so that readers can easily browse or search is a challenging task. Existing methods include classification, clustering, and chronological or geographical ordering, which only provides a partial view of the relations among news articles. To better utilize cross-document relations in organizing news articles, in this paper, we propose a novel approach to organize news archives by exploiting their near-duplicate relations. First, we use a sentence-level statistics-based approach to near-duplicate copy detection, which is language independent, simple but effective. Since content-based approaches are usually time consuming and not robust to term substitutions, near-duplicate detection approach can be used. Second, by extracting the cross-document relations in a block-sharing graph, we can derive a near-duplicate clustering by cross-document relations in which users can easily browse and find out unnecessary repetitions among documents. From the experimental results, we observed high efficiency and good accuracy of the proposed approach in detecting and clustering near-duplicate documents in news archives.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "The steady-state performance of heat pumps can be predicted From the cooling capacity and the input power required by the system. The key element for such an analysis is the \"map-based\" modeling approach which requires that the performance characteristics of a heat pump and its components, the compressor, evaporator, condenser, and flow control devices be expressed as simple algebraic functions. This study explores the efficacy of a math-package, MATLAB and \"map-based\" modeling approach to predict off-design-operating performance of a single air-source heat pump operating in the cooling mode. Also, the paper describes a useful tool that can be used to enhance students' capability to analyze projects involving thermodynamic systems. (C) 2000 John Wiley ge Suns, Inc.",
    "actual_venue": "Computer Applications In Engineering Education"
  },
  {
    "abstract": "This paper presents an overview of the ImageCLEF 2017 evaluation campaign, an event that was organized as part of the CLEF (Conference and Labs of the Evaluation Forum) labs 2017. ImageCLEF is an ongoing initiative (started in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval for providing information access to collections of images in various usage scenarios and domains. In 2017, the 15th edition of ImageCLEF, three main tasks were proposed and one pilot task: (1) a LifeLog task about searching in LifeLog data, so videos, images and other sources; (2) a caption prediction task that aims at predicting the caption of a figure from the biomedical literature based on the figure alone; (3) a tuberculosis task that aims at detecting the tuberculosis type from CT (Computed Tomography) volumes of the lung and also the drug resistance of the tuberculosis; and (4) a remote sensing pilot task that aims at predicting population density based on satellite images. The strong participation of over 150 research groups registering for the four tasks and 27 groups submitting results shows the interest in this benchmarking campaign despite the fact that all four tasks were new and had to create their own community.",
    "actual_venue": "Clef"
  },
  {
    "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
    "actual_venue": "Annual Conference On Neural Information Processing Systems"
  }
]