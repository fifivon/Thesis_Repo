[
  {
    "abstract": "Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces. To be faithful and reliable, the representation is usually required to preserve proximity relationships. In practice, methods like multidimensional scaling try to fulfill this requirement by preserving pairwise distances in the low-dimensional representation. However, such a simplification does not easily allow for local scalings in the representation. It also makes these methods suboptimal with respect to recent quality criteria that are based on distance rankings. This paper addresses this issue by introducing a dimensionality reduction method that works with ranks. Appropriate hypotheses enable the minimization of a rank-based cost function. In particular, the scale indeterminacy that is inherent to ranks is circumvented by representing data on a space with a spherical topology.",
    "actual_venue": "Neural Networks"
  },
  {
    "abstract": "We construct an intrusion-resilient symmetric-key authenticated key exchange (AKE) protocol in the bounded retrieval model. The model employs a long shared private key to cope with an active adversary who can repeatedly compromise the user's machine and perform any efficient computation on the entire shared key. However, we assume that the attacker is communication bounded and unable to retrieve too much information during each successive break-in. In contrast, the users read only a small portion of the shared key, making the model quite realistic in situations where storage is much cheaper than bandwidth. The problem was first studied by Dziembowski [Dzi06a], who constructed a secure AKE protocol using random oracles. We present a general paradigm for constructing intrusion-resilient AKE protocols in this model, and show how to instantiate it without random oracles. The main ingredients of our construction are UC-secure password authenticated key exchange and tools from the bounded storage model.",
    "actual_venue": "TCC"
  },
  {
    "abstract": "Computer-based assessment (CBA) is yet to have a significant impact on high-stakes educational assessment, but the equivalence between CBA and paper-and-pencil (P&P) test scores will become a central concern in education as CBA increases. It is argued that as CBA and P&P tests provide test takers with qualitatively different experiences, the impact of individual differences on the testing experience, and so statistical equivalence of scores, needs to be considered. As studies of score equivalence have largely ignored individual differences such as computer experience, computer anxiety and computer attitudes, the purpose of this paper is to highlight the potential effects of these. It is concluded that each of these areas is of significance to the study of equivalence and that the often inconsistent findings result from the rapid changes in exposure to technology.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "Recently, people have been interested in sharing what they are watching on TV, allowing the development of Social TV Applications often based on mobile devices. In this context, this paper proposes IRTR (Improved Real-Time TV-channel Recognition): a new method aimed at recognizing in real time (live) what people are watching on TV without any active user interaction. IRTR uses the audio signal of the TV program recorded by smartphones and is performed through two steps: i) fingerprint extraction and ii) TV channel real-time identification. Step i) is based on the computation of the Audio Fingerprint (AF). The AF computation has been taken from the literature and has been improved in terms of power consumption and computation speed to make the smartphone implementation feasible by using an ad hoc cost function aimed at selecting the best set of AF parameters. Step ii) is aimed at deciding the TV channel the user is watching. This step is performed using a likelihood estimation algorithm proposed in this paper. The consumed power, computation and response time, and correct decision rate of IRTR, evaluated through experimental measures, show very satisfying results such as a correct decision rate of about 95%, about 2s of computation time, and above 90% power saving with respect to the literature.",
    "actual_venue": "Mobile Computing, IEEE Transactions  "
  },
  {
    "abstract": "In this paper, the mean square exponential stability is investigated for a class of discrete-time stochastic neural networks with time-varying delays and norm-bounded uncertainties. Based on Lyapunov stability theory and stochastic approaches, delay-dependent criteria are derived to ensure the robust exponential stability in the mean square for the addressed system. Meantime, by using the numerically efficient Matlab LMI Toolbox, a example is presented to show the usefulness of the derived LMI-based stability condition.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "Wireless capsule endoscopy (WCE) is a revolutionary imaging technique that enables direct inspection of the gastrointestinal tract in a non-invasive way. However, viewing the large amounts of images is a very time-consuming and labor intensive task for clinicians. In this paper, we propose an automatic bleeding detection method in the WCE images. We propose a two-stage saliency map extraction method to highlight bleeding regions where the first-stage saliency map is created by means of different color channels mixer and the second-stage saliency map is obtained from the visual contrast in the RGB color space. Followed by an appropriate fusion strategy and threshold, we localize the bleeding areas in the WCE images. Then we extract statistic color features in the corresponding saliency region and non-saliency region respectively and fuse them together to represent the whole WCE images. Finally Support Vector Machine (SVM) is applied to carry out the experiment on 800 sample WCE images. Experiment result achieves an accuracy of 95.89%, sensitivity of 98.77% and specificity of 93.45%. This inspiring result demonstrates that the proposed method is very effective in detecting bleeding patterns in the WCE images. Our comparison studies with several state-of-the-art bleeding detection methods confirm that the proposed method achieves much better results than those of the alternative techniques.",
    "actual_venue": "International Conference On Robotics And Automation"
  },
  {
    "abstract": "To provide secure roaming services for mobile users in Global Mobility Networks, many schemes have been proposed in recent years. However, most of them focus only on authentication and fail to satisfy many practical security requirements such as user anonymity and untraceability. To address this problem, we propose a privacy-preserving authentication scheme based on elliptic curve cryptography. The proposed scheme is provably secure under a formal model that satisfies all practical security requirements. Compared with existing authentication schemes, ours enjoys better performance in terms of computation cost and security. Copyright © 2015John Wiley & Sons, Ltd.",
    "actual_venue": "Security And Communication Networks"
  },
  {
    "abstract": "Research based on the utilization of big data has led to new opportunities and developments in contemporary urban planning and smart city construction. This research is based on open big data of networks, such as the website Where to Go and public comments, spatial syntax theory and technical methods using the current layout of urban parking facilities in Shinan District as a case study of the lack of parking facilities and their unreasonable layout. Based on a quantitative analysis of the distribution of the traffic network and parking facilities on the urban scale and the block scale of the Shinan District of Qingdao, we found that the thermal density of Baidu is closely related to the spatial syntactic parameters of the urban area on the large scale and local scale. A field investigation was performed to analyze the traffic network of the Southern District of Qingdao and propose comprehensive optimization strategies to provide technical support for the construction of intelligent shared parking facilities in cities. This study solves a series of problems, such as unreasonable parking facility allocation around scenic spots in tourist cities, and can also be used as a reference for the optimization of parking facility layouts in similar cities.",
    "actual_venue": "Computer Communications"
  },
  {
    "abstract": "In many-to-one wireless sensor networks, there exists an unbalanced power consumption problem; that is, nodes near the data sink or base station, called hot spots, have a high probability of forwarding a high amount of packets and die early. Most of previous work is dedicated to constructing a long-lived aggregation tree, while leaving the hot spot problem unsolved. When the locations of sinks are given and thus fixed, this work attempts to alleviate the hot spot problem by exploiting techniques to balance power consumption for nodes in the tree by switching multiple fixed sinks. The first balancing technique is called manipulating multiple long-lived trees (MMLT), which constructs multiple long-lived data aggregation trees for multiple sinks, and cyclically switches a data aggregation tree to another, after a sink collects information for a predefined period of time. To reduce overhead, we further propose another technique which constructs only a single tree and shares it with all sink nodes. To evaluate the performance of these two balancing techniques, we run simulations with various setup environments and compare them with the balanced tree method.",
    "actual_venue": "MSN"
  },
  {
    "abstract": "In this paper we describe the theory, architecture, implementation, and performance of a multi-modal passive biometric verification system that continually verifies the presence/ participation of a logged-in user. We assume that the user logged in using strong authentication prior to the starting of the continuous verification process. While the implementation described in the paper combines a digital camera-based face verification with a mouse-based fingerprint reader, the architecture is generic enough to accommodate additional biometric devices with different accuracy of classifying a given user from an imposter. The main thrust of our work is to build a multi-modal biometric feedback mechanism into the operating system so that verification failure can automatically lock up the computer within some estimate of the time it takes to subvert the computer. This must be done with low false positives in order to realize a usable system. We show through experimental results that combining multiple suitably chosen modalities in our theoretical framework can effectively do that with currently available off-the-shelf components.",
    "actual_venue": "Acsac"
  },
  {
    "abstract": "This paper presents an analysis of the correlation of annotated information unit (textual) tags and geographical identification metadata geotags. Despite the increased usage of geotagging in collaborative tagging systems, most current research focuses on textual tagging alone in solving the tag search problem. This may result in difficulties to search for precise and relevant information within the given tag space. For example, inconsistencies like polysemy, synonyms, and word inflections with plural forms complicate the tag search problem. Therefore, more work needs to be done to include geotag information with existing tagging information for analysis. In this paper, to make geotagging possible to be used in analysis with tagging, we prove that there is a strong correlation between tagging and geotagging information. Our approach uses tag similarity and geographical distribution similarity to determine inter-relationships among tags and geotags. From our initial experiments, we show that the power law is established between tag similarity and geographical distribution similarity: this means that tag similarity and geographical distribution similarity has a strong correlation and the correlation can be used to find more relevant tags in the tag space. The power law confirms that there is an increased relationship between tagging and geotagging and the increased relationship is scalable in size of tags and geotags. Also, using both geotagging and tagging information instead of only tagging, we show that the uncertainty between derived and actual similarities among tags is reduced.",
    "actual_venue": "SSM"
  },
  {
    "abstract": "This document specifies a set of Internet Control Message Protocol (ICMP) messages for use with version 6 of the Internet Protocol (IPv6).",
    "actual_venue": "RFC"
  },
  {
    "abstract": "Accurate models of the feel of physical objects are essential to improving the realism of haptic simulations. This paper presents a method for automatically obtaining experimentally based models of general passive, nonlinear devices for use in haptic playback applications, with specific emphasis on modeling switches and buttons. The method, based on the exponentially weighted least-squares (EWLS), allows estimation of position- and direction-dependent parameters of a general nonlinear model. Results are presented for two push-button switches.",
    "actual_venue": "Roma"
  },
  {
    "abstract": "The maximum diversity problem (MDP) consists of identifying, in a population, a subset of elements, characterized by a set of attributes, that present the most diverse characteristics among the elements of the subset. The identification of such solution is an NP-hard problem. Some heuristics are available to obtain approximate solutions for this problem. In this paper, we propose different GRASP heuristics for the MDP, using distinct construction procedures and including a path-relinking technique. Performance comparison among related work and the proposed heuristics is provided. Experimental results show that the new GRASP heuristics are quite robust and are able to find high-quality solutions in reasonable computational times.",
    "actual_venue": "J Heuristics"
  },
  {
    "abstract": "We show the existence of codes for anytime coding over compound channels - i.e. codes that operate without knowledge of the transition probabilities of the communication channel. The anytime error exponent achieved is the same as if the decoder were given the probabilities governing the realized channel and allowed to do maximum likelihood decoding. Because of the equivalence between system stabilization over noisy feedback channels and communication over noisy channels, the results immediately give sufficient conditions for stabilization over compound channels.",
    "actual_venue": "International Symposium On Modeling And Optimization In Mobile, Ad Hoc And Wireless Networks And Workshops, Vols"
  },
  {
    "abstract": "Vast amounts of clinical information are generated daily on patients in the health care setting. Increasingly, this information is collected and stored for its potential utility in advancing health care. Knowledge-based systems, for example, might be able to apply rules to the collected data to determine whether a patient has a certain condition. Often, however; the underlying knowledge needed to write such rules is not welt understood. How could these clinical data be useful then? Use of machine learning is one answer. We present a pipeline for discovering the knowledge needed for event detection in medical time-series data. We demonstrate how this process can be applied in the development of intelligent patient monitoring for the intensive care unit (ICU). Specifically, we develop a system for detecting Otrue alarms situations in the ICU, wherecurrently as many as 86% of bedside monitor alarms are false.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "Classic combined cycle power plant models are often too complex for power system dynamic analysis, and hard to estimate the parameters accurately. The performances of parameter identification procedures have been significantly reduced by high-dimensional searches and strong nonlinear relationships. A new non-linear model is proposed to be suitable for the parameter identification procedure in this paper. An identification method based on an improved genetic algorithm (GA) is used for the modeling of a 400MW combined cycle power plant. The whole system is divided into six parts and an artificial disturbance is recorded for the identification of each part. The results show great consistence between identified model responses and the experimental data.",
    "actual_venue": "Icnc"
  },
  {
    "abstract": "Mission statements are considered to be one of the most popular management tools in the world - and also one of the most frustrating. To secure their success, it has been recommended that mission statements be communicated and disseminated to as many internal and external stakeholders as possible. One means for doing this is through the Internet and the posting of an organization's mission statement on its company Web site. But who is doing this? What types of organizations are using the World Wide Web to advertise their missions? Where is a mission statement typically located in a corporate Web site? And what are some of the motivations that an organization has for posting or not posting its mission? These are questions which have not yet been addressed by previous research and which this preliminary study sought to answer.",
    "actual_venue": "Internet Research"
  },
  {
    "abstract": "An algorithm for modifying the shape of a Bezier curve is proposed that uses the formula for the difference of two Bezier curves. This algorithm is iterative, and it can be applied to tensor products and triangular Bezier patches in addition to curves. The shape of the curve (surface) can be modified by two independent sets of shape parameters. The algorithm can be combined with the subdivision algorithm, and, in this case, it is possible to modify smaller sections of the curves and surfaces. The algorithm is efficient, especially in modeling forms that deviate slightly from a simpler, basic form.",
    "actual_venue": "Computer-Aided Design"
  },
  {
    "abstract": "The objective of foreground segmentation is to extract the desired foreground object from input videos. Over the years, there have been significant amount of efforts on this topic. Nevertheless, there still lacks a simple yet effective algorithm that can process live videos of objects with fuzzy boundaries (e.g., hair) captured by freely moving cameras. This paper presents an algorithm toward this goal. The key idea is to train and maintain two competing one-class support vector machines at each pixel location, which model local color distributions for both foreground and background, respectively. The usage of two competing local classifiers, as we have advocated, provides higher discriminative power while allowing better handling of ambiguities. By exploiting this proposed machine learning technique, and by addressing both foreground segmentation and boundary matting problems in an integrated manner, our algorithm is shown to be particularly competent at processing a wide range of videos with complex backgrounds from freely moving cameras. This is usually achieved with minimum user interactions. Furthermore, by introducing novel acceleration techniques and by exploiting the parallel structure of the algorithm, near real-time processing speed (14 frames/s without matting and 8 frames/s with matting on a midrange PC & GPU) is achieved for VGA-sized videos.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "A new approach for the automatic estimation of dense 3D deformation fields is proposed. In the first step, template propagation (an advanced block matching strategy) produces not only a large set of point correspondences, but also a quantitative measure of the \"registration quality\" for each point pair. Subsequently, the deformation field is obtained by a method based on Wendland radial base functions. This method has been adapted to incorporate \"registration quality\" into regularization, where Morozov's discrepancy principle has been applied to give intuitive meaning to the regularization parameter. The main advantage of the presented algorithm is the ability to perform an elastic registration in the presence of large deformations with minimum user interaction. Applying the method, complicated respiratory motion patterns in 3D MR images of the thorax have been successfully determined. The complete procedure takes less than one hour on a standard PC for MR image pairs (256脳256脳75 voxels) showing a 40 mm displacement of the diaphragm.",
    "actual_venue": "Miccai"
  },
  {
    "abstract": "The ubiquitous computing scenario brings many new problems such as coping with the limited processing power of mobile devices, frequent disconnections, the migration of code and tasks between heterogeneous devices, and others. Current practical approaches to the ubiquitous computing problem usually rely upon traditional computing paradigms conceived back when distributed applications where not a concern. In this paper we propose UbiHolo, a new end-to-end model oriented to development and execution of applications over largescale ubiquitous computing systems. UbiHolo is based on the Holoparadigm (in short, Holo), a new software paradigm oriented to the development of distributed computer systems. After that we show that UbiHolo is scalable, and that it is possible to create and manage execution of applications over large-scale ubiquitous environments without a great impact on application's user response times. Our results are obtained through a simulation of a Holo system with up to 950 nodes, which was implemented over the p2psim simulator.",
    "actual_venue": "Ds-Rt"
  },
  {
    "abstract": "This note presents an alternate approach to solving the inverse problem of logic by applying a decomposition technique based on Boolean equations.",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "Phylogeny inference has moved in recent years from the analysis of a single or few proteins to that of whole proteomes. However, the reconstruction of evolutionary trees for big number of species poses a significant computational challenge when using complete proteomes, even when relatively fast pairwise sequence comparison algorithms are used. We present a distributed approach that relies on the computation of distance measures based on maximal shared substrings within a bounded Hamming distance. The distributed system we built to implement this approach is flexible in that it supports a variety of design choices. It is based on the Spark framework and covers all the steps required by our approach, starting from the initial indexing of a set of FASTA sequences up to producing a report detailing the distances among these sequences, ranked according to a user-defined measure. Here we apply it to compare all proteins of selected organisms, divide them into groups and perform the comparisons within each group separately. The groups include: the functionally characterized proteins, the ribosomal proteins, and the unannotated proteins. We compute the average distances within the groups and evaluate their relationship and ability to capture the evolutionary closeness of organisms. We run experiments on selected species using a Hadoop computing cluster running Spark. The results show that the system implementing our approach is scalable and accurate.",
    "actual_venue": "Theoretical Computer Science"
  },
  {
    "abstract": "Starting from a theorem on the distance matrix of a projective linear code, one introduces an axiomatic definition of a strongly regular normed space. it is then shown that every such normed space admits a representation by means of a projective code. As a particular case, this yields a one-to-one correspondence between two-weight projective codes over prime fields and some strongly regular graphs.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "A method for real-time estimation of weather, especially the amount of rainfall, by analyzing CCTV images, is much cheaper than one using the existing expensive weather observation equipment. In this paper, we propose a method to find an estimation model function whose input is CCTV images and output is the amount of rainfall. Using CCTV images, we propose an algorithm for selecting the number and size of the region of interest optimized for rainfall estimation, generating a data pattern graph showing a clear distinction from the number of regions of interest, clustering the pattern data graphs, and estimating the amount of rainfall. Experiments using real CCTV images show that the estimation accuracy is greater than 80%. (C) 2018 Elsevier B.V. All rights reserved.",
    "actual_venue": "Future Generation Computer Systems-The International Journal Of Escience"
  },
  {
    "abstract": "We consider the dynamics of small networks of cou- pled cells. We usually assume asymmetric inputs and no global or local symmetries in the network and consider equivalence of net- works in this setting; that is, when two networks with different architectures give rise to the same set of possible dynamics. Fo- cusing on transitive (strongly connected) networks that have only one type of cell (identical cell networks) we address three questions relating the network structure to dynamics. The first question is how the structure of the network may force the existence of in- variant subspaces (synchrony subspaces). The second question is how these invariant subspaces can support robust heteroclinic at- tractors. Finally, we investigate how the dynamics of coupled cell networks with different structures and numbers of cells can be re- lated; in particular we consider the sets of possible \"inflations\" of a coupled cell network that are obtained by replacing one cell by many of the same type, in such a way that the original network dynamics is still present within a synchrony subspace. We illus- trate the results with a number of examples of networks of up to six cells.",
    "actual_venue": "J Nonlinear Science"
  },
  {
    "abstract": "Current speech synthesis technology is difficult to understand in everyday noise situations. Although there is a significant body of work on how humans modify their speech in noise, the results have yet to be implemented in a synthesizer. Algorithms capable of processing and incorporating these modifications may lead to improved speech intelligibility of assistive communication aids and more generally of spoken dialogue systems. We describe our efforts in building the Loudmouth synthesizer which emulates human modifications to speech in noise. A perceptual experiment indicated that Loudmouth achieved a statistically significant gain in intelligibility compared to a standard synthesizer in noise.",
    "actual_venue": "Assets"
  },
  {
    "abstract": "Nowadays, Urban Waste Collection process has become crucial to ensure cities’ wealth and viability. The growth of urban centers and the rapid expansion of industry led to a revision of plans and waste collection routes to increase their efficiency and effectiveness. Traditional approaches for the Vehicle Routing Problem and the Waste Collection Problem are not taking into account some factors such as the huge amount of available information produced by the Internet of Things devices.",
    "actual_venue": "Icwe"
  },
  {
    "abstract": "We present an evaluation of retransmission strategies over local area networks. Expressions are derived for the expectation and the variance of the transmission time of the go-back-n and the selective repeat protocols in the presence of errors. These are compared to the expressions for blast with full retransmission on error (BFRE) derived by Zwaenepoel [Zwa 85]. We conclude that go-back-n performs almost as well as selective repeat and is very much simpler to implement while BFRE is stable only for a limited range of messages sizes and error rates. We also present a variant of BFRE which optimally checkpoints the transmission of a large message. This is shown to overcome the instability of ordinary BFRE. It has a simple state machine and seems to take full advantage of the low error rates of local area networks. We further investigate go-back-n by generalizing the analysis to an upper layer transport protocol, which is likely to encounter among other things, variable delays due to protocol overhead, multiple connections, process switches and operating system scheduling priorities.",
    "actual_venue": "Sigmetrics"
  },
  {
    "abstract": "A software architecture style identifies classes of software architectures that present distinguishable commonalties. One major problem in the specification of software architectures is when evolution includes reconfiguration and mobility. With this in mind and understanding the relevance of visual languages specially at the design level, we present in this paper a graphical model using hyperedge replacement systems as a formal model to represent styles and their reconfigurations. To model the reconfiguration of styles we present two approaches. The first approach uses Synchronized Hyperedge Replacement Systems with the addition of name mobility to model dynamic reconfiguration. The second approach models consistent reconfigurations with respect to a style over the rewriting system derivations, based on the typing power of the lambda-calculus.",
    "actual_venue": "Electronic Notes In Theoretical Computer Science"
  },
  {
    "abstract": "We propose to use multiple signature sequences (instead of one) per user in a multicarrier direct-sequence spread spectrum multiple access system in Rician fading channels. Every user has a distinct set of spreading sequences, with a different spreading sequence for each carrier in each user's set. By analysis and computer simulation, we show that when these sets of sequences are chosen to be mutually orthogonal complementary sets of sequences, multiple access interference (MAI) is reduced in Rician fading channels. We also show that we can pack symbols more closely together, resulting in a higher data rate. The proposed system with a distinct set of deterministic spreading sequences per user has lower MAI and higher data rate than another system with one distinct random spreading sequences per user in Rician fading channels.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "This paper investigates the distributed state estimation problem for a class of sensor networks described by discrete-time stochastic systems with stochastic switching topologies. In the sensor network, the redundant channel and the randomly varying nonlinearities are considered. The stochastic Brownian motions affect both the dynamical plant and the sensor measurement outputs. Through available output measurements from each individual sensor, the distributed state estimators are designed to approximate the states of the networked dynamic system. Sufficient conditions are established to guarantee the convergence of the estimation error systems for all admissible stochastic disturbances and randomly varying nonlinearities. Then, the distributed state estimator gain matrices are derived using the linear matrix inequality method. Moreover, a numerical example is given to verify the effectiveness of the theoretical results.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "In this paper, we discuss a number of issues related to the design of evaluation tests for comparing interactive music systems for improvisation. Our testing procedure covers rehearsal and performance environments, and captures the experiences of a musician/participant as well as an audience member/observer. We attempt to isolate salient components of system behavior, and test whether the musician or audience are able to discern between systems with significantly different behavioral components. We report on our experiences with our testing methodology, in comparative studies of our London and ARHS improvisation systems (1).",
    "actual_venue": "Nime"
  },
  {
    "abstract": "Degeneration and loss of articular cartilage in Osteoarthritis (OA) is difficult to measure, because changes are small and localised. We present a method that uses statistical shape models of the knee bones to define an anatomically consistent frame of reference across a population, providing sensitive measures of cartilage morphology in anatomically equivalent regions of interest. Bone and cartilage were manually segmented from Magnetic Resonance Images (MRI) of volunteers' knees. Dense correspondences were defined across all subjects by constructing Minimum Description Length (MDL) statistical shape models of the bones. Regions of interest were manually delineated on the mean bone shapes provided by the models, and propagated to each individual in an anatomically consistent manner, using the model-based correspondences. We show that this approach results in precise measurements that can be used to detect small localised changes in cartilage thickness. Results are reported for an OA study, in which significant focal loss of cartilage was detected over 6 months in a cohort of just 31 patients.",
    "actual_venue": "Ieee International Symposium On Biomedical Imaging: Macro To Nano, Vols And"
  },
  {
    "abstract": "In this paper we propose an SDN based dynamic path selection for HTTP-based video streaming. MPEG-DASH is a recently proposed standard allowing rate adaptation over HTTP. On the other hand, Software Defined Networking (SDN) is a new network architecture, which allows determining routes of packet flows by an external controller software. In this study we develop an optimization model aiming to obtain maximum throughput for DASH services by selecting the optimal paths for video packet flows over SDN. The simulations show that the clients in the proposed system receive better QoE in terms of video bitrate, outage duration and startup delay when compared to the clients running in Internet's best effort.",
    "actual_venue": "Icce-Berlin"
  },
  {
    "abstract": "In this paper, we propose an adaptive Forward Error Correction (FEC) coding algorithm at the Medium Access Control (MAC) layer used in wireless networks. Our algorithm is based on the lookup table architecture, including a distance lookup table and a bit error rate lookup table. These tables will store the best value of the FEC codes based on different distances and bit error rates. Because radio channels change over time, the bit error rate is always changing, or in the case of mobile nodes, when the transmission distance changes, the bit error rate also changes, so previously proposed algorithms take longer to find the optimal value of the FEC code for data transmission. Our proposed algorithm, however, is based on 2 lookup tables, and thus, it can always quickly select an optimal FEC code during early data transmissions and achieve high performance. We compare our algorithm with other methods based on performance metrics such as the recovery overhead of FEC codes, energy efficiency, and peak-signal-to-noise ratio values in the case of image transmission. Our simulation indicates that the proposed algorithm achieves better performances and proves the correctness of the proposed lookup table architecture.",
    "actual_venue": "International Journal Of Communication Systems"
  },
  {
    "abstract": "Availability in wireless visual sensor networks is a major design issue that is directly related to applications monitoring quality. For targets monitoring, visual sensors may be deployed to cover most or all of targets, and monitoring quality may be focused on how well a set of targets are being covered. However, targets may have different dimensions and it is expected that large targets may be only partially viewed by source nodes, which may affect coverage quality and lead to a state of unavailability. In this context, this article analyzes the effect of target’s size on effective coverage in wireless visual sensor networks. A new coverage metric, the Effective Target Viewing ETV, is proposed to measure monitoring quality over a set of targets, which is exploited as a fundamental parameter for availability assessment. Results show that ETV can be used as a practical coverage metric when assessing availability in wireless visual sensor networks.",
    "actual_venue": "J Electrical And Computer Engineering"
  },
  {
    "abstract": "Collision among moving objects in space is one of the most common risks in daily life. In this context, we have developed an abstract model that allows to detect the presence of risk of future collisions among objects from the video content analysis. Our proposal carries out several stages. First, a camera calibration process calculates the real location of object in scene. Then, we estimate the object speed and their future trajectory in order to predict possible collisions. All the information of the objects is described in an ontology. Using the properties of objects (such as location, speed, trajectories), we have defined a fuzzy rule that permits to identify whether an object is in danger because another could hit him. The use of fuzzy logic results in two points: the collision detection is gradual and the model can be adjusted through membership functions to fuzzy concepts. Furthermore, the proposed model is easily adaptable to any situation and can be applied on various fields. With the aim of testing our proposal, we have focused on pedestrian accidents, a case of special interest since a lot of pedestrians die or are injured in traffic accidents daily. We have developed an application based on our model that is able to predict, in real time, the traffic accidents where a vehicle could run over a pedestrian. The obtained results in the experimental stage show a high performance of the system.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "The new cellular communication standard 3GPP Long Term Evolution (LTE) promises high throughputs and low latencies, thus enabling even more bandwidth-demanding and real-time critical services for end-users. This is of particular interest for vehicle manufacturers who in the future intend to offer a huge variety of cooperative driver assistance services with different quality of service (QoS) settings. In this paper we analyze the suitability of LTE for future automotive off-board services in terms of transmission delays and reliability under various QoS settings. Our investigations are based on extensive LTE system-level simulations under different load conditions and network deployments as well as on a theoretical delay analysis. The results show that an accurate selection of the LTE QoS parameters is crucial in order to meet the delay and reliability requirements of future automotive applications, especially in high-load network conditions.",
    "actual_venue": "Vehicular Networking Conference"
  },
  {
    "abstract": "This paper demonstrates, to our best knowledge, the first attempt on gender and ethnicity identification from silhouetted face profiles using a computer vision technique. The results achieved, after testing on 441 images, show that silhouetted face profiles have a lot of information, in particular, for ethnicity identification. Shape context based matching [1] was employed for classification. The test samples were multi-ethnic. Average accuracy for gender was 71.20% and for ethnicity 71.66%. However, the accuracy was significantly higher for some classes, such as 83.41% for females (in case of gender identification) and 80.37% for East and South East Asians (in case of ethnicity identification).",
    "actual_venue": "Image Processing"
  },
  {
    "abstract": "This paper presents a modeling approach in generating random flow breakdowns on congested freeways and capturing subsequent wave propagation among heterogeneous drivers. The approach is intended for predicting travel time variability caused by such stochastic phenomena. It is assumed that breakdown may occur at different flow levels with some probability and would sustain for a random duration. This is modeled at the microscopic level by considering speed changes that are initiated by a leading vehicle and propagated by the following vehicles with correlated–distributed behavioral parameters. Numerical results from a Monte Carlo simulation demonstrate that the proposed stochastic modeling approach produces a realistic macroscopic traffic flow behavior and can be used to generate travel time distributions.",
    "actual_venue": "Ieee Transactions Intelligent Transportation Systems"
  },
  {
    "abstract": "During a police investigation, officers often have to sort through hundreds of photographs to identify a suspect. To aid this task, we at the Institute of Systems Science developed and implemented a flexible database system that can retrieve faces using personal information, fuzzy and free-text descriptors, and classification trees.<>",
    "actual_venue": "Ieee Multimedia"
  },
  {
    "abstract": "We propose a tool for summarizing communication patterns between multiple hosts from traffic data with a small memory space, using a 2-D bitmap. Here, we focus on the communication pattern between pairs of source-destination hosts; these represents spatial communication patterns. By analyzing communication patterns using a bitmap, we can identify a super spreader, which is a host that sends packets to many destinations, or analyze the relationship between two source hosts. For the latter purpose, we present an application of the bitmap to calculate the similarity of hosts based on their peer-hosts patterns.",
    "actual_venue": "Saint Workshops"
  },
  {
    "abstract": "This paper concerns with the existence and the mean square exponential stability of periodic solutions of stochastic interval neural networks with mixed time delays. By applying Lyapunov functional, inequality technique, Ito^,s differential formula and fixed point theorem, the criteria are derived in terms of linear matrix inequalities. Three numerical examples are provided to illustrate the effectiveness of the proposed solutions.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Presently, ship-mounted cranes are playing more and more important roles in modern ocean transportation and logistics. Different from traditional land-fixed crane systems, ship-mounted cranes present much more complicated nonlinear dynamical characteristics and they are persistently influenced by different mismatched disturbances due to harsh sea environments, e.g., sea waves, ocean currents, sea ...",
    "actual_venue": "Ieee Transactions On Systems, Man, And Cybernetics: Systems"
  },
  {
    "abstract": "Transformations of trees and rewriting of terms can be found in various settings e.g. transformations of abstract syntax trees in compiler construction and program synthesis.",
    "actual_venue": "Esop"
  },
  {
    "abstract": "The aim of this paper is to describe an assembly line paradigm that exploits the latest trends in handling and assembly technologies. Mobile robotic units, flexible and exchangeable end effectors, equipped with automated integration capabilities are used to achieve a highly reconfigurable and autonomous line as opposed to the existing fixed assembly lines, which are difficult to be reconfigured. In a study for the automotive assembly case, the benefits of the new paradigm over the traditional assembly schemes are quantified with the help of different simulated operational conditions.",
    "actual_venue": "International Journal Of Computer Integrated Manufacturing"
  },
  {
    "abstract": "Older adults have difficulty using and learning to use smart phones, in part because the displays are too small to provide effective interactive help. Our work explores the use of a large display to temporarily augment the small phone display to support older adults during learning episodes. We designed and implemented a learning system called Help Kiosk which contains unique features to scaffold the smart phone learning process for older adults. We conducted a mixed-methods user study with 16 older adults (55+) to understand the impact of this unique design approach, comparing it with the smart phone's official printed instruction manual. We found Help Kiosk gave participants more confidence that they were doing the tasks correctly, and helped minimize the need to switch their attention between the instructions and their phone.",
    "actual_venue": "Assets"
  },
  {
    "abstract": "Holograms can reconstruct complete optical wavefronts, capturing images that have a three-dimensional appearance and can be observed from different perspectives. Museum exhibits often use optical hologram technology because it permits presentation of 3D objects with almost no loss in visual quality. Optical holograms are static, however, and lack interactivity. Combining 3D computer graphical elements with stereoscopic presentation techniques provides an alternative that allows interactivity.",
    "actual_venue": "International Conference On Computer Graphics And Interactive Techniques"
  },
  {
    "abstract": "In this work the Anomalous Pattern algorithm is explored as an initialization strategy to the Fuzzy K-Means (FCM), with the sequential extraction of clusters, one by one, that simultaneously allows to determine the number of clusters. The composed algorithm, Anomalous Pattern Fuzzy Clustering (AP-FCM), is applied in the segmentation of Sea Surface Temperature (SST) images for the identification of Coastal Upwelling. Two independent data samples of two upwelling seasons, in a total of 61 SST images covering large diversity of upwelling situations, are analysed. Results show that by tuning the AP-FCM stop conditions it fits a good number of clusters providing an effective segmentation of the SST images whose spatial visualization of fuzzy membership closely reproduces the original images. Comparing the AP-FCM with the FCM using several validation indices to determine the number of clusters shows the advantage of the AP-FCM since FCM typically leads to under or over-segmented images. Quantitative assessment of the segmentations is accomplished through ROC analysis with ground-truth maps constructed from the Oceanographers' annotations. Compared to FCM, the number of iterations of the AP-FCM is significantly decreased.",
    "actual_venue": "Ideal"
  },
  {
    "abstract": "We consider the single hop broadcast packet erasure channel (BPEC) with two multicast sessions (each of them destined to a different group of N users) and regularly available instantaneous receiver ACK/NACK feedback. Using the insight gained from recent work on BPEC with unicast and degraded messages [1], [2], we propose a virtual queue based session-mixing algorithm, which does not require knowledge of channel statistics and achieves capacity for N = 2 and iid erasures. Since the extension of this algorithm to N > 2 is not straightforward, we describe a simple algorithm which outperforms standard timesharing for arbitrary N and is actually asymptotically better than timesharing, for any finite N, as the erasure probability goes to zero. We finally provide, through an information-theoretic analysis, sufficient but not necessary asymptotic conditions between N and n (the number of transmissions) for which the achieved sum rate, under any coding scheme, is essentially identical to that of timesharing.",
    "actual_venue": "Network Coding"
  },
  {
    "abstract": "We investigate a novel intuitionistic modal logic, called Propositional Lax Logic, with promising applications to the formal verification of computer hardware. The logic has emerged from an attempt to express correctness up to behavioural constraints — a central notion in hardware verification — as a logical modality. The resulting logic is unorthodox in several respects. As a modal logic it is special since it features a single modal operator O that has a flavour both of possibility and of necessity. As for hardware verification it is special since it is an intuitionistic rather than classical logic which so far has been the basis of the great majority of approaches. Finally, its models are unusual since they feature worlds with inconsistent information and furthermore the only frame condition is that the O-frame be a subrelation of the -frame. We provide the motivation for Propositional Lax Logic and present several technical results. We investigate some of its proof-theoretic properties, and present a cut-elimination theorem for a standard Gentzen-style sequent presentation of the logic. We further show soundness and completeness for several classes of fallible two-frame Kripke models. In this framework we present a concrete and rather natural class of models from hardware verification such that the modality O models correctness up to timing constraints.",
    "actual_venue": "CSL"
  },
  {
    "abstract": "Future Exascale computing systems with a large number of processors, memory elements and interconnection links, are expected to experience multiple, complex faults, which affect both applications and operating-runtime systems. A variety of algorithms, frameworks and tools are being proposed to realize and/or verify the resilience properties of computations that guarantee correct results on failure-prone computing systems. We analytically show that certain resilient computation problems in presence of general classes of faults are undecidable, that is, no algorithms exist for solving them. We first show that the membership verification in a generic set of resilient computations is undecidable. We describe classes of faults that can create infinite loops or non-halting computations, whose detection in general is undecidable. We then show certain resilient computation problems to be undecidable by using reductions from the loop detection and halting problems under two formulations, namely, an abstract programming language and Turing machines, respectively. These two reductions highlight different failure effects: the former represents program and data corruption, and the latter illustrates incorrect program execution. These results call for broad-based, well-characterized resilience approaches that complement purely computational solutions using methods such as hardware monitors, co-designs, and system-and application-specific diagnosis codes.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Reconstructing image structure can be accomplished using a geometric image inpainting algorithm. This is done following three main steps: the first step consists of locating the damaged region. In the second step, the structure of the corrupted contours is reconstructed by matching each damaged contour with its corresponding one in a way to obtain a visually plausible image. In the third step, the found couples are joined using a curve fitting technique. The search space of candidate solutions dramatically increases as the number of contours increases, which makes the search of the optimal solution by a traditional deterministic method infeasible. A good choice to solve this problem is the use of meta-heuristics. In this paper, a discrete particle swarm optimisation DPSO is used to find the best correspondence between contours using their curvature values as a quality metric. Additional information is used, which is the mean ordinate of each contour. It adds spatial information of the location of damaged contours. The experimental results and comparisons with the genetic algorithm show the efficiency of the DPSO for geometric image inpainting.",
    "actual_venue": "International Journal Of Intelligent Engineering Informatics"
  },
  {
    "abstract": "We discuss the legacy of Alan Turing and his impact on computability and analysis.",
    "actual_venue": "Lecture Notes In Logic"
  },
  {
    "abstract": "The representation of geographical objects with vague or fuzzy boundaries still poses a challenge to current geographical information systems. The paper presents a geometric account to deal with spatial vagueness. This approach is based on ideas of the theory of supervaluation. To capture vague spatial information current geographical information systems mainly employ fuzzy set theory and fuzzy logic. The proposed geometric theory is contrasted with fuzzy theories regarding the representation of vague spatial objects and the inferences that can be drawn about the objects. Opposed to fuzzy theories, the proposed theory does not rely on a numerical representation to model spatial vagueness, but is still compatible with it. Therefore, the approach is able to support spatial databases in qualitative spatial inferences.",
    "actual_venue": "Cosit"
  },
  {
    "abstract": "This paper investigates noise cancellation problem of memristive neural networks. Based on the reproducible gradual resistance tuning in bipolar mode, a first-order voltage-controlled memristive model is employed with asymmetric voltage thresholds. Since memristive devices are especially tiny to be densely packed in crossbar-like structures and possess long time memory needed by neuromorphic synapses, this paper shows how to approximate the behavior of synapses in neural networks using this memristive device. Also certain templates of memristive neural networks are established to implement the noise cancellation.",
    "actual_venue": "Neural Networks"
  },
  {
    "abstract": "This study evaluates the utility of a publication power approach (PPA) for assessing the quality of journals in the field of artificial intelligence. PPA is compared with the Thomson-Reuters Institute for Scientific Information (TR) 5-year and 2-year impact factors and with expert opinion. The ranking produced by the method under study is only partially correlated with citation-based measures (TR), but exhibits close agreement with expert survey rankings. A simple average of TR and power rankings results in a new ranking that is highly correlated with the expert survey rankings. This evidence suggests that power ranking can contribute to evaluating artificial intelligence journals. © 2012 Wiley Periodicals, Inc.",
    "actual_venue": "Jasist"
  },
  {
    "abstract": "This paper presents two new coding theorems on a (2, 2)-threshold scheme with an opponent who impersonates one of the shareholders. In the (2, 2)-threshold scheme an encoder blockwisely generates two shares Xn and Yn from n secrets Sn and a uniform random number En, where Sn is generated from a general source. There are three kinds of inputs to a decoder, (Nn., Yn), (X̅n, Xn) and (Xn, Y̅n), where X̅n and Y̅n are fraudulent shares generated by the opponent. The decoder judges whether the input is legitimate or not under negligible decoding error probability that vanishes as n → ∞. The two coding theorems given in this paper characterize the minimum attainable rates of Xn, Yn and En and the maximum attainable exponent of the probability of the successful impersonation attack. It turns out that the (2, 2)-threshold scheme with a cheater is related to not only hypothesis testing but also optimistic coding of general sources and channels.",
    "actual_venue": "Information Theory And Its Applications"
  },
  {
    "abstract": "Recently, a base solution has been adopted for supporting multicast listener mobility in Proxy Mobile IPv6 (PMIPv6). This solution brings multicast listener support into PMIPv6 by placing multicast routing functions at LMA while MAGs provide MLD proxy functions. Nevertheless, it does not address specific optimizations and performances issues such as handover latency, tunnel overhead, non-route optimization, etc. Specially, this paper focuses on handover performance in terms of service disruption time. The theoretical and simulation results show that through the utilization of multicast context transfer the service disruption time can be reduced significantly. By tuning the behavior of the IGMP/MLD for routers, we can also achieve a similar result, but make a dramatically increasing multicast-signaling. Thus, the impact of multicast-related signaling on the wireless link is studied to suggest the maximum number of listeners supported by one MAG. An enhanced multicast context transfer function is also proposed for group multicast mobility to reduce the number of signaling messages.",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "We propose a distributed machine-learning architecture to predict trustworthiness of sensor services in Mobile Edge Computing (MEC) based Internet of Things (IoT) services, which aligns well with the goals of MEC and requirements of modern IoT systems. The proposed machine-learning architecture models training a distributed trust prediction model over a topology of MEC-environments as a Network Lasso problem, which allows simultaneous clustering and optimization on large-scale networked-graphs. We then attempt to solve it using Alternate Direction Method of Multipliers (ADMM) in a way that makes it suitable for MEC-based IoT systems. We present analytical and simulation results to show the validity and efficiency of the proposed solution.",
    "actual_venue": "Ieee International Conference On Web Services"
  },
  {
    "abstract": "The article Breast cancer classification in pathological images based on hybrid features, written by Cuiru Yu, Houjin Chen, Yanfeng Li, Yahui Peng, Jupeng Li and Fan Yang, was originally published electronically on the publisher’s internet portal (SpringerLink) on March 16, 2019 with open access.",
    "actual_venue": "Multimedia Tools And Applications"
  },
  {
    "abstract": "Sink scheduling, in the form of scheduling multiple sinks among the available sink sites to relieve the level of traffic burden, is shown to be a promising scheme in wireless sensor networks (WSNs). However, the problem of maximizing the network lifetime via sink scheduling remains quite a challenge since routing issues are tightly coupled. Previous approaches on this topic either suffer from poor performance due to a lack of joint considerations, or are based on relaxed constraints. Therefore, in this paper, we aim to fill in the research blanks. First, we develop a novel notation Placement Pattern (PP) to bound time-varying routes with the placement of sinks. This bounding technique transforms the problem from time domain into pattern domain, and thus, significantly decreases the problem complexity. Then, we formulate this optimization in a pattern-based way and create an efficient Column Generation (CG) based approach to solve it. Simulations not only demonstrate the efficiency of the proposed algorithm but also substantiate the importance of sink mobility for energy-constrained WSNs.",
    "actual_venue": "Ad Hoc Networks"
  },
  {
    "abstract": "The Unicode Standard, version 1.1, and ISO/IEC 10646-1:1993 jointly define a 16 bit character set which encompasses most of the world's writing systems. 16-bit characters, however, are not compatible with many current applications and protocols, and this has led to the development of a few so-called UCS transformation formats (UTF), each with different characteristics. UTF-8, the object of this memo, has the characteristic of preserving the full US-ASCII range: US-ASCII characters are encoded in one octet having the usual US-ASCII value, and any octet with such a value can only be an US-ASCII character. This provides compatibility with file systems, parsers and other software that rely on US-ASCII values but are transparent to other values.",
    "actual_venue": "RFC"
  },
  {
    "abstract": "A method for designing and implementing reactive object-oriented programs with explicit emphasis of states is suggested. The method relies on the automata-based programming (SWITCH-technology) and the UML notation. The UniMod tool based on this method, which is a plug-in module for the Eclipse platform, is described.",
    "actual_venue": "Programming And Computer Software"
  },
  {
    "abstract": "Protein classification by machine learning algorithms is now widely used in structural and functional annotation of proteins. The Protein Classification Benchmark collection (http://hydra.icgeb.trieste.it/benchmark) was created in order to provide standard datasets on which the performance of machine learning methods can be compared. It is primarily meant for method developers and users interested in comparing methods under standardized conditions. The collection contains datasets of sequences and structures, and each set is subdivided into positive/negative, training/test sets in several ways. There is a total of 6405 classification tasks, 3297 on protein sequences, 3095 on protein structures and 10 on protein coding regions in DNA. Typical tasks include the classification of structural domains in the SCOP and CATH databases based on their sequences or structures, as well as various functional and taxonomic classification problems. In the case of hierarchical classification schemes, the classification tasks can be defined at various levels of the hierarchy (such as classes, folds, superfamilies, etc.). For each dataset there are distance matrices available that contain all vs. all comparison of the data, based on various sequence or structure comparison methods, as well as a set of classification performance measures computed with various classifier algorithms.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "In this paper, we are concerned with the stability of the error bounds for semi-infinite convex constraint systems. Roughly speaking, the error bound of a system of inequalities is said to be stable if all its “small” perturbations admit a (local or global) error bound. We first establish subdifferential characterizations of the stability of error bounds for semi-infinite systems of convex inequalities. By applying these characterizations, we extend some results established by Azé and Corvellec [SIAM J. Optim., 12 (2002), pp. 913-927] on the sensitivity analysis of Hoffman constants to semi-infinite linear constraint systems.",
    "actual_venue": "Siam Journal On Optimization"
  },
  {
    "abstract": "In lower mammals, locomotion seems to be mainly regulated by subcortical and spinal networks. On the contrary, recent evidence suggests that in humans the motor cortex is also significantly engaged during complex locomotion tasks. However, a detailed understanding of cortical contribution to locomotion is still lacking especially during stereotyped activities. Here, we show that cortical motor areas finely control leg muscle activation during treadmill stereotyped walking. Using a novel technique based on a combination of Reliable Independent Component Analysis, source localization and effective connectivity, and by combining electroencephalographic (EEG) and electromyographic (EMG) recordings in able-bodied adults we were able to examine for the first time cortical activation patterns and cortico-muscular connectivity including information flow direction. Results not only provided evidence of cortical activity associated with locomotion, but demonstrated significant causal unidirectional drive from contralateral motor cortex to muscles in the swing leg. These insights overturn the traditional view that human cortex has a limited role in the control of stereotyped locomotion, and suggest useful hypotheses concerning mechanisms underlying gait under other conditions.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "In this article, an original algebraic method is used for the estimation of state variables. The estimation is used to implement a position control scheme for DC motors. In addition, the estimation of the Coulombs friction coefficient of the servo motor model is also investigated. The approach is based on elementary algebraic manipulations which lead to specific formulaes for the unmeasured states. The state estimation algorithm is verified by simulations.",
    "actual_venue": "Engineering Letters"
  },
  {
    "abstract": "Contextual polarity ambiguity is an important problem in sentiment analysis. Many opinion keywords carry varying polarities in different contexts, posing huge challenges for sentiment analysis research. Previous work on contextual polarity disambiguation makes use of term-level context, such as words and patterns, and resolves the polarity with a range of rule-based, statistics-based or machine learning methods. The major shortcoming of these methods lies in that the term-level features sometimes are ineffective in resolving the polarity. In this work, opinion-level context is explored, in which intra-opinion features and inter-opinion features are finely defined. To enable effective use of opinion-level features, the Bayesian model is adopted to resolve the polarity in a probabilistic manner. Experiments with the Opinmine corpus demonstrate that opinion-level features can make a significant contribution in word polarity disambiguation in four domains.",
    "actual_venue": "Cognitive Computation"
  },
  {
    "abstract": "In this paper, we propose a soft IP generator which can add or remove PCM codec modules arbitrarily. It can be applied to PCM codec IP designs that need to change their related modules, corresponding to different working environments. It also can help us to easily manage our soft IP modules, produce optimized modules, and remove unnecessary modules, in order to reduce the implementation cost. In addition, users can implement their own Verilog HDL code of PCM codec by following our predefined interface specification, and integrate it with our optimal PCM codec module to produce the users' own optimized system.",
    "actual_venue": "FPT"
  },
  {
    "abstract": "Adapting the speed of a processor is an effective method to reduce energy consumption. This paper studies the optimal way to scale speed to balance response time and energy consumption under processor sharing scheduling. It is shown that using a static rate while the system is busy provides nearly optimal performance, but having a wider range of available speeds increases robustness to different traffic loads. In particular, the dynamic speed scaling optimal for Poisson arrivals is also constant-competitive in the worst case. The scheme that equates power consumption with queue occupancy is shown to be 10-competitive when power is cubic in speed.",
    "actual_venue": "Perform Eval"
  },
  {
    "abstract": "The increasing demand of industry for enabling Business to Business and Application-to-Application communication has led to a growing requirement for Service Oriented Architecture. Web Services are based on Service Oriented Architecture which enable application-to-application communication over the internet and easy accessibility to heterogeneous applications and devices. As web services proliferate and become more sophisticated and interdependent, the issues regarding their publication and discovery become of utmost importance. This paper proposes design of A Discovery cum Publishing Engine for web service discovery with refined searching mechanism which uses service rating techniques for efficient and effective web service discovery within optimum response time. We have used Data Mining Techniques to narrow down the search space in UBRs. Further the proposed engine has an ability to publish or search web service across multiple UBRs. In addition to this an extended design of service registry is proposed to store service rating data along with the service information. The Engine publishes the web services in UBR by following a classification scheme and performs a validation test on discovered web services. Service reviews and rating have been utilized to help a user for selection of appropriate service.",
    "actual_venue": "Services"
  },
  {
    "abstract": "Structure-preserving design has emerged as an important principle in Knowledge Engineering for developing knowledge-based systems. The principle prescribes the construction of increasingly more detailed models of the functionality of the knowledge-based system, while the information content and structure of the initial model are maintained throughout the process. It is, however, not trivial to come up with such models: their construction is still more an art than a science. Libraries with reusable components provide generic structures ready for selection and adaptation to the specific requirements at hand. We show the construction of a conceptual and formal model of a diagnostic reasoner through the use of two respective libraries, following the structure-preserving design principle. We include a proof that the high-level conceptual specification of the reasoner is ensured by the formal model presented. (C) 1997 Academic Press Limited.",
    "actual_venue": "Int J Hum-Comput Stud"
  },
  {
    "abstract": "A fast context independent HV partitioning scheme for fractal image compression technique of grey scale images is proposed. The proposed partitioning scheme partitions middle of range either horizontally or vertically to create to sub-ranges if the range is not covered well by any domain. The decision to select one of the two possibilities i.e. horizontal or vertical partitioning is done only by a simple checking which side of the range is larger than the other. It does not depend on the context of the range to compute the partitioning point that speed up the partitioning. One variant of the same is also proposed where the decision to select one of the two sides of range is done by computing the pixel value differences of the middle vertical lines and the middle horizontal lines and determining which is greater than other. The fractal image compression for grey scale image with the proposed partitioning schemes offer better compression rates than the quadtree partitioning scheme maintaining almost same compression times with improved PSNRs. Though the compression rates are not as well as offered by HV partitioning scheme, the proposed schemes are much faster than the same.",
    "actual_venue": "Ised Proceedings Of The International Symposium On Electronic System Design"
  },
  {
    "abstract": "Optical motion capture can be classified as an inference problem: given the data produced by a set of cameras, the aim is to extract the hidden state, which in this case encodes the posture of the subject's body. Problems with motion capture arise due to the multi-modal nature of the likelihood distribution, the extremely large dimensionality of its state-space, and the narrow region of support of local modes. There are also problems with the size of the data and the difficulty with which useful visual cues can be extracted from it, as well as how informative these cues might be. Several algorithms exist that use stochastic methods to extract the hidden state, but although highly parallelisable in theory, such methods produce a heavy computational overhead even with the power of today's computers. In this paper we assume a set of pre-calibrated cameras and only extract the subject's silhouette as a visual cue. In order to describe the 2D silhouette data we define a 2D model consisting of conic fields. The resulting likelihood distribution is differentiable w.r.t. the state, meaning that its global maximum can be located fast using gradient ascent search, given manual initialisation at the first frame. In this paper we explain the construction of the model for tracking a human hand; we describe the formulation of the derivatives needed, and present initial results on both real and simulated data.",
    "actual_venue": "Isvc"
  },
  {
    "abstract": "The major problem faced by the users of web search is the quality and quantity of the results return by any search engine. Personalisation is a popular remedy for this problem. There are many factors that affect personalisation and many systems have been proposed to address a specific issue of personalisation in web search. No single system exists in literature that incorporates the entire spectrum of factors that affect personalisation of web search. This paper aims towards providing an architecture that integrates the various personalisation factors for effective search through the world wide web.",
    "actual_venue": "Ijcat"
  },
  {
    "abstract": "A large spectrum of scientific applications, some generating data volumes exceeding petabytes, are currently being ported on clouds to build on their inherent elasticity and scalability. One of the critical needs in order to deal with this \"data deluge\" is an efficient, scalable and reliable storage. However, the storage services proposed by cloud providers suffer from high latencies, trading performance for availability. One alternative is to federate the local virtual disks on the compute nodes into a globally shared storage used for large intermediate or checkpoint data. This collocated storage supports a high throughput but it can be very intrusive and subject to failures that can stop the host node and degrade the application performance. To deal with these limitations we propose DataSteward, a data management system that provides a higher degree of reliability while remaining non-intrusive through the use of dedicated compute nodes. DataSteward harnesses the storage space of a set of dedicated VMs, selected using a topology-aware clustering algorithm, and has a lifetime dependent on the deployment lifetime. To capitalize on this separation, we introduce a set of scientific data processing services on top of the storage layer, that can overlap with the executing applications. We performed extensive experimentations on hundreds of cores in the Azure cloud: compared to state-of-the-art node selection algorithms, we show up to a 20% higher throughput, which improves the overall performance of a real life scientific application up to 45%.",
    "actual_venue": "Trustcom/Ispa/Iucc"
  },
  {
    "abstract": "The development of an automated algorithm for the categorization of normal and cancerous colon mucosa is reported. Six features based on texture analysis were studied. They were derived using the co-occurrence matrix and were angular second moment, entropy, contrast, inverse difference moment, dissimilarity, and correlation. Optical density was also studied. Forty-four normal images and 58 cancerous images from sections of the colon were analyzed. These two groups were split equally into two subgroups: one set was used for supervised training and the other to test the classification algorithm. A stepwise selection procedure showed that correlation and entropy were the features that discriminated most strongly between normal and cancerous tissue (P",
    "actual_venue": "Ieee Transactions On Information Technology In Biomedicine"
  },
  {
    "abstract": "Hybrid Petri nets with general transitions (HPnGs) provide a formalism for modeling safety-critical systems and evaluating their dependability with means of model checking. HPnGs form a restricted subclass of Stochastic Hybrid Automata and allow discrete, continuous and stochastic variables. Previously, discrete-event simulation and Statistical Model Checking (SMC) have been used to overcome the restrictions of existing analytical approaches, e.g., to a limited number of random variables. Also when simulating, the evolution of continuous variables has been restricted to piecewise-linear trajectories, where derivatives do not change between two events. Here, we extend the modeling formalism, the simulation and SMC approach to variables with a non-linear continuous evolution. The core idea of this extension lies in transforming the input system into a so-called second-order quantized state system. A case study on the Kinetic Battery Model validates our approach by comparing results to those obtained by Matlab.",
    "actual_venue": "Ieee International Symposium On Modeling, Analysis, And Simulation Of Computer And Telecommunication Systems"
  },
  {
    "abstract": "In this paper, a Bayesian restoration technique for multiple observations of hyperspectral (HS) images is presented. As a prototype problem, we assume that a low-spatial-resolution HS observation and a high-spatial-resolution multispectral (MS) observation of the same scene are available. The proposed approach applies a restoration on the HS image and a joint fusion with the MS image, accounting for the joint statistics with the MS image. The restoration is based on an expectation-maximization algorithm, which applies a deblurring step and a denoising step iteratively. The Bayesian framework allows to include spatial information from the MS image. To keep the calculation feasible, a practical implementation scheme is presented. The proposed approach is validated by simulation experiments for general HS image restoration and for the specific case of pansharpening. The experimental results of the proposed approach are compared with pure fusion and deconvolution results for performance evaluation.",
    "actual_venue": "Ieee Trans Geoscience And Remote Sensing"
  },
  {
    "abstract": "Estimation of missing digital information is mostly addressed by 1- or 2-D signal processing methods; however, this problem can emerge in multi-dimensional data including 3-D images. Examples of 3-D images dealing with missing edge information are often found using dental micro-CT, where the natural contours of dental enamel and dentine are partially dissolved or lost by caries. In this paper, we ...",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "In the large-scale parallel computing environment, resource allocation and energy efficient techniques are required to deliver the quality of services (QoS) and to reduce the operational cost of the system. Because the cost of the energy consumption in the environment is a dominant part of the owner's and user's budget. However, when considering energy efficiency, resource allocation strategies become more difficult, and QoS (i.e., queue time and response time) may violate. This paper therefore is a comparative study on job scheduling in large-scale parallel systems to: (a) minimize the queue time, response time, and energy consumption and (b) maximize the overall system utilization. We compare thirteen job scheduling policies to analyze their behavior. A set of job scheduling policies includes (a) priority-based, (b) first fit, (c) backfilling, and (d) window-based policies. All of the policies are extensively simulated and compared. For the simulation, a real data center workload comprised of 22385 jobs is used. Based on results of their performance, we incorporate energy efficiency in three policies i.e., (1) best result producer, (2) average result producer, and (3) worst result producer. We analyze the (a) queue time, (b) response time, (c) slowdown ratio, and (d) energy consumption to evaluate the policies. Moreover, we present a comprehensive workload characterization for optimizing system's performance and for scheduler design. Major workload characteristics including (a) Narrow, (b) Wide, (c) Short, and (d) Long jobs are characterized for detailed analysis of the schedulers' performance. This study highlights the strengths and weakness of various job scheduling polices and helps to choose an appropriate job scheduling policy in a given scenario.",
    "actual_venue": "Cluster Computing"
  },
  {
    "abstract": "In this paper we present a method of visual salient region detection based on depth maps estimated from 2D images. Depth estimation aims at better understanding spatial scene layout and relationship between objects. From depth maps we extract geometry related features that are further fused with color contrast. We solve saliency detection problem in segment-wise domain that allows prediction of objects rather than separate pixels. Modelling of saliency is done using conditional random field that allows for pairwise dependencies of segments. Parameters tuning is done by learning from ground-truth data. The evaluation has shown feasibility and good performance of the proposed method.",
    "actual_venue": "Multimedia Signal Processing"
  },
  {
    "abstract": "Large regions of many images are filled with visual texture, in which a viewer is not concerned with the exact pixel values. In image coding, it is advantageous to describe such regions in terms of their boundaries and textural properties. A textural desription can be much more compact than a precise description of pixel values. For a coding system to work, it is necessary to have an automated method for generating compact texture descriptions; the synthesized textures must appear satisfying to the human viewer. We have adapted the Heeger and Bergen algorithm(11) to the coding problem. The algorithm decomposes an image into subbands with a steerable pyramid, and characterizes the texture in terms of the subband histograms and the pixel histogam. Since the subband histograms all have a similar form, we can describe each one with a low-order parametric model. The resulting textural descriptor is quite compact. We show examples with both still images and video sequences.",
    "actual_venue": "Human Vision And Electronic Imaging"
  },
  {
    "abstract": "This paper describes a new system for semi-automatically segmenting the background, subcutaneous fat, interstitial fat, muscle, bone, and bone marrow from magnetic resonance images (MRI's) of volunteers for a new osteoarthritis study. Our system first creates separate right and left thigh images from a single MR image containing both legs. The subcutaneous fat boundary is very difficult to detect in these images and is therefore interactively defined with a single boundary. The volume within the boundary is then automatically processed with a series of clustering and morphological operations designed to identify and classify the different tissue types required for this study. Once the tissues have been identified, the volume of each tissue is determined. and a single, false colored, segmented image results. We quantitatively compare the segmentation in three different ways. In our first method we simply compare the tissue volumes of the resulting segmentations performed independently on both the left and right thigh. A second quantification method compares our results temporally with three image sets of the same volunteer made one month apart including a month of leg disuse. Our final quantification methodology compares the volumes of different tissues detected with our system to the results of a manual segmentation performed by a trained expert. The segmented image results of four different volunteers using images acquired at three different times suggests that the system described in this paper provides more consistent results than the manually segmented set. Furthermore, measurements of the left and right thigh and temporal results for both segmentation methods follow the anticipated trend of increasing fat and decreasing muscle over the period of disuse.",
    "actual_venue": "Proceedings Of The Society Of Photo-Optical Instrumentation Engineers"
  },
  {
    "abstract": "We present a general algorithm of image based regression that is applicable to many vision problems. The proposed regressor that targets a multiple-output setting is learned using boosting method. We formulate a multiple-output regression problem in such a way that overfitting is decreased and an analytic solution is admitted. Because we represent the image via a set of highly redundant Haar-like features that can be evaluated very quickly and select relevant features through boosting to absorb the knowledge of the training data, during testing we require no storage of the training data and evaluate the regression function almost in no time. We also propose an efficient training algorithm that breaks the computational bottleneck in the greedy feature selection process. We validate the efficiency of the proposed regressor using three challenging tasks of age estimation, tumor detection, and endocardial wall localization and achieve the best performance with a dramatic speed, e.g., more than 1000 times faster than conventional data-driven techniques such as support vector regressor in the experiment of endo-cardial wall localization.",
    "actual_venue": "Computer Vision, Iccv Tenth Ieee International Conference"
  },
  {
    "abstract": "Network-on-Chip (NoC) is a promising solution for efficient interconnection between processor cores in Chip-Multi-Processor (CMP). This paper is focusing on the energy-efficient design of buffers, a group of the most important components in NoC. From our investigation, an overwhelming majority of \"zero\" is contained in the packets transmitting in NoC for CMP. A zero-efficient buffer design is proposed as well as the error control scheme. Compared with conventional design, up to 43% energy consumption can be saved. We use a 90nm CMOS process in our simulation.",
    "actual_venue": "Proceedings Of The Conference On Design, Automation And Test In Europe"
  },
  {
    "abstract": "People naturally express themselves through facial gestures and expressions. Our goal is to build a facial gesture human-computer interface for use in robot applications. We have implemented an interface that tracks a person's facial features in real time (30Hz). Our system does not require special illumination nor facial makeup. By using multiple Kalman filters we accurately predict and robustly track facial features. This is despite disturbances and rapid movements of the head (including both translational and rotational motion). Since we reliably track the face in real-time we are also able to recognize motion gestures of the face. Our system can recognize a large set of gestures (13) ranging from \"yes\", \"no\" and \"may be\" to detecting winks, blinks and sleeping.",
    "actual_venue": "FG"
  },
  {
    "abstract": "Purpose - This paper aims to report on a web usability study and to identify and prioritise key web interface usability factors (WIUFs) for web sites of 36 student-related online services categorised into three groups: personal services, purchase services and study-related web sites. Design/methodology/approach - In this study, involving 400 student internet users (SIUs), 12,310 data points were collected and analysed using a multiple linear regression test. Seven WIUFs were tested: use of colour and font (UCF), use of graphics and multimedia (UGM), clarity of goals in web site (CGW), trustworthiness of web site (TOW), interactivity of web site (IOW), ease of web navigation (EWN), and download speed of web site (DSOW). Findings - The study results reveal that every online service category has a different set of crucial WIUFs. SIUs' web usability preferences were compared with those of general internet users. Research limitations/implications - The participants were all Malaysians; therefore, generalising the findings to all SIUs will require a confirmatory study with SIUs from other parts of the world. Practical implications - Web developers can use the results to design usable web sites for specific online service categories. Originality/value - The research offers a simpler alternative to measure web usability and to determine which WIUFs are crucial for a specific online service category with consideration of the users' role. This study overcomes some weaknesses of previous studies, i.e. small sample size, no consideration of product-task relationship, no specific customer group and cumbersome procedures.",
    "actual_venue": "Online Information Review"
  },
  {
    "abstract": "The following paper deals with criterion used to measure communication protocol efficiency in Wireless Sensor Networks. As energy is a crucial characteristic of those networks, it is necessary to pay attention both to the energy consumption and to the distribution of energy consumption, when using communication protocols, so as to increase the lifetime of the whole network. Our aim is to present and discuss criterion designed to analyze communication protocol effectiveness. When designing, for example, communication protocols, it is really important to measure performances with a suitable metric according to our application, else it would be difficult to analyze and to improve a protocol. In this paper, we draw a list of existing criterion, and then introduce two new ones : Average Node Percentage and Monitored Interest Point Percentage. We also point out the relevance regarding the required application for each criterion.",
    "actual_venue": "Cisis: International Conference On Complex, Intelligent And Software Intensive Systems, Vols And"
  },
  {
    "abstract": "In our previous work, the k-value pseudo random sequence generated by power residue symbol has been researched. The sequence is generated by applying a primitive polynomial over odd characteristics field, trace function, and power residue symbol. And the sequence has some interesting features such as period, periodic autocorrelation, and linear complexity. In this paper, by applying a new step into the process of generating sequence, a new generating method of k-value sequence is derived. As with the previous work, it also has some interesting features such as period, periodic autocorrelation, and periodic crosscorrelation. From the experimental results, these features are examined.",
    "actual_venue": "Candar"
  },
  {
    "abstract": "This paper proposes a new approach for slow and matched disturbance suppression in digital sliding mode. The previous value of disturbance is extracted from switching function and used to make disturbance estimation, and later used in control to cancel disturbance effects. The control function is linear in state and in disturbance estimate. It may be considered chattering free since its value is zero in equilibrium if there are no disturbances.",
    "actual_venue": "Eurocon - International Conference Computer As A Tool"
  },
  {
    "abstract": "In a mam memory database (WB), the pri- mary copy of the database may be stored in a volatile memory When a crash occurs, a reload of the database from archive memory to main memory must be performed. It is essential that an efficient reload scheme be used to ensure that the expectations of high performance database systems are met. This implies that the overall performance measures of any potential reload algorithm should not be measured simply by reload time, but by its impact on overall system performance. This paper presents four different reload algorithms that aim at fast response time of transactions and high throughput of the overall system, Simulation studies comparing the algorithms indicate that the best overall approach is one based on frequency of access.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "In this paper, we develop an output feedback adaptive control framework for continuous-time minimum phase multivariable dynamical systems for output stabilization and command following. The approach is based on a nonminimal state space realization that generates an expanded set of states using the filtered inputs and filtered outputs and their derivatives of the original system. Specifically, a direct adaptive controller for the nonminimal state space model is constructed using the expanded states of the nonminimal realization and is shown to be effective for multi-input, multi-output linear dynamical systems with unmatched system uncertainties and unstable dynamics. Two illustrative numerical examples are provided to demonstrate the efficacy of the proposed approach.",
    "actual_venue": "American Control Conference"
  },
  {
    "abstract": "Tenant selection is an important strategic and operational problem for large property investment and management companies. This paper presents an evolutionary prototyping project developed in conjunction with a long term programme of establishing a property management information system for a known company. The project aims at developing a tenant selection decision support system (TSDSS) which employs a multi-criteria analysis technique as its evaluation tool. TSDSS also introduces a new method for determining the alternatives to be considered in the selection process, so that the values of some criteria, which represent the characteristics of the combinations of the chosen tenants, can be obtained. The alternative generation method has actually improved the practicality of using Multi-Criteria Analysis (MCA) techniques in some real life situations. However, from this development, some basic problems of using this technique have been revealed.",
    "actual_venue": "Decision Support Systems"
  },
  {
    "abstract": "QR-decomposition accelerators are attractive SoC components for many applications with a wide range of specifications. A new family of highly area- and energy-efficient, modular two-way linear-array QRD architectures based on the Givens algorithm and CORDIC rotations is proposed. The template architecture allows for implementations of real-/complex-valued and integer/floating-point QRDs. An accurate algebraic cost model enables cross-level optimization over architecture, micro-architecture and circuit level using a rich set of parameters. Quantitative results for exemplary applications are presented for implementations in 40-nm CMOS, proving the significant improvement of efficiency.",
    "actual_venue": "System Chip"
  },
  {
    "abstract": "Various writing assistance tools have been developed through efforts in the areas of natural language processing with different degrees of success of curriculum integration depending on their functional rigor and pedagogical designs. In this paper, we developed a system, WriteAhead, that provides six types of suggestions when non-native graduate students of English from different disciplines are composing journal abstracts, and assessed its effectiveness. The method involved automatically building domain-specific corpora of abstracts from the Web via domain names and related keywords as query expansions, and automatically extracting vocabulary and n-grams from the corpora in order to offer writing suggestions. At runtime, learners' input in the writing area of the system actively triggered a set of corresponding writing suggestions. This abstract writing assistant system facilitates interactions between learners and the system for writing abstracts in an effective and contextualized way, by providing suggestions such as collocations or transitional words. For assessment of WriteAhead, we compared the writing performance of two groups of students with or without using the system, and adopted student perception data. Findings show that the experiment group wrote better, and most students were satisfied with the system concerning most suggestion types, as they can effectively compose quality abstracts through provided language supports from WriteAhead.",
    "actual_venue": "J Comp Assisted Learning"
  },
  {
    "abstract": "In this paper, a run-time technique based on inspector-executor scheme is proposed to find available parallelism on loops in this paper. Our inspector can determine the wavefronts by building a DEF-USE table. Additionally, the process of inspector for finding the wavefronts, can be parallelized fully without any synchronization. Our executor can perform the loop iterations concurrently. For each wavefront in a loop, the auto-adapted function is used to get a tailored thread number rather than using fixed thread number for execution. Experimental results show that our new parallel inspector can handle complex data dependency patterns and reduce itself execution time obviously. Besides, the new partitioning strategy for executor can also improve the performance of run-time parallelization obviously",
    "actual_venue": "Icpads"
  }
]