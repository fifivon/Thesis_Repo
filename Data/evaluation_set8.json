[
  {
    "abstract": "A variable gain amplifier (VGA) is designed for a GSM subsampling receiver. The VGA is implemented in a 0.35-µm CMOS process and approximately occupies 0.64 mm2. It operates at an IF frequency of 246 MHz. The VGA provides a 60-dB digitally controlled gain range in 2-dB steps. The overall gain accuracy is less than 0.3 dB. The current is 9 mA at 3 V supply. The noise figure at maximum gain is 8.7 dB. The IIP3 is --4 dBm at minimum gain, while the OIP3 is -- 1 dBm at maximum gain. The group delay is 1.5 ns across 5-MHz bandwidth.",
    "actual_venue": "Ieee Trans Vlsi Syst"
  },
  {
    "abstract": "We study the relation of autoreducibility and mitoticity for polylog-space many-one reductions and log-space many-one reductions. For polylog-space these notions coincide, while proving the same for log-space is out of reach. More precisely, we show the following results with respect to nontrivial sets and many-one reductions. 1 polylog-space autoreducible ${\\,\\mathop{\\Leftrightarrow}\\,}$ polylog-space mitotic 1 log-space mitotic log-space autoreducible (logn ·loglogn)-space mitotic 1 relative to an oracle, log-space autoreducible log-space mitotic The oracle is an infinite family of graphs whose construction combines arguments from Ramsey theory and Kolmogorov complexity.",
    "actual_venue": "Isaac"
  },
  {
    "abstract": "We address a problem of sampling and reconstructing periodic piecewise polynomials based on the theory for signals with a finite rate of innovation (FRI signals) from samples acquired by a sine kernel. This problem was discussed in a previous paper. There was, however, an error in a condition about the sine kernel. Further, even though the signal is represented by parameters, these explicit values are not obtained. Hence, in this paper, we provide a correct condition for the sine kernel and show the procedure. The point is that, though a periodic piecewise polynomial of degree R is defined as a signal mapped to a periodic stream of differentiated Diracs by R + 1 time differentiation, the mapping is not one-to-one. Therefore, to recover the stream is not sufficient to reconstruct the original signal. To solve this problem, we use the average of the target signal, which is available because of the sine sampling. Simulation results show the correctness of our reconstruction procedure. We also show a sampling theorem for FRI signals with derivatives of a generic known function.",
    "actual_venue": "Ieice Transactions On Fundamentals Of Electronics Communications And Computer Sciences"
  },
  {
    "abstract": "We focus on the problem of specialization in a Description Logics (DL) representation, specifically the ALN language. Standard approaches to learning in these representations are based on bottom-up algorithms that employ the lcs operator, which, in turn, produces overly specific (overfitting) and still redundant concept definitions. In the dual (top-down) perspective, this issue can be tackled by means of an ILP downward operator. Indeed, using a mapping from DL descriptions onto a clausal representation, we define a specialization operator computing maximal specializations of a concept description on the grounds of the available positive and negative examples.",
    "actual_venue": "HIS"
  },
  {
    "abstract": "A virtual enterprise (VE) is an organization intended to cope with the rapidly changing manufacturing environment. Organization building is important in virtual domains because it has largely been affecting the success of VEs. However, the process of forming a VE is based on self-determination by the participants. This paper adopts a bargaining model under a scenario of incomplete information to formalize the formation process, considers the characteristics of the VE formation process, presents the pricing strategies for the corresponding bargaining, and verifies the correctness and validity of the pricing strategies using computer simulation. This paper breaks through the relative research that compares the formation process with partner selection from the core enterprise’s perspective and also provides the basis for the intelligent information platform of VE, whose key part is pricing software.",
    "actual_venue": "Expert Systems With Applications"
  },
  {
    "abstract": "This paper studies the performance of single-layered neural networks. This study begins with the performance of single-layered neural networks trained using the outer-product rule. The outer-product rule is a suboptimal learning scheme, resulting under certain assumptions from optimal least-squares training of single-layered neural networks with respect to their analog output. Extensive analysis reveals the improvement on the network performance caused by its optimal least-squares training. The effect of the training scheme on the performance of single-layered neural networks with binary output is exhibited by experimentally comparing the performance of single-layered neural networks trained with respect to their analog and binary output.",
    "actual_venue": "Biological Cybernetics"
  },
  {
    "abstract": "This paper presents an exact minimization algorithm for fixed polarity Reed-Muller expressions (FPRMs) for incompletely specified functions. For an n-variable function with /spl alpha/ unspecified minterms there are 2/sup n+/spl alpha// distinct FPRMs. A minimum FPRM is one with the fewest products. The minimization algorithm is based on the multi-terminal binary decision diagrams. Experimental results for a set of functions are shown. The algorithm can be extended to obtain exact minimum Kronecker expressions for incompletely specified functions.",
    "actual_venue": "Design Automation Conference"
  },
  {
    "abstract": "Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput ...",
    "actual_venue": "Ieee Transactions On Very Large Scale Integration Systems"
  },
  {
    "abstract": "Skin detection is frequently used as the first step for the tasks of face and gesture recognition in perceptual interfaces for human-computer interaction and communication. Thus, it is important for the researchers using skin detection to choose the optimal method for their specific task. In this paper, we propose a novel method of measuring the performance of skin detection for a task. We have created an evaluation framework for the task of hand detection and executed this assessment using a large dataset containing 17 million pixels from 225 images taken under various conditions. The parameter set of the skin detection has been trained extensively. Five colorspace transformations with and without the illuminance component coupled with two color modeling approaches have been evaluated. The results indicate that the best performance is achieved by transforming to SCT colorspace, using the illuminance component, and modeling the distribution with the histogram approach. Some conclusions such as the SCT colorspace being one of the best colorspaces are consistent with our previous work, while findings such as the YUV colorspace performing well in this work when it was one of the worst in our previous work are different. This indicates that the performance measured at the pixel-level might not be the ultimate indicator for the performance at the task-level of hand detection. We believe that the users of skin detection will find our task-based results to be more relevant than the traditional pixel-level results. However, we acknowledge that an evaluation is limited by its specific dataset and evaluation protocols.",
    "actual_venue": "J Visual Communication And Image Representation"
  },
  {
    "abstract": "A secure roaming protocol for mobile networks is proposed. Roaming has been analysed in some schemes from the security point of view; however, there are vulnerabilities in most of them and so the claimed security level is not achieved. The scheme offered by Wan et al. recently is based on hierarchical identity-based encryption, in which the roaming user and the foreign network mutually authenticate each other without the help of the home network. Although the idea behind this proposal is interesting, it contradicts technical considerations such as routing and billing. The proposed protocol makes use of similar functions used in Wan et al.'s scheme but contributes a distinguished structure that overcomes the previous shortcomings and achieves a higher possible level of security in mobile roaming as well as enhancing the security of the key issuing procedure.",
    "actual_venue": "Information Security, Iet"
  },
  {
    "abstract": "Contemporary society is seriously threatened with food as part of the world due to the continuous increase in world population, the degradation and decline of agricultural lands due to high industrialization, climate change and the aging of the population. Therefore, modern society is studying different solutions to solve human food. In this paper, a framework for precision agriculture using IoT Gateway is proposed for solving human food, and the productivity of crops must be increased first. IoT solution through architecture, platforms and IoT standards, or the use of interoperable IoT technologies beyond the adopters in particular, simplifying existing proposals. Connecting different sensors, connected devices, developing intelligent breeding systems as much as possible. One of our aims is to manage and challenges. We provide a techniques and technologies applications during our work. The result shows that the advantages of various types of sensors for agriculture services in their decision making. And a proposed architecture for Agriculture Mobile services based on Sensor Cloud substructure that helps farms and IoT applications are effective in intelligent farming system.",
    "actual_venue": "Csii"
  },
  {
    "abstract": "Moving object classification in far-field video is a key component of smart surveillance systems. In this paper, we propose a reliable system for person-vehicle classification which works well in challenging real-word conditions, including the presence of shadows, low resolution imagery, perspective distortions, arbitrary camera viewpoints, and groups of people. Our system runsin real-time (30 Hz) on conventional machines and has low memory consumption. We achieved accurate results by relying on powerful discriminative features, including a novel measure of object deformation based on differences of histograms of oriented gradients. We also provide an interactive user interface, enabling users to specify regions of interest for each class and correct for perspective distortions by specifying different sizes indifferent positions of the camera view. Finally, we use anautomatic adaptation process to continuously update the parameters of the system so that its performance increases for a particular environment. Experimental results demonstrate the effectiveness of our system in standard dataset and a variety of video clips captured with our surveillance cameras.",
    "actual_venue": "Avss"
  },
  {
    "abstract": "To recognize the objects in an image and to understand the image content, a computer system has to first separate the foreground objects from the background. Image segmentation and figure-ground segregation are, therefore, essential for computer image understanding. This paper describes a system called OLAG (Object-LAyer Grouping) for image segmentation and figure-ground segregation. OLAG consists of several incremental refinement steps which use colour and other visual cues such as size and compactness for grouping the image pixels. It produces, as an end result, a set of layers each containing an object or object part. Figure and ground relationships among the objects are inferred, giving their relative depths. It is shown that interesting and useful segmentation results can be obtained from the system.",
    "actual_venue": "Proceedings International Conference On Image Processing"
  },
  {
    "abstract": "A serial-parallel hybrid (SPH) fuel cell system which include control and protection loops are presented in this article. The hybrid system is formed by a Fuel Cell (FC), an Auxiliary Storage Device (ASD), and the current-controlled dc-dc converters responsible for the management of the energy between FC and ASD in a serial-parallel topology. The main advantages of the selected converter are its voltage step up and step down properties, high efficiency, and low input and output current ripples. This allows it to be positioned in the different FC hybrid system localizations with a suitable design of its control. Moreover, having the same module for all the system converters simplifies the design and construction tasks. The theoretical analyses have been simulated on a 48 V, 1200 W dc bus.",
    "actual_venue": "SSD"
  },
  {
    "abstract": "In this work, synthesis of facial animation is done by modelling the mapping between facial motion and speech using the shared Gaussian process latent variable model. Both data are processed separately and subsequently coupled together to yield a shared latent space. This method allows coarticulation to be modelled by having a dynamical model on the latent space. Synthesis of novel animation is done by first obtaining intermediate latent points from the audio data and then using a Gaussian Process mapping to predict the corresponding visual data. Statistical evaluation of generated visual features against ground truth data compares favourably with known methods of speech animation. The generated videos are found to show proper synchronisation with audio and exhibit correct facial dynamics.",
    "actual_venue": "Isvc"
  },
  {
    "abstract": "This paper takes as its premise that the web is a place of action, not just information, and that the purpose of global data is to serve human needs. The paper presents several component technologies, which together work towards a vision where many small micro-applications can be threaded together using automated assistance to enable a unified and rich interaction. These technologies include data detector technology to enable any text to become a start point of semantic interaction; annotations for web-based services so that they can link data to potential actions; spreading activation over personal ontologies, to allow modelling of context; algorithms for automatically inferring 'typing' of web-form input data based on previous user inputs; and early work on inferring task structures from action traces. Some of these have already been integrated within an experimental web-based (extended) bookmarking tool, Snip!t, and a prototype desktop application On Time, and the paper discusses how the components could be more fully, yet more openly, linked in terms of both architecture and interaction. As well as contributing to the goal of an action and activity-focused web, the work also exposes a number of broader issues, theoretical, practical, social and economic, for the Semantic Web.",
    "actual_venue": "J Web Sem"
  },
  {
    "abstract": "In our previous work we elaborated a multifrequency test generation method (TPG) for detecting parametric and catastrophic faults in linear analog circuits. The method was formulated as an optimization problem which was solved by Sequential Quadratic Programming (SQP), a non-linear programming method available in MATLAB. Such standard optimization methods are based on and process local information and consequently cannot guarantee a global optimum. In this paper we propose a method based on Constraint Logic Programming (CLP) that solves the optimization problem in TPG as a series of Constraint Satisfaction Problems (CSPs). Our TPG method is fully automatic and provides right and guaranteed bounds on the global optima of a nonlinear function. The TPG method was implemented in CLP(BNR) Prolog. First, we illustrate the effectiveness of our approach on a number of nonlinear functions known to be difficult, and then we apply it to a realistic electronic circuit in the context of TPG. The two methods produce the same results except for one case where SQP falls into a local minimum. This could lead to a wrong test selection. Moreover, while the TPG took over a week of work using SQP, it was solved in a matter of minutes using CLP",
    "actual_venue": "VTS"
  },
  {
    "abstract": "Wireless communication penetrates more and more areas of our everyday lives. Turbo-Codes provide good forward-error correction to improve the data transfer reliability. They are used in current standards and future system designers considers them promising candidates. Dedicated hardware, however, is too expensive to use in a new and still rapidly changing system; due to the non-recurring engineering and mask costs.In this paper we therefore present a scalable Turbo-Decoder architecture targeted towards FPGA implementation for low-volume devices. It allows to optimally exploit the given hardware resources on FPGA to match the desired system throughput. Our design is ported to the Xilinx Virtex-II family. On the Virtex-II 3000, we achieve a maximum throughput of 26Mbit/s at 84MHz with a latency of 185μs.",
    "actual_venue": "Sbcci"
  },
  {
    "abstract": "Let k be a field of characteristic zero. Certain classes of fixed point free actions of the additive group of k on affine n-space over Ic are known to be conjugate to global translations (i.e. to admit equivariant slices). These classes include actions on complex three space for which the invariant ring contains a variable, and certain generalizations of such actions to affine space of any dimension. Methods to construct an equivariant slice for these classes are presented.",
    "actual_venue": "International Journal Of Algebra And Computation"
  },
  {
    "abstract": "Recent decades have seen BCI applications as a novel and promising new channel of communication, control and entertainment for disabled and healthy people. However, BCI technology can be prone to errors due to the basic emotional state of the user: the performance of reactive and active BCIs decrease when user becomes stressed or bored, for example. Passive-BCI is a recent approach that fuses BCI technology with cognitive monitoring, providing valuable information about the user's intentions, the situational interpretations and mainly the emotional state. In this work, an architecture composed by passive-BCI co-working with SSVEP-BCI is proposed, with the aim of improving the performance of the reactive-BCI. The possibility of adjusting recognition characteristics of SSVEP-BCIs using a passive-BCI output is evaluated. In this sense, two ways to recover the accuracy of SSVEP are presented in this paper: 1) Adjusting of Amplitude of the SSVEP and 2) Adjusting of Frequency of the SSVEP response. The results are promising, because accuracy of SSVEP-BCI can be recovered in the case that it was reduced by the BCI user's emotional state.",
    "actual_venue": "Embc"
  },
  {
    "abstract": "In this paper, we present an analytical model to determine the network connectivity probability of one dimensional linear vehicular ad hoc network VANET in the presence of Nakagami fading. In particular, we focus on the probability of being able to convey messages from a source vehicle to a destination vehicle, which may be multiple hops away. This analysis takes into account the variability of the channel and how it affects the network connectivity of a linear VANET. In our model, the communication range of each vehicle is modeled as a random variable due to channel fading. The analytical results are used to study the effect of parameters like path loss exponent and vehicle density on the network connectivity probability. This facility is particularly useful for distributing traffic information related to road safety, weather, and navigation without the need for expensive infrastructure.",
    "actual_venue": "Icdcn"
  },
  {
    "abstract": "Cosegmentation methods segment multiple related images jointly, exploiting their shared appearance to generate more robust foreground models. While existing approaches assume that an oracle will specify which pairs of images are amenable to cosegmentation, in many scenarios such external information may be difficult to obtain. This is problematic, since coupling the \"wrong\" images for segmentation-even images of the same object class-can actually deteriorate performance relative to single-image segmentation. Rather than manually specify partner images for cosegmentation, we propose to automatically predict which images will cosegment well together. We develop a learning-to-rank approach that identifies good partners, based on paired descriptors capturing the images' amenability to joint segmentation. We compare our approach to alternative methods for partnering images, including basic image similarity, and show the advantages on two challenging datasets.",
    "actual_venue": "Computer Vision - Eccv , Pt"
  },
  {
    "abstract": "Of all research problems associated with the security of",
    "actual_venue": "Cikm Proceedings Of The Second International Conference On Information And Knowledge Management"
  },
  {
    "abstract": "Energy plays a fundamental role in an economy. Turkey has the world's 15th largest GDP-Purchasing power parity and 17th largest Nominal GDP. Economists and political scientists classify Turkey as a newly industrialized country. In this study, an alternative model for Turkey’s energy consumption is proposed for the time between 1980 and 2004. Artificial neural network based model (ANN) is preferred as a forecasting tool. Gross domestic product (GDP), which is based on purchasing power parity, industrial production index and total population are utilized in the model. It is found that the energy consumption has direct relations with the Industrial Production Index. Moreover, population and GDP has causality effects.",
    "actual_venue": "Icmla"
  },
  {
    "abstract": "Distributed virtual environments such as massive multi-player games require multiple servers to balance computational load. This paper investigates the architecture of a unified environment where the virtual online world is not partitioned according to rigid boundaries, but according to an adaptive paradigm. Since it is difficult to develop an optimal load balancing algorithm for a unified environment, we propose an optimistic scheme that quickly converges. The cost of frequent migrations is reduced by following a push/push data exchange model. We analyze the computational time costs of such a system and give simulation results to gauge its performance. The simulation results confirm that our load balancing scheme is efficient and can support large numbers of clients.",
    "actual_venue": "Nossdav"
  },
  {
    "abstract": "Space-filling curves, particularly, Hilbert curves, have been extensively used to maintain spatial locality of multi-dimensional data in a wide variety of applications. A window query is an important query operation in spatial (image) databases. Given a Hilbert curve, a window query reports its corresponding orders without the need to decode all the points inside this window into the corresponding...",
    "actual_venue": "Iet Image Processing"
  },
  {
    "abstract": "Various kinds of faults must be tested in CMOS circuits. Testing of an excessive delay as well as testing of a stuck-open fault requires a sequence of two successive input vectors. It has been observed by many authors that there are some similarities between delay testing and stuck-open testing. In this paper it is shown that if all the delay faults have been robustly tested in a combinational circuit, then all the stuck-open faults have been robustly tested too.",
    "actual_venue": "Euro-Dac"
  },
  {
    "abstract": "The traditional measurement method of pilot's mental workload is subjective measurement, which is difficult to implement real-time measurement. Much work has done on the eye activity measurement for mental workload, but most of the work just found the relationship between eye movement indexes and mental workload, which did not show how to evaluate mental workload by eye activity measurement. The goal of the research is to find the application method of eye activity measurement on pilot's mental workload. The aim of this article is to search for the eye movement indices, which can reflect the mental workload in basic flight task. The button operation experiment is achieved by C programming software. There are nine experiment trials of different interval time. Subjects were required to press the keyboard key corresponding with the letter appearing on the screen. The eye indices were recorded by eye tracking. Subjective survey was conducted on time pressure and mental workload. Operation results were recorded in experimental procedures. In our experiment, the analysis of experimental results showed that pupil size, average fixation time, fixation frequency, saccade frequency and average saccade velocity changed with the mental workload significantly. No certain conclusion was obtained concerning the relation between blink frequency and mental workload. The research results will provide experimental basic for the workload measurement of flight task by eye activity movement.",
    "actual_venue": "Indin"
  },
  {
    "abstract": "Our work is motivated by the problem of managing data on storage devices, typically a set of disks. Such storage servers are used as web servers or multimedia servers, for handling high demand for data. As the system is running, to exhibit good performance, it needs to respond dynamically to changes in demand for different data items. There are known algorithms for mapping demand to a layout. When the demand changes, a new layout can be computed. In this work we study thedata migration problem, which arises when we need to change one layout to another quickly. This problem has been studied earlier where for each disk a new layout has been prescribed. However, to apply these algorithms effectively, we identify another problem that we refer to as the correspondence problem, whose solution has a significant impact on the overall solution for the data migration problem. We study algorithms for the data migration problem in more detail and identify variations of the basic algorithm that seem to improve performance in practice, even though some of the variations have poor worst-case behavior.",
    "actual_venue": "Algorithmica"
  },
  {
    "abstract": "We study graph searching games where a number of cops try to capture a robber that is hiding in a system of tunnels modelled as a graph. While the current position of the robber is unknown to the cops, each cop can see a certain radius d around his position. For the case d = 1 these games have been studied by Fomin, Kratsch and Müller [7] under the name domination games.We are primarily interested in questions concerning the complexity and monotonicity of these games. We show that dominating games are computationally much harder than standard graph searching games where the cops only see their own vertex and establish strong non-monotonicity results for various notions of monotonicity which arise naturally in the context of domination games. Answering a question of [7], we show that there exists graphs for which the shortest winning strategy for a minimal number of cops must necessarily be of exponential length. On the positive side, we establish tractability results for graph classes of bounded degree.",
    "actual_venue": "WG"
  },
  {
    "abstract": "We prove a variation of Easton's lemma for strongly proper that, unlike the stronger principle IGMP, GMP together with 2(omega) <= omega(2) is consistent with the existence of an omega(1)-distributive nowhere c.c.c. forcing poset of size omega(1). We introduce model, and prove that many of the strong consequences of the principle of stationarily many weakly guessing models. Using Namba forcing, we are stationarily many indestructibly weakly guessing models which have covered by any countable set in the model.",
    "actual_venue": "Journal Of Symbolic Logic"
  },
  {
    "abstract": "Informative numerical representations of amino acid residues are essential for successful in silico modeling or establishing the structure-activity relationships of proteins. A straightforward approach is adopted here for representing more than 500 amino acid indices from the AAindex database by a set of uncorrelated scales, satisfying the VARIMAX criterion. Different measures are considered in order to demonstrate the improved interpretability of the current scales as compared to previously published ones. Performance is also addressed in a classification problem of G-protein coupled receptors, and is found to be similar or higher than the performance achieved by six other scale sets. Finally, a unique correspondence between numerical indices and mutation matrices is derived and discussed in light of the evolutionary conservation of amino acid properties. Conclusions from this study highlight the discord between ease of interpretation of amino acid scales and their relevance to protein structure conservation, as well as general considerations for designing custom scale sets.",
    "actual_venue": "Journal Of Computational Biology"
  },
  {
    "abstract": "A complete description of trust relationship is key to construct a high precision trust model. But most of existing models not only miss the negative information and the hesitation information of trust, but also ignore the discordance between text comments and ratings. To solve the problems, a dynamic trust evaluation model based on the affect intensity is proposed. In the model, the intensity of sentimental polarities are calculated from words in comments. The corresponding relation between evaluations of trust property and the emotional intensity vector can be also described. Time series weights to distinguish the importance of transaction at different time are determined by the inverse form of exponential distribution. So the dynamic attenuation of trust can be described. To improve the polymerization ability of trust information, the local trust, the feedback trust and the overall trust are calculated by operators of fuzzy logic. The experimental results show the proposed model can effectively describe the trust relationship between nodes to identify and eliminate various malicious attacks significantly. Copyright © 2016 John Wiley & Sons, Ltd.",
    "actual_venue": "Security And Communication Networks"
  },
  {
    "abstract": "Finding the smallest eigenvalue of a given square matrix A of order n is computationally very intensive problem. The most popular method for this problem is the Inverse Power Method which uses LU-decomposition and forward and backward solving of the factored system at every iteration step. An alternative to this method is the Resolvent Monte Carlo method which uses representation of the resolvent matrix [I – qA]$^{\\rm -{\\it m}}$ as a series and then performs Monte Carlo iterations (random walks) on the elements of the matrix. This leads to great savings in computations, but the method has many restrictions and a very slow convergence. In this paper we propose a method that includes fast Monte Carlo procedure for finding the inverse matrix, refinement procedure to improve approximation of the inverse if necessary, and Monte Carlo power iterations to compute the smallest eigenvalue. We provide not only theoretical estimations about accuracy and convergence but also results from numerical tests performed on a number of test matrices.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": "While most current planning methods have focused on the development of scalable approximate algorithms, they often neglect the important aspect of providing algorithmic performance guarantees, or their tightness is sacrificed to improve efficiency. In contrast, we address a challenging problem of solving POMDP planning problems approximately with a focus on solution quality to estimate the quality of such approximations and to decide when a satisfactory plan is available. 1) We demonstrate that the original task of optimizing POMDP controllers can be approached by its reformulation as the equivalent problem of marginal-MAP inference in a novel single-DBN generative model, which guarantees that the control policies computed by probabilistic inference over this model are optimal in the traditional sense. 2) We further solve a POMDP problem approximately with bounded performance guarantees by translating a corresponding marginal-MAP inference problem into its variational form, and developing two Bayesian variational inference algorithms to (i) approximate the marginal-MAP inference, and (ii) compute the upper bound of the solution. 3) The proposed approach to optimizing parameters of POMDP controllers by marginal-MAP inference with bounded performance guarantees is evaluated on several POMDP benchmark problems and the performance of the implemented variational algorithms is compared to previously developed methods.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "In this paper, we investigate the use of conversational agents to scaffold on-line collaborative learning discussions through an approach called academically productive talk. In contrast to past work, which has involved using agents to elevate the conceptual depth of collaborative discussion by leading students in groups through directed lines of reasoning, this approach lets students follow their own lines of reasoning and promotes productive practices such as explaining, stating agreement and disagreement, and reading and revoicing the statements of other students. We contrast two types of academically productive talk support for a discussion about 9th grade biology and show that one type in particular has a positive effect on the overall conversation, while the other is worse than no support. This positive effect carries over onto participation in a full-class discussion the following day. We use a sociolinguistic style analysis to investigate how the two types of support influence the discussion and draw conclusions for redesign. In particular, our findings have implications for how dynamic micro-scripting agents such as those scaffolding academically productive talk can be used in consort with more static macro- and micro- scripting.",
    "actual_venue": "ITS"
  },
  {
    "abstract": "The Modulated Wideband Converter (MWC) is one of the promising sub-Nyquist sampling architectures for sparse wideband signal sensing, cognitive radio applications and so on. In order to design an MWC-based RF receiver that meets a target RF specification, noise figure (NF) of the MWC has to be well-defined by its design properties. In this paper, we investigate a comprehensive explanation for NF of MWC by an analytic approach based on a proposed notation of an average noise figure (ANF) of the MWC. Consequently, the analysis is proven with simulation results in order to demonstrate its feasibility.",
    "actual_venue": "Ieee Transactions On Circuits And Systems -Regular Papers"
  },
  {
    "abstract": "In this paper, we analyze the best relay selection scheme for the soft-decision-and-forward (SDF) cooperative networks with multiple relays. The term 'best relay selection' implies that the relay having the largest end-to-end signal-to-noise ratio is selected to transmit in the second phase transmission. The approximate performances in terms of pairwise error probability (PEP) and bit error rate (BER) are analyzed and compared with the conventional multiple-relay transmission scheme where all the relays participate in the second phase transmission. Using the asymptotics of the Fox's H-function, the diversity orders of the best relay selection and conventional relay scheme for the SDF cooperative networks are derived. It is shown that both have the same full diversity order. The numerical results show that the best relay selection scheme outperforms the conventional one in terms of bit error rate.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Many objects found in domestic environments are reflectionally symmetric. In this paper, we present a system that can visually track moving objects by their reflectional symmetry in real time. Apart from the assumption of symmetry, the tracking system does not require any prior object models of the target, such as its colour and shape. The system is robust to shadows and specular reflections. It can also deal with transparent objects. Block motion detection is used in conjunction with symmetry for object tracking. A Kalman filter is used to estimate the object state. Predictions from the Kalman filter is used to improve the efficiency of the symmetry detector. The tracker provides a real time segmentation of an object by searching for motion that is symmetric about the object's mirror line. The tracking system also generates a rotated bounding box, aligned with the object's symmetry line, which can be used as a window for other image processing operations. The final system can track single objects in 640times480 videos at over 40 frames per second using a standard notebook PC",
    "actual_venue": "Iros"
  },
  {
    "abstract": "In personal navigation, pedestrian dead reckoning (PDR) systems based on low-cost self-contained sensors exploit the kinematics of human walking, and are well suited for indoor use and in urban canyons where GPS signals are degraded or not available. Considering the electromyography (EM G), which measures electrical potentials generated by muscle contractions from human body, would reflect the muscle activities during human locomotion, this kind of biomedical signal can be utilized to capture human walking characteristics in PDR. The work presented in this paper is the consecutive step of our pilot studies in further developing a novel and robust PDR solution using wearable EMG sensors to measure walking steps. Our PDR solution includes the EMG-based activity classification, step occurrence detection, and step length estimation, as well as the position calculation with the heading from a two-axis digital compass. To avoid step misdetection, two kinds of activities: walking normally and standing still, are classified via the hidden Markov model classifier fed by the sample entropy features extracted from the raw EMG data. Some EMG statistical parameters are also investigated to establish the optimized step length model. To validate the feasibility and effectiveness of this method, several field tests were conducted by a male tester in two experimental sessions, to demonstrate the effectiveness and practicability of the method using EMG to measure walking steps. Furthermore, the results indicate the performance of the PDR solution is comparable to that of the GPS under open-sky environments. © 2010 IEEE.",
    "actual_venue": "Upinlbs"
  },
  {
    "abstract": "A mode control strategy for a two-mode hybrid electric vehicle (HEV) with a fixed-gear mode is proposed, considering wheel torque demand while minimizing fuel consumption. First, the dynamic equations of a power-split HEV are derived, and these equations consist of two electrically variable transmission modes and four fixed-gear modes. Using the dynamic equations, the optimal operation mode is selected from the viewpoint of the maximum available output torque with respect to the driver's demand and vehicle velocity. Next, an optimized mode selection map is constructed using optimization to minimize fuel consumption while satisfying the required wheel torque. We note that fixed-gear mode uses the fixed-gear mode 1 to implement a higher gear ratio if the vehicle speed is low and if the required output torque is large, as in a conventional automatic transmission shift map, and selects the fixed-gear mode 4 to implement a lower gear ratio if the vehicle speed is high and if the required output torque is low. A vehicle simulation was performed to verify the mode control algorithm of the two-mode HEV, and the results were compared with vehicle experimental results. The simulation results showed that the mode control strategy for the two-mode HEV gave satisfactory performance, similar to the experiment results.",
    "actual_venue": "Ieee T Vehicular Technology"
  },
  {
    "abstract": "The paper provides a short introduction to the European perspectives on the role of teaching, education and training in context of achieving a sustainable innovation eco-system as e.g. the European Technology Platform ARTEMIS is aiming at. Studies and examples from different European projects are discussed, including the ARTEMIS Strategic Research Agenda and Multi-annual Strategic Plan and the ARTEMIS Working Group on Education and Training, the ideas of three ARTEMIS projects (R3-COP, MBAT and SafeCer) to deliver supporting training material to promote more widespread use of the outcomes of the projects, the study (report) performed within the FP6 European Integrated Project DECOS, and experiences with the European Master in Embedded Computing Systems (EMECS).",
    "actual_venue": "Software Engineering And Advanced Applications"
  },
  {
    "abstract": "AbstractWe extend the hazardous-materials hazmat network design problem to account for the time-dependent road closure as a policy tool to reduce hazmat transport risk by altering carriers' departure times and route choices. We formulate the time-dependent network design problem using an alternative-based model with each alternative representing a combined path and departure-time choice. We also present an extended model that can not only account for consecutive time-based road closure policies but also allow stopping at the intermediate nodes of the network in the routing/scheduling decisions of the carriers. Heuristic algorithms based on column generation and label setting are presented. To illustrate the advantages that can be gained through the use of our methodology, we present results from numerical experiments based on a transportation network from Buffalo, New York. To investigate the impact of the extensions, we consider three versions of the problem by gradually refining the model. We show that under consideration of extensions, the design policies are more applicable and effective.The online appendix is available at https://doi.org/10.1287/trsc.2016.0698.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "Classical authentication and authorization in grid environments can become a user management issue due to the flat nature of credentials based on X.509 certificates. While such credentials are able to identify user affiliations, such systems typically leave out a crucial aspect in user management and resource allocation: privilege levels. Shibboleth-based authentication mechanisms facilitate the secure communication of such user attributes within a trust federation. This paper describes a role-based access control framework that exploits Shibboleth attribute handling and CAS (Community Authorization Services) within a Grid environment. Users are able obtain appropriate access levels to resources outside of their domain on the basis of their native privileges and resource policies. This paper describes our framework and discusses issues of security and manageability.",
    "actual_venue": "Ica3Pp"
  },
  {
    "abstract": "Connecting the Information Systems/Information Technology (IS/IT) strategy with business strategy has become a crucial issue. The level of integration between business strategies requires the explanation of interrelationships, in order to achieve business goals with the available resources and market conditions. This research is a holistic examination of the relationship between IS, business strategy and marketing in 'X' Airlines. Marketing is considered as a core activity. The research design involves the qualitative method. It covers the literature on IS integration, a case study approach to understand the 'how' and 'why' of using IS in X. The research discusses existing frameworks, both theoretical and practical, of strategic IS integration. The results show that there are relationships of the traditional kind as the back-office support of IS for business strategy and marketing within X Airlines.",
    "actual_venue": "Ijbis"
  },
  {
    "abstract": "Motivation Predicting the secondary structure of an ribonucleic acid (RNA) sequence is useful in many applications. Existing algorithms [based on dynamic programming] suffer from a major limitation: their runtimes scale cubically with the RNA length, and this slowness limits their use in genome-wide applications. Results We present a novel alternative O(n(3))-time dynamic programming algorithm for RNA folding that is amenable to heuristics that make it run in O(n) time and O(n) space, while producing a high-quality approximation to the optimal solution. Inspired by incremental parsing for context-free grammars in computational linguistics, our alternative dynamic programming algorithm scans the sequence in a left-to-right (5-to-3) direction rather than in a bottom-up fashion, which allows us to employ the effective beam pruning heuristic. Our work, though inexact, is the first RNA folding algorithm to achieve linear runtime (and linear space) without imposing constraints on the output structure. Surprisingly, our approximate search results in even higher overall accuracy on a diverse database of sequences with known structures. More interestingly, it leads to significantly more accurate predictions on the longest sequence families in that database (16S and 23S Ribosomal RNAs), as well as improved accuracies for long-range base pairs (500+ nucleotides apart), both of which are well known to be challenging for the current models. Availability and implementation Our source code is available at https://github.com/LinearFold/LinearFold, and our webserver is at http://linearfold.org (sequence limit: 100000nt). Supplementary information Supplementary data are available at Bioinformatics online.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "In this article we discuss a freely downloadable educational software tool for illustrating project scheduling and project management concepts. The tool features exact and heuristic scheduling procedures and visualizes project networks, project schedules, resource profiles, activity slacks, and project duration distributions. (c) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 19: 327-336, 2011; View this article online at wileyonlinelibrary.com; DOI 10.1002/cae.20314",
    "actual_venue": "Computer Applications In Engineering Education"
  },
  {
    "abstract": "We are working on a new concept for decentralized medium access control (MAC), termed decentralized time-synchronized channel swapping (DT-SCS). Under the proposed DT-SCS and its associated MAC-layer protocol, wireless nodes converge to synchronous beacon packet transmissions across all IEEE802.15.4 channels, with balanced numbers of nodes in each channel. This is achieved by reactive listening mechanisms, based on pulse coupled oscillator techniques. Once convergence to the multichannel time-synchronized state is achieved, peer-to-peer channel swapping can then take place via swap requests and acknowledgments made by concurrent transmitters in neighboring channels. Our implementation of DT-SCS reveals that our proposal comprises an excellent candidate for completely decentralized MAC-layer coordination in WSNs by providing for quick convergence to steady state, high bandwidth utilization, high connectivity and robustness to interference and hidden nodes. The demo will showcase the properties of DT-SCS and will also present its behaviour under various scenarios for hidden nodes and interference, both experimentally and with the help of visualization of simulation results.",
    "actual_venue": "Ipsn"
  },
  {
    "abstract": "With the widespread adoption of Internet of Things and cloud computing in different industry sectors, an increasing number of individuals or organizations are outsourcing their Industrial Internet of Things (IIoT) data in the cloud server to achieve cost saving and collaboration (e.g., data sharing). However, in this environment, preserving the privacy of data remains a key challenge and inhibitin...",
    "actual_venue": "Ieee Transactions Industrial Informatics"
  },
  {
    "abstract": "The IEEE802.21 protocol has provided a mechanism to perform a seamless handover within heterogeneous networks guaranteeing a higher level of mobility. In this paper, we discuss different characteristics and challenges facing 4G mobility. We propose an efficient user zoning handover technique based on the IEEE802.21 protocol. The proposed approach connects users to a preset network upon entering a zone. The process is guided by the MIIS server.",
    "actual_venue": "Next Generation Mobile Applications, Services And Technologies"
  },
  {
    "abstract": "We present an efficient method to evaluate distances between dynamic obstacles and a number of points of interests (e.g., placed on the links of a robot) when using multiple depth cameras. A depth-space oriented discretization of the Cartesian space is introduced that represents at best the workspace monitored by a depth camera, including occluded points. A depth grid map can be initialized off li...",
    "actual_venue": "Ieee Robotics And Automation Letters"
  },
  {
    "abstract": "When a crowd's motivations are not triggered, they may not necessarily commit their best efforts, even if they have the knowledge to answer an open call. Drawing on the incentive theory, we introduce a top-down process model for an online crowdsourcing campaign that addresses the crowd's motivations. This model is in contrast to the traditional bottom-up process model, where the crowd self-selects an open call based on their knowledge. We adopt a longitudinal case study method and examine two online crowdsourcing campaigns that represent both models. The findings suggest that the campaign that follows the top-down model generated high-quality ideas, while the bottom-up case was considered a failure. We further enrich the top-down model by developing a four-stage guidance model that addresses the crowd's differing motivations in each stage. This research contributes to the crowdsourcing literature and helps better attract the qualifying crowd, thereby leading to greater campaign success likelihood.",
    "actual_venue": "Journal Of Computer Information Systems"
  },
  {
    "abstract": "•The output characteristic curve of the solar panel converts from a single peak curve into a multi-peaks curve under the partial shading conditions.•This maximum power point tracking method is modified on the basis of the GSA.•IGSA-MPPT is not only reduced the tracking time, but also improved the tracking accuracy compared to PSO-MPPT and GSA-MPPT.•The average tracking time of IGSA-MPPT was reduced by 0.023 s and 0.0116 s.•The average increase rates of maximum power increased by 1.7071% and 0.7001% compared with PSO-MPPT and GSA-MPPT.",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "A 3.3 GHz DCO that achieves a minimum frequency quantization step of 150 Hz without any dithering is presented. The fine digital tuning is obtained through a capacitive degeneration of a portion of the transistor switching pair used in a classical LC-tank oscillator. The DCO exhibits a phase noise of -127.5 dBc/Hz@1 MHz drawing 16 mA from a 1.8 V supply, resulting in an FoM of 183 dBc/Hz. The active area is 700 Â¿m Ã 450 Â¿m.",
    "actual_venue": "Solid-State Circuits Conference Digest Of Technical Papers"
  },
  {
    "abstract": "Approaches to abnormality detection in crowded scene largely rely on supervised methods using discriminative models. In this paper, we presents a novel and efficient unsupervised learning method for video analysis. We start from visual saliency, which has been used in several vision tasks, e.g., image classification, object detection, and foreground segmentation. To detect saliency regions in video sequences, we propose a new approach for detecting spatiotemporal visual saliency based on the phase spectrum of the videos, which is easy to implement and computationally efficient. With the proposed algorithm, we also study how the spatiotemporal saliency can be used in two important vision tasks, saliency prediction and abnormality detection. The proposed algorithm is evaluated on several benchmark datasets with comparison to the state-of-the-art methods from the literature. The experiments demonstrate the effectiveness of the proposed approach to spatiotemporal visual saliency detection and its application to the above vision tasks.",
    "actual_venue": "Ieee Winter Conference On Applications Of Computer Vision"
  },
  {
    "abstract": "Timing acquisition is critical to enabling the potential of ultra-wideband (UWB) radios in high-speed, short-range indoor wireless networking. An effective timing acquisition method should not only operate at a low sampling rate to reduce implementation complexity and synchronization time, but also be able to collect sufficient signal energy in order to operate in a reasonable transmit SNR regime. Energy capture for time-hopping impulse-radio transmissions in dense multipath is particularly challenging during the synchronization phase, in the absence of reliable channel and timing information. In this paper, we develop an efficient sampling strategy for correlation-based receivers to accomplish adequate energy capture at a low cost, using a noisy correlation template constructed directly from the received waveform. Merging our sampling operation based on noisy template with low-complexity timing acquisition schemes, we derive enhanced cyclostationarity-based blind synchronizers, as well as data-aided maximum likelihood timing offset estimators, all operating at a low frame rate. Both analysis and simulations confirm evident improvement in timing performance when using our noisy template, which makes our low-complexity timing acquisition algorithms attractive for practical UWB systems operating in dense multipath.",
    "actual_venue": "Eurasip J Adv Sig Proc"
  },
  {
    "abstract": "We present an index structure to support the approximate keyword search in text databases. In an approximate keyword search query, the user presents a query word Q and a tolerance value k (k\\geqslant 0), and wishes to find all documents in the database that contain the query word Q or any other word in the vocabulary that matches Q approximately (We say that two words match each other approximately if the edit distance between them does not exceed the tolerance value k. In a typical text database application, a user will choose k = 1, 2, 3, or 4). Our index structure is built on the underlying vocabulary of the text database. The new technique has two principal componentsa new data structure called the V-tree and its partition methods for clustering words in the vocabulary into subgroups. We have implemented our index structure and conducted experiments on real-world data. Our experiments show that even for very large text database, the construction of our index and a search for keywords that match the query word approximately can be done quickly. Our implemntation makes it clear that the V-tree data structure can be easily integrated into existing access structures built on the database such as the inverted index file.",
    "actual_venue": "CIT"
  },
  {
    "abstract": "The use of social media such as Twitter has changed our life routines. Previous studies have found consistent diurnal patterns of user activities on social media platforms. However, the temporal organization of human behavior is partly socially constructed and is determined by numerous factors other than the diurnal cycle. The current study argues that peer influence incurred by social networks is one of these potential factors. To test our hypotheses, we collected a random sample of active Twitter users (N = 5066), their followers and followees (N = 424,984), and all available tweets posted by these users. Results suggest that the temporal patterns between self-posting and interaction behavior differ across individuals. Users’ daily activity rhythms are more similar to their followees’ rhythms than to their followers’ rhythms. Despite the fact that the self-selection mechanism (homophily) cannot be ignored, peer influence seems to be an equally likely mechanism explaining such similarity.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "The uniform prior distribution is often seen as a mathematical description of noninformativeness. This paper uses the well-known Three Prisoners Paradox to examine the impossibility of maintaining noninformativeness throughout hierarchization. The Paradox has been solved by Bayesian conditioning over the choice made by the Warder when asked to name a(nother) prisoner who will be shot. We generalize the paradox to situations of N prisoners, k executions and m announcements made by the Warder. We then extend the consequences of hierarchically placing uniform and symmetrical priors (for example in the classical N = 3, k = 2, m = 1 scenario) for the probability p of the Warder naming Prisoner B, say. We prove that breaks of indifference and neutrality caused by assignment of uniform and symmetrical priors in lieu of degenerate indifference probabilities hold in general.",
    "actual_venue": "Synthese"
  },
  {
    "abstract": "This paper deals with zero-sum stochastic differential games with long-run average payoffs. Our main objective is to give conditions for existence and characterization of bias and overtaking optimal equilibria. To this end, first we characterize the family of optimal average payoff strategies. Then, within this family, we impose suitable conditions to determine the subfamilies of bias and overtaking equilibria. A key step to obtain these facts is to show the existence of solutions to the average payoff optimality equations. This is done by the usual “vanishing discount” approach. Finally, a zero-sum game associated to a certain manufacturing process illustrates our results.",
    "actual_venue": "J Optimization Theory And Applications"
  },
  {
    "abstract": "Existing Photometric Stereo methods provide reasonable surface reconstructions unless the irradiance image is corrupted with noise and effects of digitisation. However, in real world situations the measured image is almost always corrupted, so an efficient method must be formulated to denoise the data. Once noise is added at the level of the images the noisy Photometric Stereo problem with a least squares estimate is transformed into a non-linear discrete optimization problem depending on a large number of parameters. One of the computationally feasible methods of performing this non-linear optimization is to use many smaller local optimizations to find a minimum (called 2D Leap-Frog). However, this process still takes a large amount of time using a single processor, and when realistic image resolutions are used this method becomes impractical. This paper presents a parallel implementation of the 2D Leap-Frog algorithm in order to provide an improvement in the time complexity. While the focus of this research is in the area of shape from shading, the iterative scheme for finding a local optimum for a large number of parameters can also be applied to any optimization problems in Computer Vision. The results presented herein support the hypothesis that a high speed up and high efficiency can be achieved using a parallel method in a distributed shared memory environment.",
    "actual_venue": "Iccvg"
  },
  {
    "abstract": "3-D flash memory faces a number of challenges, including thermal issues and process variation. The high temperature will cause charge loss and lead to the fluctuation of threshold voltage. To address the thermal issue of 3-D flash memory, this paper presents \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">ThermAlloc</italic>\n, a novel thermal-aware physical space allocation strategy for open-channel solid-state drive with 3-D charge trapping flash memory. ThermAlloc permutes the allocation of physical blocks. Consecutively accessed logical blocks are distributed to different physical locations in order to prevent the accumulation of hotspots. The objective is to postpone garbage collection operations and keep the distribution of block temperature well under control. We demonstrate the viability of the proposed technique using a set of extensive experiments. Experimental results show that ThermAlloc can reduce the peak temperature by 26.94% with 3.15% extra worst-case response time in comparison with the baseline scheme.",
    "actual_venue": "Ieee Transactions On Computer-Aided Design Of Integrated Circuits And Systems"
  },
  {
    "abstract": "Build systems are used in all but the smallest software projects to invoke the right build tools on the right files in the right order. A build system must be sound (after a build, generated files consistently reflect the latest source files) and efficient (recheck and rebuild as few build units as possible). Contemporary build systems provide limited efficiency because they lack support for expressing fine-grained file dependencies. We present a build system called pluto that supports the definition of reusable, parameterized, interconnected builders. When run, a builder notifies the build system about dynamically required and produced files as well as about other builders whose results are needed. To support fine-grained file dependencies, we generalize the traditional notion of time stamps to allow builders to declare their actual requirements on a file's content. pluto collects the requirements and products of a builder with their stamps in a build summary. This enables pluto to provides provably sound and optimal incremental rebuilding. To support dynamic dependencies, our rebuild algorithm interleaves dependency analysis and builder execution and enforces invariants on the dependency graph through a dynamic analysis. We have developed pluto as a Java API and used it to implement more than 25 builders. We describe our experience with migrating a larger Ant build script to pluto and compare the respective build times.",
    "actual_venue": "Conference On Object-Oriented Programming Systems, Languages, And Applications"
  },
  {
    "abstract": "This paper describes applications of continuous-time feed-forward Sigma-Delta (ΣΔ) modulators to control DC-DC converters as follows. We propose to use continuous-time feed-forward ΣΔ controllers in DC-DC converters, and show that their transient response is faster than discrete-time and/or feedback-type ΣΔ controllers. We also show that second-order ΣΔ controllers have superior performance to first-order ones. SPICE and Matlab simulations substantiate these results.",
    "actual_venue": "Apccas"
  },
  {
    "abstract": "Genomics is an important emerging scientific field that relies on meaningful data visualization as a key step in analysis. Specifically, most investigation of gene expression microarray data is performed using visualization techniques. However, as microarrays become more ubiquitous, researchers must analyze their own data within the context of previously published work in order to gain a more complete understanding. No current method for microarray visualization and analysis enables biology researchers to observe the greater context of data that surrounds their own results, which severely limits the ability of researchers draw novel conclusions. Here we present a system, called HIDRA, that visually integrates the simultaneous display of multiple microarray datasets to identify important parallels and dissimilarities. We demonstrate the power of our approach through examples of real-world biological insights that can be observed using HIDRA that are not apparent using other techniques.",
    "actual_venue": "Zurich"
  },
  {
    "abstract": "JYAG is the Java implementation of a real-time, general-purpose, template-based generation system (YAG, Yet Another Generator). JYAG enables interactive applications to adapt natural language output to the interactive context without requiring developers to write all possible output strings ahead of time or to embed extensive knowledge of the grammar of the target language in the application. Currently, designers of interactive systems who might wish to include dynamically generated text face a number of barriers; for example designers must decide (1) How hard will it be to link the application to the generator? (2) Will the generator be fast enough? (3) How much linguistic information will the application need to provide in order to get reasonable quality output? (4) How much effort will be required to write a generation grammar that covers all the potential outputs of the application? The design and implementation of our template-based generation system, JYAG, is intended to address each of these concerns.",
    "actual_venue": "Aaai/Iaai"
  },
  {
    "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) - a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks: phonetic transcription from acoustic & articulatory measurements, recommending hashtags, and recommending friends on a dataset of Twitter users.",
    "actual_venue": "Workshop On Representation Learning For Nlp"
  },
  {
    "abstract": "We introduce Secant-Tangents AveRaged (STAR) Stochastic Approximation (SA), a new SA algorithm that estimates the gradient using a hybrid estimator, which is a convex combination of a symmetric finite difference and an average of two direct gradient estimators. For the deterministic weight sequence that minimizes the variance of the STAR gradient, we prove that for quadratic functions, the mean squared error (MSE) of the STAR-SA algorithm using this weight sequence is strictly less than that of the classical SA methods of Robbins-Monro (RM) and Kiefer-Wolfowitz (KW). We also prove convergence of the STAR-SA algorithm for general concave functions. Furthermore, we illustrate its effectiveness through numerical experiments by comparing the MSE of the STAR-SA algorithm against RM and KW for simple quadratic functions with various steepness and noise levels.",
    "actual_venue": "Ifac Proceedings Volumes"
  },
  {
    "abstract": "We introduce analogues of the lattice of non-crossing set partitions for the classical reflection groups of types B and D . The type B analogues (first considered by Montenegro in a different guise) turn out to be as well-behaved as the original non-crossing set partitions, and the type D analogues almost as well-behaved. In both cases, they are EL-labellable ranked lattices with symmetric chain decompositions (self-dual for type B ), whose rank-generating functions, zeta polynomials, rank-selected chain numbers have simple closed forms.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "Connected-dominating-set (CDS) is a representative technique for constructing a virtual backbone of wireless networks. Most of existing works on CDS aim at minimizing the size of the CDS, i.e., constructing the minimum CDS (MCDS), so as to reduce the communication overhead over the CDS. However, MCDS may not work well in cognitive radio networks (CRNs) where communication links are prone to failure due to the unpredictable activities of primary users. A MCDS without consideration of stochastic activities of primary users easily becomes invalid when the primary users reclaim the licensed spectrum. In this work, we assume that the activities of primary users follow the exponential distribution. Our problem is to maximize the lifetime of the CDS while minimizing the size of the CDS, where the lifetime of a CDS is defined as the expected duration that the CDS is maintained valid. We show that the problem is NP-hard and propose a three-phase algorithm. Our basic idea is to apply a pruning-based approach to maximize the lifetime of the CDS. Given a CRN, we prove that our algorithm can compute a CDS such that i) the lifetime of the CDS is maximized (optimal); and ii) the size of the CDS is upper-bounded. To the best of our knowledge, it is the first time in the literature that CDS in CRNs is studied and an effective algorithm is proposed.",
    "actual_venue": "Networking"
  },
  {
    "abstract": "By viewing the ancestral recombination graph as defining a sequence of trees, we show how possible evolutionary histories consistent with given data can be constructed using the minimum number of recombination events. In contrast to previously known methods, which yield only estimated lower bounds, our method of detecting recombination always gives the minimum number of recombination events if the right kind of rooted trees are used in our algorithm. A new lower bound can be defined if rooted trees with fewer constraints are used. As well as studying how often it actually is equal to the minimum, we test how this new lower bound performs in comparison to some other lower bounds. Our study indicates that the new lower bound is an improvement on earlier bounds. Also, using simulated data, we investigate how well our method can recover the actual site-specific evolutionary relationships. In the presence of recombination, using a single tree to describe the evolution of the entire locus clearly leads to lower average recovery percentages than does our method. Our study shows that recovering the actual local tree topologies can be done more accurately than estimating the actual number of recombination events.",
    "actual_venue": "Journal Of Computational Biology"
  },
  {
    "abstract": "This paper considers an optimization viewpoint of decisiontheoretic rough set model. An optimization problem is proposed by considering the minimization of the decision cost. Based on the optimization problem, cost functions and thresholds used in decision-theoretic rough set model can be learned from the given data automatically. An adaptive learning algorithm Alcofa is proposed. Another significant inference drawn from the solution of the optimization problem is a minimum cost based attribute reduction. The attribute reduction can be interpreted as finding the minimal attribute set to make the decision cost minimum. The optimization viewpoint can bring some new insights into the research on decision-theoretic rough set model.",
    "actual_venue": "Rskt"
  },
  {
    "abstract": "In the past decade, several higher-order crystal plasticity models have been developed to properly capture size effects, dislocation pile-up and patterning. Here we consider a formulation which accounts for the presence and behavior of both positive and negative dislocations in terms of densities. We derive an implicit finite element implementation for the continuum crystal plasticity model including dislocation transport, using a generalised continuum expression for the short-range dislocation interactions, by discretizing the two governing non-linear transport equations in time and space. The resulting non-linear algebraic equations are solved by an incremental-iterative solution scheme. We compare the resulting numerical solutions with discrete dislocation simulations. This analysis shows the capabilities of the implicit FEM framework to solve continuum dislocation transport in crystal plasticity with the added energetic dislocation interactions.",
    "actual_venue": "Adv Model And Simul In Eng Sciences"
  },
  {
    "abstract": "A cinemagraph is a new type of medium that infuses a static image with the dynamics of one particular region. It is in many ways intermediate between a photograph and a video, and has a number of attractive potential applications, such as the creation of dynamic scenes for games and interactive environments. However, creating cinemagraphs is time consuming and requires certain level of proficiency on photo editing techniques. In this paper, we present a fully automatic approach that creates cinemagraphs from video sequences. Specifically, we view cinemagraph construction as a constrained optimization problem that seeks a sub-volume in video with the maximum cumulative flow fields. The problem can be efficiently solved by a branch and-bound search scheme. A user survey is conducted to understand user preferences and demonstrate the performance of the proposed approach. The findings of this study should provide information for various design choices for an easy and versatile authoring tool for cinemagraphs.",
    "actual_venue": "Acm Multimedia"
  },
  {
    "abstract": "Reducing potentially preventable readmissions has been identified as an important issue for decreasing Medicare costs and improving quality of care provided by hospitals. Based on previous research by medical professionals, preventable readmissions are caused by such factors as flawed patient discharging process, inadequate follow-ups after discharging, and noncompliance of patients on discharging and follow up instructions. It is also found that the risk of preventable readmission also may relate to some patient's characteristics, such as age, health condition, diagnosis, and even treatment specialty. In this study, using both general demographic information and individual past history of readmission records, we develop a risk prediction model based on hierarchical nonlinear mixed effect framework to extract significant prognostic factors associated with patient risk of 30-day readmission. The effectiveness of our proposed approach is validated based on a real dataset from four VA facilities in the State of Michigan. Simultaneously explaining both patient and population based variations of readmission process, such an accurate model can be used to recognize patients with high likelihood of discharging non-compliances, and then targeted post-care actions can be designed to reduce further rehospitalization.",
    "actual_venue": "Automation Science And Engineering"
  },
  {
    "abstract": "Presents two simple and computationally efficient schemes for force tracking using impedance control. The schemes generate the reference position trajectory required to produce a desired contact force despite lack of knowledge of the environmental stiffness and location. The first scheme uses direct adaptive control to generate the reference position online as a function of the force tracking-error. The second scheme utilizes an indirect adaptive strategy in which the environmental parameters are estimated online, and the required reference position is computed based on these estimates. Simulation studies are presented for a 7-DOF Robotics Research arm using full arm dynamics and demonstrate that the adaptive schemes are able to compensate for uncertainties in both the environmental stiffness and location, so that the end-effector applies the desired contact force while exhibiting the specified impedance dynamics",
    "actual_venue": "Intelligent Robots And Systems , Iros , Proceedings Of The Ieee/Rsj International Conference"
  },
  {
    "abstract": "Following related work in law and policy, two notions of prejudice have come to shape the study of fairness in algorithmic decision-making. Algorithms exhibit disparate treatment if they formally treat people differently according to a protected characteristic, like race, or if they intentionally discriminate (even if via proxy variables). Algorithms exhibit disparate impact if they affect subgroups differently. Disparate impact can arise unintentionally and absent disparate treatment. The natural way to reduce disparate impact would be to apply disparate treatment in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practiceu0027s contested legal status, several papers have proposed trying to eliminate both forms of unfairness simultaneously, introducing a family of algorithms that we denote disparate learning processes (DLPs). These processes incorporate the protected characteristic as an input to the learning algorithm (e.g.~via a regularizer) but produce a model that cannot directly access the protected characteristic as an input. In this paper, we make the following arguments: (i) DLPs can be functionally equivalent to disparate treatment, and thus should carry the same legal status; (ii) when the protected characteristic is redundantly encoded in the nonsensitive features, DLPs can exactly apply any disparate treatment protocol; (iii) when the characteristic is only partially encoded, DLPs may induce within-class discrimination. Finally, we argue the normative point that rather than masking efforts towards proportional representation, it is preferable to undertake them transparently.",
    "actual_venue": "Arxiv: Machine Learning"
  },
  {
    "abstract": "A novel compensation scheme for timing synchronization error in GPP-SDR LTE system is proposed in this article. The properties of timing synchronization error are analyzed, and it is exploited to design a compensation scheme suitable to GPP-SDR baseband processing capacity. Furthermore, the applicability of the scheme is assessed in theory analysis and tested by simulations and experiments. It is revealed from the results that the BER of baseband processing with timing synchronization error compensation is about 10-4 while that without compensation is about 0.1. © 2012 IEEE.",
    "actual_venue": "Iccc"
  },
  {
    "abstract": "This contribution describes a case-based decision support system which is intended for being used in the field of Structural Health Monitoring to support the assessment of structures (by using the example of lamp posts). Interpreting measuring data manually is a very complex task, time-consuming and influenced by the subjectivity of civil engineers. Therefore, the engineers shall be supported in assessing measuring data by using a case-based decision support system. A measurement of a structure and a manual assessment by an engineer represent a case in a case base. Similar cases/structures shall be retrieved and made available for assessing new measurements. For supporting the assessment of simple structures (lamp posts), a case-based system shall be provided for interpreting measuring data semi-automatically to make suggestions about lamp posts' condition. Thereby, time and costs can be reduced more than 90% by the use of computer-aided assessment in comparison with the manual interpretation.",
    "actual_venue": "Iccbr"
  },
  {
    "abstract": "We consider two styles of executing a single job or an algorithm: either the job is subdivided into tasks, each of which is executed. on a separate processor, or the entire job is executed on a single processor, that has the same capacity as the sum of the processors in the earlier case. The algorithm is abstracted as consisting of a number of tasks with dependencies among them. Our model of dependencies among tasks allows sequential execution, parallel execution, synchronization, and spawning of tasks. The model assumes that the dependencies are known before the job begins, and a task in not preempted after its execution begins. With the usual assumptions such as exponential distribution of task execution times, and Poisson arrival of input data, we are able to show that the centralized execution completes the job faster than the decentralized execution only for a certain range of parameters of algorithms. We also give counterexamples that show that, contrary to popular belief, the reverse is true for some values of parameters of algorithms.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "Objective The aim of this study was to measure the effect of an electronic heparin-induced thrombocytopenia (HIT) alert on provider ordering behaviors and on patient outcomes. Materials and Methods A pop-up alert was created for providers when an individual's platelet values had decreased by 50% or to < 100000/mm(3) in the setting of recent heparin exposure. The authors retrospectively compared inpatients admitted between January 24, 2008 and August 24, 2008 to a control group admitted 1 year prior to the HIT alert. The primary outcome was a change in HIT antibody testing. Secondary outcomes included an assessment of incidence of HIT antibody positivity, percentage of patients started on a direct thrombin inhibitor (DTI), length of stay and overall mortality. Results There were 1006 and 1081 patients in the control and intervention groups, respectively. There was a 33% relative increase in HIT antibody test orders (p-0.01), and 33% more of these tests were ordered the first day after the criteria were met when a pop-up alert was given (p=0.03). Heparin was discontinued in 25% more patients in the alerted group (p=0.01), and more direct thrombin inhibitors were ordered for them (p=0.03). The number who tested HIT antibody-positive did not differ, however, between the two groups (p=0.99). The length of stay and mortality were similar in both groups. Conclusions The HIT alert significantly impacted provider behaviors. However, the alert did not result in more cases of HIT being detected or an improvement in overall mortality. Our findings do not support implementation of a computerized HIT alert.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "3D kinect camera systems are essential for real- time imaging of 3D treatment space that consists of both the patient anatomy as well as the treatment equipment setup. In this paper, we present the technical details of a 3D treatment room monitoring system that employs a scalable number of calibrated and co-registeredKinect v2 cameras. The monitoring system tracks radiation gantry and treatment couch positions, and tracks the patient and immobilization accessories. The number and positions of the cameras were selected to avoid line-of-sight issues and to adequately cover the treatment setup. The cameras were calibrated with a calibration error of 0.1 mm. Our tracking system evaluation show that both gantry and patient motion could be acquired at a rate of 30 frames per second. The transformations between the cameras yielded a 3D treatment space accuracy of < 2 mm error in a radiotherapy setup within 500mm around the isocenter.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "The last four decades have produced a number of significant advances in the developments of computer models to simulate and investigate the electrical activity of cardiac tissue. The tissue descriptions that underlie these simulations have been built from a combination of clever insight and careful comparison with measured data at multiple scales. Tissue models have not only led to greater insights into the mechanisms of life-threatening arrhythmias but have been used to engineer new therapies to treat the consequences of cardiac disease. This paper is a look back at the early years in the cardiac modeling and the challenges facing the field as models move toward the clinic.",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "The paper features MAVE, a knowledge-based system for answer validation through deep linguistic processing and logical inference. A relaxation loop is used to determine a robust indicator of logical entailment. The system not only validates answers directly, but also gathers evidence by proving the original question and comparing results with the answer candidate. This method boosts recall by up to 13%. Additional indicators signal false positives. The filter increases precision by up to 14.5% without compromising recall of the system.",
    "actual_venue": "Clef"
  },
  {
    "abstract": "This paper aims to explore the research status of task design used in virtual worlds and the student's long-term engagement in the use of virtual worlds. Content analysis is conducted to investigate the common designed language activities in the research area. The results show that role-playing are the most common designed activities and Vygotsky's sociocultural theory was significantly used as the theoretical frameworks among the selected studies. However, only one of the studies examined students' further use of the virtual worlds beyond the experiment assigned time. This short paper might shed light on future exploration about the student's long term engagement in the virtual reality environments.",
    "actual_venue": "Icalt"
  },
  {
    "abstract": "Feature selection is a process commonly used in machine learning, wherein a subset of the features available from the data are selected for application of a learning algorithm. Feature selection is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy and efficiency. In this paper, we propose a new information distance to measure the relevancy of two features. Unlike the information measure in previous feature selection works, our proposed information distance meets the condition of triangle inequality. We use InfoDist to feature selection and the experimental results showed it has a better performance.",
    "actual_venue": "Fskd"
  },
  {
    "abstract": "Publish/Subscribe architectures have been widely studied and applied in wired networks. However, their deployment on mobile ad hoc networks still presents a lot of challenges. This work proposes and analyzes a solution for such networks using the nodes' movement to disseminate publications to the whole network with few transmissions. Our proposal does not build dissemination trees which incur in a high cost to keep them updated due to constant changes in the topology, and does not even require beacons exchanges in order to sustain a neighborhood table. Our experiments show that we are able to achieve better results than a gossip-based algorithm and other solutions found in the literature.",
    "actual_venue": "Bone"
  },
  {
    "abstract": "In a structured peer-to-peer network, frequent join and leave of peer members cause huge maintenance overhead. To deal with this churn problem, we proposed a sector-based routing model (SBRM). The key space is divided into several sectors and each sector has a peer member acting as the relay proxy of the sector. Then the cost of join and leave can be greatly reduced. In this paper we choose Chord as the underlying structured network and perform a series of simulation. Analysis and simulation results show that SBRM achieve lower communication cost when a member joined or left the system, and message routing path length is also shortened.",
    "actual_venue": "Mobility Conference"
  },
  {
    "abstract": "The limited capacity of battery power becomes one of the major constraints in the applications of Internet of things (IoT). Ambient energy harvesting technologies and wireless energy transfer technologies have appeared to resolve the energy supply problem, making it possible for the sensor nodes to operate perpetually. In this paper, we focus on energy efficiency maximization and network throughput optimization problems for energy cooperation in Energy Harvesting Cooperative Wireless Sensor Networks (EHC-WSNs). In order to maximize the efficiency of energy charging phase, a Region-based Proactive Energy Cooperation (RPEC) charging strategy is developed, which is used to charge the life-critical cooperators or receivers in time. By introducing a novel metric that converts optimal forwarder selection from the multi-dimensional problem to one-dimensional problem, an Energy-Neutral-based Opportunistic Cooperative Routing (ENOCR) algorithm is proposed to optimize the relay nodes selection and improve the network throughput. Extensive simulations show that the proposed Opportunistic Energy Cooperation Mechanism (OECM) can significantly improve energy efficiency and network lifetime.",
    "actual_venue": "Monet"
  },
  {
    "abstract": "AbstractSocial media platform owners often choose to provide tighter integration with their own complementary applications i.e., first-party applications as compared to that with other complementary third-party applications. We study the impact of such integration on consumer demand for first-party applications and competing third-party applications by exploring Facebook's integration of Instagram, an application in its photo-sharing application ecosystem. We find that consumers obtain additional value from Instagram after its integration with Facebook, leading to a large increase in the use of Instagram for Facebook photo sharing. Further, we find that the growth of Instagram's user base has a positive spillover effect on big third-party applications and a negative spillover effect on small third-party applications in Facebook's photo-sharing ecosystem. As a result, while small third-party applications face reduced demand after integration, big third-party applications experience a small increase in demand. Thus, the overall demand for the entire photo-sharing application ecosystem actually increases, which suggests that Facebook's integration strategy benefits the complementary market overall. Our results highlight the role of platform integration for first-party applications and the application ecosystem overall, and they have implications for strategic management of first-party applications in the presence of third-party applications.This paper was accepted by Anandhi Bharadwaj, information systems.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "We consider the important crowdsourcing problem of estimating worker confusion matrices, or sensitivities and specificities for binary classification tasks. In addition to providing diagnostic insights into worker performance, such estimates enable robust online task routing for classification tasks exhibiting imbalance and asymmetric costs. However, labeled data is often expensive and hence estimates must be made without much of it. This poses a challenge to existing methods. In this paper, we propose a novel model that captures the correlations between entries in confusion matrices. We applied this model in two practical scenarios: (1) an imbalanced classification task in which workers are known to belong to groups and (2) a multitask scenario in which labels for the same workers are available in more than one labeling task. We derive an efficient variational inference approach that scales to large datasets. Experiments on two real world citizen science datasets (biomedical citation screening and galaxy morphological classification) demonstrate consistent improvement over competitive baselines. We have made our source code available.",
    "actual_venue": "UAI"
  },
  {
    "abstract": "Reconstruction of object surfaces from sparse measures is an ill-posed inverse problem which requires a priori knowledge to be regularized. This problem becomes more difficult whenever an active method is used and a scattering medium is present between the signal source and the scene observed. This paper describes a reconstruction method that can be applied to solve a microwave imaging problems. The goal of the proposed algorithm is to obtain a pixel-based representation of 2-D object slices. The objects are assumed to be inhomogeneous dielectric scatterers in a microwave electromagnetic field. The method is based on the hypothesis that the observed field is a Markov Random Field (MRF), and consists in finding the field configuration that maximizes the a posteriori probability measure associated with the MRF model. A specific probabilistic measure that is based on a weak-membrane regularizing constraint as an a priori model and on an observation model using a near-field hypothesis is proposed. A classical stochastic optimization approach (i.e., simulated annealing with a Metropolis sampler) is adopted to find the probabilistic maximum. The capabilities and effectiveness of the method are evaluated and compared with those of other approaches requiring matrix inversion. Finally, simulation results are reported that show better reconstructions, than those obtained by other microwave image domain methods.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "The problem of learning from both labeled and unlabeled data is considered. In this paper, we present a novel semisupervised multimodal dimensionality reduction (SSMDR) algorithm for feature reduction and extraction. SSMDR can preserve the local and multimodal structures of labeled and unlabeled samples. As a result, data pairs in the close vicinity of the original space are projected in the nearby of the embedding space. Due to overfitting, supervised dimensionality reduction methods tend to perform inefficiently when only few labeled samples are available. In such cases, unlabeled samples play a significant role in boosting the learning performance. The proposed discriminant technique has an analytical form of the embedding transformations that can be effectively obtained by applying the eigen decomposition, or finding two close optimal sets of transforming basis vectors. By employing the standard kernel trick, SSMDR can be extended to the nonlinear dimensionality reduction scenarios. We verify the feasibility and effectiveness of SSMDR through conducting extensive simulations including data visualization and classification on the synthetic and real-world datasets. Our obtained results reveal that SSMDR offers significant advantages over some widely used techniques. Compared with other methods, the proposed SSMDR exhibits superior performance on multimodal cases.",
    "actual_venue": "Computational Intelligence"
  },
  {
    "abstract": "Purpose - The purpose of this paper is to explore and expound the factors that impinge on the adoption and usage of industrial e-markets.Design/methodology/approach - A review of the literature on e-market adoption was followed by in-depth interviews with senior managers in buyer, supplier, and e-market organisations. Senior level executives (15) reported barriers and challenges to the adoption and usage of e-markets operating in the aerospace and defence and higher education sectors. The interview data is transcribed, coded and analysed using the qualitative data analysis programme QSR N6.Findings - The paper found a number of barriers and challenges related risk perception, knowledge deficits, trust, firm size, and organisational readiness that moderate the adoption and usage of e-markets in the sectors.Research limitations/implications - There is unequal representation of buyer and supplier organisations between the two e-market sectors. However, the case material enabled the research question to be answered and did not compromise the aims of the research.Practical implications - The case material presented in the paper can help academic researchers, managers, practitioners and other professionals better understand the barriers that impinge on e-market adoption and find practical ways to mitigate those barriers.Originality/value - Recently, research on the barriers and challenges to e-markets has been largely anecdotal and patchy with a paucity of studies noting factors that are likely to be conducive to e-markets success. This study departs from such studies by offering empirical. evidence of the factors in moderating the uptake of e-markets.",
    "actual_venue": "Industrial Management And Data Systems"
  },
  {
    "abstract": "In general, Transmission Control Protocol (TCP), e.g., TCP NewReno, considers all losses to be a sign of congestion. It decreases the sending rate whenever a loss is detected. Integrating the network coding (NC) into protocol stack and making it cooperate with TCP (TCP/NC) would provide the benefit of masking packet losses in lossy networks, e.g., wireless networks. TCP/NC complements the packet loss recovery capability without retransmission at a sink by sending the redundant combination packets which are encoded at the source. However, TCP/NC is less effective under heavy and bursty loss which often occurs in fast fading channel because the retransmission mechanism of the TCP/NC entirely relies on the TCP layer. Our solution is TCP/NC with enhanced retransmission (TCP/NCwER), for which a new retransmission mechanism is developed to retransmit more than one lost packet quickly and efficiently, to allow encoding the retransmitted packets for reducing the repeated losses, and to handle the dependent combination packets for avoiding the decoding failure. We implement and test our proposal in Network Simulator 3. The results show that TCP/NCwER overcomes the deficiencies of the original TCP/NC and improves the TCP goodput under both random loss and burst loss channels.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "This paper describes algorithms for generating low intermodulation-distortion (IMD) two-tone sinewaves, for communication application ADC testing, using an arbitrary waveform generator (AWG) or a multi-bit ΣΔ DAC inside an SoC. The nonlinearity of the DAC generates distortion components, and we propose here eight methods to precompensate for the IMD using DSP algorithms and produce low-IMD two-tone signals. Theoretical analysis, simulation, and experimental results all demonstrate the effectiveness of our approach.",
    "actual_venue": "Asian Test Symposium"
  },
  {
    "abstract": "This memo describes a conflict between TCP [RFC793] and DiffServ [RFC2475] on the use of the three leftmost bits in the TOS octet of an IPv4 header [RFC791]. In a network that contains DiffServ-capable nodes, such a conflict can cause failures in establishing TCP connections or can cause some established TCP connections to be reset undesirably. This memo proposes a modification to TCP for resolving the conflict.",
    "actual_venue": "RFC"
  },
  {
    "abstract": "As digital imaging techniques continue to advance, new image compression standards are needed to keep the transmission time and storage space low for increasing image sizes. The Joint Photographic Expert Group (JPEG) fulfilled this need with the ratification of the JPEG2000 standard in December of 2000. JPEG2000 adds many features to image compression technology but also increases the computational complexity of traditional encoders. To mitigate the added computational complexity, the JPEG2000 algorithm allows processing parts in parallel, increasing the benefits of implementing the algorithm in application specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). A ﬂexible FPGA implementation of the JPEG2000 binary arithmetic decoder, the core component of the JPEG2000 decoding algorithm, is presented in this paper. The proposed JPEG2000 binary arithmetic decoder reduces the amount of resources used on the FPGA allowing 17% more entropy block decoders to fit on chip and consequently increasing the throughput by 35% beyond previous designs.",
    "actual_venue": "Digital Lmage Computing: Techniques And Applications"
  },
  {
    "abstract": "Software-defined networking (SDN) is deemed as a promising direction to offer generalizability of wireless sensor networks (WSN). To introduce SDN into WSNs, however, means a series of non-trivial challenges due to the wireless and ad-hoc nature of WSNs. In this paper, we present our study towards a software-defined architecture for multi-function wireless sensor networks. Our proposal called Pangu is built upon the opportunistic routing protocol stack and introduces the concept of modality properties of sensor nodes. It enables centralized network control over a WSN while preserving the flexibility of underlying ad-hoc routing. We tackle the critical problems of the architecture design by presenting three essential components of Pangu. Moreover, we implement Pangu on a real-world testbed and evaluate it with various experiments.",
    "actual_venue": "Ieee International Conference On Parallel And Distributed Systems"
  },
  {
    "abstract": "We extend the reach of functional encryption schemes that are provably secure under simple assumptions against unbounded collusion to include function-hiding inner product schemes. Our scheme is a private key functional encryption scheme, where ciphertexts correspond to vectors $$\\\\vec {x}$$, secret keys correspond to vectors $$\\\\vec {y}$$, and a decryptor learns $$\\\\langle \\\\vec {x}, \\\\vec {y} \\\\rangle $$. Our scheme employs asymmetric bilinear maps and relies only on the SXDH assumption to satisfy a natural indistinguishability-based security notion where arbitrarily many key and ciphertext vectors can be simultaneously changed as long as the key-ciphertext dot product relationships are all preserved.",
    "actual_venue": "Iacr Cryptology Eprint Archive"
  }
]