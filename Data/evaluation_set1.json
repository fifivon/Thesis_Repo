[
  {
    "abstract": "In this paper we consider a single-server, cyclic polling system with switch-over times. A distinguishing feature of the model is that the rates of the Poisson arrival processes at the various queues depend on the server location. For this model we study the joint queue length distribution at polling epochs and at the server's departure epochs. We also study the marginal queue length distribution at arrival epochs, as well as at arbitrary epochs (which is not the same in general, since we cannot use the PASTA property). A generalised version of the distributional form of Little's law is applied to the joint queue length distribution at customer's departure epochs in order to find the waiting time distribution for each customer type. We also provide an alternative, more efficient way to determine the mean queue lengths and mean waiting times, using Mean Value Analysis. Furthermore, we show that under certain conditions a Pseudo-Conservation Law for the total amount of work in the system holds. Finally, typical features of the model under consideration are demonstrated in several numerical examples.",
    "actual_venue": "Queueing Syst"
  },
  {
    "abstract": "Low-order Whitney elements are widely used for electromagnetic field problems. Higher-order approximations are receiving increasing interest, but their definition remains unduly complex. In this paper we propose a new simple construction for Whitney $p$-elements of polynomial degree higher than one that use only degrees of freedom associated to $p$-chains. We provide a basis for these elements on simplicial meshes and give a geometrical localization of all degrees of freedom. Properties of the higher-order Whitney complex are deeply investigated.",
    "actual_venue": "Siam J Numerical Analysis"
  },
  {
    "abstract": "The security of digital signatures depends not only on the cryptographic strength of the digital signature algorithms used, but also on the integrity of the platform on which the digital signature application is running. Breach of platform integrity due to unintentional or intentional malfunctioning has the potential of wrongly imposing liability on, or wrongly taking liability away from signing parties. This problem is amplified by the fact that digital signatures may be generated on platforms that are not under the control of the signing party, and that there can be strong financial incentives for trying to manipulate the systems used for digital signatures. In practice it is extremely difficult to assess the integrity of a general purpose computing platform, so that digital signing on such platforms in principle is untrustworthy. This paper describes a method for robust WYSIWYS (What You See Is What You Sign) that ensures the integrity of digital documents and their digital signatures. This method can only be directly applied to documents written with traditional ASCII characters. For more advanced formatting a specific layout definition language must defined.",
    "actual_venue": "Artificial Intelligence And Symbolic Computation"
  },
  {
    "abstract": "This article studies characteristic properties of synchronous and asynchronous message communications in distributed systems. Based on the causality relation between events in computations with asynchronous communications, we characterize computations which are realizable with synchronous communications, which respect causal order, or where messages between two processes are always received in the order sent. It is shown that the corresponding computation classes form a strict hierarchy. Furthermore, an axiomatic definition of distributed computations with synchronous communications is given, and it is shown that several informal characterizations of such computations are equivalent when they are formalized appropriately. As an application, we use our results to show that the distributed termination detection algorithm by Dijkstra et al. is correct under a weaker synchrony assumption than originally stated.",
    "actual_venue": "Distributed Computing"
  },
  {
    "abstract": "The traditional methods of estimating economic contribution rate of education (ECRE) are based on hard computing such as statistical methods, which ignore both the long-term effect of education and the lagged effect of education on economy growth. This paper proposes the fusion method of neural networks, fuzzy systems and genetic algorithms made in the realm of soft computing to estimate the ECRE. Firstly, a target system (a country or a region) is categorized softly according to the level of Science and Technology (S&T) progress. Secondly, potential human capital stock and actual human capital stock in the same cluster are calculated and set up the internal correlation between them (fuzzy mapping). Thirdly, we conceptualize actual human capital as one production factor, joined with the other two production factors, land and fixed asset, to set up the fuzzy mapping to economic growth. Finally, we obtain the ECRE through two marginal rates, namely marginal economic growth to actual human capital stock, and marginal actual human capital to potential human capital. This method greatly reduces the bias in the ECRE that results from the indirect and lagged effects of education. It therefore identifies the effect of education on economic growth more explicitly. Based on the level of S&T progress, 31 provinces in China could be classified into three clusters. The first cluster (developed S&T) has an ECRE of 11.60%, and contains two provinces; the second cluster (developing S&T) has an ECRE of 8.82%, and contains 11 provinces; the third cluster (underdeveloped S&T) has an ECRE of 1.49% and contains 18 provinces.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "Multiple Sequence Alignment (MSA) is the process of aligning three or more nucleotides/amino-acids sequences at the same time. It is an NP-complete optimization problem where the time complexity of finding an optimal alignment raises exponentially when the number of sequences to align increases. In the multiobjective version of the MSA problem, we simultaneously optimize the alignment accuracy and conservation. In this work, we present a parallel scheme for a multiobjective version of a memetic metaheuristic: Hybrid Multiobjective Memetic Metaheuristics for Multiple Sequence Alignment (H4MSA). In order to evaluate the parallel performance of H4MSA, we use several datasets with different number of sequences (up to 1000 sequences) and compare its parallel performance against other well-known parallel approaches published in the literature, such as MSAProbs, T-Coffee, Clustal O and MAFFT. On the other hand, the results reveals that parallel H4MSA is around 25 times faster than the sequential version with 32 cores.",
    "actual_venue": "Trustcom/Bigdatase/Ispa"
  },
  {
    "abstract": "A US law, the electronic signatures (E-Sign) in Global and National Commerce Act (signed by then President Clinton on 30 June 2000 with an effective date of 1 October 2000), grants electronic signatures legal validity equivalent to traditional handwritten counterparts. The intention of this law is to cut costs while providing more stringent security. In the emerging e-commerce arena, electronic signatures hold great potential for facilitating secure electronic transactions. But signatures are used in many critical business processes that occur prior to or independent of final transactions. Contract development and numerous other processes entail a series of draft modifications and sign-offs. Can electronic signatures provide cost savings and security in these activities? In this paper, we (i) detail fundamentals and the current status of electronic signatures; (ii) describe the integration of electronic signatures with electronic verification and authentication technologies; (iii) explore e-commerce applications, especially document management processes, that could benefit from adopting electronic signatures; and (iv) propose modifications to the electronic signature process to enable innovative document management processes. We propose modifications using partial document ownership, soft signatures, and hard signatures.",
    "actual_venue": "Information And Management"
  },
  {
    "abstract": "The application of Dynamic Voltage Scaling (DVS) to reduce energy consumption may have a detrimental impact on the quality of manufacturing tests employed to detect permanent faults. This paper analyses the influence of different voltage/frequency settings on fault detection within a DVS application. In particular, the effect of supply voltage on different types of delay faults is considered. This paper presents a study of these problems with simulation results. We have demonstrated that the test application time increases as we reduce the test voltage. We have also shown that for newer technologies we do not have to go to very low voltage levels for delay fault testing. We conclude that it is necessary to test at more than one operating voltage and that the lowest operating voltage does not necessarily give the best fault cover.",
    "actual_venue": "European Test Symposium"
  },
  {
    "abstract": "An important issue in open agent systems such as the Internet is the discovery of service providers by potential consumers (requesters). This paper is concerned with services that involve the ongoing provision of up-to-date information to requesters. We explore three separate issues: subscription to an information provider for ongoing provision of information; monitoring for new information providers; and maintaining awareness of when providers disappear from the system. We explore several models for how this functionality may best be provided, with emphasis on the ways in which certain choices affect the overall system; and provide an analysis of preferred design options for environments with different characteristics.",
    "actual_venue": "Aois"
  },
  {
    "abstract": "Operation Execution Groups (OEGs) address the problem of efficient execution of a sequence of long-distance remote object invocations in widely distributed, cross-organizational systems. OEGs are designed to improve the performance of operation processing with respect to client response time, using distributed replication that can be implemented without significant impact on existing server configurations and server objects. We present the general pattern of our solution, and describe and discuss a CORBA-based OEG-implementation.",
    "actual_venue": "Tools"
  },
  {
    "abstract": "Internet of Things (IoT) systems and applications are increasingly deployed for critical use cases and therefore exhibit an increasing need for dependability. Data provenance deals with the recording, management and retrieval of information about the origin and history of data. We propose that the introduction of data provenance concepts into the IoT domain can help create dependable and trustworthy IoT systems by recording the lineage of data from basic sensor readings up to complex derived information created by software agents. In this paper, we present a data provenance model for IoT systems that is geared towards providing a generic mechanism for assuring the correctness and integrity of IoT applications and thereby reinforcing their trustworthiness and dependability for critical use cases.",
    "actual_venue": "Icsoc Workshops"
  },
  {
    "abstract": "In this paper we present and evaluate a map-based RGB-D SLAM (Simultaneous Localization and Mapping) system employing a novel idea of combining efficient visual odometry and a persistent map of 3D point features used to jointly optimize the sensor (robot) poses and the feature positions. The optimization problem is represented as a factor graph. The SLAM system consists of a front-end that tracks the sensor frame-by-frame, extracts point features, and associates them with themap, and a back-end that manages and optimizes the map. We propose a robust approach to data association, which combines efficient selection of candidate features from the map, matching of visual descriptors guided by the sensor pose prediction from visual odometry, and verification of the associations in both the image plane and 3D space. The improved accuracy and robustness is demonstrated on publicly available data sets.",
    "actual_venue": "Robot : Second Iberian Robotics Conference: Advances In Robotics"
  },
  {
    "abstract": "In the real world problems, we may face the cases when parameters of linear programming problems are not known exactly. In such cases, parameters can be treated as random variables or possibilistic variables. The probability distribution which random variables obey are not always easily obtained because they are assumed to be obtained by strict measurement owing to the cardinality of the probability. On the other hand, the possibility distribution restricting possibilistic variables can be obtained rather easily because they are assumed to be obtained from experts' perception owing to the ordinality of possibility. Then possibilistic programming approach would be convenient as an optimization technique under uncertainty. In this talk, we review possibilistic linear programming approaches to robust optimization. Possibilistic linear programming approaches can be classified into three cases: optimizing approach, satisficing approach and two-stage approach [1].Because the third approach has not yet been very developed, we focus on the other two approaches. First we review the optimization approach. We describe a necessarily optimal solution [2] as a robust optimal solution in the optimization approach. Because a necessarily optimal solution do not always exist, necessarily soft optimal solutions [3] have been proposed. In the necessarily soft optimal solutions, the optimality conditions is relaxed to an approximate optimality conditions. The relation to minimax regret solution [4,5] is shown and a solution procedure for obtaining a best necessarily soft optimal solution is briefly described. Next we talk about the modality constrained programming approach [6]. A robust treatment of constraints are introduced. Then the necessity measure optimization model and necessity fractile optimization model are described as treatments of an objective function. They are models from the viewpoint of robust optimization. The simple models can preserve the linearity of the original problems. We describe how much we can generalize the simple models without great loss of linearity. A modality goal programming approach [7] is briefly introduced. By this approach, we can control the distribution of objective function values by a given goal. Finally, we conclude this talk by giving future topics in possibilistic linear programming [8,9,10].",
    "actual_venue": "Mdai"
  },
  {
    "abstract": "Asingle-stage bridgeless ac-dc PFC converter using a lossless passive snubber and valley switching is proposed. The proposed converter is based on a two-stage bridgeless boost-flyback converter. In the proposed converter, the conduction losses are reduced by removing an input full-bridge diode rectifier. The boost inductor is designed to be operated in the discontinuous-conduction mode for achieving high power factor. In the flyback module, the couple inductor that provides input-output electrical isolation for safety is designed to be operated in the critical-conduction mode for low RMS current and low turn-on switching loss by using valley-switching operation. Because of the lossless snubber circuit, the voltage spike of switch is clamped, and the leakage inductor energy is recycled. The snubber capacitor is used as a dc-bus capacitor, which is divided into two capacitors. In addition, some input power is directly conducted to the output, and the remaining power is stored in dc-bus capacitor. So, low-voltage rating capacitors can be used as the dc-bus capacitor and power transfer efficiency is improved. The presented theoretical analysis is verified on an output 48-V and 60-Wexperimental prototype.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Context: Mastroeni and Zanardini introduced the notion of semantics-based data dependences, both at concrete and abstract domains, that helps in converting the traditional syntactic Program Dependence Graphs (PDGs) into more refined semantics-based (abstract) PDGs by disregarding some false dependences from them. As a result, the slicing techniques based on these semantics-based (abstract) PDGs result in more precise slices. Aim: The aim of this paper is to further refine the slicing algorithms when focusing on a given property. Method: The improvement is obtained by (i) applying the notions of semantic relevancy of statements and semantic data dependences, and (ii) combining them with conditional dependences. Result: We provide an abstract slicing algorithm based on semantics-based abstract Dependence Condition Graphs (DCGs) that enable us to identify the conditions for dependences between program points.",
    "actual_venue": "Science Of Computer Programming"
  },
  {
    "abstract": "A number of continuous-phase modulation methods suitable for digital and mobile radio applications has been grouped in a common class, which has been named 12PM3 (12-state, phase modulation with correlation over three consecutive bits). This paper introduces the definition of this class of modulation schemes, and presents some of its features. With reference to the scheme that gives the best spectral compactness, some effects of typical implementation imperfections of the modulation circuits are discussed. The performance of some 12PM3 modulators associated with a frequency discrimination receiver is then computed with reference to both AWGN (additive white Gaussian noise) channels and the typical propagation conditions envisaged for SCPC/FDMA digital land mobile radio systems in urban environment. The adopted pre and postdiscrimination filters of the receiver are optimized for the best performance. Different decision techniques are considered, namely 2-TH (2-threshold), 4-TH, mixed 2-TH/4-TH and MLSE (maximum-likelihood sequence estimation) techniques.",
    "actual_venue": "Ieee Journal On Selected Areas In Communications"
  },
  {
    "abstract": "Due to the strong increase of processing units available to the end user, expressing parallelism of an algorithm is a major challenge for many researchers. Parallel applications are often expressed using a task-parallel model (task graphs), in which tasks can be executed concurrently unless they share a dependency. If these tasks can also be executed in a data-parallel fashion, e.g., by using MPI or OpenMP, then we call it a mixed-parallel programming model. Mixed-parallel applications are often modeled as directed a cyclic graphs (DAGs), where nodes represent the tasks and edges represent data dependencies. To execute a mixed-parallel application efficiently, a good scheduling strategy is required to map the tasks to the available processors. Several algorithms for the scheduling of mixed-parallel applications onto a homogeneous cluster have been proposed. MCPA (Modified CPA) has been shown to lead to efficient schedules. In the allocation phase, MCPA considers the total number of processors allocated to all potentially concurrently running tasks as well as the number of processors in the cluster. In this article, it is shown how MCPA can be extended to obtain a more balanced workload in situations where concurrently running tasks differ significantly in the number of operations. We also show how the allocation procedure can be tuned in order to deal not only with regular DAGs (FFT), but also with irregular ones. We also investigate the question whether additional optimizations of the mapping procedure, such as packing of allocations or backfilling, can reduce the make span of the schedules.",
    "actual_venue": "Cluster, Cloud And Grid Computing"
  },
  {
    "abstract": "The degree of predictability of a sequence can be measured by its entropy and it is closely related to its repetitiveness and compressibility. Entropic profiles are useful tools to study the under- and over-representation of subsequences, providing also information about the scale of each conserved DNA region. On the other hand, compact classes of repetitive motifs, such as maximal motifs, have been proved to be useful for the identification of significant repetitions and for the compression of biological sequences. In this paper we show that there is a relationship between entropic profiles and maximal motifs, and in particular we prove that the former are a subset of the latter. As a further contribution we propose a novel linear time linear space algorithm to compute the function Entropic Profile introduced by Vinga and Almeida in [18], and we present some preliminary results on real data, showing the speed up of our approach with respect to other existing techniques.",
    "actual_venue": "Algorithms In Bioinformatics"
  },
  {
    "abstract": "In this paper, we propose a new fuzzy interpolative reasoning method for sparse fuzzy rule-based systems based on the ratio of fuzziness of polygonal rough-fuzzy sets, where the values of the antecedent variables and the consequence variables in the fuzzy rules are represented by polygonal rough-fuzzy sets. The experimental results show that the proposed fuzzy interpolative reasoning method outperforms the existing method for fuzzy interpolative reasoning in sparse fuzzy rule-based systems.",
    "actual_venue": "Intelligent Information And Database Systems, Pt"
  },
  {
    "abstract": "This paper presents a design method of critically sampled graph wavelet transforms (CSGWTs) utilizing real-valued biorthogonal linear-phase wavelets for regular signals. Their filter characteristics are equivalent to those of biorthogonal linear-phase wavelets and can be expressed by real-valued closed-form with the sum of sinusoidal waves in the graph spectral domain. The proposed CSGWTs satisfy the perfect reconstruction condition for graph signals. Since the proposed filters are smooth functions, they are well-behaved even if we use a lower-order polynomial approximation. The performance of the proposed CSGWTs is evaluated by comparison with existing CSGWTs.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "Use cases and user interface prototypes are agile techniques used to specify required functionalities of an object-oriented\n system. Testing can find most errors of the software and ensure that the requirement specifications are satisfied by the application.\n We can derive test cases from use cases and run them in user interfaces, but we do it in a mechanical way. In this paper,\n we propose a new approach for automating the generation of user interface prototypes and test cases for web applications.\n Starting from a formalization of the requirements based on controlled use cases extended with the system glossary and the\n user interface specifications, we automatically generate web pages and test cases which are passed as parameters for an automated\n web testing tool.",
    "actual_venue": "Isse"
  },
  {
    "abstract": "The particle swarm optimization algorithm includes three vectors associated with each particle: inertia, personal, and social influence vectors. The personal and social influence vectors are typically multiplied by random diagonal matrices (often referred to as random vectors) resulting in changes in their lengths and directions. This multiplication, in turn, influences the variation of the particles in the swarm. In this paper we examine several issues associated with the multiplication of personal and social influence vectors by such random matrices, these include: (1) Uncontrollable changes in the length and direction of these vectors resulting in delay in convergence or attraction to locations far from quality solutions in some situations (2) Weak direction alternation for the vectors that are aligned closely to coordinate axes resulting in preventing the swarm from further improvement in some situations, and (3) limitation in particle movement to one orthant resulting in premature convergence in some situations. To overcome these issues, we use randomly generated rotation matrices (rather than the random diagonal matrices) in the velocity updating rule of the particle swarm optimizer. This approach makes it possible to control the impact of the random components (i.e. the random matrices) on the direction and length of personal and social influence vectors separately. As a result, all the above mentioned issues are effectively addressed. We propose to use the Euclidean rotation matrices for rotation because it preserves the length of the vectors during rotation, which makes it easier to control the effects of the randomness on the direction and length of vectors. The direction of the Euclidean matrices is generated randomly by a normal distribution. The mean and variance of the distribution are investigated in detail for different algorithms and different numbers of dimensions. Also, an adaptive approach for the variance of the normal distribution is proposed which is independent from the algorithm and the number of dimensions. The method is adjoined to several particle swarm optimization variants. It is tested on 18 standard optimization benchmark functions in 10, 30 and 60 dimensional spaces. Experimental results show that the proposed method can significantly improve the performance of several types of particle swarm optimization algorithms in terms of convergence speed and solution quality.",
    "actual_venue": "Journal Of Heuristics"
  },
  {
    "abstract": "This paper introduces an original method for target tracking in wireless sensor networks that combines machine learning and Kalman filtering. A database of radio-fingerprints is used, along with the ridge regression learning method, to compute a model that takes as input RSSI information, and yields, as output, the positions where the RSSIs are measured. This model leads to a position estimate for each target. The Kalman filter is used afterwards to combine the model's estimates with predictions of the target's positions based on acceleration information, leading to more accurate ones.",
    "actual_venue": "Sensor Array And Multichannel Signal Processing Workshop"
  },
  {
    "abstract": "Karasev conjectured that for any set of 3k lines in general position in the plane, which is partitioned into 3 color classes of equal size k, the set can be partitioned into k colorful 3-subsets such that all the triangles formed by the subsets have a point in common. Although the general conjecture is false, we show that Karasev's conjecture is true for lines in convex position. We also discuss possible generalizations of this result.",
    "actual_venue": "Computational Geometry"
  },
  {
    "abstract": "This paper focuses on the degree of freedom and number of subdeterminants in a Pearson residual in a multiway contingency table. The results show that multidimensional residuals are represented as linear sum of determinants of 2 x 2 submatrices, which can be viewed as information granules measuring the degree of statistical dependence. Geometrical interpretation of Pearson residual is investigated. Furthermore, the number of subdeterminants in a residual is equal to the degree of freedom in (2)-test statistic. Since the way of calculation of the number of subdeterminants corresponds to the construction of a statistical model for a contingency table, it has been found that the combinatorics of the number subdeterminants is closely related with permutation of attributes in a given table, where symmetric group may play an important role.",
    "actual_venue": "International Journal Of Intelligent Systems"
  },
  {
    "abstract": "Multi-objective design optimization of a nano-CMOS voltage-controlled oscillator (VCO).Frequency of oscillation, average dynamic power and leakage power are the target objectives.The novel game-theoretic differential evolution (GTDE) algorithm is proposed.Differential evolution (DE), particle swarm optimization (PSO) and game-theoretic differential evolution (GTDE) algorithms are implemented.New improved local optimum for the design of the nano-CMOS VCO was obtained. Engineering problems presenting themselves in a multiobjective setting have become commonplace in most industries. In such situations the decision maker (DM) requires several solution options prior to selecting the best or the most attractive solution with respect to the current industrial circumstances. The weighted sum scalarization approach was employed in this work in conjunction with three metaheuristic algorithms: particle swarm optimization (PSO), differential evolution (DE) and the improved DE algorithm (GTDE) (which was enhanced using ideas from evolutionary game theory). These methods are then used to generate the approximate Pareto frontier to the nano-CMOS voltage-controlled oscillator (VCO) design problem. Some comparative studies were then carried out to compare the proposed method as compared to the standard DE approach. Examination on the quality of the solutions across the Pareto frontier obtained using these algorithms was carried out using the hypervolume indicator (HVI).",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "•In post-digital society print newspapers are sustainable.•Print and electronic media sustainability are compared.•Sustainable practices of media use are analyzed.•Newspapers contribute to information on sustainability.",
    "actual_venue": "Telematics And Informatics"
  },
  {
    "abstract": "Machine translation is often not enough for people to engage across languages due to translation errors and lack of cultural background. In addressing these challenges, my dissertation explores how AI-augmented analytics can improve computer-mediated communication between speakers of different native languages. First, to support better sense making of foreign language posts in social media, I designed SenseTrans, a tool that adds contextual information using AI-analytics such as sentiment analysis. In my future work, I intend to explore 1) how people perceive, interpret and make use of AI-generated information, and 2) how AI-augmented analytics could be applied to other settings.",
    "actual_venue": "Cscw Companion"
  },
  {
    "abstract": "The standard Sugeno integral has several equivalent ways to be introduced. This equivalence fails when generalizing the standard capacities into level dependent capacities. We discuss several possible types of Sugeno integral based on level dependent capacities. An illustrative example is added. Relations between different types of level dependent capacities-based Sugeno integrals are presented, showing that there are exactly three different types of studied integrals.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "This work presents a new proposal FPGA-based for low cost measurement of AM/AM and AM-PM distortion curves in RF Power Amplifiers through DSP Builder tool into a Development Board. A practical comparison between state-of-the-art reported works based on measurements of AM/AM and AM-PM distortion curves is discussed, in terms of low cost measurement, by using a FPGA development board and a PC to obtain the PA distortion curves adding flexibility to read the PA behavior. The main contribution is the full control of the digital PA behavior based on the phase to amplitude conversion principle simulated in Matlab-Simulink. This test bed is able to recalculate the known AM/AM and AM/PM measurements stored as LUT in a previous development board but also a real PA as DUT can be added.",
    "actual_venue": "Electronics, Communications And Computers"
  },
  {
    "abstract": "In this paper, we focus on blind source cell-phone identification problem. It is known various artifacts in the image processing pipeline, such as pixel defects or unevenness of the responses in the CCD sensor, black current noise, proprietary interpolation algorithms involved in color filter array [CFA] leave telltale footprints. These artifacts, although often imperceptible, are statistically stable and can be considered as a signature of the camera type or even of the individual device. For this purpose, we explore a set of forensic features, such as binary similarity measures, image quality measures and higher order wavelet statistics in conjunction SVM classifier to identify the originating cell-phone type. We provide identification results among 9 different brand cell-phone cameras. In addition to our initial results, we applied a set of geometrical operations to original images in order to investigate how much our proposed method is robust under these manipulations.",
    "actual_venue": "Proceedings Of The Society Of Photo-Optical Instrumentation Engineers"
  },
  {
    "abstract": "There is a continuous function which is one-to-one mapping from the support of the original fuzzy set into the support of the image fuzzy set. With the aid of the extension principle, this paper investigates the entropy relationship between the original fuzzy set and image fuzzy set. We also derive some useful formulas by which the entropy of the image fuzzy set can be obtained directly without calculating its membership function. (C) 1999 Elsevier Science B.V. All rights reserved.",
    "actual_venue": "Fuzzy Sets And Systems"
  },
  {
    "abstract": "Implementation of carrier-sensing-based medium access control (MAC) protocols on inexpensive reconfigurable radio platforms has proven challenging due to long and unpredictable delays associated with both signal processing on a general purpose processor (GPP) and the interface between the RF front-end and the GPP. This paper describes the development and implementation of a split-functionality architecture for a contention-based carrier-sensing MAC, in which some of the functions reside on an FPGA (field programmable gate array) and others reside in the GPP. We provide an FPGA-based implementation of a carrier sensing block and develop two versions of a CSMA MAC protocol based upon this block. We experimentally test the performance of the resulting protocols in a multihop environment in terms of end-to-end throughput and required frame retransmissions. We cross-validate these results with a network simulator with modules modified to reflect the mean and variance of delays measured in components of the real software-defined radio system.",
    "actual_venue": "Cognitive Radio Oriented Wireless Networks"
  },
  {
    "abstract": "The connection between Bayesian statistics and the technique of regularization for inverse problems has been given significant attention in recent years. For example, Bayes' law is frequently used as motivation for variational regularization methods of Tikhonov type. In this setting, the regularization function corresponds to the negative-log of the prior probability density; the fit-to-data function corresponds to the negative-log of the likelihood; and the regularized solution corresponds to the maximizer of the posterior density function, known as the maximum a posteriori (MAP) estimator of the unknown, which in our case is an image. Much of the work in this direction has focused on the development of techniques for efficient computation of MAP estimators (or regularized solutions). Less explored in the inverse problems community, and of interest to us in this paper, is the problem of sampling from the posterior density. To do this, we use a Markov chain Monte Carlo (MCMC) method, which has previously appeared in the Bayesian statistics literature, is straightforward to implement, and provides a means of both estimation and uncertainty quantification for the unknown. Additionally, we show how to use the preconditioned conjugate gradient method to compute image samples in cases where direct methods are not feasible. And finally, the MCMC method provides samples of the noise and prior precision (inverse-variance) parameters, which makes regularization parameter selection unnecessary. We focus on linear models with independent and identically distributed Gaussian noise and define the prior using a Gaussian Markov random field. For our numerical experiments, we consider test cases from both image deconvolution and computed tomography, and our results show that the approach is effective and surprisingly computationally efficient, even in large-scale cases.",
    "actual_venue": "Siam Journal On Scientific Computing"
  },
  {
    "abstract": "Social network games in Facebook are played by millions of players on daily basis. Due to their design characteristics, new challenges for game design and playability evaluations arise. We present a study where 18 novice inspectors evaluated a social game using playability heuristics. The objective is to explore possible domain-specific playability problems and to examine how the established heuristics suit for evaluating social games. The results from this study show that some implementations of the social games design characteristics can cause playability problems and that the established heuristics are suitable for evaluating social games. The study also revealed that inspectors had problems in interpreting cause and effect of the found problems.",
    "actual_venue": "Advances In Computer Entertainment"
  },
  {
    "abstract": "In this paper, we investigate the end-to-end performance of dual-hop relaying systems with beamforming over Nakagami-m fading channels. Our analysis considers semi-blind (fixed-gain) relays with single antennas, and source and destination nodes equipped with multiple antennas. Closed-form expressions for the outage probability (OP), moment generating function (MGF), and generalized moments of the end-to-end signal-to-noise ratio (SNR) are derived. The proposed expressions apply to general operating scenarios with distinct Nakagami-m fading parameters and average SNRs between the hops. The influence of the power imbalance, fading parameters, and antenna configurations on the overall system performance are analyzed and discussed through representative numerical examples. Furthermore, the exactness of our formulations is validated by means of Monte Carlo simulations.",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "In the context of finite metric spaces with integer distances, we investigate the new Ramsey-type question of how many points can a space contain and yet be free of equilateral triangles. In particular, for finite metric spaces with distances in the set {1,..., n}, the number D-n is defined as the least number of points the space must contain in order to be sure that there will be an equilateral triangle in it. Several issues related to these numbers are studied, mostly focusing on low values of n. Apart from the trivial D-1 = 3, D-2 = 6, we prove that D-3 = 12, D-4 = 33 and 81 less than or equal to D-5 less than or equal to 95.",
    "actual_venue": "The Electronic Journal Of Combinatorics"
  },
  {
    "abstract": "This paper presents a domotic house gateway capable of seamlessly interacting with different devices from heterogeneous domotic systems and appliances. Such a gateway also provides the possibility to automate device cooperation through an embedded rule-based engine, which can be dynamically and automatically updated to accommodate necessities and anticipate users' actions. Some practical applications will show the effectiveness of the system.",
    "actual_venue": "Acm Symposium On Applied Computing"
  },
  {
    "abstract": "The problem of selecting secondary indices for a file so as to minimize the expected transaction cost was frequently analyzed before. We prove that it is NP-complete by reducing the MINIMUM SET COVER problem to it.",
    "actual_venue": "Sigmod Record"
  },
  {
    "abstract": "Differential signaling has been widely used in high-speed interconnects. Signal integrity issues, such as inter-symbol interference (ISI) and crosstalk between the differential pair, however, still cause significant timing jitter and amplitude noise and heavily limit the performance of the differential link. The pre-emphasis filter is commonly used to reduce ISI but may potentially change the crosstalk behavior. In this paper, we first propose formula-based jitter and noise models considering the combined effect of ISI, crosstalk, and pre-emphasis filter. With the same set of input patterns, experiment shows our models achieve within 5% difference compared with SPICE simulation. By utilizing these formula-based models, we then develop algorithms to directly find out the input patterns for worst-case jitter and worst-case amplitude noise through pseudo-Boolean optimization (PBO) and mathematical programming. In addition, a heuristic algorithm is proposed to further reduce runtime. Experiments show our algorithms obtain more reliable worst-case jitter and noise compared with pseudorandom bit sequences simulation and, meanwhile, reduce runtime by 25× when using a general PBO solver and by 150× when using our proposed heuristic algorithm.",
    "actual_venue": "Ieee Transactions On Very Large Scale Integration Systems"
  },
  {
    "abstract": "The problem of fine-grained object recognition is very challenging due to the subtle visual differences between different object categories. In this paper, we propose a task-driven progressive part localization (TPPL) approach for fine-grained object recognition. Most existing methods follow a two-step approach that first detects salient object parts to suppress the interference from background scenes and then classifies objects based on features extracted from these regions. The part detector and object classifier are often independently designed and trained. In this paper, our major finding is that the part detector should be jointly designed and progressively refined with the object classifier so that the detected regions can provide the most distinctive features for final object recognition. Specifically, we develop a part-based SPP-net (Part-SPP) as our baseline part detector. We then establish a TPPL framework, which takes the predicted boxes of Part-SPP as an initial guess, and then examines new regions in the neighborhood using a particle swarm optimization approach, searching for more discriminative image regions to maximize the objective function and the recognition performance. This procedure is performed in an iterative manner to progressively improve the joint part detection and object classification performance. Experimental results on the Caltech-UCSD-200-2011 dataset demonstrate that our method outperforms state-of-the-art fine-grained categorization methods both in part localization and classification, even without requiring a bounding box during testing.",
    "actual_venue": "Ieee Trans Multimedia"
  },
  {
    "abstract": "In this paper, we investigate a multi-node multi-antenna wireless-powered sensor networks (WPSN) comprised of one power beacon and multiple sensor nodes. The proposed beam-splitting technique enables a power beacon to split microwave energy beams towards multiple nodes for simultaneous charging. We propose an optimal joint beam-splitting and energy neutral control algorithm for perpetual operation of multiple sensor nodes. We have implemented a real WPSN testbed and conducted experiments. We have shown that the proposed algorithm can successfully keep all sensor nodes alive by optimally splitting energy beams to sensor nodes.",
    "actual_venue": "Ieee International Conference On Communication Workshop"
  },
  {
    "abstract": "This paper presents one solution of the video media server (VMS) extension for an Android based digital television (DTV) set-top box. Proposed VMS extension will provide simultaneous live playback and personal video recorder (PVR) functions on various DTV services, as well as adaptive streaming of DTV content on second screen devices over MPEG-DASH protocol. Furthermore, VMS can be controlled by second screen devices.",
    "actual_venue": "Ieee International Conference On Consumer Electronics - Berlin"
  },
  {
    "abstract": "Although the application of data fusion in information retrieval has yielded good results in the majority of the cases, it has been noticed that its achievement is dependent on the quality of the input result lists. In order to tackle this problem, in this paper we explore the combination of only the n-top result lists as an alternative to the fusion of all available data. In particular, we describe a heuristic measure based on redundancy and ranking information to evaluate the quality of each result list, and, consequently, to select the presumably n-best lists per query. Preliminary results in four IR test collections, containing a total of 266 queries, and employing three different DF methods are encouraging. They indicate that the proposed approach could significantly outperform the results achieved by fusion all available lists, showing improvements in mean average precision of 10.7%, 3.7% and 18.8% when it was used along with Maximum RSV, CombMNZ and Fuzzy Borda methods.",
    "actual_venue": "Cicling"
  },
  {
    "abstract": "We present an SMT encoding of a generalized version of the subterm criterion and evaluate its implementation in TTT2.",
    "actual_venue": "Arxiv: Logic In Computer Science"
  },
  {
    "abstract": "Online music faces two significant challenges: on the demand side, limited willingness of the end consumer to pay for digital content can be found and forces music providers to (re-)evaluate new direct and indirect revenue models. On the supply side, uncertainty exists whether online music is a private or a public good due to the lack of efficient exclusion mechanisms. Innovative business models like filesharing communities, music service provider, subscription and super-distribution are described and positioned in a scenario matrix.",
    "actual_venue": "Wirtschaftsinformatik"
  },
  {
    "abstract": "We introduce the notion of functional stream derivative, generalising the notion of input derivative of rational expressions (Brzozowski 1964) to the case of stream functions over arbitrary input and output alphabets. We show how to construct Mealy automata from algebraically specified stream functions by the symbolic computation of functional stream derivatives. We illustrate this construction in full detail for various bitstream functions specified in the algebraic calculus of the 2-adic numbers. This work is part of a larger ongoing effort to specify and model component connector circuits in terms of (functions and relations on) streams.",
    "actual_venue": "Electronic Notes In Theoretical Computer Science"
  },
  {
    "abstract": "Self Modifying CGP (SMCGP) is a developmental form of Cartesian Genetic Programming(CGP). It is able to modify its own phenotype during execution of the evolved program. This is done by the inclusion of modification operators in the function set. Here we present the use of the technique on several different sequence generation and regression problems.",
    "actual_venue": "Eurogp"
  },
  {
    "abstract": "We present the design and implementation details of a time-division demultiplexing/multiplexing based scan architecture using serializer/deserializer. This is one of the key DFT features implemented on NVIDIA's Fermi family GPU (Graphic Processing Unit) chips. We provide a comprehensive description on the architecture and specifications. We also depict a compact serializer/deserializer module design, test timing consideration, design rule and test pattern verification. Finally, we show silicon data collected from Fermi GPUs.",
    "actual_venue": "Ieee Vlsi Test Symposium"
  },
  {
    "abstract": "We present a structured learning approach to semantic annotation of RGB-D images. Our method learns to reason about spatial relations of objects and fuses low-level class predictions to a consistent interpretation of a scene. Our model incorporates color, depth and 3D scene features, on which an energy function is learned to directly optimize object class prediction using the loss-based maximum-margin principle of structural support vector machines. We evaluate our approach on the NYU V2 dataset of indoor scenes, a challenging dataset covering a wide variety of scene layouts and object classes. We hard-code much less information about the scene layout into our model then previous approaches, and instead learn object relations directly from the data. We find that our conditional random field approach improves upon previous work, setting a new state-of-the-art for the dataset.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "In this work we propose the method for a rather unexplored problem of computer vision - discriminatively trained dense surface normal estimation from a single image. Our method combines contextual and segment-based cues and builds a regressor in a boosting framework by transforming the problem into the regression of coefficients of a local coding. We apply our method to two challenging data sets containing images of man-made environments, the indoor NYU2 data set and the outdoor KITTI data set. Our surface normal predictor achieves results better than initially expected, significantly outperforming state-of-the-art.",
    "actual_venue": "Computer Vision - Eccv , Pt"
  },
  {
    "abstract": "The evaluation of research beyond scientific impact is increasingly required, but has not yet been widely applied. One reason is that data necessary for the evaluation of the societal impact of science are often not available in sufficient quantities or suitable form. This paper describes first results from a research project that develops improved documentation to serve evaluation beyond scientific impact. Firstly, we refer to the need to do this, and to the specific challenges for data assessment in this area. Secondly, we describe the concept for documenting achievements of research beyond scientific impact with a research information system that integrates the relevant parts of research proposals and reports. This enables data to be provided without causing additional effort for scientists, and makes them usable for scientists, research funding agencies and research institutions for different purposes, including evaluation. Compatibility of the system with interoperability standards (e.g. CERIF) is also taken into account. The concept is currently being developed and tested from the user perspectives of scientists, research funding agencies and evaluators.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "The graphical structure of a simple closed-loop supply chain network utilizing in this paper.",
    "actual_venue": "Applied Soft Computing"
  },
  {
    "abstract": "The 2-sum embedding conjecture (Lee-Sidiropoulos, FOCS 2009) states that all the shortest-path metrics supported on a family of graphs F admit a uniformly bi-Lipschitz embedding into L1 if and only if the same property holds for the closure of F under edge sums. The problem has an equivalent formulation in terms of multi-commodity flow/cut gaps and appears to be a fundamental step in resolving the well-studied question of which families of graphs admit an approximate multi-commodity max-flow/min-cut theorem. The 2-sum conjecture is known to be true when F = {K3}, where Kn denotes the complete graph on n vertices; this is equivalent to the seminal result of Gupta, Newman, Rabinovich, and Sinclair (Combinatorica, 2004) on embeddings of series-parallel graphs into L1. For F = {K4}, the conjecture was confirmed by Chakrabarti, Jaffe, Lee, and Vincent (FOCS 2008). In the present paper, we prove the conjecture holds for any finite family of graphs F. In fact, we obtain a quantitatively optimal result: Any path metric on a graph formed by 2-sums of arbitrarily many copies of Kn embeds into L1 with distortion O(log n). This is a consequence of a more general theorem that characterizes the L1 distortion of the 2-sum closure of any family F in terms of constrained embeddings of the members of F.",
    "actual_venue": "Symposium On Computational Geometry"
  },
  {
    "abstract": "Many studies have shown that it is possible to recognize people by the way they walk. However, there are a number of covariate factors that affect recognition performance. The time between capturing the gallery and the probe has been reported to affect recognition the most. To date, no study has shown the isolated effect of time, irrespective of other covariates. Here we present the first principled study that examines the effect of elapsed time on gait recognition. Using empirical evidence we have shown for the first time that elapsed time does not affect recognition significantly in the short to medium term. By controlling clothing, a Correct Classification Rate (CCR) of 95% has been achieved over 9 months, on a dataset of nearly 2000 gait sequences/samples. We have created a new multimodal temporal database to enable the research community to investigate various gait and face covariates in a formal manner. Our results show that gait can be used as a reliable biométrie over time and at a distance. We have demonstrated that clothing drastically affects performance regardless of elapsed time. A move towards developing appearance invariant recognition algorithms is essential.",
    "actual_venue": "Biometrics: Theory, Applications And Systems"
  },
  {
    "abstract": "The process of constructing a parametric quadratic polynomial with four data points is discussed and a new method for determining knots in parametric curve interpolation is presented. The method has a parametric polynomial reproduction degree of two, i.e., an interpolation scheme which reproduces quadratic polynomials would reproduce parametric quadratic polynomials if the new method is used to construct knots in the interpolation process. Testing results on the efficiency of the new method are also included.",
    "actual_venue": "Computer Aided Geometric Design"
  },
  {
    "abstract": "Game manuals and tutorial scenarios are insufficient for new players to learn games of deep complexity such as highly realistic tactical simulations of modern battlefields. Adding post-game after-action reviews improves the situation, but these typically do not provide guidance during the mission and tend to focus on quantitative feedback, rather than specifics about what the player did wrong and how to improve. Intelligent tutoring system (ITS) technology provides a higher level of interactivity and a more specific qualitative analysis to guide players during game play. This use of an AI technology is demonstrated with the integration of an ITS component with the tactical simulation Armored Task Force (ATF) resulting in a combined system called the the Virtual Combat Training Center (V-CTC). V-CTC simulates the Army's combat training center at Fort Irwin and its instructors, called observer / controllers. The ATF game itself was modified to send an event stream over TCP-IP sockets to the ITS component, which interprets the events and acts accordingly. V-CTC was originally intended for a military context: either classroom use, field instruction, or embedded deployment. However, in non-military games, tutors (or non-player characters acting in that role) may well enhance the gaming experience of players. Such players might otherwise become frustrated with learning very challenging games, or simply fail to appreciate the tactical possibilities and depth of strategy possible in a well-designed game.",
    "actual_venue": "Aiide"
  },
  {
    "abstract": "The Bip intelligent business solution is the basics for all business systems. It is a total solution for enterprise or SME business application implementation. This business process building block offers an easy-to-use practical tool to customize business needs in a short time, usually in hands on the user. In line with the latest application development universe, user-maintained rule-based application engine with easily customizable screen flows are the basics provided in the building block. Toppling with the latest business intelligence capability and enhanced graphic interface makes the framework a good landing base towards a swift to the long-standing programming platform.",
    "actual_venue": "AMT"
  },
  {
    "abstract": "A fundamental challenge in networked systems is detection and removal of suspected malicious nodes. In reality, detection is always imperfect, and the decision about which potentially malicious nodes to remove must trade off false positives (erroneously removing benign nodes) and false negatives (mistakenly failing to remove malicious nodes). However, in network settings this conventional tradeoff must now account for node connectivity. In particular, malicious nodes may exert malicious influence, so that mistakenly leaving some of these in the network may cause damage to spread. On the other hand, removing benign nodes causes direct harm to these, and indirect harm to their benign neighbors who would wish to communicate with them. We formalize the problem of removing potentially malicious nodes from a network under uncertainty through an objective that takes connectivity into account. We show that optimally solving the resulting problem is NP-Hard. We then propose a tractable solution approach based on a convex relaxation of the objective. Finally, we experimentally demonstrate that our approach significantly outperforms both a simple baseline that ignores network structure, as well as a state-of-the-art approach for a related problem, on both synthetic and real-world datasets.",
    "actual_venue": "Adaptive Agents And Multi Agents Systems"
  },
  {
    "abstract": "small number of informed agents can control opinion formation in complex networks.To infuence the society, we affect the nodes with small degrees are connected to hubs.Small in-degree with large out-degree provides efficient influence and propagation.Informed agents are more infuential in disassortative networks than assortative ones.The proposed method is more effective than using high centrality in opinion formation. Control of collective behavior is one of the most desirable goals in many applications related to social networks analysis and mining. In this work we propose a simple yet effective algorithm to control opinion formation in complex networks. We aim at finding the best spreaders whose connection to a reasonable number of informed agents results in the best performance. We consider an extended version of the bounded confidence model in which the uncertainty of each agent is adaptively controlled by the network. A number of informed agents with the desired opinion value is added to the network and create links with other agents such that large portion of the network follows their opinions. We propose to connect the informed agents to nodes with small in-degrees and high out-degree that are connected to high in-degree nodes. Our experimental results on both model and real social networks show superior performance of the proposed method over the state-of-the-art heuristic methods in the facet of opinion formation models.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.",
    "actual_venue": "Leipzig"
  },
  {
    "abstract": "In 3GPP (Third Generation Partnership Project) Release5 (R5), HSDPA (High-Speed Downlink Packet Access) has been standardized to enhance the data rate of downlink packet transmission on top of WCDMA (Wideband CodeDivision Multiple Access) network. Even HSDPA supports up to 10 Mbps data rate through HS-DSCHs (High Speed-Downlink Shared Channels), an UE (User Equipment) suffers from significant handover latency since HSDPA standard does not supporting soft handover. Therefore, in this paper, we propose a new soft handover mechanism for HSDPA to reduce the handover latency by utilizing DCH (DedicatedChannel) instead of HS-DSCH. Simulation results show that the proposed soft handover mechanism reduces the packet IAT (Inter Arrival Time) less than 15 msec while HSDPA’s hard handover suffers from 190 msec packet IAT. It means that the proposed mechanism guarantees the minimum packet IAT requirement of the typical VoIP service in 20 msec. Furthermore, the proposed mechanism employees no service disruption for the data connection while an UE performs the soft handover procedure.",
    "actual_venue": "Iitsi"
  },
  {
    "abstract": "Purpose: To describe the structure of the social network of junior high school students from a low socioeconomic status and assess the association between centrality measurements and academic performance. In the centrality positions (eigenvector, closeness, degree, and betweenness), the female gender and “only study” were significant predictors of high academic performance. The density of the student social network was presented by homophilic associations that hint at the existence of subcultures at school.",
    "actual_venue": "Social Networks"
  },
  {
    "abstract": "With the increased popularity of polymorphic and register spring attacks, exploit signatures intrusion detection systems (IDS) can no longer rely only on exploit signatures. Vulnerability signatures that pattern match based on properties of the vulnerability instead of the exploit should be employed. Recent research has proposed three classes of vulnerability signatures but its approach cannot address complex vulnerabilities such as the ASN.1 Double-Free. Here we introduce Petri nets as a new class of vulnerability signature that could potentially be used to detect other types of vulnerabilities. Petri nets can be automatically generated and are represented as a graph making it easier to understand and debug. We analyzed it along side the three other classes of vulnerability signatures in relation to the Windows ASN.1 vulnerability. The results were very promising due to the very low false positive rate and 0% false negative rate. We have shown that Petri nets are a very efficient, concise, and effective way of describing signatures (both vulnerability and exploit). They are more powerful than regular expressions and still efficient enough to be practical. Comparing with the other classes, only Turing machines provided a better identification rate but they incur significant performance overhead.",
    "actual_venue": "ISC"
  },
  {
    "abstract": "The lack of a unified control plane does not allow current optical networks to dynamically provision new optical paths. The IETF standardization body has proposed the Generalized Multi-Protocol Label Switching standard as a solution to this problem. Their efforts however focus on the creation of end-to-end optical trails. This concept is convenient for major service providers, whose network may span a large area, but may not suit smaller network operators. We believe that the future trend of the telecommunication industry is made up of a global network formed by many inter-linked operators of small and medium size. A novel type of network based on a distributed and disaggregated architecture seems to be the best solution to dynamic optical path establishment.In this paper we present an optical testbed implementing the concept of Optical IP Switching. Central to the system is the optical router that creates new lightpaths depending on encountered traffic flows. The decision making process is completely distributed, and perfectly fits the disaggregated network view. The testbed currently links two universities in Dublin, and may in future be extended to join other existing research networks.",
    "actual_venue": "International Conference On Testbeds And Research Infrastructures For The Development Of Networks And Communities"
  },
  {
    "abstract": "The formal methods for security protocols guarantee the security properties of protocols. Instantiation Space Logic is a new\n security protocol logic, which has a strong expressive power. Compositional Logic is also a useful security protocol logic.\n This paper analyzes the relationship between these two logics, and interprets the semantics of Compositional Logic in Instantiation\n Space model. Through our work, the interpreted Compositional Logic can be extended more easily. Moreover, those security protocols\n described in Compositional Logic can be automatically verified by the verifier of Instantiation Space. The paper also proves\n that the expressive power of Instantiation Space Logic, which can not be completely interpreted by Compositional Logic, is\n stronger than Compositional Logic.",
    "actual_venue": "Frontiers Of Computer Science In China"
  },
  {
    "abstract": "Deep architectures have been used in transfer learning applications, with the aim of improving the performance of networks designed for a given problem by reusing knowledge from another problem. In this work we addressed the transfer of knowledge between deep networks used as classifiers of digit and shape images, considering cases where only the set of class labels, or only the data distribution, changed from source to target problem. Our main goal was to study how the performance of knowledge transfer between such problems would be affected by varying the number of layers being retrained and the amount of data used in that retraining. Generally, reusing networks trained for a different label set led to better results than reusing networks trained for a different data distribution. In particular, reusing for less classes a network trained for more classes was beneficial for virtually any amount of training data. In all cases, retraining only one layer to save time consistently led to poorer performance. The results obtained when retraining for upright digits a network trained for rotated digits raise the hypothesis that transfer learning could be used to better deal with image classification problems in which only a small amount of labelled data is available for training.",
    "actual_venue": "Icmla"
  },
  {
    "abstract": "The 5G wireless networks will support massive connectivity mainly due to device-to-device communications. An enabling technology for device-to-device links is the dynamical spectrum access. The devices, which are equipped with cognitive radios, are to be allowed to reuse spectrum occupied by cellular links in an opportunistic manner [1]. The dynamical spectrum availability makes cognitive users switch between channels. Switching leads to communication overhead, delay, and energy consumption. The performance degrades even more in the presence of security threats. It is important to countermeasure security threats while meeting a desired quality of service. In this paper, we analytically model the impact of spectrum dynamics on the performance of mobile cognitive users in the presence of cognitive jammers. The spectrum occupancy is modeled as a two-state Markov chain. Our contribution is proposing a pseudorandom time hopping technique to countermeasure jamming. We achieve an analytical solution of jamming probability, switching and error probability. Based on our findings, our proposed technique out performs the frequency hopping anti-jamming technique.",
    "actual_venue": "Ieee Globecom Workshops"
  },
  {
    "abstract": "The specification and validation of security protocols often requires viewing function calls - like encryption/decryption and the generation of fake messages explicitly as actions within the process semantics. Following this approach, this paper introduces a symbolic framework based on value-passing processes able to handle symbolic values like fresh nonces, fresh keys, fake addresses and fake messages. The main idea in our approach is to assign to each value-passing process a formula describing the symbolic values conveyed by its semantics. In such symbolic processes, called constrained processes, the formulas are drawn from a logic based on a message algebra equipped with encryption, signature and hashing primitives. The symbolic operational semantics of a constrained process is then established through semantic rules updating formulas by adding restrictions over the symbolic values, as required for the process to evolve. We then prove that the logic required from the semantic rules is decidable. We also define a bisimulation equivalence between constrained processes; this amounts to a generalisation of the standard bisimulation equivalence between (non-symbolic) value-passing processes. Finally, we provide a complete symbolic bisimulation method for constructing the bisimulation between constrained processes.",
    "actual_venue": "Journal Of Universal Computer Science"
  },
  {
    "abstract": "Presented here are methods for successfully controlling software maintenance activity so that present systems will be more useful and less expensive to support. While it is based on experience at Los Alamos National Laboratory, it is not based on solutions developed and implemented there. Los Alamos is presently struggling with the problems identified in this paper and is impacted by them to the same extent as the rest of industry. An idea has emerged from this struggle: The deterioration of production software is basically a quality control problem the rate of which can and should be minimized. Many data processing shops currently have two options concerning old (over five years), marginally useful systems; pay the high cost of supporting them or undertake a rewrite. If the principles presented in this paper are applied, a third option may become available; prolonging the useful life of software by making it more cost-effective to support.",
    "actual_venue": "Afips National Computer Conference"
  },
  {
    "abstract": "In this paper we present a heuristic algorithm for the well-known Unconstrained Quadratic 0–1 Programming Problem. The approach is based on combining solutions in a genetic paradigm and incorporates intensification algorithms used to improve solutions and speed up the method. Extensive computational experiments on instances with up to 500 variables are presented and we compare our approach both with powerful heuristic and exact algorithms from the literature establishing the effectiveness of the method in terms of solutions quality and computing time.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "AbstractIn-situ analytics have been increasingly adopted by leadership scientific applications to gain fast insights into massive output data of simulations. With the current practice, systems buffer the output data in DRAM for analytics processing, constraining it to DRAM capacity un-used by the simulation. The rapid growth of data size requires alternative approaches to accommodating data-rich analytics, such as using solid-state disks to increase effective memory capacity. For this purpose, this paper explores software solutions for exploring the deep memory hierarchies expected on future high-end machines. Leveraging the fact that many analytics are sensitive to data features (regions-of-interest) hidden in the data being processed, the approach incorporates the knowledge of the data features into in-situ data management. It uses adaptive index creation/refinement to reduce the overhead of index management. In addition, it uses data features to predict data skew and improve load balance through controlling data distribution and placement on distributed staging servers. The experimental results show that such feature-guided optimizations achieve substantial improvements over state-of-the-art approaches for managing output data in-situ.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "In this work, a MIMO feed forward control for the air charge, i.e. the mass of air in the cylinders of a turbo charged spark ignition (SI) engine is presented. The control inputs are the angle of the throttle plate and the opening of the wastegate. There are many publications in this field of application. Often the control task is separated into an air charge control with the throttle plate as control input and a boost pressure control with the wastegate opening as control input. These approaches often lack the ability to precisely track reference trajectories in the transition between suction mode operation and turbocharged operation. The approach presented here is based on exact input-output linearization and enables a perfect tracking of air charge reference trajectories in both modes of operation and the transition between them.",
    "actual_venue": "Control Applications"
  },
  {
    "abstract": "Many applications deal with knowledge in the form of 'if-then rules'. In numerical data spaces the condition part of such rules is often based on intervals where the values of single variables are allowed to be within the ranges of the intervals. The interval rules can be interpreted geometrically as hyper rectangles. They can be derived heuristically by adaptive learning. In a previous approach we used cuts of membership functions of neuro-fuzzy rules for the pre-initialization of interval rules, that were reduced in their dimensions. As we showed before, such rules can be optimized by evolutionary or by ant colony algorithms to a problem-specific criterion. We demonstrate how interval rules in the chemical area of virtual screening can be optimized to characterize molecules as novel drugs. Mainly, a comparison between evolutionary and ant colony optimization is given, with and without using the neuro-fuzzy pre-initialization of interval borders. The results show that pre-initialization is more useful for the evolutionary optimization paradigm",
    "actual_venue": "Evolutionary Computation, The Ieee Congress"
  },
  {
    "abstract": "Thousands of criminal events are reported in newspapers and social networks every day. They describe violent acts that include actors, places, times, causes and any information concerning them. Verbal and nominal phrases are used to characterize and expose criminal events, which employ an important variety of natural language structures in the newspapers. In addition, causes, times and spaces of criminal events, use linguistic phrases to represent them in text. All of them need to be extracted as a pattern recognition process in order to extract criminal events from text and the information that concerns them. The extracted events, as a knowledge base, are very useful for information retrieval tasks. Therefore, this paper presents an approach based on pattern recognition in order to extract criminal events from Spanish text, by populating and enriching an ontology model. Ontology population and enrichment involve the instantiation of criminal events and their cause relationships. An evaluation process is carried out with a set of manually tagged newspapers with categories of specific events, and shows promising results.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "A major approach for palmprint recognition today is to extract fea- ture vectors corresponding to individual palmprint images and to perform palmprint matching based on some distance metrics. One of the difficult problems in feature-based recognition is that the match- ing performance is significantly influenced by many parameters in feature extraction process, which may vary depending on environ- mental factors of image acquisition. This paper presents a palmprint recognition algorithm using phase-based image matching. The use ofphase components in2D (two-dimensional)discrete Fouriertrans- forms of palmprint images makes possible to achieve highly robust palmprint recognition. Experimental evaluation using a palmprint image database clearly demonstrates an efficient matching perfor- mance of the proposed algorithm. Index Terms— image processing, pattern recognition, security, identification of persons, image recognition, pattern matching",
    "actual_venue": "Atlanta, Ga"
  },
  {
    "abstract": "Recently, several authors have proposed serial decomposition techniques for solving approximately or exactly large Markov Decision Processes. Many of these techniques rely on a clever partitioning of the state space in roughly independent parts. The different parts only communicate through relatively few communicating states. The efficiency of these decomposition methods clearly depends on the size of the set of communicating states : the smaller, the better. However, the task of finding such a decomposition with few communicating states is often (if not always) left to the user. In this paper, we present automated decomposition techniques for MDPs. These techniques are based on methods which have been developed for graph partitioning problems.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "Diversity within base classifiers has been recognized as an important characteristic of an ensemble classifier. Data and feature sampling are two popular methods of increasing such diversity. This is exemplified by Random Forests (RFs), known as a very effective classifier. However real-world data remain challenging due to several issues, such as multi-class imbalance, data redundancy, and class noise. Ensemble margin theory is a proven effective way to improve the performance of classification models. It can be used to detect the most important instances and thus help ensemble classifiers to avoid the negative effects of the class noise and class imbalance. To obtain accurate classification results, this paper proposes the Ensemble-Margin Based Random Forests (EMRFs) method, which combines RFs and a new subsampling iterative technique making use of computed ensemble margin values. As for comparative analysis, the learning techniques considered are: SVM, AdaBoost, RFs and the Subsample based Random Forests (SubRFs). The SubRFs uses Out-Of-Bag (OOB) estimation to optimize the training size. The effectiveness of EMRFs is demonstrated on both balanced and imbalanced datasets.",
    "actual_venue": "Knowledge Based Systems"
  },
  {
    "abstract": "A formal approach to deal with several types of grids in the context of multigrid algorithms is presented. The approach serves as a framework for describing the definition and manipulation of grids as well as the specification of typical grid algorithms. Furthermore, it is useful to define the transformation of high-level grid algorithms into parallel programs. The proposed method is used within the Suspense specification and transformation system to handle simple and staggered logically rectangular grids.",
    "actual_venue": "I4Cs"
  },
  {
    "abstract": "EMISAR is a high-resolution (2×2 m), fully polarimetric, dual-frequency (L- and C-band) synthetic aperture radar (SAR) system designed for remote-sensing applications. The SAR is operated at high altitudes on a Gulfstream G-3 jet aircraft. The system is very well calibrated and has low sidelobes and low cross-polar contamination. Digital technology has been utilized to realize a flexible and highly stable radar with variable resolution, swath width, and imaging geometry. Thermal control and several calibration loops have been built into the system to ensure system stability and absolute calibration. Accurately measured antenna gains and radiation patterns are included in the calibration. The processing system is developed to support data calibration, which is the key to most of the current applications. Recent interferometric enhancements are important for many scientific applications",
    "actual_venue": "Geoscience And Remote Sensing, Ieee Transactions"
  },
  {
    "abstract": "The significance of intangible resources for business success has evidently increased. Analyses have indicated some intellectual capital factors as the most prominent and important, yet the impact on the intra- and inter-organisational innovation ecosystems has not been analysed thoroughly. This contribution seeks to close this gap and draw conclusions with regard to drivers of innovation and related differences between manufacturing and service enterprises. The analysis of the correlation between intellectual capital and innovation capabilities allows statements regarding those intellectual capital factors, onto which enterprises should focus to foster innovation. The qualitative content-related analysis of the 38 intellectual capital statements of German enterprises with regard to new and different types of innovation management methodologies allows the identification of existing gaps in intellectual capital management.",
    "actual_venue": "International Journal Of Knowledge And Learning"
  },
  {
    "abstract": "Selective visualization is a solution for visualizing data of large size and dimensionality. In this paper a neu, method is proposed for effectively rendering certain chosen parts among the fall set of data in terms of a colour buffer, referred to as the virtual plane, for storing intermediate results. By this method, scientists may concentrate their attention on the contents of data in which they are interested. Besides, the method could be easily integrated with all the current divert volume rendering techniques, especially progressive refinement methods and selective methods. Copyright (C) 1999 John Wiley & Sons, Ltd.",
    "actual_venue": "Journal Of Visualization And Computer Animation"
  },
  {
    "abstract": "Recognizing the location and orientation of a mobile device from captured images is a promising application of image retrieval algorithms. Matching the query images to an existing georeferenced database like Google Street View enables mobile search for location related media, products, and services. Due to the rapidly changing field of view of the mobile device caused by constantly changing user attention, very low retrieval times are essential. These can be significantly reduced by performing the feature quantization on the handheld and transferring compressed Bag-of-Feature vectors to the server. To cope with the limited processing capabilities of handhelds, the quantization of high dimensional feature descriptors has to be performed at very low complexity. To this end, we introduce in this paper the novel Multiple Hypothesis Vocabulary Tree (MHVT) as a step towards real-time mobile location recognition. The MHVT increases the probability of assigning matching feature descriptors to the same visual word by introducing an overlapping buffer around the separating hyperplanes to allow for a soft quantization and an adaptive clustering approach. Further, a novel framework is introduced that allows us to integrate the probability of correct quantization in the distance calculation using an inverted file scheme. Our experiments demonstrate that our approach achieves query times reduced by up to a factor of 10 when compared to the state-of-the-art.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "A design procedure for complete substitution-permutation encryption networks is presented. The cryptographically important property of completeness is achieved after three iterations, the minimum possible number for all networks of size N = i . n^2 (i = 1,2,...,n), where n is the size of the substitution function used. The permutation stage is constructed by choosing a single member of a class of cryptographically equivalent permutations for all the network rounds, hence having the advantage of simplifying the network implementation. An algorithm for generating members of the class of cryptographically equivalent permutations is given.",
    "actual_venue": "Computers And Security"
  },
  {
    "abstract": "The importance of an abstract approach to a computation theory over general data types has been stressed by Tucker in many of his papers. Berger and Seisenberger recently elaborated the idea for extraction out of proofs involving (only) abstract reals. They considered a proof involving coinduction of the proposition that any two reals in [-1, 1] have their average in the same interval, and informally extract a Haskell program from this proof, which works with stream representations of reals. Here we formalize the proof, and machine extract its computational content using the Minlog proof assistant. This required an extension of this system to also take coinduction into account.",
    "actual_venue": "Mathematical Structures In Computer Science"
  },
  {
    "abstract": "Motivation: Orphan genes, also known as ORFans, are newly evolved genes in a genome that enable the organism to adapt to specific living environment. The gene content of every sequenced genome can be classified into different age groups, based on how widely/narrowly a gene's homo-logs are distributed in the context of species taxonomy. Those having homologs restricted to organisms of particular taxonomic ranks are classified as taxonomically restricted ORFans. Results: Implementing this idea, we have developed an open source program named ORFanFinder and a free web server to allow automated classification of a genome's gene content and identification of ORFans at different taxonomic ranks. ORFanFinder and its web server will contribute to the comparative genomics field by facilitating the study of the origin of new genes and the emergence of lineage-specific traits in both prokaryotes and eukaryotes.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "A robot sensing system is described that uses multiple sources of information to construct an internal representation of its environment. Initially, object models are used to form the basic representations. These are modified by processes that operate on sequences of sensory information, obtained from sensors that move about in the environment. Two representations are constructed. One is a description of the spatial layout of the environment, represented as an octree, while the other is an object- and feature-based representation. The system handles both expected and unexpected objects, and attempts to register its internal representation with the external world using a variety of predictive, sensory-processing, and matching procedures.",
    "actual_venue": "Icra"
  },
  {
    "abstract": "The spectral and radiometric quality of airborne imaging spectrometer data is affected by the anisotropic reflectance behavior of the imaged surface. Illumination and observation angle-dependent patterns of surface reflected radiation propagate into products, hinder quantitative assessment of biophysical/biochemical parameters, and decrease the comparability of data from multiple flight lines. The Ross–Li model, originally developed for multiangular observations, can be inverted to estimate and correct for surface anisotropy effects. This requires land cover be stratified into distinct types of scattering behavior. When the observations subsumed in these classes cover a range of view angles, a pseudo multiangular view on the surface can be employed to invert the Ross–Li model. A discrete land cover classification, however, bears the risk of inappropriate scattering correction resulting in spatial artifacts in the corrected data, predominantly in transition regions of two land cover types (e.g., soil and sparse vegetation with varying fractions). We invert the Ross–Li model on continuous land cover fraction layers. We decompose land cover in dominating structural types using linear spectral unmixing. Ross–Li kernel weights and formulations are estimated for each type independently; the correction is then applied pixel-wise according to the fractional distribution. The corrected Airborne Prism EXperiment imaging spectrometer data show significant reduction of anisotropic reflectance effects of up to 90% (average 60% to 75%, ), measured in the overlapping regions of adjacent flight lines. No spatial artifacts or spectral irregularities are observed after correction.",
    "actual_venue": "Geoscience And Remote Sensing, Ieee Transactions"
  },
  {
    "abstract": "Heat flux expressions are derived for multibody potential systems by extending the original Hardy's methodology and modifying Admal & Tadmor's formulas. The continuum thermomechanical quantities obtained from these two approaches are easy to compute from molecular dynamics (MD) results, and have been tested for a constant heat flux model in two distinctive systems: crystalline iron and polyethylene (PE) polymer. The convergence criteria and affecting parameters, i.e. spatial and temporal window size, and specific forms of localization function are found to be different between the two systems. The conservation of mass, momentum, and energy are discussed and validated within this atomistic-continuum bridging.",
    "actual_venue": "Journal Of Computational Physics"
  },
  {
    "abstract": "In this work we present a new room impulse response simulation for spherical microphone arrays taking into account source directivity. We calculate the emission angle of the sound ray leaving the source based on the location of the image and the receiver using Allen and Berkley's image method. We provide an implementation of a room impulse response simulator for a spherical microphone array including a directional source with arbitrary directivity. We validate our implementation considering the zeroth and the first-order reflections. Our results show a worst-case directional gain error of 7% in comparison with theoretical predictions.",
    "actual_venue": "European Signal Processing Conference"
  },
  {
    "abstract": "A 3D finite element ANSYS model of the thermal laser stimulation (OBIRCH, TIVA) effect is presented. Using this model we will answer questions related to the localization and the rapidity of laser heating. Furthermore, the temperature increase and the resistance change for a 1 mum aluminum line will be given. Thermal laser stimulation results on a test structure are compared with the simulations. All the conclusions obtained in this study underline the ability of thermal laser stimulation to precisely localize metallic short type faults in ICs. (C) 2001 Elsevier Science Ltd. All rights reserved.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "A multibit ΔΣ analog-to-digital converter can achieve high resolution with a lower order modulator and lower oversampling ratio than a single-bit design. However, in a multihit ΔΣ modulator, quantization level errors in the internal multibit quantizer can limit the ΔΣ modulator's signal-to-noise-and-distortion and spurious-free dynamic range. For a CMOS ΔΣ analog-to-digital converter using a flash analog-to-digital converter as its internal quantizer, comparator input offset errors are a significant source of quantization level errors. This paper presents a dynamic element matching (DEM) technique, comparator offset DEM, that modulates the sign of the comparator input offsets with a random sequence and causes the offset errors to appear as white noise and attenuated spurious tones. Measured performance of a prototype ΔΣ modulator IC shows that comparator offset DEM enables it to achieve 98-dB peak signal-to-noise-and-distortion and 105-dB spurious-free dynamic range. Analysis and simulation of comparator offset DEM in a flash analog-to-digital converter with a periodic input and uniform dither give insight into its operation and quantify the spur attenuation it provides",
    "actual_venue": "Ieee Transactions On Circuits And Systems -Analog And Digital Signal Processing"
  },
  {
    "abstract": "We present an application of Granular Computing and Three-way decisions to intelligence analysis. In particular we extend the Analysis of Competing Hypotheses with an additional perspective devoted to support analysts in reasoning with groups of hypotheses that can be equivalent on the basis of partial and incomplete evidence, and in classifying these groups of hypotheses with respect to a decisional attribute of interest for the analyst, such as dangerous or safe. Creating and reasoning with granules and multi-level granular structures give to our approach an added value when dealing with a large number of evidence and hypotheses. Three-way decision making offers the possibility of a rapid understanding of how granules of hypotheses approximate a class of dangerous hypotheses, with clear benefits when analysts have to take decision on classifying a group of hypotheses or setting a proper level of attention to group of equivalent hypotheses.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "The typical Software Defined Networking (SDN) Data Center (DC) of the future will likely be comprised of a software overlay and a hardware underlay, i.e., a physical network. From an architectural perspective, there are two distinct possibilities for such DCs. The first is a loosely coupled model where the software overlay and the hardware underlay are kept independent and managed separately. In this model, the software overlay is transparent and virtually invisible to the hardware underlay. The second is a tightly coupled model, where the software overlay and the hardware underlay work in cooperation, with the hardware underlay having visibility into the software overlay. This paper analyzes the impact of these approaches from a technology and business standpoint. It also presents some fundamentally different ideas for maximizing the value of the underlay in a Commercial-off-the-shelf (COTS) ASICs environment.",
    "actual_venue": "Computing, Networking And Communications"
  },
  {
    "abstract": "We propose a new approach to build context-aware interactive applications. Our approach enables us to use existing GUI-based interactive applications although a variety of interaction devices can be used to control them. This means that we can adopt traditional GUI toolkits to build context-aware applications. The paper describes design and implementation of our middleware that enables existing applications to be context-aware, and presents some examples to show the effectiveness of our approach.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Hyoid movement data attained from videofluoroscopic swallowing study were analyzed.SVM was employed to classify the data as normal or dysfunctional swallowing.Features extracted from hyoid movement were selected to minimize redundancy.Feature selection results would present a deeper understanding of dysphagia pathophysiology.The proposed method with an outstanding discrimination performance would be useful as an adjunct diagnostic tool. To evaluate classification performance of a support vector machine (SVM) classifier for diagnosing swallowing difficulty based on the hyoid movement data attained from videofluoroscopic swallowing study, the hyoid kinematics during the swallowing of 2źmL of liquid barium solution were analyzed for 90 healthy volunteers and 116 dysphagic stroke patients. SVM was used to classify the kinematic results as normal or dysfunctional swallowing. Various kernel functions and kernel parameters were used for optimization. Features were selected to find an optimal feature subset and to minimize redundancy. Accuracy, sensitivity, specificity, and area under a receiving operating characteristic curve (AUC) were used to assess the discrimination performance. In 19 out of 26 features, mean comparison revealed a significant difference between healthy subjects and dysphagic patients. By reducing the number of features to 10, an AUC of 0.9269 could be reached. Common features showing the best classification in both kernel functions included forward maximum excursion time, upward maximum excursion time, maximum excursion length, upward maximum velocity time, upward maximum acceleration time, maximum acceleration, maximum acceleration time, and mean acceleration. SVM-based classification method with the use of kernel functions showed an outstanding (AUC of 0.9269) discrimination performance for either healthy or dysphagic hyoid movement during swallowing. We expect that this classification method will be useful as an adjunct diagnostic tool by providing automatic detection of swallowing dysfunction as well as a research tool providing deeper understanding of pathophysiology.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "Numerical solutions of the Euler equations using real gas equations of state (EOS) often exhibit serious inaccuracies. The focus here is the van der Waals EOS and its variants (often used in supercritical fluid computations). The problems are not related to a lack of convexity of the EOS since the EOS are considered in their domain of convexity at any mesh point and at any time. The difficulties appear as soon as a density discontinuity is present with the rest of the fluid in mechanical equilibrium and typically result in spurious pressure and velocity oscillations. This is reminiscent of well-known pressure oscillations occurring with ideal gas mixtures when a mass fraction discontinuity is present, which can be interpreted as a discontinuity in the EOS parameters. We are concerned with pressure oscillations that appear just for a single fluid each time a density discontinuity is present. The combination of density in a nonlinear fashion in the EOS with diffusion by the numerical method results in violation of mechanical equilibrium conditions which are not easy to eliminate, even under grid refinement.",
    "actual_venue": "Journal Of Computational Physics"
  },
  {
    "abstract": "Presents a novel estimator for the time-dependent spectrum of a nonstationary signal. By modeling the signal, at any given frequency, as having a time-varying amplitude accurately represented by an orthonormal basis expansion, the authors are able to compute a minimum mean-squared error estimate of this time-varying amplitude. Repeating the process over all frequencies, they obtain a power distribution as a function of time and frequency that is consistent with the Wold-Cramer evolutionary spectrum. Based on the model assumptions, the authors develop the evolutionary periodogram (EP) for nonstationary signals, an estimator analogous to the periodogram used in the stationary case. They also derive the time-frequency resolution of the new estimator. The approach is free of some of the drawbacks of the bilinear distributions and of the short-time Fourier transform spectral estimates. It is guaranteed to produce nonnegative spectra without the cross-term behavior of the bilinear distributions, and it does not require windowing of data in the time domain. Examples illustrating the new estimator are given",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "Internet technologies, including the Web, Web browsers, and search engines, have provided unprecedented opportunities for conducting business online. Together, these technologies enable the creation of Web-based information systems, which are changing market dynamics and the way companies conduct business. This article presents a framework for an internetworked business model using Web-based information systems.",
    "actual_venue": "Information Systems Management"
  },
  {
    "abstract": "Typical autonomous robotics systems incorporate multiple cameras, LIDAR sensors and sophisticated computing resources. In this paper we present a software framework for utilizing any array of multiple cameras with sufficient field-of-view (FOV) overlap as a light field imaging system. We show that the typical linear arrays that exist on autonomous cars are sufficient to capture stable time resolved light fields even when moving at highway speeds. We elaborate on the potential pitfalls associated with such a technique namely loss of calibration between cameras due to high frequency vibrations and sudden shocks associated with driving over potholes and highlight a method that can compensate for such effects. We demonstrate that the light fields collected by simple linear arrays can be processed in real time for a wide variety of useful applications including occlusion removal, for signal enhancement in featureless images captured in very low light, for reflection removal and for improved visibility in extreme conditions associated with snow and heavy rain.",
    "actual_venue": "Ieee/Rsj International Conference On Intelligent Robots And Systems"
  },
  {
    "abstract": "This paper presents new decentralized optimal strategies for Cooperative Adaptive Cruise Control (CACC) of a car platoon under string-stability constraints. Two related scenarios are explored in the article: in the first one, a linear-quadratic regulator in the presence of measurable disturbances is synthesized, and the string-stability of the platoon is enforced over the controller's feedback and feedforward gains. In the second scenario, H2- and H∞-performance criteria, respectively accounting for the desired group behavior and the string-stability of the platoon, are simultaneously achieved using the recently-proposed compensator blending method. An analytical study of the impact of actuation/communication delays and uncertain model parameters on the stability of the multi-vehicle system, is also conducted. The theory is illustrated via numerical simulations.",
    "actual_venue": "Control Conference"
  },
  {
    "abstract": "We describe the improved properties of the NMHDECAY program, that is designed to compute Higgs and sparticle masses and Higgs decay widths in the NMSSM. In the version 2.0, Higgs decays into squarks and sleptons are included, accompanied by a calculation of the squark, gluino and slepton spectrum and tests against constraints from LEP and the Tevatron. Further radiative corrections are included in the Higgs mass calculation. A link to MicrOMEGAs allows to compute the dark matter relic density, and a rough (lowest order) calculation of BR(b→sγ) is performed. Finally, version 2.1 allows to integrate the RGEs for the soft terms up to the GUT scale.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "This paper presents the neural dynamical network to compute a best rank-one approximation of a real-valued tensor. We implement the neural network model by the ordinary differential equations (ODE), which is a class of continuous-time recurrent neural network. Several new properties of solutions for the neural network are established. We prove that the locally asymptotic stability of solutions for ODE by constructive an appropriate Lyapunov function under mild conditions. Furthermore, we also discuss how to use the proposed neural networks for solving the tensor eigenvalue problem including the tensor H-eigenvalue problem, the tensor Z-eigenvalue problem, and the generalized eigenvalue problem with symmetric-definite tensor pairs. Finally, we generalize the proposed neural networks to the computation of the restricted singular values and the associated restricted singular vectors of real-valued tensors. We illustrate and validate theoretical results via numerical simulations.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Wireless sensor networks (WSNs) can provide real-time information about geospatial environments, and so have the potential to play an important role in the monitoring of geographic phenomena. The research reported in this paper uses WSNs to provide salient information about spatially distributed dynamic fields, such as regional variations in temperature or concentration of a toxic gas. The focus is on topological changes to areas of high-activity that occur during the evolution of the field. Topological changes investigated include region merging and splitting, and hole formation or elimination. Such changes are formally characterized, and an algorithm is developed that detects such changes by means purely of in-network processing. The efficiency of this algorithm is investigated both theoretically and using simulation experiments.",
    "actual_venue": "GIS"
  },
  {
    "abstract": "Most students experience learning difficulties in various points during their school years. When new contents are presented the reactions can he different depending on the individual characteristics. In this paper we present an architecture that intends to combine agent technology with computational models of personality and emotion. within an ubiquitous learning system that is able to support the learning process among a group of students coordinated by an instructor.",
    "actual_venue": "Ambient Intelligence And Future Trends - International Symposium On Ambient Intelligence"
  },
  {
    "abstract": "We apply a general procedure of the author to choose penalty parameters in total variation denoising. This is an automatic method of tuning parameter choice for total variation denoising. The method is computationally much simpler than cross-validation.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "This paper investigates the synchronization problem of multiple rigid bodies under directed communication topology. Unlike the existing designs relying on continuous-time information exchanges, a novel sampled-data networking strategy is introduced, which is more reliable and practical in realistic communication networks. Specifically, we focus on the analysis of continuous-time rigid body dynamics with non-uniform discrete-time information exchanges in the communication network. In particular, the sampling period is assumed to be time-varying and governed by a Markov chain. Furthermore, the relative angular velocity information between neighbouring rigid bodies is not required, such that the communication network burden can be further reduced. Another important and distinct feature of our synchronization scheme is that the communication energy consumption can be effectively decreased, since the communication network is utilized more efficiently. Based on model transformation and stochastic stability analysis, sufficient conditions are established to guarantee that the synchronization can be achieved. Finally, a numerical example is provided to show the effectiveness and benefits of our proposed results. (C) 2015 Elsevier B.V. All rights reserved.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "This paper presents a novel full-depletion Si X-ray detector based on silicon-on-insulator pixel (SOIPIX) technology using a pinned depleted diode structure, named the SOIPIX-PDD. The SOIPIX-PDD greatly reduces stray capacitance at the charge sensing node, the dark current of the detector, and capacitive coupling between the sensing node and SOI circuits. These features of the SOIPIX-PDD lead to low read noise, resulting high X-ray energy resolution and stable operation of the pixel. The back-gate surface pinning structure using neutralized p-well at the back-gate surface and depleted n-well underneath the p-well for all the pixel area other than the charge sensing node is also essential for preventing hole injection from the p-well by making the potential barrier to hole, reducing dark current from the Si-SiO2 interface and creating lateral drift field to gather signal electrons in the pixel area into the small charge sensing node. A prototype chip using 0.2 mu m SOI technology shows very low readout noise of 11.0 e(rms)(-), low dark current density of 56 pA/cm(2) at -35 degrees C and the energy resolution of 200 eV(FWHM) at 5.9 keV and 280 eV (FWHM) at 13.95 keV.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "Binarization techniques have been developed in the document analysis community for over 30 years and many algorithms have been used successfully. On the other hand, document analysis tasks are more and more frequently being applied to multimedia documents such as video sequences. Due to low resolution and lossy compression, the binarization of text included in the frames is a non trivial task. Existing techniques work without a model of the spatial relationships in the image, which makes them less powerful. We introduce a new technique based on a Markov Random Field (MRF) model of the document. The model parameters (clique potentials) are learned from training data and the binary image is estimated in a Bayesian framework. The performance is evaluated using commercial OCR software.",
    "actual_venue": "Pattern Recognition, Proceedings International Conference"
  },
  {
    "abstract": "Herein, we describe extended work-stealing strategies for Stack Threads/MP, in which thieves steal from the bottom of a victim's logical stack not just the bottommost task but multiple chained tasks. These new strategies offer two advantages: reducing the total cost of stealing tasks and reducing the total idle time. In addition, these strategies attempt to preserve the sequential execution order of tasks in the chain. We evaluated these extended work-stealing strategies by using the unbalanced tree search (UTS) benchmark and could demonstrate its advantages over the original work-stealing strategy and other OpenMP task implementations and Cilk implementation as well. Extended work-stealing strategies exhibit significant improvement with respect to the UTS benchmark, even if the task is very fine-grain and non-uniform.",
    "actual_venue": "Ipdps Workshops"
  },
  {
    "abstract": "Off-label drug use refers to using marketed drugs for indications that are not listed in their FDA labeling information. Such uses are very common and sometimes inevitable in clinical practice. To some extent, off-label drug uses provide a pathway for clinical innovation, however, they could cause serious adverse effects due to lacking scientific research and tests. Since identifying the off-label uses can provide a clue to the stakeholders including healthcare providers, patients, and medication manufacturers to further the investigation on drug efficacy and safety, it raises the demand for a systematic way to detect off-label uses. Given data contributed by health consumers in online health communities (OHCs), we developed an automated approach to detect off-label drug uses based on heterogeneous network mining. We constructed a heterogeneous healthcare network with medical entities (e.g. disease, drug, adverse drug reaction) mined from the text corpus, which involved 50 diseases, 1,297 drugs, and 185 ADRs, and determined 13 meta paths between the drugs and diseases. We developed three metrics to represent the meta-path-based topological features. With the network features, we trained the binary classifiers built on Random Forest algorithm to recognize the known drug-disease associations. The best classification model that used lift to measure path weights obtained F1-score of 0.87, based on which, we identified 1,009 candidates of off-label drug uses and examined their potential by searching evidence from PubMed and FAERS.",
    "actual_venue": "BCB"
  },
  {
    "abstract": "Objective: This study reports the development and psychometric evaluation of the Smartphone for Clinical Work Scale (SCWS) to measure nurses' use of smartphones for work purposes. Methods: Items were developed based on literature review and a preliminary study. After expert consultations and pilot testing, a 20-item scale was administered in January-June 2017 to 517 staff nurses from 19 tertiary-level general hospitals in Metro Manila, Philippines. Exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) were used to evaluate construct validity. Structural equation modeling (SEM) was used to test the predictive validity of SCWS on perceived work productivity. Results: EFA results show that 15 out of 20 items loaded on five factors: communication with clinicians via call and text, communication with doctors via instant messaging, information seeking, communication with nurses via instant messaging, and communication with patients via call and text. CFA results suggest that the five factors that form SCWS have adequate fit to the data, thus supporting construct validity. SEM results suggest predictive validity since SCWS was positively associated with perceived work productivity. Conclusions: The 15-item SCWS showed satisfactory psychometric properties for use in future studies. These studies can focus on identifying factors associated with nurses' use of smartphones for work purposes.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "This paper is devoted to the problem of control de- sign of an underwater vehicle/manipulator (UVM) system com- posed of a free navigating platform equipped with a robot ma- nipulator. This composite system is driven by actuators and sen- sors having substantially different bandwidth characteristics due to their nature. Such difference allows for a mathematical setup which can be naturally treated by standard singular perturbation theory. On the basis of this analysis, two control laws are pro- posed. The first is a simplification of the computed torque con- trol law which only requires partial compensation for the slow-sub- system (vehicle dynamics). Feedback compensation is only needed to overcome the coupling effects from the arm to the basis. The second aims at replacing this partial compensation by a robust nonlinear control that does not depend on the model parameters. The closed-loop performance of this controller is close to that of the model-based compensation. Both control laws are shown to be closed-loop stable in the sense of the perturbation theory. A com- parative study between a linear partial derivative (PD) controller, a partial model-based compensation, and the nonlinear robust feed- back is presented at the end of this paper.",
    "actual_venue": "Ieee Trans Contr Sys Techn"
  },
  {
    "abstract": "Pattern matching is a commonly used operation in many applications including image processing, computer and network security, bioinformatics, among many others. Aho-Corasick (AC) algorithm is one of the well-known pattern matching techniques and it is intensively used in computer and network security. In order to meet the real-time performance requirements imposed on these security applications, developing a high-speed parallelization technique is essential for the AC algorithm. In this paper, we present a new memory efficient parallelization technique which efficiently places and caches the input text data and the reference data in the on-chip shared memories and texture caches of the Graphic Processing Unit (GPU). Furthermore, the new approach efficiently schedules memory accesses in order to minimize the overhead in loading data to the on-chip shared memories. The approach cuts down the effective memory access latencies and leads to significant performance improvements. Experimental results on Nvidia GeForce 9500GT GPU shows up to 15-times speedup compared with a serial version on 2.2Ghz Core2Duo Intel processor, and 15Gbps throughput performance.",
    "actual_venue": "Hpcc-Icess"
  },
  {
    "abstract": "In the area of robotic vision we are interested in developing algorithms for the control of a robot manipulator by means of visual feedback. Image analysis and exploitation of the image features are therefore steps of importance. This paper deals with the problem of three-dimensional localization of objects representing circular patterns. The first part of the paper is concerned with the parametrization of elliptic curves and the second part is dedicated to the estimation of an object's position and orientation",
    "actual_venue": "Image Processing, Proceedings, International Conference"
  },
  {
    "abstract": "This paper describes an evolution and standardization trends of the wireless channel modeling activities towards IMT-Advanced. In particular, the new developed and standardized ITU-R IMT-Advanced multiple-input-multiple-output (MIMO) Channel Model (IMT-Adv MCM) is detailed. The IMT-Adv MCM is compared with the well-known the 3GPP/3GPP2 Spatial Channel Model (SCM), and their main similarities and differences are pointed out. The performance of MIMO systems is greatly influenced by the spatial-temporal correlation properties of the underlying MIMO channels. Here, we investigate the spatial-temporal correlation characteristics of the 3GPP/3GPP2 SCM and the IMT-Adv MCM in term of their spatial multiplexing and spatial diversity gains. The main goals of this paper are to summarize the current state of the art, as well as to point out the gaps in the wireless channel modeling works, and thus hopefully to stimulate research in these areas.",
    "actual_venue": "Pimrc"
  },
  {
    "abstract": "This paper describes drum sound identification for poly- phonic musical audio signals. It is difficult to identify drum sounds in such signals because acoustic features of those sounds vary with each musical piece and precise templates for them cannot be prepared in advance. To solve this problem, we propose new template-adaptation and template- matching methods. The former method adapts a single seed template prepared for each kind of drums to the correspond- ing drum sound appearing in an actual musical piece contain- ing sounds of various musical instruments. The latter method then uses a carefully-designed distance measure that can de- tect all the onset times of each drum in the same piece by using the corresponding adapted template. The onset times of bass and snare drums in any piece can thus be identified even if their timbres are different from prepared templates. Experimental results with our methods showed that the ac- curacy of identifying bass and snare drums in popular music was about 90%.",
    "actual_venue": "Sapa@Interspeech"
  },
  {
    "abstract": "The emergence of low-cost 3D printers steers the investigation of new geometric problems that control the quality of the fabricated object. In this paper, we present a method to reduce the material cost and weight of a given object while providing a durable printed model that is resistant to impact and external forces. We introduce a hollowing optimization algorithm based on the concept of honeycomb-cells structure. Honeycombs structures are known to be of minimal material cost while providing strength in tension. We utilize the Voronoi diagram to compute irregular honeycomb-like volume tessellations which define the inner structure. We formulate our problem as a strength--to--weight optimization and cast it as mutually finding an optimal interior tessellation and its maximal hollowing subject to relieve the interior stress. Thus, our system allows to build-to-last 3D printed objects with large control over their strength-to-weight ratio and easily model various interior structures. We demonstrate our method on a collection of 3D objects from different categories. Furthermore, we evaluate our method by printing our hollowed models and measure their stress and weights.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "With the burgeoning popularity & necessity of all things wireless, the radio spectrum has become a scarce utility; cognitive radio (CR) is now the relevant technology under development that enables one to utilize the spectrum more efficiently. Here we propose a new artficial neural network (ANN) model that predicts the channel capacity of the received signal. This information is analyzed theoretically which is subsequently verified by a suitable simulation scheme for identifying possible white space in a given band.",
    "actual_venue": "Ieee Eurocon"
  },
  {
    "abstract": "Summary form only given. The current molecular biology and systems biology is featured by the rapid accumulation of high-throughput genomics and proteomics data like microarray and mass spectrometry (MS) data. Through our study on microarray and MS data, we have observed that the cancer classification and gene/biomarker selection task has many unique characteristics that distinguish itself from other standard pattern recognition tasks. Due to the extremely small sample size, the reliable assessment of the classification accuracy becomes a major question. For gene/biomarker selection, a key question is the significance of the selected genes/marker. We studied these questions with both simulated and real microarray and MS data. We developed a perturbation-based method for estimating the distribution of error rates of a support vector machine classifier. For evaluating the statistical significance of gene lists selected by sophisticated machine learning methods, we defined the problem of rank significance of genes and developed a heuristic strategy for estimating this significance. These questions highlight two important aspects of the pattern recognition problems in high-throughput computational molecular biology. The awareness of such questions is a key for properly applying computational methods to practical data and for developing new methods that really target the scientific questions",
    "actual_venue": "Iccta"
  },
  {
    "abstract": "In order to address the existing problem that only the measurement of uncertainty is provided for the tolerance results, a calculating method of sphericity error evaluation standard uncertainties studied according to new Geometrical Product Specification and Verification(GPS). Sphericity error evaluation mathematical model based on minimum area method is raised. Sphericity error value is obtained by using improved Particle swarm optimization (PSO) algorithm. Standard uncertainty specific calculating method is raised through studying each element's trans-mission coefficient and correlation coefficient which affect standard uncertainty. And sphericity error evaluation of the graphical interface is developed. On the strength of real cases, the method proposed in this article can be used in the new GPS system to evaluate the sphericity errors accurately, visually and conveniently.",
    "actual_venue": "Iscid"
  },
  {
    "abstract": "The green design of electromechanical products is a pivotal link to manufacturing industry. The question on how to design green products must be answered by excellent designers using both advanced design methods and effective assessment techniques of design alternatives. Making an objective and precise assessment of green designs is of increasing importance to ensure sustainable development. This ...",
    "actual_venue": "Ieee Transactions On Systems, Man, And Cybernetics: Systems"
  },
  {
    "abstract": "The problem of designing communication networks that can survive the loss of any single fink is studied. Such problems can be formulated as minimum cost 2-edge connected subgraph problems in a complete graph. The linear programming cutting plane approach has been used effectively for related problems in [Schwerpunktprogramm der Deutshen Forschungsgemeinschaft, Anwendungsbezogene Optimierung und Steuerung, Report No. 188, 1989], where problem-specific cutting planes that define facets of the underlying integer polyhedra are used. This paper introduces a new class of valid inequalities.for the polytope associated with the minimum cost 2-edge connected subgraph problem, and necessary and sufficient conditions for these inequalities to be facet-inducing for this polytope are given.",
    "actual_venue": "Siam J Discrete Math"
  },
  {
    "abstract": "In the light of ongoing progresses of research on artificial intelligent systems exhibiting a steadily increasing problem-solving ability, the identification of practicable solutions to the value alignment problem in AGI Safety is becoming a matter of urgency. In this context, one preeminent challenge that has been addressed by multiple researchers is the adequate formulation of utility functions or equivalents reliably capturing human ethical conceptions. However, the specification of suitable utility functions harbors the risk of   for which no final consensus on responsible proactive countermeasures has been achieved so far. Amidst this background, we propose a novel socio-technological ethical framework denoted Augmented Utilitarianism which directly alleviates the perverse instantiation problem. We elaborate on how augmented by AI and more generally science and technology, it might allow a society to craft and update ethical utility functions while jointly undergoing a dynamical ethical enhancement. Further, we elucidate the need to consider embodied simulations in the design of utility functions for AGIs aligned with human values. Finally, we discuss future prospects regarding the usage of the presented scientifically grounded ethical framework and mention possible challenges.",
    "actual_venue": "AGI"
  },
  {
    "abstract": "ABSTRACT..............................................................................................................viii  1. INTRODUCTION ................................................................................................... 1  2. STATISTICAL METHODS FOR CLASSIFICATION.......................................... 5  2.1 Maximum,Likelihood Classification ............................................................. 6 2.2 Minimum,Distance Classification ................................................................. 9 2.3 Spatial-Spectral Classification....................................................................... 9 2.4 Class Separability ........................................................................................ 12 2.4.1 Divergence ........................................................................................ 14 2.4.2 Jeffries-Matusita distance ................................................................. 17",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "Automated performance modeling and performance prediction of parallel programs are highly valuable in many use cases, such as in guiding task management and job scheduling, offering insights of application behaviors, assisting resource requirement estimation, etc. The performance of parallel programs is affected by numerous factors, including but not limited to hardware, system software, applications, algorithms, and input parameters, thus an accurate performance prediction is often a challenging and daunting task. In this study, we focus on automatically predicting the execution time of parallel programs (more specifically, MPI programs) with different inputs, at different scale, and without domain knowledge. We model the correlation between the execution time and domain-independent runtime features. These features include values of variables, counters of branches, loops, and MPI communications. Through automatically instrumenting an MPI program, each execution of the program will output a feature vector and its corresponding execution time. After collecting data from executions with different inputs, a random forest machine learning approach is used to build an empirical performance model, which can predict the execution time of the program with a new input. Our experiments and analyses of three parallel programs, Graph500, GalaxSee and SMG2000, on three different systems show that our method performs well, with less than 20% error in predictions on average.",
    "actual_venue": "Ieee International Symposium On Parallel And Distributed Processing With Applications And Ieee International Conference On Ubiquitous Computing And Communications"
  },
  {
    "abstract": "A new parallel algorithm has been developed for second-order Moller-Plesset perturbation theory (MP2) energy calculations. Its main projected applications are for large molecules, for instance, for the calculation of dispersion interaction. Tests on a moderate number of processors (2-16) show that the program has high CPU and parallel efficiency. Timings are presented for two relatively large molecules, taxol (C47H51NO14) and luciferin (C11H8N2O3S2), the former with the 6-31G* and 6-311G** basis sets (1032 and 1484 basis functions, 164 correlated orbitals), and the latter with the aug-cc-pVDZ and aug-cc-pVTZ basis sets (530 and 1198 basis functions, 46 correlated orbitals). An MP2 energy calculation on C130H10 (1970 basis functions, 265 con-elated orbitals) completed in less than 2 h on 128 processors. (c) 2006 Wiley Periodicals, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "BACKGROUND: Identifying the active site of an enzyme is a crucial step in functional studies. While protein sequences and structures can be experimentally characterized, determining which residues build up an active site is not a straightforward process. In the present study a new method for the detection of protein active sites is introduced. This method uses local network descriptors derived from protein three-dimensional structures to determine whether a residue is part of an active site. It thus does not involve any sequence alignment or structure similarity to other proteins. A scoring function is elaborated over a set of more than 220 proteins having different structures and functions, in order to detect protein catalytic sites with a high precision, i.e. with a minimal rate of false positives. RESULTS: The scoring function was based on the counts of first-neighbours on side-chain contacts, third-neighbours and residue type. Precision of the detection using this function was 28.1%, which represents a more than three-fold increase compared to combining closeness centrality with residue surface accessibility, a function which was proposed in recent years. The performance of the scoring function was also analysed into detail over a smaller set of eight proteins. For the detection of 'functional' residues, which were involved either directly in catalytic activity or in the binding of substrates, precision reached a value of 72.7% on this second set. These results suggested that our scoring function was effective at detecting not only catalytic residues, but also any residue that is part of the functional site of a protein. CONCLUSION: As having been validated on the majority of known structural families, this method should prove useful for the detection of active sites in any protein with unknown function, and for direct application to the design of site-directed mutagenesis experiments.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "Cet article propose un nouvel algorithme de filtrage RII adaptatif, basé sur la formulation dite d'erreur d'équation (EE). L'algorithme permet la compensation du biais inhérent à la formulation de l'erreur d'équation. Une étude des propriétés de la nouvelle méthode montre que la stabilité associée à l'algorithme d'EE peut être conservée. De plus, moyennant des conditions suffisamment générales, les estimées de paramètres sont consistantes. Certaines des méthodes proposées récemment pour résoudre le problème du biais de l'algorithme d'EE sont des cas particuliers de l'algorithme général présenté ici. Des simulations informatiques sont incluses afin d'illustrer et de comparer les propriétés de convergence de l'algorithme proposé.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "BCI video games are making brain training increasingly popular and available; yet scientific evidence to support its efficacy is lacking. Real-life descriptions of BCI video games deployments in concrete scenarios are urgently needed. In this paper, we report a use case of the development and pilot-testing of a BCI video game designed to support children with autism when attending to Neurofeedback training sessions, called FarmerKeeper. Caring for children with autism may impose new cognitive, motor, behavioral, and attention challenges that current solutions targeted for other populations may not address. The goal of the game is to maintain children’s attention above a threshold to control a runner who is seeking for lost farm animals. FarmerKeeper uses a consumer-grade BCI headset to read user’s attention. We evaluated FarmerKeeper’s usability and user experience through a 4-weeks deployment study with 12 children with autism. Our quantitative results show FarmerKeeper outperforms a commercial BCI video game used for neurofeedback training, and qualitatively, FarmerKeeper could successfully support children with autism when attending to neurofeedback training sessions by possibly improving their attention and reducing their anxiety. We close reflecting on our design aspects and discussing directions for future work.",
    "actual_venue": "Multimedia Tools And Applications"
  },
  {
    "abstract": "Intrusion detection system (IDS) has been introduced and broadly applied to prevent unauthorized access to system resource and data for several years. However, many problems are still not well resolved in most of IDS, such as detection evasion, intrusion containment. In order to resolve these problems, we propose a novel flexible architecture VNIDA which is based on virtual machine monitor (VMM) and has no-intrusive behavior to target system after studying popular IDS architectures. In this architecture, a separate intrusion detection domain (IDD) is added to provide intrusion detection services for all virtual machines. Specially, an IDD helper is introduced to take response to the intrusions according to the security policies. Moreover, event sensors and IDS stub, as the core components of IDS, are separately isolated from target systems, so strong reliability is also achieved in this architecture. To show the feasibility of the VNIDA, we implement a prototype based on the proposed architecture. Based on the prototype, we employed some rootkits to evaluate our VNIDA, and the results shows that VNIDA has the ability to detect them efficiently, even some potential intrusions. In addition, system performance evaluation also shows that VNIDA only introduce less than 1.25% extra overhead.",
    "actual_venue": "Wkdd"
  },
  {
    "abstract": "A blackboard system in which the experts operate concurrently is described and implemented. The parallel program, which carries out a Penrose tiling of a planar area, provides insight into, and requires resolution of, many of the problems which must be addressed in implementing a more general concurrent blackboard system. A more versatile distributed knowledge system is also proposed. This model is based on a tree of concurrent blackboard systems in which partial solutions to various subproblems are developed at interior nodes and are piped to the root where the overall problem solution is arrived at.",
    "actual_venue": "Iea/Aie"
  },
  {
    "abstract": "The past decade has seen a dramatic rise in the number international joint ventures (IJV) and a great majority of them fail. Although the research literature continues to expand, there is an imbalance to the attention paid to the process and operational management side of IJVs. We begin this article with a survey of the literature and we segment the research stream into the life cycle phases of an IJV: the IJV input, process, and output schools of investigation. Then, we present an overview of the Cathode Ray Tube (CRT) industry and the competitive forces that influenced LG and Philips to form a strategic joint venture. Next, which is the core of this article, we use the disciplines of system dynamics to perform strategy analyses of the dynamics of the LG-Philips IJV. The fundamental intent of this paper is to demonstrate how system dynamics is an effective analytic instrument to derive strategic measures, as well as countermeasures to strengthen the IJV operations and to weaken competitors and blunt the impact of their actions. We show how we can model technology-intensive businesses, in the context of industries and markets, by means of systems with intricate and technology-idiosyncratic feedback characteristics, but whose complex system behaviour is substantially more easily discernable from their graphic representations. Fourth, we close this article with a discussion on the limitations of our analysis and potential areas for further research.",
    "actual_venue": "International Journal Of Technology Management"
  },
  {
    "abstract": "Most researches on the massive multiple-input multiple-output (MIMO) systems are based on the assumption that the MIMO channels are independent, in reality, the above assumption is very difficult to realize when the number of multiple antennas are very large. Hence the simulation on massive MIMO systems under correlated channels has some degree of guiding significance for practical application of massive MIMO systems. Simulation results show that the performance of massive MIMO systems becomes worse when the correlation coefficients of the channels increase. And when the channel correlation coefficient becomes smaller than a certain small value, the performance of massive MIMO systems will be enhanced quickly as the number of Base Station (BS) antennas increases; while the channel correlation coefficient is larger than the certain value, the performance is enhanced slowly as the number of BS antennas increases.",
    "actual_venue": "Ieee International Conference On Networks"
  },
  {
    "abstract": "The Multi-Path Transmission Control Protocol (MPTCP) is the new concurrent multi-path transfer extension for the widely-deployed Transmission Control Protocol (TCP). Of course, having multiple and possibly highly dissimilar paths for transmission is a challenge for the management of the send and receive buffers, since optimal throughput is desired with a reasonable allocation of the limited memory resources in MPTCP endpoints. This is particularly important when many MPTCP connections have to be handled simultaneously. This paper measures out the required MPTCP buffer size in the real-world Internet testbed NORNET, comparing theoretical size and real size to analyse MPTCP performance. The experiment shows that multi-path transmission can effectively increase the application payload throughput, and greatly improve the robustness of the data transmission. As an important point of this paper, we can show that appropriate buffer size settings can increase the payload throughput, while not wasting resources. This paper has certain significance for further accurately determining the optimal buffer size settings for multi-path transmission in large-scale Internet setups.",
    "actual_venue": "Ieee International Conference On Advanced Information Networking And Applications"
  },
  {
    "abstract": "In this paper, we propose an architecture for a service able to manage, enrich, and support the interpretation of the scientific data produced during the evaluation of information access and extraction components of a Digital Library Management System (DLMS). Moreover, we describe a first prototype, which implements the proposed service.",
    "actual_venue": "Ecdl"
  },
  {
    "abstract": "The design of OBS networks that guarantee QoS provisioning for different classes of traffic is a major topic under current\n research. In this work we formulate a unified framework for studying QoS in OBS networks with a GMPLS-based control plane.\n We use this framework in order to investigate two problems. First, the configuration of the parameters of an aggregation strategy\n so that a given Forwarding Equivalency Class observes its corresponding QoS requirements. Second, we address the problem of\n planning a whole OBS network given a series of QoS constraints for each one of the Forwarding Equivalency Classes in the network.\n The presented QoS framework constitutes a valuable tool for studying QoS-related issues in OBS networks.",
    "actual_venue": "Photonic Network Communications"
  },
  {
    "abstract": "A modified general matching method is proposed for tracking moving points on a talking face. Contradictory to the early algorithms in this area, we begin the search from the finest level. If the texture information at the finest level is not sufficient for point tracking, we let the included texture information “grow” to the coarse level step by step. With this fine-to-coarse “growing” method, the searching templates are adapted to the texture information surrounding the tracked points. This procedure can reduce mismatches due to texture insufficiency in the coarse-to-fine procedure. It can also reduce the interference from unrelated points. Although, we emphasize the searching direction change, the main idea of this approach is to let the tracking templates “grow” from the smallest scale to a larger scale according to the texture information surrounding the tracked points. This approach is more adaptive compared with the traditional fixed template tracking. The procedure employs Gabor wavelet feature vectors, and examples illustrate the gains from this approach",
    "actual_venue": "Icpr"
  },
  {
    "abstract": "In the paper, a new method for solving the nonlinear Klein-Gordon equation is proposed. To this end, the nonlinear partial differential equation of Klein-Gordon is transformed into an equivalent non-linear integral equation through a transformation. Here, a hyperbolic type of Green's function is newly incorporated into the transformation in such a way that nonlinearities due to large wave motion are effectively taken into account. Based on the equivalent integral equation, a functional iteration procedure is constructed for solving the equation. The method proposed here is a semi-analytical one, not only fairly simple but straightforward to apply. Mathematical analysis is performed on the method's convergence and uniqueness. An illustrative example of a solitary wave (or soliton) is presented to investigate the validity of the method, resulting in numerical solutions, appearing to be higher accurate compared to the usual 2nd order centered difference scheme in a stable manner. In fact, just a few iterations are enough for the numerical solution.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "Cleanroom software engineering is a theory-based, team-oriented engineering process for developing very high quality software under statistical quality control. The Cleanroom process combines formal methods of object-based box structure specification and design, function-theoretic correctness verification, and statistical usage testing for reliability certification to produce software approaching zero defects. Management of the Cleanroom process is based on a life cycle of development and certification of a pipeline of user-function increments that accumulate into the final product. Teams in IBM and other organizations that use the process are achieving remarkable quality results with high productivity. A phased implementation of the Cleanroom process enables quality and productivity improvements with an increased control of change. An introductory implementation involves the application of Cleanroom principles without the full formality of the process; full implementation involves the comprehensive use of formal Cleanroom methods; and advanced implementation optimizes the process through additional formal methods, reuse, and continual improvement. The AOEXPERT/MVS™ project, the largest IBM Cleanroom effort to date, successfully applied an introductory level of implementation. This paper presents both the implementation strategy and the project results.",
    "actual_venue": "Ibm Systems Journal"
  },
  {
    "abstract": "Traditionally, medical imaging repositories have been supported by indoor infrastructures with huge operational costs. This paradigm is changing thanks to cloud outsourcing which not only brings technological advantages but also facilitates inter-institutional workflows. However, communication latency is one main problem in this kind of approaches, since we are dealing with tremendous volumes of data. To minimize the impact of this issue, cache and prefetching are commonly used. The effectiveness of these mechanisms is highly dependent on their capability of accurately selecting the objects that will be needed soon.",
    "actual_venue": "International Journal For Computer Assisted Radiology And Surgery"
  },
  {
    "abstract": "Piezoelectric materials constitute a class of intelligent materials that are helpful for monitoring the structural integrity. The principle of the piezoelectric impedance-based structural health monitoring technique is to measure the electrical impedance of a piezoelectric patch attached to a structure in a certain frequency range. Electrical impedance variations indicate physical changes in the structure due to the coupling between the electrical impedance and the mechanical impedance. Traditional methods use an impedance analyser that increases the inspection cost. The objective of this work is to introduce an electronic circuit for the piezoelectric impedance-based structural health monitoring. The circuit can monitor the electrical impedance variations of a piezoelectric patch attached to a structure. The frequency range is from 7.47 to 277.29 kHz. This frequency range covers the sensitive range of the piezoelectric structural integrity. The power consumption of the circuit is 18.15 mW. The chip area is 1.03 mm x 2.30 mm. The cost of the final design will be much lower than that of an impedance analyser. Using a wireless communication circuit, a sensor network might be established in the future.",
    "actual_venue": "Circuits, Devices And Systems, Iet"
  },
  {
    "abstract": "Head-Up Games [19, 20] attempt to combine the technological benefits of modern electronic games with the social and physical advantages of traditional games. To demonstrate this concept, a Head-Up Game for 9- to 11-year-old children was designed and developed iteratively, with intensive involvement of children for play-testing. This paper describes and reflects on the game's design process and the implications regarding the concept of Head-Up Games. The final game, Stop the Bomb, was found to be physically and socially stimulating, understood and enjoyed by the target group, and preferred over a nonelectronic version of the game at first encounter.",
    "actual_venue": "Bcs Hci"
  },
  {
    "abstract": "There is recently great interest in haplotype block structure and haplotype tagging SNPs (htSNPs) in the human genome for its implication on htSNPs-based association mapping strategy for complex disease. Different definitions have been used to characterize the haplotype block structure in the human genome, and several different performance criteria and algorithms have been suggested on htSNPs selection.A heuristic algorithm, generalized branch-and-bound algorithm, is applied to the searching of minimal set of haplotype tagging SNPs (htSNPs) according to different htSNPs performance criteria. We develop a software htSNPer1.0 to implement the algorithm, and integrate three htSNPs performance criteria and four haplotype block definitions for haplotype block partitioning. It is a software with powerful Graphical User Interface (GUI), which can be used to characterize the haplotype block structure and select htSNPs in the candidate gene or interested genomic regions. It can find the global optimization with only a fraction of the computing time consumed by exhaustive searching algorithm.htSNPer1.0 allows molecular geneticists to perform haplotype block analysis and htSNPs selection using different definitions and performance criteria. The software is a powerful tool for those focusing on association mapping based on strategy of haplotype block and htSNPs.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "Traditional SLAs, defined by average delay or packet loss, often camouflage the instantaneous performance perceived by end-users. We define a set of metrics for service availability to quantify the performance of IP backbone networks and capture the impact of routing dynamics on packet forward- ing. Given a network topology and its link weights, we propose a novel technique to compute the associated service availability by taking into account transient routing dynamics and operational conditions, such as BGP table size and traffic distributions. Even though there are numerous models for characterizing topologies, none of them provide insights on the expected perfor- mance perceived by end customers. Our simulations show that the amount of service disruption experienced by similar networks (i.e., with similar intrinsic properties such as average out-degree or network diameter) could be significantly different, making it imperative to use new metrics for characterizing networks. In the second part of the paper, we derive goodness factors based on service availability viewed from three perspectives: ingress node (from one node to many destinations), link (traffic traversing a link), and network-wide (across all source-destination pairs). We show how goodness factors can be used in various applications and describe our numerical results. I. INTRODUCTION Service-Level Agreements (SLAs) offered by today's Inter- net Service Providers (ISPs) are based on four metrics: end-to- end delay, packet loss, data delivery rate and port availability. The first three metrics are usually computed network-wide and averaged over a relatively long period of time. For the fourth metric, the term \"port\" refers to the point at which a customer's link attaches to the edge of an ISP's network. Port availability therefore refers to the fraction of time this port is operational and measures a customer's physical connectivity to the ISP's network. None of these SLA metrics capture the ability of the network to carry customer traffic to Internet destinations at any point in time. The main problem with the existing SLA specifications is that they do not capture the effect of instantaneous network conditions like failures and congestions. A recent study (1) shows that failures occur on a daily basis due to a variety of reasons (e.g., fiber cut, router hardware/software failures, and human errors) and can impact the quality-of-service (QoS) delivered to customers. When a link/node fails, all routers will independently compute a new path around the failure. At that time, routers may lack or have inconsistent forwarding 0This research project was supported by National Science Foundation CAREER grant 238348 and University of California Micro with matching funds from Fujitsu and Sprint",
    "actual_venue": "Iwqos"
  },
  {
    "abstract": "This paper analyzes the delay suffered by messages in a clocked, packet-switched, square Banyan network with k x k output-buffered switches by approximating the flow processes in the network with Markov chains. We recursively approximate the departure process of buffers of the nth stage in terms of thqt at the n -- lst stage. We show how to construct the transition matrix for the Markov chain at each stage of the network and how to solve for the stationary distribution of the delay in the queues of that stage. The analytical results are compared with simulation results for several cases. Finally, we give a method based on this approximation and the technique of coupling to compute upper bounds on the time for the system to approach steady state.",
    "actual_venue": "Sigmetrics"
  },
  {
    "abstract": "Magnitude of equalization applied by a receiver equalization circuit varies across silicon process and environmental conditions. We propose a novel method to auto calibrate a programmable receiver equalization circuit to a target gain equalization value without the use of any external test equipment or channel. This method is built upon on-chip eye monitoring and internal loop back capabilities, which are used to measure the gain equalization value. By executing this on-chip gain equalization measurement for various equalizer settings, the setting which produces equalization that is closest to the target value can be determined. This has been implemented in 45nm CMOS for a PCI Express 2.0 transceiver hardware running at 5Gbps. Lab results with test silicon demonstrate the on-chip eye height measurement capabilities.",
    "actual_venue": "Vlsi Design"
  },
  {
    "abstract": "With the rapidly development and large scale applications of electronic product code (EPC) networks, researchers and customers have turned their attention to the security problems. In this paper, we discuss the design methodology of trustworthy ONS, an object naming system of EPC network, and propose an adjusted public key infrastructure (PKI), called LPKI, for trustworthy ONS. It enhances the security of ONS using a new encryption encode/decode strategy of EPC, and improves reliability of the certificate authority by a new multiple customer relation model. Our preliminary experiments show that the proposed method does provide the needed security with reasonable performance, and hence is feasible for building trustworthy ONS.",
    "actual_venue": "Acis-Icis"
  },
  {
    "abstract": "A number of studies indicate that project-based learning enhances a student's motivation and in-depth understanding, while the CSCL environment promotes collaboration within the project. However, we know little about how teachers or curriculum designers should design a course utilizing the project-based learning approach according to real-world activities. In this study, we investigate an undergraduate cognitive science course that combines CSCL classroom activities with observational project activities in educational fields. As a result we identified three requirements of a project-based learning design to promote integration between classroom knowledge and authentic field activities: 1) Parallel-structured course involving both disciplinary and project activities; 2) Reality of the project activities; and 3) accessibility of the project content. In the conclusion, we discuss how these findings should guide the development of CSCL-based, project-oriented courses.",
    "actual_venue": "Cscl"
  },
  {
    "abstract": "An algorithm is presented that allows one to perform skeletonization of large maps with much lower memory requirements than with the straightforward approach. The maps are divided into overlapping tiles, which are skeletonized separately, using a Euclidean distance transform. The amount of overlap is controlled by the maximum expected width of any map component and the maximum size of what is considered as a small component. Next, the skeleton parts are connected again at the middle of the overlap zones. Some examples are given for efficient memory utilization in tiling an A0 size map into a predefined number of tiles or into tiles of a predefined (square) size. The algorithm is also suited for a parallel implementation of skeletonization.",
    "actual_venue": "Icdar"
  },
  {
    "abstract": "We establish formulas for computing/estimating the regular and Mordukhovich coderivatives of implicit multifunctions defined by generalized equations in Asplund spaces. These formulas are applied to obtain conditions for solution stability of parametric variational systems over perturbed smooth-boundary constraint sets.",
    "actual_venue": "J Global Optimization"
  },
  {
    "abstract": "A test of the performance of two probabilistic classifiers (random forests and multinomial logit models) in automatically defining cancer cases has been carried out on 5608 subjects, registered by the Venetian Tumour Registry (RTV) during the years 1987-1996 and manually checked for possible second cancers that occurred during the 1997-1999 period. An eightfold cross-validation was performed to estimate the classification error; 63 predictive variables were entered into the model fitting. The random forest allows to automatically classify 45% of subjects with a classification error lower than 5%, while the corresponding error is 31% for the multilogit model. The performance of the former classifier is appealing, indicating a potential drop of manually checked cases from 1750 to 960 per incidence year with a moderate error rate. This result suggests to refine the approach and extend it to other categories of manually treated cases.",
    "actual_venue": "Journal Of Biomedical Informatics"
  },
  {
    "abstract": "We consider the projected subgradient method for solving generalized mixed variational inequalities. In each step, we choose an @e\"k-subgradient u^k of the function f and w^k in a set-valued mapping T, followed by an orthogonal projection onto the feasible set. We prove that the sequence is weakly convergent.",
    "actual_venue": "Oper Res Lett"
  },
  {
    "abstract": "This paper discusses a basic system which realizes scaled manipulation. Tools for scaled manipulation are necessary when we manipulate an extremely large or small size target. This type of tool has to scale size and force up or down in a bilateral way, but time scaling should also be considered in order to build up an ideal environment such that we can feel our body size is scaled to the target size. This paper proposes a time scaling method which can be used in bilateral force-position control for unknown targets.",
    "actual_venue": "Advanced Robotics"
  },
  {
    "abstract": "We have proposed a method for diagnosing analog circuits, which is realized by combining the operation-region model and the X-Y zoning method. In the method, since we developed a data processing method to handle data discretely, we cloud implement a diagnosis procedure based on the preset test which is a diagnostic method for digital circuits. In this paper, we analyze results of the proposed diagnosis method by changing several parameters for diagnosing a circuit and show their characteristics. Moreover, we propose a new data processing method to obtain a short diagnostic sequence length. We demonstrate the effectiveness of the proposed method by applying it to ITC'97 benchmark circuits with hard faults and soft faults.",
    "actual_venue": "DFT"
  },
  {
    "abstract": "In this paper, we present a novel fast S-box algorithm without lookup table method, and novel fast optional hardware architecture for MixColumn and Inverse MixColumn module with only 5 XOR gate delay. We use on-the-fly key schedule architecture for both encryption and decryption. Furthermore, we implement a memoryless AES cipher with proposed S-box architecture and fast MixColumn and Inverse MixColumn module by adopting a pipeline method to obtain high throughput as 1.454 Gbits/sec under 125 MHz using 0.25 m CMOS technology and the hardware cost is about 80 K gate counts. According to our knowledge, our hardware architecture is the first memoryless AES cipher including encryption and decryption function.",
    "actual_venue": "Iscas"
  },
  {
    "abstract": "A public-key asymmetric robust watermarking algorithm based on signal with special correlation characteristic is proposed in this paper. This algorithm is designed to permit pubic watermark detection while preventing the watermark from being removed without the private keys. A signal, which is pseudorandom sequence with special auto-correlation characteristic, is used as watermark, and the signal's characteristic is the foundation of this algorithm. Therefore we first construct the method of generating the pseudorandom sequence and prove its auto-correlation characteristic. Then we describe the algorithm in detail. Experiment results show that the algorithm is valid and robust to common signal distortions and malicious attacks.",
    "actual_venue": "Wscg"
  },
  {
    "abstract": "The main theme of this paper is the automatic segmentation of Arabic words using Mathematical Morphology tools. The proposed algorithm has been tested with a set of Arabic words written by different writers ranging from poor to acceptable quality, The initial experimental results are very encouraging and promising.",
    "actual_venue": "Document Analysis And Recognition, , Proceedings Of The Fourth International Conference"
  },
  {
    "abstract": "In Multi-Input Multi-Output (MIMO) systems, Maximum-Likelihood (ML) decoding is equivalent to finding the closest lattice point in an N-dimensional complex space. In general, this problem is known to be NP hard. In this paper, we propose a quasi-maximum likelihood algorithm based on Semi-Definite Programming (SDP). We introduce several SDP relaxation models for MIMO systems, with increasing complexity. We use interior-point methods for solving the models and obtain a near-ML performance with polynomial computational complexity. Lattice basis reduction is applied to further reduce the computational complexity of solving these models'.",
    "actual_venue": "Ieee International Symposium On Information Theory , Vols And"
  },
  {
    "abstract": "Information resources in today's cyber communities over the World Wide Web are increasingly growing in size with an ever increasing pace of change. As information demand increases, more knowledge management and retrieval applications need to exhibit a degree of resilience towards information change, and must be able to handle incremental changes in a reasonable time. In this paper we are defining a new system that utilizes new conceptual methods using the notion of pseudo maximal rectangles (i.e. the union of all non enlargeable rectangles containing a pair (a,b) of a binary relation) for managing incremental information organization and structuring in a dynamic environment. The research work in hand focuses on managing changes in an information store relevant to a specific domain of knowledge attempted through addition and deletion of information. The incremental methods developed in this work should support scalability in change-prone information stores and be capable of producing updates to end users in an efficient time. The paper will also discuss some algorithmic aspects and evaluation results concerning the new methods.",
    "actual_venue": "Ramics"
  },
  {
    "abstract": "The development of new web services by composition of existing services is becoming an extensive approach. This has resulted in transactions that span in multiple web services. These business transactions may be unpredictable and long in duration. Thus they may not be acceptable to lock resources exclusively for such long period. Two-phase commit is also not suitable for transactions with some long sub-transactions. Compensation is a way to ensure transaction reliability. However, rolling back a previously completed transaction is potentially expensive. Thus, tentative holding is another option. This paper presents a transaction management model for web service composition. We apply the approach of tentative hold and compensation for the composite transaction. We also present a multi-dimension negotiation model for the service composition.",
    "actual_venue": "ADC"
  },
  {
    "abstract": "Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",
    "actual_venue": "Systems Science And Cybernetics, Ieee Transactions"
  },
  {
    "abstract": "This paper introduces a variation on Kak's three-stage quanutm key distribution protocol which allows for defence against the man in the mid- dle attack. In addition, we introduce a new protocol, which also offers similar resiliance against such an attack.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "Cloud computing makes available a vast amount of computation and storage resources in the pay-as-you-go manner. However, the users of cloud storage have to trust the providers to ensure the data privacy and confidentiality. In this paper, we present the Privacy-enhancing Image-based Collaborative File System (PicFS), a network file system that steganographically encodes itself into images and provides anonymous uploads and downloads from a media sharing website. PicFS provides plausible deniability by preventing traffic and image analysis by any third party from revealing the existence of PicFS or compromising its data. Because all accesses are anonymized, users of PicFS are dissociated from their data, which protects users against being compelled to release their keys. For further security and ease of use, we develop a method for automatically generating a large set of non-suspicious images to serve as input to the system. Our prototype leverages a number of existing technologies, including the F5 algorithm for steganography, Quick-Flickr for Flickr API access, Tor for anonymization, and FUSE-J for user-level file system calls. We show that the PicFS is indeed practical as the prototype demonstrates satisfactory performance in the real-world environment.",
    "actual_venue": "Parallel And Distributed Systems"
  },
  {
    "abstract": "High reconstructed performance compressed video sensing (CVS) with low computational complexity and memory requirement is very challenging. In order to reconstruct the high quality video frames with low computational complexity, this paper proposes a tensor-based joint sparseness regularization CVS reconstruction model FrTVCST (fractional-order total variation combined with sparsifying transform), in which a high-order tensor fractional-order total variation (FrTV) regularization and a tensor discrete wavelet transform (DWT) L0 norm regularization are combined. Furthermore, an approach for choosing the regularization parameter that controls the influence of the two terms in this joint model is proposed. Afterwards, a tensor gradient projection algorithm extended from smoothed L0 (SL0) is deduced to solve this combined tensor FrTV and DWT joint regularization constrained minimization problem, using a smooth approximation of the L0 norm. Compared with several state-of-the-art CVS reconstruction algorithms, such as the Kronecker compressive sensing (KCS), generalized tensor compressive sensing (GTCS), N-way block orthogonal matching pursuit (N-BOMP), low-rank tensor compressive sensing (LRTCS), extensive experiments with commonly used video data sets show the competitive performance of the proposed algorithm with respect to the peak signal-to-noise ratio (PSNR) and subjective visual quality. A FrTV combined with tensor DWT for CS video reconstruction model is proposed.A method for estimating the regularization parameter is proposed.A tensor smoothed L0 algorithm is developed to solve this reconstruction model.The algorithm has a higher PSNR and better detail preservation.",
    "actual_venue": "Sig Proc: Image Comm"
  },
  {
    "abstract": "In this paper, we propose to integrate fingerprint and ECG authentification implemented on wearable devices for personal identify. The identification rate of fingerprint authentication is 96%, ECG authentication with fixed threshold method and variable threshold method are 92.6% and 95.9% respectively. We have combined fingerprint and ECG authentication as a novel multiple authentication method. Meanwhile, we have reduced the complexity of algorithms and used less ECG wave range in order to implement on wearable devices. The identification rate has been up to 92% to achieve high accuracy and high security goal using wearable devices.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "A new gait pattern is addressed and recognized in this paper. We use a multi-view part detector to detect the body parts of each participant. Multi-gait consisting of more than one participant is tracked using hierarchical association. We use a high-dimension exemplar-based method to realize gait image inpainting and use a tensor's lowest rank to complete a two-value sequence completion. We use multiple linear tensors to describe multi-gait and realize recognition by a segmented accumulated energy map. The experimental results indicate that the methodology achieves high multi-gait recognition accuracy and has good robustness to dress, carried objects and view variance.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "ccording to the current state of research, it seems uncontroversial that the Pedagogical Content Knowledge (PCK) of teachers is a crucial factor for the success of teaching and learning in the context of many school subjects. Yet, the research about PCK in the subject of computer science (CS) is still sparse. Thus, we are working on a conceptualization of PCK for computer science (CS) that is based on literature on the one hand and empirically validated on the other. As a first step towards this goal, we have developed a category system from a set of publications from general pedagogy as well as from educational research in other subjects. Additionally, we have compared this system with the outcomes of a former survey among teachers about the preparation of lessons. Currently, we are coding all curricula for teacher education in Germany with this category system and preparing interviews among experts, applying the Critical Incident Technique.",
    "actual_venue": "Icer"
  },
  {
    "abstract": "The proliferation of digital data has resulted in a mushrooming of data-intensive applications, especially in the area of unstructured data processing. Given the growing popularity of unstructured data processing applications (e.g., FlickrTM, Google MapsTM), it is important to rethink system architectures to efficiently run these applications, from both the performance and power viewpoints. In this paper, we revisit active storage, which proposed offloading computation to disk drive processors, as a possible system architecture for these applications. Unlike previous work, we evaluate the microarchitectural aspects of active storage and perform an in-depth examination of the design of the offload processors. Using a set of unstructured data processing benchmarks, we examine two choices along the I/O path where the computation can be offloaded in existing system architectures -- a disk drive processor and a disk array controller. Our evaluation demonstrates that there are interesting tradeoffs in the choice of each location and that microarchitectural enhancements to these processors can provide significant performance boosts. We show that active storage architectures can provide large power savings, by using lower-power processors along the I/O path, while exploiting the data-level parallelism on the storage side of the system.",
    "actual_venue": "Conf Computing Frontiers"
  },
  {
    "abstract": "Perceptual image quality assessment (IQA) and sparse signal representation have recently emerged as high-impact research topics in the field of image processing. Here we make one of the first attempts to incorporate the structural similarity (SSIM) index, a promising IQA measure, into the framework of optimal sparse signal representation and approximation. In particular, we introduce a novel image denoising scheme where a modified orthogonal matching pursuit algorithm is proposed for finding the best sparse coefficient vector in maximum-SSIM sense for a given set of linearly independent atoms. Furthermore, a gradient descent algorithm is developed to achieve SSIM-optimal compromise in combining the input and sparse dictionary reconstructed images. Our experimental results show that the proposed method achieves better SSIM performance and provide better visual quality than least square optimal denoising methods.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "In this paper we propose a novel scheduling framework for a dynamic real-time environment with energy constraints. This framework dynamically adjusts the CPU voltage/frequency so that no task in the system misses its deadline and the total energy savings of the system are maximized. In this paper we consider only realistic, discrete-level speeds. Each task in the system consumes a certain amount of energy, which depends on a speed chosen for execution. The process of selecting speeds for execution while maximizing the energy savings of the system requires the exploration of a large number of combinations, which is too time consuming to be computed online. Thus, we propose an integrated heuristic methodology which executes an optimization procedure in a. low computation time. This scheme allows the scheduler to handle power-aware real-time tasks with low cost while maximizing the use of the available resources and without jeopardizing the temporal constraints of the system. Simulation results show that our heuristic methodology is able to generate power-aware scheduling solutions with near-optimal performance.",
    "actual_venue": "Ieee Real Time Technology And Applications Symposium"
  },
  {
    "abstract": "Receiving large number of data packets at different baud rates and different sizes at gateways in very high-speed network routers may lead to a congestion problem and force them to drop some packets. Several algorithms have been developed to control this problem. Random Early Detection (RED) algorithm is well commonly used. In this paper, we present an FPGA implementation of a modified version of RED able to run as fast as 10 Gbps. Furthermore, we discuss three enhancements of the RED algorithm leading a better performance suitable for FPGA implementation.",
    "actual_venue": "Ieee International Conference On Field Programmable Technology, Proceedings"
  },
  {
    "abstract": "The new business landscape ushered in by e-business has revolutionized business operations but, to date, has not integrated well with internal knowledge management initiatives. Through the development of e-business focused knowledge, organizations can accomplish three critical tasks: (1) evaluate what type of work organizations are doing in the e-business environment (know-what); (2) understand how they are doing it (know-how); and (3) determine why certain practices and companies are likely to undergo change for the foreseeable future (know-why). In this paper we take a process perspective and reflect upon the value e-business knowledge contributes in the enhancement of three core operating processes: customer relationship management, supply chain management, and product development management. Understanding how e-business impacts these core processes and the subprocesses within them, and then leveraging that knowledge to enhance these processes, is key to an organization's success in deriving superior marketplace results. In this paper, therefore, we highlight the central role knowledge management plays in diagnosing and managing e-business-driven changes in organizations.",
    "actual_venue": "Ibm Systems Journal"
  },
  {
    "abstract": "In-network data aggregation is a useful technique to reduce redundant data and to improve communication efficiency. Traditional data aggregation schemes for wireless sensor networks usually rely on a fixed routing structure to ensure data can be aggregated at certain sensor nodes. However, they cannot be applied in highly mobile vehicular environments. In this paper, we propose an adaptive forwarding delay control scheme, namely Catch-Up, which dynamically changes the forwarding speed of nearby reports so that they have a better chance to meet each other and be aggregated together. The Catch-Up scheme is designed based on a distributed learning algorithm. Each vehicle learns from local observations and chooses a delay based on learning results. The simulation results demonstrate that our scheme can efficiently reduce the number of redundant reports and achieve a good trade-off between delay and communication overhead.",
    "actual_venue": "Ieee Trans Parallel Distrib Syst"
  },
  {
    "abstract": "This paper describes the practical possibilities of mobile computing in the field of material flow management and as a solution for structured data gathering. The aim is a significant reduction of barriers for small and medium sized enterprises (SME) in order to facilitate the introduction and implementation of an integrated material flow management. Furthermore the overall quality of data gathering, and thus the quality of the data itself should profit from the approach. The state of the art is data gathering by using pencil and paper, which should be replaced by using mobile applications. There are defined data gathering methods in the field of environmental protection and business optimization such as Value Stream Mapping (VSM), Material Stream Mapping (MSM), Environmental Value Stream Mapping (EVSM) and the standardized checklist procedure proclaimed by the German Federal Environment Agency (UBA). All of them were investigated for their ability to automatize given processes. A concept of a promising and future orientated data gathering method was elaborated, which enabled the users to reuse the collected data for different visual analysis in a single software product. The implementation was made in the research project MOEBIUS, sponsored by the German Federal Ministry of Education and Research (BMBF). Within this project other mobile applications were also implemented, with the distinct focus on EMIS specific aspects as “waste”, “energy and water” and “legal compliance”, which are not discussed in this paper. For all of the invented mobile applications there was the requirement for small, simple and modular design, especially when considering the focused user group of SME. The advantage for SME is that they are able to compose different components to one software solution and in that way fit their characteristic requirements based on their company profile or specified use cases.",
    "actual_venue": "Enviroinfo"
  },
  {
    "abstract": "Memory protection mechanisms have become important in embedded systems because programs are becoming larger and more complex, and the failure of one program can corrupt other programs. In order to isolate failures and to prevent the failure of one program from propagating throughout the system, memory protection is required. Recently, memory protection is also required in safety-critical embedded systems. In embedded systems, the memory protection mechanisms are used the memory management unit (MMU) function in a CPU. However, the overhead cost of system calls to the OS is very large because the system calls are implemented by a software trap, which decreases the system performance. The goal of the present study is to provide a lightweight memory protection mechanism in the privilege memory space in order to protect a real-time OS from unintended behaviors of application programs in the privilege memory space. An application program in the privilege level in an embedded system, which is accessible to registers in peripheral devices and can execute privilege instructions for the embedded system, is crucial. We design and implement the light memory protection mechanism in the privilege memory space in real-time OS using MMU in ARM processor. We show that our memory protection mechanism is effective in a real application because of very small increase of execution time.",
    "actual_venue": "Distributed Computing Systems Workshops"
  },
  {
    "abstract": "Using energy generated with fossil fuel causes global warming due to the greenhouse effect, which threatens our environment. One of the challenges for New Generation Networks (NGN) is then the reduction of energy consumption, in particular at the BSs (Base Stations) which use about 85% of the total network energy. We contribute to the research with a mathematical model that calculates the total power consumption of a BS and enlightens the way to minimize it. First, we analyze the power consumed at every different component of the BS. Second, based on the cost incurred in turning off the BS's power amplifiers, we show how to decide whether it is convenient to keep the BS idle during those intervals in which no traffic has to be sent, or to turn off the amplifiers. Our model is evaluated by means of numerical examples, and shows that interesting power gain can be obtained under a large spectrum of load conditions.",
    "actual_venue": "Online Conference Green Communications"
  },
  {
    "abstract": "Let Γ be a finite multigraph; we denote by χ(Γ, x, y) the dichromatic polynominal of Γ, as defined by W. T. Tutte in 1953. We prove that, for any planar multigraph Γ with m edges, χ(Γ, −1, −1) = (−1)m · (−2)k, where 0 ≤ k ≤ m2. Furthermore, if Γ is connected, s = k − 1 turns out to be a pertinent invariant of the medial of Γ.",
    "actual_venue": "Journal Of Combinatorial Theory Series A"
  },
  {
    "abstract": "Markov Random Field (MRF) is an important tool and has been widely used in many vision tasks. Thus, the optimization of MRFs is a problem of fundamental importance. Recently, Veskler and Kumar et. al propose the range move algorithms, which are one of the most successful solvers to this problem. However, two problems have limited the applicability of previous range move algorithms: 1) They are limited in the types of energies they can handle (i.e. only truncated convex functions); 2) These algorithms tend to be very slow compared to other graph-cut based algorithms (e.g. α-expansion and αβ-swap). In this paper, we propose a generalized range swap algorithm (GRSA) for efficient optimization of MRFs. To address the first problem, we extend the GRSA to arbitrary semimetric energies by restricting the chosen labels in each move so that the energy is submodular on the chosen subset. Furthermore, to feasibly choose the labels satisfying the submodular condition, we provide a sufficient condition of the submodularity. For the second problem, unlike previous range move algorithms which execute the set of all possible range moves, we dynamically obtain the iterative moves by solving a set cover problem, which greatly reduces the number of moves during the optimization. Experiments show that the GRSA offers a great speedup over previous range swap algorithms, while it obtains competitive solutions.",
    "actual_venue": "Ieee/Cvf Conference On Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "The International Technology Roadmap for Semiconductors (ITRS) is a collaborative effort within the semiconductor industry to confront the challenges implicit in Moore's law. The roadmap's goal is to present an industry-wide consensus on the \"best current estimate\" of its R&D needs out to a 15-year horizon. As such, the ITRS provides a guide to the efforts of companies, research organizations, and governments to improve the quality of R&D investment decisions made at all levels. The 2001 Roadmap is notable because it was developed with truly international representation. In this article, representatives of the International Technology Working Groups for Design and Test showcase some of the contributions from 839 international experts seeking to address the difficult and exciting challenges facing the design and test communities and the semiconductor industry as a whole.",
    "actual_venue": "Ieee Computer"
  },
  {
    "abstract": "Echo state network is a novel kind of recurrent neural networks, with a trainable linear readout layer and a large fixed recurrent connected hidden layer, which can be used to map the rich dynamics of complex real-world data sets. It has been extensively studied in time series prediction. However, there may be an ill-posed problem caused by the number of real-world training samples less than the s...",
    "actual_venue": "Ieee Transactions Neural Networks And Learning Systems"
  },
  {
    "abstract": "In this age of electronic connectivity, where we all face viruses, hackers, eavesdropping and electronic fraud, there is indeed no time when security is not critical. Passwords provide security mechanism for authentication and protection services against unwanted access to resources. A graphical based password is one promising alternatives of textual passwords. According to human psychology, humans are able to remember pictures easily. In this paper, we have proposed a new hybrid graphical password based system, which is a combination of recognition and recall based techniques that offers many advantages over the existing systems and may be more convenient for the user. Our scheme is resistant to shoulder surfing attack and many other attacks on graphical passwords. This resistant scheme is proposed for small mobile devices (like smart phones i.e. ipod, iphone, PDAs etc) which are more handy and convenient to use than traditional desktop computer systems.",
    "actual_venue": "Ica3Pp"
  },
  {
    "abstract": "In this paper we analyze a simple model which attempts to describe the transmission of convolutionally encoded data over channels with memory using finite interleaving. The model could for example describe the performance of a slow frequency hopping system in which every frequency is either totally blocked or else noiseless, and in which interleaving depth is limited by the maximal delay allowed in the link. For this model, we provide an analytic expression for the Bit Error Rate for periodic bit and word interleavers, as well as for pseudo-random bit and word interleavers. The probability of a message error is also analyzed.",
    "actual_venue": "Mobile Communications"
  },
  {
    "abstract": "The model of memoryless Gaussian channel with deterministic interference are common in various engineering applications. In this letter we propose a non-data-aided algorithm for joint measurement of the interference level and the noise power. The algorithm applies an iterative approach based on the maximum likelihood criterion. The Cramer-Rao bound (CRB) is also derived as the performance benchmark. Simulation results show that the proposed approach has fast convergence speed and its measurement performance is almost identical to the CRB.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Timing speculation is a promising technique for improving microprocessor yield, in field reliability, and energy efficiency. Previous evaluations of the energy efficiency benefits of timing speculation have either been based on code compiled for a traditional target [2]--a processor that produces no errors, or code that relies on additional hardware support [6]. In this paper, we advocate that binaries for timing speculative processors should be optimized differently than those for conventional processors to maximize the energy benefits of timing speculation. Since the program binary determines the utilization pattern of the processor, which in turn influences the error rate of the processor and the energy efficiency of timing speculation, binary optimizations for timing speculative processors should attempt to manipulate the utilization of different microarchitectural units based on their likelihood of causing errors. An exploration of targeted and standard compiler optimizations demonstrates that significant energy benefits are possible from TS-aware binary optimization.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "The statistical distribution of image patch exemplars has been shown to be an effective approach to texture classification. In this paper, the joint distribution of pairs of patches for texture classification from single images is investigated. We developed a statistical method of examining texture that considers the spatial relationship of image patches, which is called the quantized patches co-occurrence matrix (QPCM). In our method, the images are first slipt into small image patches, and then the patches are quantized to the closest patch cluster centers (textons) which is learned form training images. By calculating how often pairs of patches with specific quantized values (texton labels) and in a specified spatial relationship occur in an image, we create the QPCM for images representation. Moreover, we developed a fusion framework for texture classification by fusing 4 QPCM functions with specified neighboring spatial relationship and 3 other statistical representations of image patches, which is called QPCM-SVM classifier. The effectiveness of the proposed texture classification methodology is demonstrated via an extensive consistent evaluation in standard benchmarks that clearly shows better performance against state-of-the-art statistical approach using image patch exemplars.",
    "actual_venue": "Proceedings Of Spie"
  },
  {
    "abstract": "This paper presents an extended architecture and a scheduling algorithm for a dataflow computer aimed at real-time processing. From the real-time processing point of view, current dataflow computers have several problems which stem from their hardware mechanisms for scheduling instructions based on data synchronization. This mechanism extracts as many eligible instructions as possible for execution of a program, then executes them in parallel. Hence, the computation in a dataflow computer is generally difficult to interrupt and schedule using software. To realize a controllable dataflow computation, two basic mechanisms are introduced for serializing concurrent processes and interrupting the execution of a process. A parallel and distributed algorithm for the scheduler is presented, with these two mechanisms, which controls and decides state transitions and execution order of the processes based on priority and execution depth, while still maintaining the number of the running state processes at a preferred value. To gear the scheduler algorithm to meet one of the requirements for real-time processing, such as time-constrained computing, a data-parallel algorithm for selection of the user-process with the current highest priority in O (x logxn) time is proposed, where n is the number of priority levels.",
    "actual_venue": "Realtime Systems"
  },
  {
    "abstract": "This paper presents a new approach to classify tentative feature matches as inliers or outliers during wide baseline image matching. After typical feature matching algorithms are run and tentative matches are created, our approach is used to classify matches as inliers or outliers to a transformation model. The approach uses the affine invariant property that ratios of areas of shapes are constant under an affine transformation. Thus, by randomly sampling corresponding shapes in the image pair we can generate a histogram of ratios of areas. The matches that contribute to the maximum histogram value are then candidate inliers. The candidate inliers are then filtered to remove any with a frequency below the noise level in the histogram. The resulting set of inliers are used to generate a very accurate transformation model between the images. In our experiments we show similar accuracy to RANSAC and an order of magnitude efficiency increase using this affine invariant-based approach.",
    "actual_venue": "Iciar"
  },
  {
    "abstract": "Multipartite quantum states that cannot be uniquely determined by their reduced states of all proper subsets of the parties exhibit some inherit 'high-order' correlation. This paper elaborates this issue by giving necessary and sufficient conditions for a pure multipartite state to be locally undetermined, and moreover, characterizing precisely all the pure states sharing the same set of reduced states with it. Interestingly, local determinability of pure states is closely related to a generalized notion of Schmidt decomposition. Furthermore, we find that locally undetermined states have some applications to the well-known consensus problem in distributed computing. To be specific, given some physically separated agents, when communication between them, either classical or quantum, is unreliable, then there exists a totally correct and completely fault-toleram, protocol for them to reach a consensus if and only if they share a priori a locally undetermined quantum state.",
    "actual_venue": "Quantum Information And Computation"
  },
  {
    "abstract": "Let $$G=(V,E)$$G=(V,E) be a simple graph and for every vertex $$v\\\\in V$$v¿V let $$L(v)$$L(v) be a set (list) of available colors. The graph $$G$$G is called $$L$$L-colorable if there is a proper coloring $$\\\\varphi $$¿ of the vertices with $$\\\\varphi (v)\\\\in L(v)$$¿(v)¿L(v) for all $$v\\\\in V$$v¿V. A function $$f:V(G) \\\\rightarrow \\\\mathbb N$$f:V(G)¿N is called a choice function of $$G$$G and $$G$$G is said to be $$f$$f-list colorable if $$G$$G is $$L$$L-colorable for every list assignment $$L$$L with $$|L(v)|=f(v)$$|L(v)|=f(v) for all $$v\\\\in V$$v¿V. Set $${{\\\\mathrm{size}}}(f)=\\\\sum \\\\nolimits _{v\\\\in V} f(v)$$size(f)=¿v¿Vf(v) and define the sum choice number$$\\\\chi _{sc}(G)$$¿sc(G) as the minimum of $${{\\\\mathrm{size}}}(f)$$size(f) over all choice functions $$f$$f of $$G$$G. It is easy to see that $$\\\\chi _{sc}(G)\\\\le |V|+|E|$$¿sc(G)≤|V|+|E| for every graph $$G$$G and that there is a greedy coloring of $$G$$G for the corresponding choice function $$f$$f and every list assignment with $$|L(v)|=f(v)$$|L(v)|=f(v). Therefore, a graph $$G$$G with $$\\\\chi _{sc}(G)=|V|+|E|$$¿sc(G)=|V|+|E| is called $$sc$$sc-greedy. The concept of the sum choice number was introduced in 2002 by Isaak. In 2006, Heinold characterized the broken wheels (or fan graphs) with respect to $$sc$$sc-greedyness and obtained some results for wheels. In this paper we extend the result for wheels and provide a complete characterization of wheels concerning this property.",
    "actual_venue": "Graphs And Combinatorics"
  },
  {
    "abstract": "AbstractWith the rapid growth of online education in recent years, Learning Analytics LA has gained increasing attention from researchers and educational institutions as an area which can improve the overall effectiveness of learning experiences. However, the lack of guidelines on what should be taken into consideration during application of LA hinders its full adoption. Therefore, this article investigates the issues that should be considered when approaching the design of LA experiences from the data preparation perspective. The obtained results highlight a validated LA framework of twenty-two designing issues that should be considered by various stakeholders in different contexts as well as a set of guidelines which can enhance designing LA experiences.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "The unprecedented success of social networking sites (SNSs) has been recently overshadowed by concerns about privacy risks. As SNS users grow weary of privacy breaches and thus develop distrust, they may restrict or even terminate their platform activities. In the long run, these developments endanger SNS platforms' financial viability and undermine their ability to create individual and social value. By applying a justice perspective, this study aims to understand the means at the disposal of SNS providers to leverage the privacy concerns and trusting beliefs of their users-two important determinants of user participation on SNSs. Considering that SNSs have a global appeal, empirical tests assess the effectiveness of justice measures for three culturally distinct countries: Germany, Russia and Morocco. The results indicate that these measures are particularly suited to address trusting beliefs of SNS audience. Specifically, in all examined countries, procedural justice and the awareness dimension of informational justice improve perceptions of trust in the SNS provider. Privacy concerns, however, are not as easy to manage, because the impact of justice-based measures on privacy concerns is not universal. Beyond theoretical value, this research offers valuable practical insights into the use of justice-based measures to promote trust and mitigate privacy concerns in a cross-cultural setting.",
    "actual_venue": "Communications Of The Association For Information Systems"
  },
  {
    "abstract": "This paper proposes an adaptive dictionary learning approach based on sub modular optimization. A candidate atom set is constructed based on multiple bases from the combination of analytic and trained dictionaries. With the low-frequency components by the analytic DCT atoms, high-resolution dictionaries can be inferred through online learning to make efficient approximation with rapid convergence. It is formulated as a combinatorial optimization for approximate sub modularity, which is suitable for sparse representation based on dictionaries with arbitrary structures. In single-image super-resolution, the proposed scheme has been demonstrated to improve the reconstruction performance in comparison with double sparsity dictionary in terms of both objective and subjective restoration quality.",
    "actual_venue": "DCC"
  },
  {
    "abstract": "We present a formal investigation of artifact-based systems, a relatively novel framework in service oriented computing, aimed at laying the foundations for verifying these systems through model checking. We present an infinite-state, computationally grounded semantics for these systems that allows us to reason about temporal-epistemic specifications. We present abstraction techniques for the semantics that guarantee transfer of satisfaction from the abstract system to the concrete one.",
    "actual_venue": "Ijcai"
  },
  {
    "abstract": "Markov Random Field, or MRF, models are a powerful tool for modeling images. While much progress has been made in algorithms for inference in MRFs, learning the pa- rameters of an MRF is still a challenging problem. In this paper, we show how variational optimization can be used to learn the parameters of an MRF. This method for learn- ing, which we refer to as Variational Mode Learning, finds the MRF parameters by minimizing a loss function that pe- nalizes the difference between ground-truth images and an approximate, variational solution to the MRF. In particular, we focus on learning parameters for the Field of Experts model of Roth and Black. In addition to demonstrating the effectiveness of this method, we show that a model based on derivative filters performs quite similarly to the Field of Ex- perts model. This suggests that the Field of Experts model, which is difficult to interpret, can be understood as impos- ing piecewise continuity on the image.",
    "actual_venue": "Minneapolis, Mn"
  },
  {
    "abstract": "The design of high-performance Multiprocessor Systems-on-Chip (MPSoCs) has proven to be an attractive challenge in embedded systems design automation. However, the complexity of such designs associated with short time-to-market constraints impose serious limitations on the exploration of different configurations and scenarios on the design space exploration. The use of virtual platforms may decrease the time-to-market of these architectures while providing the means to exploit, debug and verify architectures with different features. In this paper, we present the web-based Simplify framework, an interactive approach for MP-SoC exploration using an instruction-accurate Open Virtual Platform (OVP). The framework provides an environment to define both software and hardware properties in an intuitive way, and allows designers to validate the functionality as well as the behavior of the modeled architectures at high-abstraction levels. Based on the simulation reports generated from the framework, designers can perform further design modifications and optimizations, and re-validate the whole system in an efficient way, allowing increased design space exploration. For the evaluation of the proposed approach, a set of benchmark applications extracted from MiBench has been used. They run on five different processors (MIPS32, ARM7, OpenRISC (OR1K), PowerPC32 and Micro Blaze) on both mono and multiprocessor architectures and the experiments show considerable simulation speed-ups to obtain application profiling at instruction-level compared to existing approaches based on tracing.",
    "actual_venue": "Ipdps Workshops"
  },
  {
    "abstract": "Manifold learning algorithms have been proven to be capable of discovering some nonlinear structures. However, it is hard for them to extend to test set directly. In this paper, a simple yet effective extension algorithm called PIE is proposed. Unlike LPP, which is linear in nature, our method is nonlinear. Besides, our method will never suffer from the singularity problem while LPP and KLPP will. Experimental results of data visualization and classification validate the effectiveness of our proposed method.",
    "actual_venue": "Isnn"
  },
  {
    "abstract": "We study routing overhead due to location information collection and retrieval in mobile ad-hoc networks employing geographic routing with no hierarchy. We first provide a new framework for quantifying overhead due to control messages generated to exchange location information. Second, we compute the minimum number of bits required on average to describe the locations of a node, borrowing tools from information theory. This result is then used to demonstrate that the expected overhead is \\Omega (n^{1.5} \\log (n)), where n is the number of nodes, under both proactive and reactive geographic routing, with the assumptions that 1) nodes' mobility is independent, and 2) nodes adjust their transmission range to maintain network connectivity. Finally, we prove that the minimum expected overhead under the same assumptions is \\Theta (n \\log (n)).",
    "actual_venue": "Mobile Computing, Ieee Transactions"
  },
  {
    "abstract": "The paper considers a problem of multiple person tracking. We present the algorithm to automatic people tracking on surveillance videos recorded by static cameras. Proposed algorithm is an extension of approach based on tracking-by-detection of people heads and data association using Markov chain Monte Carlo (MCMC). Short track fragments (tracklets) are built by local tracking of people heads. Tracklet postprocessing and accurate results interpolation were shown to reduce number of false positives. We use position deviations of tracklets and revised entry/exit points factor to separate pedestrians from false positives. The paper presents a new method to estimate body position, that increases precision of tracker. Finally, we switched HOG-based detector to cascade one. Our evaluation shows proposed modifications significantly increase tracking quality.",
    "actual_venue": "Programming And Computer Software"
  },
  {
    "abstract": "Heterogeneous parallel systems including accelerators such as Graphics Processing Units (GPUs), are expected to play a major role in architecting the largest systems in the world, as well as the most powerful embedded devices. Impressive computational speedups have been reported for numerous algorithms in fields of medical image processing, digital signal processing, astrophysics, modeling and simulations. However, it is frequently assumed that the working data set of the application fits in the memory of the accelerator. In this paper, first we elevate this constraint by presenting a simple and scalable compile-time approach for processing large data sets based on I/O tiling. Second, we combine tiling with streaming in our asynchronous execution model, which enables efficient data-driven processing of large data sets on heterogeneous platforms with accelerators. Finally, we present results for several micro benchmarks and three data parallel kernels.",
    "actual_venue": "Ipdps Workshops"
  },
  {
    "abstract": "In multi-platform surveillance system, a prerequisite for successful fusion is the transformation of data from different platforms to a common coordinate system. However, some stochastic system biases arise during this transformation, and they seriously downgrade the global surveillance performance. Considering that the target state and the system biases are coupled and interactive, the authors pr...",
    "actual_venue": "Iet Signal Processing"
  },
  {
    "abstract": "In this paper we consider a zero-sum Markov stopping game on a general state space with impulse strategies and infinite time horizon discounted payoff where the state dynamics is a weak Feller-Markov process. One of the key contributions is our analysis of this problem based on \"shifted\" strategies, thereby proving that the original game can be practically restricted to a sequence of Dynkin's stopping games without affecting the optimalty of the saddle-point equilibria and hence completely solving some open problems in the existing literature. Under two quite general (weak) assumptions, we show the existence of the value of the game and the form of saddle-point (optimal) equilibria in the set of shifted strategies. Moreover, our methodology is different from the previous techniques used in the existing literature and is based on purely probabilistic arguments. In the process, we establish an interesting property of the underlying Feller-Markov process and the impossibility of infinite number of impulses in finite time under saddle-point strategies which is crucial for the verification result of the corresponding Isaacs-Bellman equations.",
    "actual_venue": "Siam Journal On Control And Optimization"
  },
  {
    "abstract": "A novel track initiation method using ants of different tasks, a kind of ant colony optimization (ACO) algorithm, is developed in this paper. For the proposed system of ants of different tasks, we assume that the number of tracks to be initiated equals the one of tasks, and moreover, ants of the same task search for a given track by collaboration, while ants of different tasks will compete with each other during the search process. In order to fulfill such behaviors, the pheromone model is established, and the corresponding objective function to be optimized is also presented. Numerical simulation results indicate that, for the case of bearings-only multi-sensor-multi-target tracking, the track initiation performance for the proposed system of ants of different tasks performs well compared to other track initiation methods.",
    "actual_venue": "Simulation Modelling Practice And Theory"
  },
  {
    "abstract": "This paper introduces a novel approach for the specialization of functional logic languages. We consider a maximally simplified abstract representation of programs (which still contains all the necessary information) and define a non-standard semantics for these programs. Both things mixed together allow us to design a simple and concise partial evaluation method for modern functional logic languages, avoiding several limitations of previous approaches. Moreover, since these languages can be automatically translated into the abstract representation, our technique is widely applicable. In order to assess the practicality of our approach, we have developed a partial evaluation tool for the multi-paradigm language Curry. The partial evaluator is written in Curry itself and has been tested on an extensive benchmark suite (even a meta-interpreter). To the best of our knowledge, this is the first purely declarative partial evaluator for a functional logic language.",
    "actual_venue": "Lpar"
  },
  {
    "abstract": "We propose a method to estimate the graph structure from data for a Markov random field (MRF) model. The method is valuable in many practical situations where the true topology is uncertain. First the similarities of the MRF variables are estimated by applying methods from information theory. Then, employing multidimensional scaling on the dissimilarity matrix obtained leads to a 2D topology estimate of the system. Finally, applying uniform thresholding on the node distances in the topology estimate gives the neighbourhood relations of the variables, hence defining the MRF graph estimate. Conditional independence properties of a MRF model are defined by the graph topology estimate thus enabling the estimation of the MRF model parameters e.g. through the pseudolikelihood estimation scheme. The proposed method is demonstrated by identifying MRF model for a telecommunications network, which can be used e.g. in analysing the effects of stochastic disturbances to the network state.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "High-quality computer graphics let mobile-device users access more compelling content. Still, the devices' limitations and requirements differ substantially from those of a PC. This survey of mobile graphics research describes current solutions in terms of specialized hardware (including 3D displays), rendering and transmission, visualization, and user interfaces.",
    "actual_venue": "Ieee Computer Graphics And Applications"
  },
  {
    "abstract": "In this paper, we present a test for small gate delay faults in combinational circuits, called a tenacious test and describe a method for generating tenacious tests. We consider a single gate delay fault in a circuit on the assumption of that each gate has some appropriate gate delay. First, we introduce a tenacious test V1, V2V1, V2",
    "actual_venue": "Asian Test Symposium"
  },
  {
    "abstract": "Previous research has found people to transfer behaviors from social interaction among humans to interactions with computers or robots. These findings suggest that people will talk to a robot which looks like a child in a similar way as people talking to a child. However, in a previous study in which we compared speech to a simulated robot with speech to preverbal, 10 months old infants, we did not find the expected similarities. One possibility is that people were targeting an older child than a 10 months old. In the current study, we address the similarities and differences between speech to four different age groups of children and a simulated robot. The results shed light on how people talk to robots in general.",
    "actual_venue": "Icuwb), Ieee International Conference"
  },
  {
    "abstract": "Regression-based methods have revolutionized 2D landmark localization with the exploitation of deep neural networks and massive annotated datasets in the wild. However, it remains challenging for 3D landmark localization due to the lack of annotated datasets and the ambiguous nature of landmarks under the 3D perspective. This paper revisits regression-based methods and proposes an adversarial voxe...",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "The stability of scheduled multiaccess communica- tion with random coding and independent decoding of messages is investigated. The number of messages that may be scheduled for simultaneous transmission is limited to a given maximum value, and the channels from transmitters to receiver are quasi- static, flat, and have independent fades. Requests for messa ge transmissions are assumed to arrive according to an i.i.d. arrival process. Then, we show the following: (1) in the limit of large message alphabet size, the stability region has an interference limited information-theoretic capacity interpretation, (2) state- independent scheduling policies achieve this asymptotic stability region, and (3) in the asymptotic limit corresponding to immedi- ate access, the stability region for non-idling scheduling policies is shown to be identical irrespective of received signal powers. I. I NTRODUCTION",
    "actual_venue": "Ieee International Symposium On Information Theory"
  },
  {
    "abstract": "One of the most interesting features of genomes (both coding and non-coding regions) is the presence of relatively short tandemly repeated DNA sequences known as tandem repeats (TRs). We developed a new PC-based stand-alone software analysis program, combining sequence motif searches with keywords such as organs, tissues, cell lines or development stages for finding exact, inexact and compound, TRs. Tandem Repeats Analyzer 1.5 (TRA) has several advanced repeat search parameters/options over other repeat finder programs as it does not only accept GenBank, FASTA and expressed sequence tag (EST) sequence files but also does analysis of multifiles with multisequences. Advanced user-defined parameters/options let the researchers use different motif lengths search criteria for varying motif lengths simultaneously. The outputs show statistical results to be evaluated by the user. The discovery of TRs in ESTs could be useful for both gene mapping and association studies and discovering TRs located in coding regions of important genes that are expressed under various conditions of environment, stress, organ, tissue and development stage.In this paper, we demonstrated applications of TRA using 175 899 ESTs sequences for three Arabidopsis spp. downloaded from GenBank. The EST-SSRs/ESTs ratios were found 43.1%, 15.3% and 2.34% in A.lyrata, A.thaliana and A.halleri, respectively. Analysis revealed that organs, tissues and development stages possessed different amounts of repeats and repeat compositions. This indicated that the distribution of TRs among the tissues or organs may not be random differing from the untranscribed repeats found in genomes.The program can be obtained free by anonymous FTP from ftp.akdeniz.edu.tr/Araclar/TRA.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "We develop and compare multilevel algorithms for solving constrained nonlinear variational problems via interior point methods. Several equivalent formulations of the linear systems arising at each iteration of the interior point method are compared from the point of view of conditioning and iterative solution. Furthermore, we show how a multilevel continuation strategy can be used to obtain good initial guesses (“hot starts”) for each nonlinear iteration. Some minimal surface and volume-constrained image registration problems are used to illustrate the various approaches.",
    "actual_venue": "Siam J Scientific Computing"
  },
  {
    "abstract": "In this experience paper we present a case study in using logic programming in a pervasive computing project in the healthcare domain. An expert system is used to detect healthcare activities in a pervasive hospital environment where positions of people and things are tracked. Based on detected activities an activity-driven computing infrastructure provides computational assistance to healthcare staff on mobile-and pervasive computing equipment. Assistance range from simple activities like fast log-in into the electronic patient medical record system to complexact ivities like signing for medicine given to specific patients. We describe the role of logic programming in the infrastructure and discuss the benefits and problems of using logic programming in a pervasive context.",
    "actual_venue": "Iclp"
  },
  {
    "abstract": "In this paper we discuss the design of parallel interactive continuous media servers suitable for the implementation of scalable server-based media delivery services like Video-on-Demand or Teleshopping. The main design problems for the development of such servers is to ensure the just-in-time delivery of media elements in order to maximize the Quality of Service and to minimize the buffer size at the user site. Just-in-time delivery means that the media elements should be sent as late as possible to the users but early enough to ensure a continuous replay of the media. This is important because clients have to provide buffer space for data arriving to early. The Quality of Service measures the number of data elements arrived in time at the user side. Thus, the real-time properties of the internal communication network as well as the congestion arising at the disks are of highest importance. We present models for parallel media servers and a very simple scheduler that is fully distributed and can therefore easily be implemented on a scalable parallel continuous media server. For each requested data element the scheduler sends a request to the storage subsystems at a point of time only depending on the deadline of that request, i.e. the time the data has to be delivered to the user, and the length of the path the data has to be routed through the internal network of the parallel server. In order to minimize the buffer space at the user site, and to maximize the Quality of Service, we develop timing strategies for the scheduler using simulation results as well as analytical observations.",
    "actual_venue": "Parallel Computing"
  },
  {
    "abstract": "Collaborative learning techniques are still largely disregarded within higher education. This paper investigates the reasons why the practice of collaborative learning within a tertiary environment is still seen as largely problematical.",
    "actual_venue": "Icce"
  },
  {
    "abstract": "Flexibly adapting behavior in dynamic environments relies on fronto-limbic networks that include the amygdala, orbitofrontal cortex, and striatum. Animal work demonstrates that interactions among these regions are critical for flexible feedback-guided learning, but it remains unknown to what extent such anatomical–functional interactions operate in humans. Here, we use connectivity analyses in both structural and functional MRI to further our understanding of how brain circuits work in conjunction to promote goal-directed behavior. In particular, fiber tracking based on diffusion-weighted imaging provides information about anatomical connectivity between brain structures, and functional MRI provides estimates of functional connectivity between structures. We found that, during a feedback-guided reversal learning task, the strength of estimated white matter tracts from the amygdala to the hippocampus, orbitofrontal cortex, and ventral striatum predicted both how subjects adapted their behavior following positive and negative feedback, and the functional connectivity (estimated from functional MRI time series) between the amygdala and these regions. In addition, we identified a dissociation between an amygdala-hippocampus circuit that predicted response switching, and an amygdala-orbitofrontal cortex circuit that predicted learning following rule reversals. These findings provide novel insights into how the anatomy and functioning of amygdala-related brain circuits mediate different aspects of feedback-guided learning behavior.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "To guarantee Quality of Service(QoS) for real-time applications strictly, we have proposed an output buffer control mechanism in IP routers, confirmed its effectiveness through simulations and implemented a prototype. This mechanism can guarantee QoS strictly within single router. In this paper, we propose the control scheme of mutual cooperation between IP routers equipped with the above-mentioned mechanism by using one of the signaling protocols. Our proposed scheme aims to stabilize End-to-End (EM) delay of a flow within target delay. Also, reserved resources are dynamically updated between IP routers with our mechanism to improve E2E packet loss rate. We perform the implemental design of our scheme and empirical evaluation through the implementation. As one of results, we validated quantitatively that our scheme improves the quality of video picture.",
    "actual_venue": "Proceedings - International Conference On Computer Communications And Networks, Icccn"
  },
  {
    "abstract": "Large-scale maintenance in industrial plants requires the entire shutdown of production units for disassembly, comprehensive inspection, and renewal. We derive models and algorithms for this so-called turnaround scheduling that include different features such as time-cost trade-off, precedence constraints, external resource units, resource leveling, different working shifts, and risk analysis. We propose a framework for decision support that consists of two phases. The first phase supports the manager in finding a good makespan for the turnaround. It computes an approximate project time-cost trade-off curve together with a stochastic evaluation. Our risk measures are the expected tardiness at time t and the probability of completing the turnaround within time t. In the second phase, we solve the actual scheduling optimization problem for the makespan chosen in the first phase heuristically and compute a detailed schedule that respects all side constraints. Again, we complement this by computing upper bounds for the same two risk measures. Our experimental results show that our methods solve large real-world instances from chemical manufacturing plants quickly and yield an excellent resource utilization. A comparison with solutions of a mixed-integer program on smaller instances proves the high quality of the schedules that our algorithms produce within a few minutes.",
    "actual_venue": "Informs Journal On Computing"
  },
  {
    "abstract": "In this paper we develop a quantum network with a mutual quantum secure direct communication scheme based on multiparty quantum secret sharing. This quantum network, assuming to contain clusters S, M, and D, shares a sequence of single photons and Greenberger-Horne-Zeilinger (GHZ) states. Each cluster is made of the same or similar quantum nodes gathered or occurring closely together. The feature of this scheme is that the communication between two clusters depends on the agreement of the third cluster. We also prove that such a quantum network is unconditionally secure.",
    "actual_venue": "Snpd"
  },
  {
    "abstract": "Wireless ad hoc networks will be an important component in future communication systems. The performance of wireless ad hoc networks can be improved by link quality-aware applications. Wireless link quality is dynamic in nature, especially in mobile scenarios. Therefore, accurate and fast packet delivery ratio estimation is a prerequisite to good performance in mobile, multi-hop and multi-rate wireless ad hoc networks. In this paper, we propose a novel packet delivery ratio estimation method that improves the accuracy and responsiveness of the packet delivery ratio estimation. The proposed link quality estimation components are implemented in a IEEE 802.11b/g test-bed. The experiment results show that the accuracy of the packet delivery ratio estimation can improve up to 50% in mobile scenarios without introducing overhead. We also show the end-to-end performance impact of this improved estimation on route selection using different routing metrics and configurations. The measurement results show that our packet delivery ratio method leads to better route selection in the form of increased end-to-end throughput compared to traditional methods, which respond slowly to the link dynamics.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "Wireless sensor network consists of large number of inexpensive tiny sensors which are connected with low power wireless communications. Most of the routing and data dissemination protocols of WSN assume a homogeneous network architecture, in which all sensors have the same capabilities in terms of battery power, communication, sensing, storage, and processing. However the continued advances in miniaturization of processors and low-power communications have enabled the development of a wide variety of nodes. When more than one type of node is integrated into a WSN, it is called heterogeneous. Multihop short distance communication is an important scheme to reduce the energy consumption in a sensor network because nodes are densely deployed in a WSN. In this paper M-EECDA (Multihop Energy Efficient Clustering & Data Aggregation Protocol for Heterogeneous WSN) is proposed and analyzed. The protocol combines the idea of multihop communications and clustering for achieving the best performance in terms of network life and energy consumption. M-EECDA introduces a sleep state and three tier architecture for some cluster heads to save energy of the network. M-EECDA consists of three types of sensor nodes: normal, advance and super. To become cluster head in a round normal nodes use residual energy based scheme. Advance and super nodes further act as relay node to reduce the transmission load of a normal node cluster head when they are not cluster heads in a round.",
    "actual_venue": "International Journal Of Computer Applications"
  },
  {
    "abstract": "This paper presents an online and integrated solution for diagnosis and healing of the image sensors based on a defective pixel detection and concealment method. The proposed defective pixel detection method is based on the evaluation and analysis of the median, range and other local dispersion parameters of the pixel blocks in the images taken by the to-be-tested image sensor. Once a defective pixel is detected, the concealment process uses a median filter, which consists of the substitution of the defective value by the median value of the pixel block. For an error detected in the image sensor, a diagnostic process can be used to determine the origin of the problem. The global detection and concealment process was implemented on an FPGA platform to evaluate the computational complexity and to confirm the feasibility of integrating the proposed solutions in image sensors.",
    "actual_venue": "Ieee Vlsi Test Symposium"
  },
  {
    "abstract": "This paper considers a situation where a video sequence is to be compressed and transmitted over a wireless channel. Our goal is to minimize the amount of distortion in the reconstructed video sequence under certain channel bandwidth and transmission power constraints, with transmission power allocated across packets. Two power allocation algorithms under more realistic assumptions than those used in previous work are proposed, allocating transmission power to packets according to their relative importance. Simulations are conducted for a time-correlated Rayleigh fading channel with multiple transmit antennas, and results demonstrate the performance improvement of the proposed methods over the conventional constant power method.",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "The following topics are dealt with: emotion recognition; learning (artificial intelligence); face recognition; feature extraction; medical signal processing; human computer interaction; behavioural sciences computing; cognition; social sciences computing; psychology.",
    "actual_venue": "International Conference On Affective Computing And Intelligent Interaction Workshops And Demos"
  },
  {
    "abstract": "Pixel-value-ordering (PVO) appears as an efficient solution for high-fidelity reversible data hiding. State-of-the-art PVO schemes split the host image into blocks, sort pixels within the blocks based on their graylevel and, finally, embed data into some differences between the sorted values. This paper investigates the use of the prediction error instead of the pixel graylevel both for ordering and embedding. The proposed prediction-error-ordering (PEO) scheme also introduces a two-stage procedure by splitting each block in two sets following a chessboard pattern and by processing set by set each block. The pixels of one set are used to classify and predict the ones of the other set and vice-versa. The proposed PEO approach outperforms the state-of-the-art PVO schemes.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "Parameter estimation for subsurface flow models is an essential step for maximizing the value of numerical simulations for future prediction and the development of effective control strategies. We propose the iterative stochastic ensemble method (ISEM) as a general method for parameter estimation based on stochastic estimation of gradients using an ensemble of directional derivatives. ISEM eliminates the need for adjoint coding and deals with the numerical simulator as a blackbox. The proposed method employs directional derivatives within a Gauss-Newton iteration. The update equation in ISEM resembles the update step in ensemble Kalman filter, however the inverse of the output covariance matrix in ISEM is regularized using standard truncated singular value decomposition or Tikhonov regularization. We also investigate the performance of a set of shrinkage based covariance estimators within ISEM. The proposed method is successfully applied on several nonlinear parameter estimation problems for subsurface flow models. The efficiency of the proposed algorithm is demonstrated by the small size of utilized ensembles and in terms of error convergence rates.",
    "actual_venue": "J Comput Physics"
  },
  {
    "abstract": "Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.",
    "actual_venue": "Ieee International Conference On Big Data"
  },
  {
    "abstract": "We focus on a setting where agents in a social network consume a product that exhibits positive local network externalities. A seller has access to data on past consumption decisions/prices for a subset of observable agents, and can target these agents with appropriate discounts to exploit network effects and increase her revenues. A novel feature of the model is that the observable agents potentially interact with additional latent agents. These latent agents can purchase the same product from a different channel, and are not observed by the seller. Observable agents influence each other both directly and indirectly through the influence they exert on the latent agents. The seller knows the connection structure of neither the observable nor the latent part of the network. Due to the presence of network externalities, an agentu0027s consumption decision depends not only on the price offered to her, but also on the consumption decisions of (and in turn the prices offered to) her neighbors in the underlying network. We investigate how the seller can use the available data to estimate the matrix that captures the dependence of observable agentsu0027 consumption decisions on the prices offered to them. We provide an algorithm for estimating this matrix under an approximate sparsity condition, and obtain convergence rates for the proposed estimator despite the high dimensionality that allows more agents than observations. Importantly, we then show that this approximate sparsity condition holds under standard conditions present in the literature and hence our algorithms are applicable to a large class of networks. We establish that by using the estimated matrix the seller can construct prices that lead to a small revenue loss relative to revenue-maximizing prices under complete information, and the optimality gap vanishes relative to the size of the network.",
    "actual_venue": "Arxiv: Social And Information Networks"
  },
  {
    "abstract": "Tabu Search (TS) is a well known local search method which has been widely used for solving AI problems. Different versions of TS have been proposed in the literature, and many features of TS have been considered and tested experimentally. The feature that is present in almost all TS variants is the so called (short-term) tabu list which is recognised as the crucial issue of TS. However, the definition of the parameters associated with the tabu list remains in most TS applications still a handcrafted activity. In this work we undertake a systematic study of the relative influence of few relevant tabu list features on the performances of TS solvers. In particular, we apply statistical methods for the design and analysis of experiments. The study focuses on a fundamental theoretical problem (GRAPH COLOURING) and on one of its practical specialisation (EXAMINATION TIMETABLING), which involves specific constraints and objectives. The goal is to determine which TS features are more critical for the good performance of TS in a general context of applicability. The general result is that, when the quantitative parameters are well tuned, the differences with respect to qualitative parameters become less evident.",
    "actual_venue": "Ecai"
  },
  {
    "abstract": "Super-resolution technology provides an effective way to increase image resolution by incorporating additional information from successive input images or training samples. Various super-resolution algorithms have been proposed based on different assumptions, and their relative performances can differ in regions of different characteristics within a single image. Based on this observation, an adaptive algorithm is proposed in this paper to integrate a higher level image classification task and a lower level super-resolution process, in which we incorporate reconstruction-based super-resolution algorithms, single-image enhancement, and image/video classification into a single comprehensive framework. The target high-resolution image plane is divided into adaptive-sized blocks, and different suitable super-resolution algorithms are automatically selected for the blocks. Then, a deblocking process is applied to reduce block edge artifacts. A new benchmark is also utilized to measure the performance of super-resolution algorithms. Experimental results with real-life videos indicate encouraging improvements with our method.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "Land surface water mapping is one of the most basic classification tasks to distinguish water bodies from dry land surfaces. In this paper, a water mapping method was proposed based on multi-scale level sets and a visual saliency model (MLSVS), to overcome the lack of an operational solution for automatically, rapidly and reliably extracting water from large-area and fine spatial resolution Synthetic Aperture Radar (SAR) images. This paper has two main contributions, as follows: (1) The method integrated the advantages of both level sets and the visual saliency model. First, the visual saliency map was applied to detect the suspected water regions (SWR), and then the level set method only needed to be applied to the SWR regions to accurately extract the water bodies, thereby yielding a simultaneous reduction in time cost and increase in accuracy; (2) In order to make the classical Itti model more suitable for extracting water in SAR imagery, an improved texture weighted with the Itti model (TW-Itti) is employed to detect those suspected water regions, which take into account texture features generated by the Gray Level Co-occurrence Matrix (GLCM) algorithm, Furthermore, a novel calculation method for center-surround differences was merged into this model. The proposed method was tested on both Radarsat-2 and TerraSAR-X images, and experiments demonstrated the effectiveness of the proposed method, the overall accuracy of water mapping is 98.48% and the Kappa coefficient is 0.856.",
    "actual_venue": "Isprs International Journal Of Geo-Information"
  },
  {
    "abstract": "The millimeter wave band will be a key component in the next generation wireless communication system (5G), and a proper channel model is needed for developing the future 5G cellular technologies. This paper introduces a 28 GHz channel sounder based on automatically rotating horn antennas with time synchronization ability between the transmitter (TX) and the receiver (RX). We have developed a sounding method for synchronized measurement using the proposed channel sounder system which records time-stamping to measure the relative propagation time from the transmitter to receiver. The proposed approach allows us to generate an omni-like channel measurements by synthesizing all directional measurements, which have been verified by comparison with the measurements of omni-directional antennas. Subsequently, the omni-directional propagation measurement results are provided in an in-building environment similar to a small shopping mall. From the measurements, the clustering analysis has been done including its power distribution. This paper provides the first channel sounder and initial results to obtain the synthesized omni-directional results in millimeter wave 28 GHz band.",
    "actual_venue": "Blackseacom"
  },
  {
    "abstract": "Coalition formation is a key topic in multiagent systems. One may prefer a coalition structure that maximizes the sum of the values of the coalitions, but often the number of coalition structures is too large to allow exhaustive search for the optimal one. But then, can the coalition structure found via a partial search be guaranteed to be within a bound from optimum? Sandholm et al. showed that it suffices to search the lowest two levels of the coalition structure graph in order to establish a worst case bound K(n). Dang et al. presented an algorithm that takes a step further to search those coalition structures whose biggest coalition's cardinality is greater than or equal to ⌈n(k − 1)/(k + 1)⌉, which is the best result known so far. Against this background, this paper reports on a novel anytime algorithm based on cardinality structure that only have to take a step further to search those coalition structures whose cardinality structure is in the CCS(n, b). Consequently, the algorithm reported in this paper is obviously better than that of Sandholm et al. (up to 1035 times faster when n=100, K=2) and Dang et al (up to 1018 times faster when n=100, K=3).",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "•A novel metric measuring the class distribution imbalance is proposed.•Theoretical properties of the proposed distribution imbalance metric are studied.•Two adaptive sampling-based approaches are proposed for imbalance learning.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "A busy system makes thousands of scheduling decisions per second, so the speed with which scheduling decisions are made is critical to the performance of the system as a whole. This article - excerpted from the forthcoming book, \"The Design and Implementation of the FreeBSD Operating System\" - uses the example of the open source FreeBSD system to help us understand thread scheduling. The original FreeBSD scheduler was designed in the 1980s for large uniprocessor systems. Although it continues to work well in that environment today, the new ULE scheduler was designed specifically to optimize multiprocessor and multithread environments. This article first studies the original FreeBSD scheduler, then describes the new ULE scheduler. The article does not describe the realtime scheduler that is also available in FreeBSD.",
    "actual_venue": "Acm Queue"
  },
  {
    "abstract": "Automatic recognition of human activities and behaviors is still a challenging problem for many reasons, including limited accuracy of the data acquired by sensing devices, high variability of human behaviors, and gap between visual appearance and scene semantics. Symbolic approaches can significantly simplify the analysis and turn raw data into chains of meaningful patterns. This allows getting rid of most of the clutter produced by low-level processing operations, embedding significant contextual information into the data, as well as using simple syntactic approaches to perform the matching between incoming sequences and models. We propose a symbolic approach to learn and detect complex activities through the sequences of atomic actions. Compared to previous methods based on context-free grammars, we introduce several important novelties, such as the capability to learn actions based on both positive and negative samples, the possibility of efficiently retraining the system in the presence of misclassified or unrecognized events, and the use of a parsing procedure that allows correct detection of the activities also when they are concatenated and/or nested one with each other. An experimental validation on three datasets with different characteristics demonstrates the robustness of the approach in classifying complex human behaviors. (C) 2014 SPIE and IS&T",
    "actual_venue": "Journal Of Electronic Imaging"
  },
  {
    "abstract": "Once we assumed that Web accessibility is a right, we implicitly state the necessity of a governance of it. Beyond any regulation, institutions must provide themselves with suitable tools to control and support accessibility on typically large scale scenarios of content and resources. No doubt, the economic impact and effectiveness of these tools affect accessibility level. In this paper, we propose an application to effectively monitor Web accessibility from a geo-political point of view, by referring resources to the specific (category of) institutions which are in charge of it and to the geographical places they are addressed to. Snapshots of such a macro level spatial-geo-political analysis can be used to effectively focus investments and skills where they are actually necessary.",
    "actual_venue": "Assets"
  },
  {
    "abstract": "Homogeneous Charge Compression Ignition (HCCI) is a promising concept for combustion engines to reduce both emissions and fuel consumption. In HCCI engines, a homogeneous air-fuel mixture auto-ignites due to compression, which is unlike traditional spark ignition or diesel engines where ignition is started with either a spark or fuel injection. HCCI combustion control is a challenging issue because there is no direct initiator of combustion in these engines. Variable Valve Timing (VVT) is one effective way to control the combustion timing in HCCI engines. VVT changes the amount of trapped residual gas and the effective compression ratio both of which have a strong effect on combustion timing. In order to control HCCI combustion, a physics based control oriented model is developed that includes the effect of trapped residual gas on combustion timing. The control oriented model is obtained by model order reduction of complex chemical kinetic reaction mechanisms. This method allows different fuels to be incorporated using a standard methodology and fills the gap between complex models with highly detailed chemical kinetics and simple black box models that have been used in model based control. The control oriented model is used to develop ignition timing PI control using simulation. The PI control modulates the trapped residual gas using variable valve timing as the actuator. The results indicate that the controller can track step changes in HCCI combustion timing.",
    "actual_venue": "American Control Conference"
  },
  {
    "abstract": "In their 1996 article, Lickteig and Roy introduced a fast “divide and conquer” variant of the subresultant algorithm which avoids coefficient growth in defective cases. The present article concerns the complexity analysis of their algorithm over effective rings endowed with the partially defined division routine. We achieve essentially the same kind of complexity bound as over effective fields. As a consequence we obtain new convenient complexity bounds for gcds, especially when coefficients are in abstract polynomial rings where evaluation/interpolation schemes are not supposed to be available.",
    "actual_venue": "Journal Of Symbolic Computation"
  },
  {
    "abstract": "Most contemporary software and other engineering methods rely on modeling and automatic translations of models into different forms. The author advocates a UML-based translation specification method that is visual and therefore easier to understand and use.",
    "actual_venue": "Ieee Software"
  },
  {
    "abstract": "Summary Nowadays, the Italian science sector is undergoing a strategic reform due to budget cuts and there is a need for measuring and evaluating research performance of public research institutes. This research presents a new measure to assess the scientific research performance of public research institutes. The new model is successfully applied to 108 public research institutes belonging to the Italian National Research Council, using data from year 2003 and displays the laboratories with high/low performance. The results are substantially stronger and quicker to obtain than those calculated by using conventional indicators. This model supports the policy-makers, who must decide about the level and direction of public funding for research and technology transfer.",
    "actual_venue": "Scientometrics"
  },
  {
    "abstract": "The impending environmental issues and growing concerns for global energy crises are driving the need for new opportunities and technologies that can meet significantly higher demand for cleaner and sustainable energy systems. This necessitates the development of transportation and power generation systems. The electrification of the transportation system is a promising approach to green the transportation systems and to reduce the issues of climate change. This paper inspects the present status, latest deployment, and challenging issues in the implementation of Electric vehicles (EVs) infrastructural and charging systems in conjunction with several international standards and charging codes. It further analyzes EVs impacts and prospects in society. A complete assessment of charging systems for EVs with battery charging techniques is explained. Moreover, the beneficial and harmful impacts of EVs are categorized and thoroughly reviewed. Remedial measures for harmful impacts are presented and benefits obtained therefrom are highlighted. Bidirectional charging offers the fundamental feature of vehicle to grid technology. In this paper, the current challenging issues due to the massive deployment of EVs, and upcoming research trends are also presented. It is envisioned that the researchers interested in such areas can find this paper valuable and an informative one-stop source.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "With the application of community detection in complex networks becoming more and more extensive, the application of more and more algorithms for community detection are proposed and improved. Among these algorithms, the label propagation algorithm is simple, easy to perform and its time complexity is linear, but it has a strong randomness. Small communities in the label propagation process are easy to be swallowed. Therefore, this paper proposes a method to improve the partition results of label propagation algorithm based on the pre-partition by circularly searching core nodes and assigning label for nodes according to similarity of nodes. First, the degree of each node of the network is calculated. We go through the whole network to find the nodes with the maximal degrees in the neighbors as the core nodes. Next, we assign the core nodes' labels to their neighbors according to the similarity between them, which can reduce the randomness of the label propagation algorithm. Then, we arrange the nodes whose labels had not been changed as the new network and find the new core nodes. After that, we update the labels of neighbor nodes according to the similarity between them again until the end of the iteration, to complete the pre-partition. The approach of circularly searching for core nodes increases the diversity of the network partition and prevents the smaller potential communities being swallowed in the process of partition. Then, we implement the label propagation algorithm on the whole network after the pre-partition. Finally, we adopt a modified method based on the degree of membership determined by the bidirectional attraction of nodes and their neighbor communities. This method can reduce the possibility of the error in partition of few nodes. Experiments on artificial and real networks show that the proposed algorithm can accurately divide the network and get higher degree of modularity compared with five existing algorithms.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "The proliferation of knowledge-sharing communities and the advances in information extraction have enabled the construction of large knowledge bases using the RDF data model to represent entities and relationships. However, as the Web and its latently embedded facts evolve, a knowledge base can never be complete and up-to-date. On the other hand, a rapidly increasing suite of Web services provide access to timely and high-quality information, but this is encapsulated by the service interface. We propose to leverage the information that could be dynamically obtained from Web services in order to enrich RDF knowledge bases on the fly whenever the knowledge base does not suffice to answer a user query. To this end, we develop a sound framework for appropriately generating queries to encapsulated Web services and efficient algorithms for query execution and result integration. The query generator composes sequences of function calls based on the available service interfaces. As Web service calls are expensive, our method aims to reduce the number of calls in order to retrieve results with sufficient recall. Our approach is fully implemented in a complete prototype system named ANGIE1. The user can query and browse the RDF knowledge base as if it already contained all the facts from the Web services. This data, however, is gathered and integrated on the fly, transparently to the user. We demonstrate the viability and efficiency of our approach in experiments based on real-life data provided by popular Web services.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "An asynchronous, distributed algorithm is presented that determines the optimal transmission rate allocation in computer networks with virtual circuit routing. The flow control problem is formulated as a gradient hill-climbing algorithm. It is distributed, since the entry node for each virtual circuit iteratively computes the rate allocation for that virtual circuit. The entry node communicates with the associated user during each interaction and obtains information about the rest of the network by sending control packets along the virtual circuit. It is shown that all of the necessary information to solve the optimization problem can be obtained in this fashion and that the algorithm will converge, even though there is no synchronization between nodes and some computations are made with outdated information.",
    "actual_venue": "Computers, IEEE Transactions  "
  },
  {
    "abstract": "Systems-of-Systems (SoS) encompass independent, heterogeneous software-intensive systems (called constituents) that collaborate among themselves to accomplish larger missions. SoS architectures are inherently dynamic, which causes frequent reconfigurations that can quickly degrade the quality of the SoS architecture, as it can deviate from its initial prescriptive architecture, a phenomenon that has been referred to as architectural drift. Then, it is important to perform a reconciliation, i.e., a propagation of changes in the current concrete architecture to its associated prescriptive architecture, making them synchronized. The main contribution of this paper is proposing Back-SoS, a model-based approach that reconciles descriptive architecture of the SoS with its intended prescriptive architecture, thus supporting the verification of conformance between architectural configurations that can be obtained at runtime and their prescriptive design.",
    "actual_venue": "Sac : Symposium On Applied Computing Pau France April"
  },
  {
    "abstract": "We present a simple yet robust signed distance field (SDF) generator based on recent GPU architectures. In our approach, the squared Euclidean distance is calculated for each triangle face in parallel, and then an optimized stream reduction process is used to find the shortest distance. During this process, the stream reduction operation acts like a parallel binary space-searching routine for each level. This process uses computations and memory bandwidth efficiently because of the massive number of CUDA threads. Signs are then determined by calculating angle-weighted pseudonormals. Unlike some previous SDF approaches that only calculate the SDF near the surface or within the bounding box, our method can calculate the SDF adaptively so that there are no limitations on proximity or regularity. We also compare our GPU-based results with a kd-tree based single CPU approach for a 3D geometry synthesis application.",
    "actual_venue": "CIT"
  },
  {
    "abstract": "Multithreaded programming is an effective way to exploit concurrency, but it is difficult to debug and tune a highly threaded program. This paper describes a performance tool called Tmon for monitoring, analyzing and tuning the performance of multithreaded programs. The performance tool has two novel features: it uses \"thread waiting time\" as a measure and constructs thread waiting graphs to show thread dependencies and thus performance bottlenecks, and it identifies \"semi-busy-waiting\" points where CPU cycles are wasted in condition checking and context switching. We have implemented the Tmon tool and, as a case study, we have used it to measure and tune a heavily threaded file system. We used four workloads to tune different aspects of the file system. We were able to improve the file system bandwidth and throughput significantly. In one case, we were able to improve the bandwidth by two orders of magnitude.",
    "actual_venue": "Sigmetrics"
  },
  {
    "abstract": "Lets(d, n) be the number of triangulations withn labeled vertices ofSd−1, the (d−1)-dimensional sphere. We extend a construction of Billera and Lee to obtain a large family of triangulated spheres. Our construction shows that logs(d, n)≥C1(d)n[(d−1)/2], while the known upper bound is logs(d, n)≤C2(d)n[d/2] logn.",
    "actual_venue": "Discrete And Computational Geometry"
  },
  {
    "abstract": "A computational system called the polynomial residue number system (PRNS) has previously been proposed and analyzed. It solves the problem of multiplying two univariate polynomials modulo (xN±1) over the modular ring Zp. In the present paper, extensions of PRNS for computing the product of two multivariate polynomials modulo a polynomial are developed. Such a number system is termed as multivariate polynomial residue number system (MPRNS). MPRNS is essentially an isomorphic representation between the ring Zp[x]/Πi=1L (xi(N i)±1) of L-variate polynomials in the indeterminate vector x=(x1, x2, …, xL) and the ring Zp(N1N2…NL). Issues related to existence of isomorphic mappings, their properties and multiplicative complexity of the resulting algorithm have been addressed. The applications of the MPRNS scheme are also presented",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "Destination prediction is an essential task for many emerging location-based applications such as recommending sightseeing places and targeted advertising according to destinations. A common approach to destination prediction is to derive the probability of a location being the destination based on historical trajectories. However, almost all the existing techniques use various kinds of extra information such as road network, proprietary travel planner, statistics requested from government, and personal driving habits. Such extra information, in most circumstances, is unavailable or very costly to obtain. Thereby we approach the task of destination prediction by using only historical trajectory dataset. However, this approach encounters the \\\"data sparsity problem\\\", i.e., the available historical trajectories are far from enough to cover all possible query trajectories, which considerably limits the number of query trajectories that can obtain predicted destinations. We propose a novel method named Sub-Trajectory Synthesis (SubSyn) to address the data sparsity problem. SubSyn first decomposes historical trajectories into sub-trajectories comprising two adjacent locations, and then connects the sub-trajectories into \\\"synthesised\\\" trajectories. This process effectively expands the historical trajectory dataset to contain much more trajectories. Experiments based on real datasets show that SubSyn can predict destinations for up to ten times more query trajectories than a baseline prediction algorithm. Furthermore, the running time of the SubSyn-training algorithm is almost negligible for a large set of 1.9 million trajectories, and the SubSyn-prediction algorithm runs over two orders of magnitude faster than the baseline prediction algorithm constantly.",
    "actual_venue": "Vldb J"
  },
  {
    "abstract": "Verursacht durch Informations- und Kommunikationstechnologien steigen Szenarien, in welchen Partizipation stattfindet. Jedoch, nur durch geschickt konzipierte und eingesetzte Informationssysteme kann Partizipation auch gelingen. Folgender Beitrag leitet aus unterschiedlichsten Formen und Erklärungsansätzen aus dem, was heute unter Partizipation begriffen wird, wichtige Probleme, Bedingungen und Erfolgsfaktoren ab. Dem gegenüber stellt er Werkzeuge, Merkmale und Konzepte für Informationssysteme und zeigt, wie diese Probleme der Partizipation verringern und Erfolgschancen erhöhen können.",
    "actual_venue": "Hmd - Praxis Wirtschaftsinform"
  },
  {
    "abstract": "Being able to analyze and interpret signal coming from electroencephalogram\n(EEG) recording can be of high interest for many applications including medical\ndiagnosis and Brain-Computer Interfaces. Indeed, human experts are today able\nto extract from this signal many hints related to physiological as well as\ncognitive states of the recorded subject and it would be very interesting to\nperform such task automatically but today no completely automatic system\nexists. In previous studies, we have compared human expertise and automatic\nprocessing tools, including artificial neural networks (ANN), to better\nunderstand the competences of each and determine which are the difficult\naspects to integrate in a fully automatic system. In this paper, we bring more\nelements to that study in reporting the main results of a practical experiment\nwhich was carried out in an hospital for sleep pathology study. An EEG\nrecording was studied and labeled by a human expert and an ANN. We describe\nhere the characteristics of the experiment, both human and neuronal procedure\nof analysis, compare their performances and point out the main limitations\nwhich arise from this study.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "The paper presents a passive ranging method for estimating distances to an object using a video sequence gathered from a moving platform. The motivation is provided by potential application to object distance estimation using video data from general aviation aircraft. The method exploits scale changes of an object in the video sequence, as inferred by processing wavelet transforms of video frames, to compute distance. The underlying principles are presented along with results of bench experiments",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Proceedings, Ieee International Conference"
  },
  {
    "abstract": "Research and application of Web text mining is an important branch in the data mining. Web is the biggest information system currently. Now people mainly use the search engine to look up Web information. The problem of getting reliable rate and comprehensive rate is increasingly convex, and it is very difficult to mine data further. Now the search engine can hardly provide individual service according to different need of different customers. However, Web text mining aims at resolving this problem. This paper discusses an Algorithm of how to follow the appointed Website or Web page according to the user's request, how to analysis, compare, sample, reserve and classify the data information combined with the Web page text contents for later use. The model of Web text mining, mining algorithm and implement technique are discussed in details. Keywords: Web text mining; individual service; Text Classification; Characteristic Vector; Similitude degree",
    "actual_venue": "SKG"
  },
  {
    "abstract": "A new generation of quantum Hall array resistance standards (QHARS) designed from GaAs/AlGaAs heterostructures with double two-dimensional electron gases (2DEGs) has been developed. First results of measurements carried out on a single Hall bar (R/sub K//4) and 50 Hall bars placed in parallel by triple connections (R/sub K//200) are presented.",
    "actual_venue": "Ieee T Instrumentation And Measurement"
  },
  {
    "abstract": "We develop a rule-based contingency misfit model and related hypotheses to test empirically the Burton and Obel (1998) multi contingency model for strategic organizational design. The model is a set of \"if-then\" misfit rules, in which misfits lead to a loss in performance; they are complements to the strategy and organizational contingency theory fit rules. Using data from 224 small- and medium-sized Danish firms, misfits are categorized and identified. Then, performance hypotheses are developed and tested using regression models. We confirm the hypotheses that firms with situational misfits or contingency misfits, or both, incur performance losses in return on assets compared with firms with no misfits. Contrary to our hypotheses, we did not find that additional misfits lead to increased performance loss. Our results suggest that just one misfit of any kind may significantly compromise performance. These results yield a deeper understanding of organizational contingency theory, as well as implications for the rule-based fit-misfit organizational design model.",
    "actual_venue": "Management Science"
  },
  {
    "abstract": "Differing from the traditional traffic, connected vehicles enable information sharing between vehicles at vicinity to facilitate cooperative path planning, which may positively affect the congestion propagation process. In this paper, we propose to modeling and simulating traffic congestion propagation in such new situation where the path planning is driven by a temporal or spatial preference with aims at investigating the effects of various factors on traffic congestion, e.g. traffic light, mobility pattern, traffic density and communication radius. Simulations show that the traffic congestion is indeed affected by the concerned factors; however, the traffic congestion fails to be mitigated persistently as the communication radius increases beyond a certain threshold. The result is helpful for understanding the traffic congestion propagation in connected vehicles.",
    "actual_venue": "Wireless Networks"
  },
  {
    "abstract": "Sometimes knowledge engineers have to come up with new ontologies for some specific domain or application even though there are already ontological works in that area. Although there are techniques for ontology de/composition, a common problem is that the existing systems often characterise some notion in a way that is not quite what the knowledge engineer needs. The change of a few notions in a given ontology can be challenging: it is not easy to understand the impact of these changes. In this paper we investigate another route. Assuming that the knowledge engineer has to deal with notions at the mesoscopic level that are cognitively clear, we propose to independently characterised them (in a sense to be discussed), and look at how they can be used to build an ontology such that it comprises only the needed notions and with the right meaning. Here we just explore some important steps of this approach, and list other problems that need to be faced.",
    "actual_venue": "Jowo@Ijcai"
  },
  {
    "abstract": "With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification Expectation-Maximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.",
    "actual_venue": "Cvpr"
  },
  {
    "abstract": "We developed a compact tactile imaging (TI) system to guide the clinician or the self-user for noninvasive detection of breast tumors. Our system measures the force distribution based on the difference in stiffness between a palpated object and an abnormality within. The average force resolution, force range, and the spatial resolution of the device are 0.02 N, 0-4 N, and 2.8 mm, respectively. To evaluate the performance of the proposed TI system, compression experiments were performed to measure the sensitivity and specificity of the system in detecting tumor-like inclusions embedded in tissue-like cylindrical silicon samples. Based on the experiments performed with 11 inclusions, having two different sizes and two different stiffnesses located at three different depths, our TI system showed an average sensitivity of 90.8 ± 8.1 percent and an average specificity of 89.8 ± 12.7 percent. Finally, manual palpation experiments were performed with 12 human subjects on the same silicon samples and the results were compared to that of the TI system. The performance of the TI system was significantly better than that of the human subjects in detecting deep inclusions while the human subjects performed slightly better in detecting shallow inclusions close to the contact surface.",
    "actual_venue": "Haptics, Ieee Transactions"
  },
  {
    "abstract": "IEEE 802.15.4 (for low-rate Wireless Personal Area Networks--WPANs) (IEEE 802.15.4 Standard-2003) and IEEE 802.15.7 (for Short-Range Wireless Optical Communication Using Visible Light) (IEEE 802.15.7 Standard--2011) are two typical standards for WPANs that support Quality-of-Service (QoS) through a Guaranteed Time Slot (GTS) mechanism to allocate a specific duration within a superframe structure for a time division multiplexing transmission. The low bandwidth utilization problem may occur in the GTS mechanism when the allocated bandwidth is less than the available bandwidth. However, this problem has not been resolved thoroughly in any of the standard or current research thus far. This paper analyzes GTS performance in QoS-guaranteed transmission and proposes a new GTS allocation scheme named Unbalanced GTS Allocation Scheme (UGAS), which improves the bandwidth resource efficiency. Our scheme tries to solve the bandwidth under-utilization problem by using Network Calculus theory based on the fluid model and greedy algorithm. The UGAS scheme divides the Contention-Free Period into time slots of different durations to support various bandwidth requirements. Time slots are allocated using an approximation QoS model to minimize under-utilization. Compared with the standard GTS allocation scheme, UGAS makes an efficient bandwidth allocation with the QoS-guaranteed model and without breaking the standard protocol. The numerical results show that the average bandwidth utilization using UGAS can be improved by 30 % as compared with the standard scheme.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "Many microarray experiments involve examining the time elapsed prior to the occurrence of a specific event. One purpose of these studies is to relate the gene expressions to the survival times. The Cox proportional hazards model has been the major tool for analyzing such data. The transformation model provides a viable alternative to the classical Cox's model. We investigate the use of transformation models in microarray survival data in this paper. The transformation model, which can be viewed as a generalization of proportional hazards model and the proportional odds model, is more robust than the proportional hazards model, because it is not susceptible to erroneous results for cases when the assumption of proportional hazards is violated. We analyze a gene expression dataset from Beer et al. [Beer, D.G., Kardia, S.L., Huang, C.C., Giordano, T.J., Levin, A.M., Misek, D.E., Lin, L., Chen, G., Gharib, T.G., Thomas, D.G., Lizyness, M.L., Kuick, R., Hayasaka, S., Taylor, J.M., Iannettoni, M.D., Orringer, M.B., Hanash, S., 2002. Gene-expression profiles predict survival of patients with lung adenocarcinoma. Nat. Med. 8 (8), 816-824] and show that the transformation model provides higher prediction precision than the proportional hazards model.",
    "actual_venue": "Computational Biology And Chemistry"
  },
  {
    "abstract": "In breast cancer cases, it is known that the ratio of correct diagnosis is affected by the breast tissue density. For this reason, automatic tissue density classification is an important process in diagnosis. In this work a method for classification of breast tissue density from mammographic images is proposed. The objective of the method is to determine which class, namely fatty, fatty-glandular and dense-glandular, the breast tissue belongs to. For this purpose, SIFT algorithm is used as the local feature extraction method, and LVQ algorithm is used for supervised classification. Test results on the MIAS dataset demonstrate that the code vectors corresponding to bag of SIFT features of each class can successfully model the breast tissue and the classification accuracy over 90% is achieved by LVQ.",
    "actual_venue": "Signal Processing And Communications Applications Conference"
  },
  {
    "abstract": "Caching has shown the success in performance improvement for many wireless communications and networking systems. In this paper, we introduce a caching mechanism for the energy harvesting based Internet of Things (IoT) sensing service. In the service, a sensor harvests energy from an environment. The energy is stored in the battery, and the sensor uses it for sensing and transmitting the reading to the user. A sensing cache can be implemented at a wireless gateway of the sensor to avoid activating the sensor too frequently, hence reducing its energy consumption. We develop an analytical model to investigate the benefit of the proposed caching mechanism. We also introduce the threshold adaptation algorithm that allows the sensing cache dynamically to adjust the parameter of caching to maximize the combined hit rate of the sensing service from multiple sensors. The performance evaluation clearly shows the tradeoff between energy consumption and caching..",
    "actual_venue": "Ieee International Conference On Communications"
  },
  {
    "abstract": "While we have a good understanding of how cyber crime is perpetrated and the profits of the attackers, the harm experienced by humans is less well understood, and reducing this harm should be the ultimate goal of any security intervention. This paper presents a strategy for quantifying the harm caused by the cyber crime of typo squatting via the novel technique of intent inference. Intent inference allows us to define a new metric for quantifying harm to users, develop a new methodology for identifying typo squatting domain names, and quantify the harm caused by various typo squatting perpetrators. We find that typo squatting costs the typical user 1.3 seconds per typo squatting event over the alternative of receiving a browser error page, and legitimate sites lose approximately 5% of their mistyped traffic over the alternative of an unregistered typo. Although on average perpetrators increase the time it takes a user to find their intended site, many typo squatters actually improve the latency between a typo and its correction, calling into question the necessity of harsh penalties or legal intervention against this flavor of cyber crime.",
    "actual_venue": "Ieee Symposium On Security And Privacy"
  },
  {
    "abstract": "We present a new multi-objective exploration method at the system level to select customized implementations for mapping tables, dynamically allocated, as encountered in telecom network, database, and multimedia applications. Our method fits in the context of embedded system synthesis for such applications, and it enables the optimization of the system-level memory management of these applications. To this end it mainly aims at trading off the average memory footprint, number of memory accesses, and memory power. Compared with existing methods, for large mapping tables, 90% (resp. 80%) of the average memory footprint (resp. power) can be saved, without decreasing the performance.",
    "actual_venue": "Msp/Ismm"
  },
  {
    "abstract": "In this paper, we present a variant of the primal affine scaling method, which we call the primal power affine scaling method. This method is defined by choosing a realr>0.5, and is similar to the power barrier variant of the primal-dual homotopy methods considered by den Hertog, Roos and Terlaky and Sheu and Fang. Here, we analyze the methods forr>1. The analysis for 0.50r2r > 2/(2r-1) and with a variable asymptotic step size αk uniformly bounded away from 2/(2r+1), the primal sequence converges to the relative interior of the optimal primal face, and the dual sequence converges to the power center of the optimal dual face. We also present an accelerated version of the method. We show that the two-step superlieear convergence rate of the method is 1+r/(r+1), while the three-step convergence rate is 1+ 3r/(r+2). Using the measure of Ostrowski, we note thet the three-step method forr=4 is more efficient than the two-step quadratically convergent method, which is the limit of the two-step method asr approaches infinity.",
    "actual_venue": "Annals Of Operations Research"
  },
  {
    "abstract": "Recent research has explored ways to obtain and use knowledge of person-object interactions. We present a novel pair of wearables, a glove and a bracelet, that detect when users interact with unobtrusively tagged objects. The glove can also report whether the grasp was with the palm or the fingertips. Both devices have been built and deployed. We present the, requirements, design and early experiences.",
    "actual_venue": "Iswc"
  },
  {
    "abstract": "Assume that a set of Demand Response Switch (DRS) devices are deployed in smart meters for autonomous demand side management within one house. The DRS devices are able to sense and control the activity of each appliance. We propose a set of appliance scheduling algorithms to 1) minimize the peak power consumption under a fixed delay requirement, and 2) minimize the delay under a fixed peak demand constraint. For both problems, we first prove that they are NP-Hard. Then we propose a set of approximation algorithms with constant approximation ratios. We conduct extensive simulations using both real-life appliance energy consumption data trace and synthetic data to evaluate the performance of our algorithms. Extensive evaluations verify that the schedules obtained by our methods significantly reduce the peak demand or delay compared with naive greedy algorithm or randomized algorithm.",
    "actual_venue": "Infocom"
  },
  {
    "abstract": "Given a large collection of images, very few of which have labels, how can we guess the labels of the remaining majority, and how can we spot those images that need brand new labels, different from the existing ones? Current automatic labeling techniques usually scale super linearly with the data size, and/or they fail when only a tiny amount of labeled data is provided. In this paper, we propose QMAS (Querying, Mining And Summarization of Multi-modal Databases), a fast solution to the following problems: (i) low-labor labeling (L3) – given a collection of images, very few of which are labeled with keywords, find the most suitable labels for the remaining ones, and (ii) mining and attention routing – in the same setting, find clusters, the top-NO outlier images, and the top-NR representative images. We report experiments on real satellite images, two large sets (1.5GB and 2.25GB) of proprietary images and a smaller set (17MB) of public images. We show that QMAS scales linearly with the data size, being up to 40 times faster than top competitors (GCap), obtaining better or equal accuracy. In contrast to other methods, QMAS does low-labor labeling (L3), that is, it works even with tiny initial label sets. It also solves both presented problems and spots tiles that potentially require new labels.",
    "actual_venue": "Icdm"
  },
  {
    "abstract": "Creating large area FPGA's is limited by defective sections and the maximum reticule print size (~3x3 cm). FPGA's are well suited for expanding into monolithic multiprint systems 2 to 9 times larger (5 - 10 cm square) we call DeciMeter Scale Integration (DMSI). DMSI expands system capacity, while producing many copies per wafer. Its design criteria is much simpler than the complex Wafer Scale Integration, but still uses defect avoidance routing around flawed blocks to build complete working systems. FPGA's have the main features required for successful DMSI systems: a repeatable cell, built in switchable flexible routing, high connec- tivity requirements between cell blocks, and flexibility with many potential applications. Mod- est changes at the periphery of FPGA's chips enables DMSI capability. Laser formed connec- tions and cuts have proven effectiveness in bypassing fabrication time defects and creating defect free working wafer scale systems. The important DMSI criteria for laser defect avoid- ance is that defect free devices should need no correction. Depending on the DMSI size and current chip yields design defect avoidance requirements vary from simple row column substi- tution to cell by cell/row column substitution with redundant signal paths. Power shorts de- fects and how they are handled prove an important limitation on chip size.",
    "actual_venue": "Austin, Tx"
  },
  {
    "abstract": "We propose an ensemble of invariant features for person re-identification. The proposed method requires no domain learning and can effectively overcome the issues created by the variations of human poses and viewpoint between a pair of different cameras. Our ensemble model utilizes both holistic and region-based features. To avoid the misalignment problem, the test human object sample is used to generate multiple virtual samples, by applying slight geometric distortion. The holistic features are extracted from a publically available pre-trained deep convolutional neural network. On the other hand, the region-based features are based on our proposed Two-Way Gaussian Mixture Model Fitting and the Completed Local Binary Pattern texture representations. To make better generalization during the matching without additional learning processes for the feature aggregation, the ensemble scheme combines all three feature distances using distances normalization. The proposed framework achieves robustness against partial occlusion, pose and viewpoint changes. In addition, the experimental results show that our method exceeds the state of the art person re-identification performance based on the challenging benchmark 3DPeS.",
    "actual_venue": "Ieee International Workshop On Multimedia Signal Processing"
  },
  {
    "abstract": "The non-zero carrier frequency offset (CFO) must be compensated for orthogonal frequency-division multiplexing (OFDM) communications since it may destroy the orthogonality among subcarriers and cause severe degradation in system performance. Several blind CFO estimation methods were developed by exploiting the virtual carriers in practical OFDM transmissions, including a highly efficient approach by rooting a polynomial. However, this rooting method is suboptimal when the noise is present. In this work, we propose an improved polynomial rooting method that is shown to be the maximum likelihood (ML) estimator for both the noisy and the noise-free case. The simulation results clearly show the effectiveness and the better performance of the newly proposed method",
    "actual_venue": "Wcnc"
  },
  {
    "abstract": "Microarchitectural information regarding various aspects of instruction execution can help processor-level stimuli generators more easily reach verification goals. While many such aspects are based on common microarchitectural concepts, their specific manifestations are highly design-specific. We propose using an automatic method for acquiring such microarchitectural knowledge and integrating it into the stimuli generator. We start by extracting microarchitectural data from simulation traces. This data is fed to a decision tree learning algorithm that produces rules for microarchitectural behavior of instructions; these rules are then integrated into the testing knowledge of the stimuli generator. This testing knowledge can provide users with the ability to better control the microarchitectural behavior of generated instructions, leading to higher quality test cases. Experimental results on the POWER7 processor showed that our proposed method can improve the microarchitectural cover-age of the design.",
    "actual_venue": "Design Automation Conference"
  },
  {
    "abstract": "One of the most difficult tasks we face in automated process planning is determination of operation sequencing. In this paper we present an approach to automatic generation of machining sequences in an object-oriented automated process planning system. Sequencing of machining operations is carried out in three phases of planning: initial planning, set-up planning, and final planning. The initial planning generates general plans including the required operations and machine cells. Two types of information are used at this stage, manufacturing process knowledge and component information, including features and associated dimensions, tolerances, surface finish, and material conditions. Based on process requirements decided in the initial planning, the set-up planning selects machines and fixtures, decides the clamping surfaces and feature accessibility, and sequences the set-ups. The final planning determines all the detailed sequences of operations based on the set-ups using the built-in manufacturing logic and heuristics. We introduce the set-up planning, the core of the planning system, to link the part model, initial planning and final detailed planning. The strategy has been implemented in an object-oriented process planning system. An example is provided to demonstrate the approach.",
    "actual_venue": "Journal Of Intelligent Manufacturing"
  },
  {
    "abstract": "For a four-layer datapath routing environment, we present an algorithm that considers all the nets simultaneously. Routing probabilities are calculated for potential routing regions and consolidated into a congestion metric. This is followed by an iterative diversion technique where the region with the maximum congestion metric is repetitively relaxed until the track probabilities crystallize into integer values of 1 and 0. We have run the algorithm on large test cases and achieved significant routability within a small number of available tracks.",
    "actual_venue": "Iccad Proceedings Of The International Conference On Computer-Aided Design"
  },
  {
    "abstract": "Novel applications of online pH determinations at temperatures from -35 degrees C to 130 degrees C in technical and biological media, which are all but ideal aqueous solutions, require new approaches to pH monitoring. The glass electrode, introduced nearly hundred years ago, and chemical sensors based on field effect transistors (ISFET) show specific drawbacks with respect to handling and long-time stability. Proton sensitive metal oxides seem to be a promising and alternative to the state-of-the-art measuring methods, and might overcome some problems of classical hydrogen electrodes and reference electrodes.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "With reducing feature sizes, more transistors can be integrated on the chip. The increased transistor budget can be utilized to improve the instruction level parallelism (ILP) exploited from the processor. However, the transistors cannot be used to arbitrarily increase the processor width and size in the hope of exploiting better ILP. In this paper, we propose an architecture where the superscalar datapath is tightly coupled with a reconfigurable unit (RFU). The reconfiguration unit is configured to execute the traces of dynamic instructions that are frequently executed. To address the data dependency issues between the instructions in the superscalar and the RFU, we propose to execute the trace on the RFU with predicted values. When the trace instructions reach the issue queue in the superscalar, the predictions are validated. In this technique, performance improvement is obtained for correct prediction, whereas no performance degradation is incurred for mispredictions. With this architecture, we observe an average instructions per cycle (IPC) improvement of about 11% over the simulated SPEC 2000 benchmarks, using a very small last value data value predictor.",
    "actual_venue": "Sigarch Computer Architecture News"
  },
  {
    "abstract": "Real-time applications such as telepresence systems present an opportunity to use embedded GPUs for compute acceleration to meet platform goals. In this paper, we develop a prototype of a portable, standalone telepresence robot that performs real-time attention-directed control using an NVIDIA Jetson TK1 embedded platform. We perform platform-specific optimizations to improve thread occupancy, optimize computation workload and improve accuracy of face detection on the embedded GPU and achieve real-time performance of 30 frames per second on the Jetson TK1 and an overall speedup of 10x compared to the ARM CPU version.",
    "actual_venue": "Proceedings Of The Design, Automation And Test In Europe Conference And Exhibition"
  },
  {
    "abstract": ":  *Formerly at the Department of Communication Studies, Linko¨ping University, Sweden. Most of this work was conducted during\n the author’s employment at the Department of Communication Studies. Recent research concerning the control of complex systems\n stresses the systemic character of the work of the controlling system, including the number of people and artefacts as well\n as the environment. This study adds to the growing body of knowledge by focusing on the internal working of such a system.\n Our vantage point is the theoretical framework of distributed cognition. Through a field study of an emergency co-ordination\n centre we try to demonstrate how the team’s cognitive tasks, to assess an event and to dispatch adequate resources, are achieved\n by mutual awareness, joint situation assessment, and the co-ordinated use of the technology and the physical arrangement of\n the co-ordination room.",
    "actual_venue": "Cognition, Technology And Work"
  },
  {
    "abstract": "Denial of Service (DoS) and Distributed Denial of service (DDoS) attacks are common place in today's computer networks. There are different types of attacks among which Flood based attacks are the major ones. Attacks detection mechanisms usually rely on statistical information of the traffic. As regards of the fact that statistical properties of DoS and DDoS attacks are very similar to those in legitimate traffics, in this paper we characterize the frequency domain of denial of service attacks instead of time domain. We consider the number of packets arriving to the node of victim as a random process which is acquired by sampling the packets number every 1 milli-second. Having created the process we find the normalized spectrum of the sampled data. The results show that the main energy of DoS and DDoS attacks are distributed in high and low frequencies respectively while energy is spread evenly through all range of frequencies in Normal TCP traffic.",
    "actual_venue": "Signal Processing And Communications Applications Conference"
  },
  {
    "abstract": "This paper presents a high level description of Domus, an architecture for cluster-oriented Distributed Hash Tables. As a data management layer, Domus supports the concurrent execution of multiple and heterogeneous DHTs, that may be simultaneously accessed by different distributed/parallel client applications. At system level, a load balancement mechanism allows for the (re)distribution of each DHT over cluster nodes, based on the monitoring of their resources, including CPUs, memory, storage and network. Two basic units of balancement are supported: vnodes, a coarse-grain unit, and partitions, a fine-grain unit. The design also takes advantage of the strict separation of object lookup and storage, at each cluster node, and for each DHT. Lookup follows a distributed strategy that benefits from the joint analysis of multiple partition-specific routing information, to shorten routing paths. Storage is accomplished through different kinds of data repositories, according to the specificity and requirements of each DHT.",
    "actual_venue": "Ppam"
  },
  {
    "abstract": "This paper presents the design and implementation of a Portable Open Source Energy Monitor (POEM) to enable developers to automatically test and measure the energy consumption of every single application component down to the control flow level. Based on existing portable power meter designs, POEM extends the state of the art of application analysxis with the energy annotation of the control flow down to the basic blocks, the call graph, and the Android API calls, allowing developers to locate energy leaks in their applications with high accuracy. Because the power consumption is tied to the system status, energy annotation is also coupled with system activities.",
    "actual_venue": "Pervasive Computing And Communication Workshops"
  },
  {
    "abstract": "The QuikSCAT scatterometer is used to accurately retrieve winds over the ocean at both high (2.5 km) and low (25 km) resolutions. In near-coastal regions, land contamination of measurements results in inaccurate wind estimates using current techniques. Here, we show that identifying land-contaminated measurements allows wind retrieval to be accurately achieved in near-coastal regions using QuikSCA...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "The logic BKc1 is the basic constructive logic in the ternary relational semantics (without a set of designated points) adequate to consistency understood as the absence of the negation of any theorem. Negation is introduced in BKc1 with a negation connective. The aim of this paper is to define the logic BKc1F. In this logic negation is introduced via a propositional falsity constant. We prove tha...",
    "actual_venue": "Logic Journal Of The Igpl"
  },
  {
    "abstract": "This paper describes a system that gives opticians Internet access from their high street shops to patient data held in a hospital Diabetes Information System (DIS), using a standard Web browser. The system is a revision of an earlier one we provided to General Practitioners (GPs), and uses a public key infrastructure with strong encryption and digitally signed messages to secure the data as it traverses the Internet. We describe the PKI and the security architecture, the DIS we chose to distribute, the changes that we made to the Web interface to tailor it to the opticians needs, the validation testing we performed, the results of the pilot testing and the feedback we obtained from the opticians. We also compare the results with our earlier work with GPs. We found that in a well-designed system the underlying PKI is virtually invisible to the users, and its security is taken for granted. Users then concentrate on the costs and benefits of the electronic application. In our system, benefits can accrue to opticians by giving them access to the latest patient data, and this can help to improve patient care. Benefits also accrue to the DIS administrators and the wider community of DIS users, in that data quality can be significantly improved. However, we found that the slow speed of Internet access via a dial up connection is a significant impediment to its frequent use. We also found that it is extremely difficult to produce a user interface that pleases everyone. Finally, in complex information systems such as this PKI, failure of just one component or administrative procedure can have a catastrophic effect on the availability of the entire system.",
    "actual_venue": "Computer Communications"
  },
  {
    "abstract": "Formal Description Techniques have been widely used for the specification of traditional networked applications. They have not been applied to the specification of new applications such as multimedia systems yet. In this paper, we examine the FDT Estelle with respect to its suitability for multimedia system specification and automatic derivation of efficient implementations. We show that it is possible to specify certain aspects of multimedia systems, but that Estelle is not sufficient for others. The derived implementations often perform badly. We show the reasons and propose to use a slightly modified Estelle syntax and semantics to solve the problems. The implemented solution was tested successfully.",
    "actual_venue": "Pstv"
  },
  {
    "abstract": "The results of the MUC-6 evaluation must be analyzed to determine whether close scores significantly distinguish systems or whether the differences in those scores are a matter of chance. In order to do such an analysis, a method of computer intensive hypothesis testing was developed by SAIC for the MUC-3 results and has been used for distinguishing MUC scores since that time. The implementation of this method for the MUC evaluations was first described in [1] and later the concepts behind the statistical model were explained in a more understandable manner in [2]. This paper gives the results of the statistical testing for the three MUC-6 tasks where a single metric could be associated with a system's performance.",
    "actual_venue": "MUC"
  },
  {
    "abstract": "Pipelined heterogeneous multiprocessor system-on-chip (MPSoC) can provide high throughput for streaming applications. In the design of such systems, time performance and system cost are the most concerning issues. By analyzing runtime behaviors of benchmarks in real-world platforms, we find that execution times of tasks are not fixed but spread with probabilities. In terms of this feature, we model execution times of tasks as random variables. In this paper, we study how to design high-performance and low-cost MPSoC systems to execute a set of such tasks with data dependencies in a pipelined fashion. Our objective is to obtain the optimal functional unit assignment and voltage selection for the pipelined MPSoC systems, such that the system cost is minimized while timing constraints can be met with a given guaranteed probability. For each required probability, our proposed algorithm can efficiently obtain the optimal solution. Experiments show that other existing algorithms cannot find feasible solutions in most cases, but ours can. Even for those solutions that other algorithms can obtain, ours can reach 30% reductions in total cost compared with others.",
    "actual_venue": "Lctes"
  },
  {
    "abstract": "Wireless mesh network is a promising new direction resulting from the recent rapid progress made in wireless communication technologies. While multiple radio, multiple channel technologies have offered a great potential for increasing network throughput, they also create new research challenges. In this paper, we propose RingMesh, a token ring-based protocol for multi-channel routing. By applying delay-guaranteed rules for joining a ring and for creating new rings, the multiple-ring protocol bounds the end-to-end packet delay within the mesh network, from the source node to a gateway that connects with the wired network. The RingMesh protocol is described as a state machine; the analytical results on bounded delay are presented We believe that both the protocol and analysis may be applied to other areas of delay-guaranteed wireless network research.",
    "actual_venue": "Ieee Conference On Local Computer Networks, Proceedings"
  },
  {
    "abstract": "In analog circuits, blocks need to be placed symmetrically to satisfy the devices matching. Different from the existing constraint-driven approaches, the proposed topological symmetry structure enables us to generate a symmetrical placement without any constraint. Simulated annealing is utilized as the framework of the optimization, and we propose new move operation to maintain the placement's topological symmetry. By inserting dummy blocks, we present a physical skewed symmetry structure allowing non-symmetry partly, so that to enhance the placement on area and wire length. Besides, we incorporate regularity into the evaluation of placement. Experiments shows that our approach generated topological complete symmetry placements without much compromise on chip area and wire length, compared to the placements with no symmetry.",
    "actual_venue": "Asp-Dac"
  },
  {
    "abstract": "We present a simple and unified framework for developing and analyzing approximation algorithms for some multiway partition problems (with or without terminals), including the k-way cut (or k- cut), multiterminal cut (or multiway cut), hypergraph partition and target split.",
    "actual_venue": "Isaac"
  },
  {
    "abstract": "This paper proposes a semantic-based P2P resource organization model R-Chord by incorporating the Resource Space Model (RSM), the P2P Semantic Link Network Model (P2PSLN) and the DHT Chord protocol. Peers provide services with each other according to the content of their resources and the related configuration information. It incorporates the classification semantics and the relational semantics to provide users and applications with a uniform view on distributed resources. Experiments show that, compared to the Chord approach, the R-Chord approach is more flexible to support semantic-based queries and can significantly decrease the average visiting number of and visiting times on peers for answering queries.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Recently, recommender system has attracted a lot of attentions, which helps users to find items of interest through utilizing the user-item interaction information and/or content information associated with users and items. The interaction information (i.e., feedback) between users and items are widely exploited to build recommendation models. The feedback data in recommender systems usually comes in the form of both explicit feedback (e.g., rating) and implicit feedback (e.g., browsing histories, click logs). Although existing works have begun to utilize either explicit or implicit feedback for better recommendation, they did not make best use of these feedback information together. In this paper, we first study the personalized ranking recommendation problem by integrating multiple feedbacks, i.e., one type of explicit feedback and multiple types of implicit feedbacks. Then we propose a unified and flexible personalized ranking framework MFPR to integrate multiple feedbacks. Moreover, as there are no readily available training data, an explicit feedback based training data generation algorithm is designed to generate item pairs with more accurate partial order consistent with the multiple feedbacks for the proposed ranking model. Extensive experiments on two real-world datasets validate the effectiveness of the MFPR model, and the integration of multiple feedbacks making up better complementary information significantly improves recommendation performance.",
    "actual_venue": "Pakdd"
  },
  {
    "abstract": "A hard disk readback signal generator designed to provide noise-corrupted signals to a channel simulator has been implemented on a Xilinx Virtex™E FPGA device. The generator simulates pulses sensed by read heads in hard drives. All major distortion and noise processes, such as intersymbol interference, transition noise, electronics noise, head and media nonlinearity, intertrack interference, and write timing error, can be generated according to the statistics and parameters defined by the user. Reconfigurable implementation enables an update of the signal characteristics in runtime. The user also has the flexibility to choose from a set of bitstreams to simulate particular combinations of noise and distortion. Such customized restructuring helps reduce the area consumption and hence virtually increase the capacity of the FPGA device. The time to generate the readback signals has been reduced by four orders compared to its software counterpart.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "This paper proposes implementing an antenna operating in the millimeter wave band of 56-64 GHz on the backside of an Integrated Circuit (IC) that uses Through Silicon Via (TSV) technology for a System in Package (SiP) approach to mixed signal design. A folded monopole antenna that utilizes a coaxial TSV feed line is selected to implement the design on the backside of the silicon die. Furthermore, the initial design is modeled using Ansys's High Frequency Structure Simulator (HFSS) to measure appropriate antenna parameters including a directivity of 1.27 dB. The final design proposes using an artificial magnetic conductor as a reflector to improve antenna directivity by 4.87 dB and eliminates propagation of electromagnetic waves back into the substrate.",
    "actual_venue": "Ieee International Midwest Symposium On Circuits And Systems"
  },
  {
    "abstract": "As online reviews become increasingly important to e-commerce and social media sites, computer scientists work on ways to ensure their authenticity.",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "The operator-based signal separation approach uses an adaptive operator to separate a signal into additive subcomponents. And different types of operator can depict different properties of a signal. In this paper, we define a new kind of integral operator which can be derived from the second kind of Fredholm integral equation. Then, we analyze the properties of the proposed integral operator and discuss its relation to the second condition of Intrinsic Mode Function (IMF). To demonstrate the robustness and efficacy of the proposed operator, we incorporate it into the Null Space Pursuit algorithm to separate several multicomponent signals, including a real-life signal.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "In this paper we present novel expressions for several performance metrics of communication systems operating over a composite\n fading environment modelled by the generalized-K distribution. Initially, for a generalized-K fading channel with arbitrary values for the small and large-scale fading parameters we derive a closed-form expression for\n the moment generating function (MGF) of the received signal-to-noise ratio (SNR) and utilize it to obtain the exact average\n symbol error probability for a variety of digital modulations using the MGF based approach. Then, for integer values of the\n small-scale fading parameter, we derive a novel closed-form expression for the cumulative distribution function of the received\n SNR, which is then used to obtain closed-form expressions for the outage probability, the average bit error probability of\n various digital modulations, and the ergodic capacity of the generalized-K fading channel.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "This paper presents a selection of results obtained during an impulse responsemeasurement campaign undertaken in Hong Kong at 1.8 GHz. The chanel probingsignal bandwidth of 60 MHz was generated by a pseudorandom sequence clockedat 30 MHz. The results of the channel impulse response measurements in theHung Hom district of Kowloon, Hong Kong are presented. The measured resultsare analyzed for CIR parameters like delay spread, average delay and thesignificant number of paths. An attempt is made to relate the measured datawith the geometry of the environmental features that surround the transmitterand the receiver.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "Spatial Psychovisual Modulation (SPVM) is a novel information display technology which aims to simultaneously generate multiple visual percepts for different viewers on a single display. The SPVM system plays an important role in information security. In a SPVM system, the viewers wearing polarized glasses can see a specific image (called personal view), and meanwhile the viewers not wearing glasses can also see a semantically meaningful image (called shared view). Researches on screen content image (SCI) are very hot recently, which have received a great amount of attention from multiple fields in multimedia signal processing. In this paper, we focus our gaze on how the users' quality-of-experience on SCIs is influenced under the SPVM display system. To this aim, we implement a comprehensive subjective quality assessment of SCIs by building a database which contains the distorted SCIs generated by a SPVM system. We run prevailing image quality methods on the newly established database, and results of experiments indicate that existing image quality metrics cannot reach a good performance, possibly due to some unique distortions, e.g. false contour and ghosting artifacts of SPVM-generated SCIs. Furthermore, we also point out some potential features which may lead to a high-performance metric by some appropriate modification and combination.",
    "actual_venue": "Advances In Multimedia Information Processing - Pcm , Pt"
  },
  {
    "abstract": "The identification of multiple change points is a problem shared by many subject areas, including disease and criminality mapping, medical diagnosis, industrial control, and finance. An algorithm based on the Product Partition Model (PPM) is developed to solve the multiple change point identification problem in Poisson data sequences. In order to address the PPM, a simple and easy way to implement Gibbs sampling scheme is derived. A sensitivity analysis is performed, for different prior specifications. The algorithm is then applied to the analysis of a real data sequence. The results show that the method is quite effective and provides useful inferences.",
    "actual_venue": "Advances In Complex Systems"
  },
  {
    "abstract": "With the continuous scaling of semiconductor technology, the life-time of circuit is decreasing so that processor failure becomes an important issue in MPSoC design. A software solution to tolerate run-time processor failure is to migrate tasks from the failed processors to the live processors when failure occurs. Previous works on run-time task migration usually aim to minimize the migration overhead with or without a given latency constraint. For streaming applications, however, it is more important to minimize the throughput degradation than the migration overhead or the latency. Hence, we propose a task remapping technique to minimize the throughput degradation assuming that the migration overhead can be amortized safely. The target multi-core system assumed in this paper consists of processor pools and each pool consists of homogeneous processors. The proposed technique is based on an intensive compile-time analysis for all possible failure scenarios. It involves the following steps; 1) Determine the static mapping of tasks onto the live processors, aiming to minimize the throughput degradation: 2) Find an optimal processor-to-processor mapping to minimize the task migration overhead: and 3) Store the resultant task remapping information that includes task mapping and processor-to-processor mapping results. Since the task remapping information is pre-computed at compile-time for all possible failure scenarios, it should be efficiently represented and stored. At run-time, we simply remap the tasks following the compile-time decision. We examine the scalability of the proposed technique on both space and run-time overhead for compile-time analysis varying the number of failed processors. Through intensive experiments, we show that the proposed technique outperforms the previous works with respect to application throughput.",
    "actual_venue": "Codes+Isss"
  },
  {
    "abstract": "Class-attribute interdependence maximization (CAIM) is one of the state-of-the-art algorithms for discretizing data for which classes are known. However, it may take a long time when run on high-dimensional large-scale data, with large number of attributes and/or instances. This paper presents a solution to this problem by introducing a graphic processing unit (GPU)-based implementation of the CAIM algorithm that significantly speeds up the discretization process on big complex data sets. The GPU-based implementation is scalable to multiple GPU devices and enables the use of concurrent kernels execution capabilities of modern GPUs. The CAIM GPU-based model is evaluated and compared with the original CAIM using single and multi-threaded parallel configurations on 40 data sets with different characteristics. The results show great speedup, up to 139 times faster using four GPUs, which makes discretization of big data efficient and manageable. For example, discretization time of one big data set is reduced from 2 h to $$",
    "actual_venue": "The Journal Of Supercomputing"
  },
  {
    "abstract": "In 1963, Corrádi and Hajnal proved that for all k≥1 and n≥3k, every graph G on n vertices with minimum degree δ(G)≥2k contains k disjoint cycles. The bound δ(G)≥2k is sharp. Here we characterize those graphs with δ(G)≥2k−1 that contain k disjoint cycles. This answers the simple-graph case of Dirac's 1963 question on the characterization of (2k−1)-connected graphs with no k disjoint cycles.",
    "actual_venue": "Journal Of Combinatorial Theory Series A"
  },
  {
    "abstract": "In this paper, we propose a new approach to develop a nonperturbative approximate solution for the Thomas-Fermi equation. This approach rests on the recently developed modification of the Adomian decomposition method. The initial slope of the Thomas-Fermi potential y^'(0) is computed by converting the obtained series solution into several diagonal Pade approximants. The proposed scheme is presented in a general way so that it can be used in applied sciences.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "A concept of a flow sensor optimized for the use in HVAC (Heating Ventilating Air Conditioning) systems is presented. The fabrication of the transducer is based on PCB (Printed Circuit Board) technology to keep costs low and allow for easy handling and replacement. The complete sensor device consists of a quantizer, a conversion circuitry, and a network link. Through interaction with the streaming fluid, the transducer generates an electrically measurable signal which allows determination of the total flow of the fluid. The measurement principle is based on a modification of the calorimetric principle. Hence, miniaturized heat sources and nearby temperature detectors have to be implemented. The behavior and performance of the sensor concept has been studied by means of finite element simulations. The quasistatic and transient simulations reveal the temperature allocation inside the sensor and the surrounding fluid and therefore allow a further optimization of the sensor for different applications.",
    "actual_venue": "Emerging Technologies And Factory Automation"
  },
  {
    "abstract": "Replay attacks presents a great risk for Automatic Speaker Verification (ASV) system. In this paper, we propose a novel replay detector based on Variable length Teager Energy Operator Energy Separation Algorithm-Instantaneous Frequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge. The key idea here is to exploit the contribution of IF in each subband energy via ESA to capture possible changes in spectral envelope (due to transmission and channel characteristics of replay device) of replayed speech. The IF is computed from narrowband components of speech signal, and DCT is applied in IF to get proposed feature set. We compare the performance of the proposed VESA-IFCC feature set with the features developed for detecting synthetic and voice converted speech. This includes the CQCC, CFCCIF and prosody-based features. On the development set, the proposed VESA-IFCC features when fused at score-level with a variant of CFCCIF and prosody based features gave the least EER of 0.12 %. On the evaluation set, this combination gave an EER of 18.33 %. However, post-evaluation results of challenge indicate that VESA-IFCC features alone gave the relatively least EER of 14.06 % (i.e., relatively 16.11 % less compared to baseline CQCC) and hence, is a very useful countermeasure to detect replay attacks.",
    "actual_venue": "Annual Conference Of The International Speech Communication Association , Vols -: Situated Interaction"
  },
  {
    "abstract": "We consider the problem of finding the M assignments with maximum probability in a probabilistic graphical model. We show how this problem can be formulated as a linear program (LP) on a particular polytope. We prove that, for tree graphs (and junction trees in general), this polytope has a particularly simple form and differs from the marginal polytope in a single inequality constraint. We use this characterization to provide an approximation scheme for non-tree graphs, by using the set of spanning trees over such graphs. The method we present puts the M -best inference problem in the context of LP relaxations, which have recently received considerable attention and have proven useful in solving difficult inference problems. We show empirically that our method often finds the provably exact M best configurations for problems of high tree-width. A common task in probabilistic modeling is finding the assignment with maximum probability given a model. This is often referred to as the MAP (maximum a-posteriori) problem. Of particular interest is the case of MAP in graphical models, i.e., models where the probability factors into a product over small subsets of variables. For general models, this is an NP-hard problem [11], and thus approximation algorithms are required. Of those, the class of LP based relaxations has recently received considerable attention [3, 5, 18]. In fact, it has been shown that some problems (e.g., fixed backbone protein design) can be solved exactly via sequences of increasingly tighter LP relaxations [13]. In many applications, one is interested not only in the MAP assignment but also in the M maximum probability assignments [19]. For example, in a protein design problem, we might be interested in the M amino acid sequences that are most stable on a given backbone structure [2]. In cases where the MAP problem is tractable, one can devise tractable algorithms for the M best problem [8, 19]. Specifically, for low tree-width graphs, this can be done via a variant of max-product [19]. However, when finding MAPs is not tractable, it is much less clear how to approximate the M best case. One possible approach is to use loopy max-product to obtain approximate max-marginals and use those to approximate the M best solutions [19]. However, this is largely a heuristic and does not provide any guarantees in terms of optimality certificates or bounds on the optimal values. LP approximations to MAP do enjoy such guarantees. Specifically, they provide upper bounds on the MAP value and optimality certificates. Furthermore, they often work for graphs with large tree-width [13]. The goal of the current work is to leverage the power of LP relaxations to the M best case. We begin by focusing on the problem of finding the second best solution. We show how it can be formulated as an LP over a polytope we call the \"assignment-excluding marginal polytope\". In the general case, this polytope may require an exponential number of inequalities, but we prove that when the graph is a tree it has a very compact representation. We proceed to use this result to obtain approximations to the second best problem, and show how these can be tightened in various ways. Next, we show how M best assignments can be found by relying on algorithms for 1",
    "actual_venue": "Nips"
  },
  {
    "abstract": "This correspondence investigates the uncertainty principles under the linear canonical transform (LCT). First, a lower bound on the uncertainty product of signal representations in two LCT domains for complex signals is derived, which can be achieved by a complex chirp signal with Gaussian envelope. Then, the tighter lower bound for real signals in two LCT domains proposed by Sharma and Joshi is also proven to hold for arbitrary LCT parameters based on the properties of moments for the LCT. The uncertainty principle for the fractional Fourier transform is a special case of the achieved results.",
    "actual_venue": "Ieee Transactions Signal Processing"
  },
  {
    "abstract": "We show in this article how the multi-stage encoding scheme proposed in [3] may be used to construct the [24, 12, 8] Golay code, and two extremal self-dual codes with parameters [32, 16, 8] and [40, 20, 8] by using an extended [8, 4, 4] Hamming base code. An extension of the construction of [3] over Z4 yields self-dual codes over Z4 with parameters (for the Lee metric over Z4) [24, 12, 12] and [32, 16, 12] by using the [8, 4, 6] octacode. Moreover, there is a natural Tanner graph associated to the construction of [3], and it turns out that all our constructions have Tanner graphs that have a cyclic structure which gives tail-biting trellises of low complexity: 16-state tail-biting trellises for the [24, 12, 8], [32, 16, 8], [40, 20, 8] binary codes, and 256-state tail-biting trellises for the [24, 12, 12] and [32, 16, 12] codes over Z4.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Clustering sensor nodes is an effective topology control method to reduce energy consumption of the sensor nodes for maximizing lifetime of Wireless Sensor Networks (WSNs). However, in a cluster based WSN, the leaders (cluster heads) bear some extra load for various activities such as data collection, data aggregation and communication of the aggregated data to the base station. Therefore, balancing the load of the cluster heads is a challenging issue for the long run operation of the WSNs. Load balanced clustering is known to be an NP-hard problem for a WSN with unequal load of the sensor nodes. Genetic Algorithm (GA) is one of the most popular evolutionary approach that can be applied for finding the fast and efficient solution of such problem. In this paper, we propose a novel GA based load balanced clustering algorithm for WSN. The proposed algorithm is shown to perform well for both equal as well as unequal load of the sensor nodes. We perform extensive simulation of the proposed method and compare the results with some evolutionary based approaches and other related clustering algorithms. The results demonstrate that the proposed algorithm performs better than all such algorithms in terms of various performance metrics such as load balancing, execution time, energy consumption, number of active sensor nodes, number of active cluster heads and the rate of convergence.",
    "actual_venue": "Swarm And Evolutionary Computation"
  },
  {
    "abstract": "A crucial aspect of bilingual communication is the ability to identify the language of an input. Yet, the neural and cognitive basis of this ability is largely unknown. Moreover, it cannot be easily incorporated into neuronal models of bilingualism, which posit that bilinguals rely on the same neural substrates for both languages and concurrently activate them even in monolingual settings. Here we hypothesized that bilinguals can employ language-specific sublexical bigram frequency and lexical orthographic neighborhood size statistics for language recognition. Moreover, we investigated the neural networks representing language-specific statistics and hypothesized that language identity is encoded in distributed activation patterns within these networks. To this end, German-English bilinguals made speeded language decisions on visually presented pseudowords during fMRI. Language attribution followed lexical neighborhood sizes both in first L1 and second L2 language. RTs revealed an overall tuning to L1 bigram statistics. Neuroimaging results demonstrated tuning to L1 statistics at sublexical occipital lobe and phonological temporoparietal lobe levels, whereas neural activation in the angular gyri reflected sensitivity to lexical similarity to both languages. Analysis of distributed activation patterns reflected language attribution as early as in the ventral stream of visual processing. We conclude that in language-ambiguous contexts visual word processing is dominated by L1 statistical structure at sublexical orthographic and phonological levels, whereas lexical search is determined by the structure of both languages. Moreover, our results demonstrate that language identity modulates distributed activation patterns throughout the reading network, providing a key to language identity representations within this shared network.",
    "actual_venue": "Journal Of Cognitive Neuroscience"
  },
  {
    "abstract": "Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the Caliński–Harabasz index, the $CS$ index, the Davies–Bouldin index, the Dunn index with two of its generalized versions, the $I$ index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolution–particle-swarm-optimization (DEPSO)-based clustering. DEPSO is a hybrid evolutionary algorithm of the stochastic optimization approach (differential evolution) and the swarm intelligence method (particle swarm optimization) that further increases the search capability and achieves higher flexibility in exploring the problem space. According to the experimental results, we find that the silhouette statistic index stands out in most of the data sets that we examined. Meanwhile, we suggest that users reach their conclusions not just based on only one index, but after considering the results of several indices to achieve reliable clustering structures.",
    "actual_venue": "Ieee Transactions On Systems, Man, And Cybernetics, B: Cybernetics"
  },
  {
    "abstract": "Smartphones contain sensors that provide information about the user of the device. Studying smartphones, therefore, provides an invaluable window into the behavioral patterns of users. In this work, we describe an open infrastructure that provides research communities with principled access to mobile devices and their sensors. Our platform, the Sensibility Testbed, is a free, community-driven platform for mobile devices. It provides secure data access to user-owned mobile devices while preserving user privacy. The unified programmable interface to sensors on heterogeneous devices in the Sensibility Testbed enables research scientists to design and deploy experiments that run across large numbers of devices, for example to measure the coverage and performance of cellular and WiFi networks. The Sensibility Testbed makes the sensing capabilities of smartphones more accessible to a broad range of researchers.",
    "actual_venue": "Infocom Workshops"
  },
  {
    "abstract": "A cyclic prefix reconstruction scheme is proposed for precoded single-carrier systems with frequency-domain equalization (SC-FDE) that employ insufficient length of cyclic prefix. For SC-FDE, cyclic prefix is employed to facilitate frequency-domain equalization at the receiver. Since inserting cyclic prefix incurs a loss in bandwidth-utilization efficiency, it is desirable to limit the length of cyclic prefix for SC-FDE. This paper designs the energy spreading transform (EST), a precoder that enables iterative reconstruction of missing cyclic prefix. The performance of the proposed scheme is shown to be close to that of SC-FDE with enough length of cyclic prefix.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "In this paper, we introduce a novel beamforming algorithm for single receiver phased array antenna systems and study its performance in terms of the convergence speed and the steady state error, through simulations and experimental tests. Neither a priori knowledge of the target's direction, nor the phase-voltage characteristic of the phase shifters are required in this algorithm. Instead, the total received power, which is the only array output, is sampled to perform the beamforming and adjust the phase shifters. Insensitivity to the variation of the phase shifter characteristics, and simultaneous calibration is an important feature of this algorithm. This algorithm has been successfully applied to a 16-element Ku-band phased array.",
    "actual_venue": "Calgary, Bc"
  },
  {
    "abstract": "The paper presents a neural network based segmentation method which can extract moving objects in video. This proposed neural network architecture is multilayer so as to match the complexity of the frames in a video stream and deal with the problems of segmentation. The neural network combines inputs that exploit spatio-temporal correlation among pixels. Each of these unit themselves produce imperfect results, but the neural network learns to combine their results for better overall segmentation, even though it is trained with noisy results from a simpler method. The proposed algorithm converges from an initial stage where all the pixels are considered to be part of the background to a stage where only the appropriate pixels are classified as background. Results are shown to demonstrate the efficacy of the method compared to a more memory intensive MoG method.",
    "actual_venue": "Proceedings Of Seventh International Conference On Bio-Inspired Computing: Theories And Applications"
  },
  {
    "abstract": "The Omni-Directional Treadmill (ODT) is a revolutionary device for locomotion in large-scale virtual environments. The device allows its user to walk or jog in any direction of travel. It is the third generation in a series of devices built for this purpose for the U.S. Army's Dismounted Infantry Training Program. We first describe the device in terms of its construction and operating characteristics. We then report on an analysis consisting of a series of locomotion and maneuvering tasks on the ODT. We observed user motions and system responses to those motions from the perspective of the user. Each task is described in terms of what causes certain motions to trigger unpredictable responses causing loss of balance or at least causing the user to become consciously aware of their movements. We con- clude that the two primary shortcomings in the ODT are its tracking system and machine control mechanism for center- ing the user on the treads.",
    "actual_venue": "Acm Symposium On User Interface Software And Technology"
  },
  {
    "abstract": "This is an extended abstract for a poster that presents a new approach that employs metaprogramming to generate optimized code for algorithms in Linear Algebra.",
    "actual_venue": "Acm Southeast Regional Conference"
  },
  {
    "abstract": "An image representation framework based on structured sparse model selection is introduced in this work. The corresponding modeling dictionary is comprised of a family of learned orthogonal bases. For an image patch, a model is first selected from this dictionary through linear approximation in a best basis, and the signal estimation is then calculated with the selected model. The model selection leads to a guaranteed near optimal denoising estimator. The degree of freedom in the model selection is equal to the number of the bases, typically about 10 for natural images, and is significantly lower than with traditional overcomplete dictionary approaches, stabilizing the representation. For an image patch of size √N × √N, the computational complexity of the proposed framework is O (N2), typically 2 to 3 orders of magnitude faster than estimation in an overcomplete dictionary. The orthogonal bases are adapted to the image of interest and are computed with a simple and fast procedure. State-of-the-art results are shown in image denoising, deblurring, and inpainting.",
    "actual_venue": "Icip"
  },
  {
    "abstract": "A very high-frequency operational transconductance amplifier (OTA) with a new feedforward-regulated cascode topology is demonstrated in this paper. Experimental results show a bandwidth of 10 GHz and a large transconductance of 11 mS. A theoretical analysis of the OTA is provided which is in very good agreement with the measured results. We also carry out a Monte Carlo simulation to determine the effect of transistor mismatches and process variations on the transconductance and input/output parasitic capacitances of the OTA. The linearity and intermodulation distortion properties of the OTA, which are of particular interest in microwave applications, are experimentally determined using a purpose-built single-stage amplifier. For high-frequency demonstration purposes we built a larger circuit: an inductor less microwave oscillator. The fabricated oscillator operates at 2.89 GHz and has a significantly larger output voltage swing and better power efficiency than other inductor less oscillators reported in the literature in this frequency range. It also has a very good phase noise for this type of oscillators: -116 dBc/Hz at 1-MHz offset.",
    "actual_venue": "Ieee Trans On Circuits And Systems"
  },
  {
    "abstract": "In this study we propose the development of an adaptive particle swarm optimization (APSO) learning algorithm to train a non-linear autoregressive (NAR) neural network, which we call PSONAR, for short term time series prediction of ocean wave elevations. We also introduce a new stochastic inertial weight to the APSO learning algorithm. Our work is motivated by the expected need for such predictions by wave energy farms. In particular, it has been shown that the phase resolved predictions provided in this paper could be used as inputs to novel control methods that hold promise to at least double the current efficiency of wave energy converter (WEC) devices. As such, we simulated noisy ocean wave heights for testing. We utilized our PSONAR to get results for 5, 10, 30, and 60 second multistep predictions. Results are compared to a standard backpropagation model. Results show APSO can outperform backpropagation in training a NAR neural network.",
    "actual_venue": "Foundations Of Computational Intelligence"
  },
  {
    "abstract": "RNA-based next-generation sequencing (RNA-Seq) provides a tremendous amount of new information regarding gene and transcript structure, expression and regulation. This is particularly true for non-coding RNAs where whole transcriptome analyses have revealed that the much of the genome is transcribed and that many non-coding transcripts have widespread functionality. However, uniform resources for raw, cleaned and processed RNA-Seq data are sparse for most organisms and this is especially true for non-human primates (NHPs). Here, we describe a large-scale RNA-Seq data and analysis infrastructure, the NHP reference transcriptome resource (http://nhprtr.org); it presently hosts data from12 species of primates, to be expanded to 15 species/subspecies spanning great apes, old world monkeys, new world monkeys and prosimians. Data are collected for each species using pools of RNA from comparable tissues. We provide data access in advance of its deposition at NCBI, as well as browsable tracks of alignments against the human genome using the UCSC genome browser. This resource will continue to host additional RNA-Seq data, alignments and assemblies as they are generated over the coming years and provide a key resource for the annotation of NHP genomes as well as informing primate studies on evolution, reproduction, infection, immunity and pharmacology.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "For problems over continuous random variables, MRFs with large cliques pose a challenge in probabilistic inference. Difficulties in performing optimization efficiently have limited the probabilistic models explored in computer vision and other fields. One inference technique that handles large cliques well is Expectation Propagation. EP offers run times independent of clique size, which instead depend only on the rank, or intrinsic dimensionality, of potentials. This property would be highly advantageous in computer vision. Unfortunately, for grid-shaped models common in vision, traditional Gaussian EP requires quadratic space and cubic time in the number of pixels. Here, we propose a variation of EP that exploits regularities in natural scene statistics to achieve run times that are linear in both number of pixels and clique size. We test these methods on shape from shading, and we demonstrate strong performance not only for Lambertian surfaces, but also on arbitrary surface reflectance and lighting arrangements, which requires highly non-Gaussian potentials. Finally, we use large, non-local cliques to exploit cast shadow, which is traditionally ignored in shape from shading.",
    "actual_venue": "Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study.",
    "actual_venue": "Information And Software Technology"
  },
  {
    "abstract": "Neste artigo, realiza-se o estudo do conceito de Responsible Web Design (RWD) que permite ao desenvolvimento web ser adaptável a diferentes tamanhos de tela e diferentes recursos dos dispositivos, como sensores e câmeras. Paralelamente ao estudo do RWD, também se estuda uma alternativa para a solução do mesmo problema, conhecida como Mobile Web Framework (MWF) da UCLA (University of California). Um comparativo é realizado através da aplicação do RWD na implementação da interface de um sistema de opinião seguindo dois modelos distintos: utilizando puro RWD e utilizando o framework da UCLA. Desta forma, obtiveram-se informações suficientes para apontar as vantagens e desvantagens das duas abordagens. Concluiu-se que o MWF ainda se encontra em fase de amadurecimento e o RWD, por outro lado, permite a flexibilidade e personalização necessária para o uso comercial.",
    "actual_venue": "IHC"
  },
  {
    "abstract": "•Heuristic global optimization schemes are able to remedy some drawbacks of back-propagation used in ANN.•An enhanced ANN training scheme is proposed using an efficient global optimization scheme.•The proposed SP-UCI-enahnced ANN shows better performances than the GA-, PSO-, SA-, and DE-based ANN.•The heuristic search optimization schemes are universally adaptable for other types of ANNs.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Major companies, especially banks, invest in interactive distance learning replacing face-to-face training. Research has shown learning gains are mostly due to a shift in instruction. In this study, a WBT about currency management of a major German bank was examined. The communicational features of the WBT comprise a discussion forum, note taking, and automatic messaging of questions and answers between experts and students. The experimental design compared a face-to-face seminar with WBT learning. The results show that WBT participants learned as much as the seminar participants, but in about 70% of the seminar's study time. Young seminar participants performed better than older ones, while WBT learning did not produce an age effect. The results of the study demonstrate that the learners in the bank tend to choose traditional learning strategies, they do not cope optimally with co-operative and selective learning strategies, and they tend to appreciate audio-visual media. Experts did not voluntarily play an active role in the discussion processes. Communicational features, however, were used quite frequently. The users who were experienced in using a CBT and showed high self esteem gained most from WBT learning.",
    "actual_venue": "Journal Of Universal Computer Science"
  },
  {
    "abstract": "This paper is concerned with model reduction for complex Markov chain models. The Kullback- Leibler divergence rate is employed as a metric to measure the difference between the Markov model and its approximation. The main result is a characterization of the solution to the o ptimal bi-partition problem. The solution is characterized by an associated eigenvalue problem, whose form is similar to the eigenvalue problems considered in Markov spectral theory for model reduction. This result is the basis of a heuristic proposed for the m-ary partition problem, resulting in a practical recursive algorithm. The results are illustrated with examples.",
    "actual_venue": "Ieee Trans Automat Contr"
  },
  {
    "abstract": "We address the problem of recognizing 3D rigid free- form objects using dense range data when the objects can be imaged from arbitrary viewpoints and the objects vary in shape and complexity. We propose a multi-level match- ing strategy that employs shape spectral analysis and fea- tures derived from theCOSMOS representations of free-form objects for fast and efficient recognition. We demonstrate that with a large model database of object views, a small set of ranked candidate matches can be selected quickly us- ing shape spectrum based matching for further verification. We propose a graph-based matching scheme for view hy- pothesis verification using COSMOS representations of ob- ject views to establish the correct identity and the pose of the sensed object. Preliminary experimental results on a database containing views of ten different objects are show n to demonstrate the effectiveness of theCOSMOS-based 3D object recognition system.",
    "actual_venue": "Pattern Recognition, Icpr Proceedings Of The International Conference"
  },
  {
    "abstract": "Recently, Shadow Graph has been proposed for recovering 3D shapes from shadows projected on curved surfaces. Unfortunately, this method requires a large computational cost. In this paper, we introduce 1D Shadow Graph which can be used for recovering 3D shapes with much smaller computational costs. We also extend our method, so that we can estimate both 3D shapes and light source positions simultaneously under a condition where 3D shapes and light sources are unknown.",
    "actual_venue": "Icpr"
  },
  {
    "abstract": "In this paper, the problem of concealing missing image blocks is casted into a framework of Bayesian estimation. The conditional expectation of the missing block vector is taken over a pilot vector of correctly decoded pixels near the missing block. Multiple observations of the missing vector and pilot vectors obtained in a neighborhood are used to approximate the expectation. We design a multiscale estimation approach with discrete cosine transform pyramid to improve estimation efficiency. The DC image of the missing block is recovered first, and then more details related to high-frequency AC coefficients are recovered successively. Moreover, the algorithm operates in an iterative mode through using estimated block to refine the searching process for the next estimation. The algorithm is found to perform very well for a wide range of block loss rates. Substantial improvement over 14 existing error concealment (inclusive of inpainting) algorithms on various images is demonstrated in our extensive experiments, under different test conditions inclusive of high-loss rates and large block sizes.",
    "actual_venue": "Ieee Trans Circuits Syst Video Techn"
  },
  {
    "abstract": "We introduce a new way of composing proofs in rule-based proof systems that generalizes tree-like and dag-like proofs. In the new definition, proofs are directed graphs of derived formulas, in which cycles are allowed as long as every formula is derived at least as many times as it is required as a premise. We call such proofs circular. We show that, for all sets of standard inference rules, circular proofs are sound. For Frege we show that circular proofs can be converted into tree-like ones with at most polynomial overhead. For Resolution the translation can no longer be a Resolution proof because, as we show, the pigeonhole principle has circular Resolution proofs of polynomial size. Surprisingly, as proof systems for deriving clauses from clauses, Circular Resolution turns out to be equivalent to Sherali-Adams, a proof system for reasoning through polynomial inequalities that has linear programming at its base. As corollaries we get: 1) polynomial-time (LP-based) algorithms that find circular Resolution proofs of constant width, 2) examples that separate circular from dag-like Resolution, such as the pigeonhole principle and its variants, and 3) exponentially hard cases for circular Resolution.",
    "actual_venue": "Theory And Applications Of Satisfiability Testing"
  },
  {
    "abstract": "In this paper, the asymptotic stability problem of genetic regulatory networks with time-varying/constant neutral delay is considered. By introducing a new Lyapunov---Krasovskii functional and applying the free weighting matrix technique, sufficient delay-dependent stability conditions are developed and presented in terms of strict linear matrix inequality, which can be easily verified by using the LMI toolbox. Finally, two numerical examples are provided to demonstrate the effectiveness and reduced conservativeness of the proposed algorithm.",
    "actual_venue": "Soft Computing - A Fusion Of Foundations, Methodologies And Applications"
  },
  {
    "abstract": "With 31 provinces and cities in Mainland China selected as the research objects, the paper explores the acting mechanism of industrial cluster, urbanization and population cluster, builds a measurement index system for the spatial–temporal coupling characteristics of industrial cluster, urbanization and population agglomeration, and then measures the coupling coordination degrees of industrial agglomeration, urbanization and population agglomeration in provinces since 2000 and researches the spatial–temporal evolution and spatial differentiation characteristics of coupling coordination degree. Research results show: ① industrial agglomeration, urbanization and population agglomeration have formed a dynamically coordinated relationship with mutual influences, mutual promotion and joint development; ② the comprehensive development levels of industrial agglomeration, urbanization and population agglomeration in China are rising year by year, but the overall coupling degree and coordination degree of the three are low, but the coupling coordination relationship will improve continuously with time, still showing obvious differences in provinces and the spatial tendency of being “high in the east and low in the west”; ③ the coupling coordination presents a continuous spatial agglomeration state, and the agglomeration intensity is “high on two ends and low in the middle”, thus forming the outstanding hot-spot in areas eastern coastal regions and the outstanding cold-spot areas in western regions; ④ the government should promote industrial agglomeration, industrial structure optimization and upgrading and urbanization, and make reasonable planning for population agglomeration degree practically and effectively, to ensure the matching and coordinated development of industrial agglomeration, population agglomeration and urban bearing capacity.",
    "actual_venue": "Cluster Computing"
  },
  {
    "abstract": "This paper presents a method for designing solid shapes containing slopes where orientation appears opposite to the actual orientation when observed from a unique vantage viewpoint. The resulting solids generate a new type of visual illusion, which we call ''impossible motion'', in which balls placed on the slopes appear to roll uphill thereby defying the law of gravity. This is possible because a single retinal image lacks depth information and human visual perception tries to interpret images as the most familiar shape even though there are infinitely many possible interpretations. We specify the set of all possible solids represented by a single picture as the solution set of a system of equations and inequalities, and then relax the constraints in such a way that the antigravity slopes can be reconstructed. We present this design procedure with examples.",
    "actual_venue": "Comput Geom"
  },
  {
    "abstract": "In this paper, we consider the design of local decision rules for distributed detection systems where decisions from peripheral detectors are transmitted over dependent nonideal channels. Under the conditional independence assumption among multiple sensor observations, we show that the optimal detection performance can be achieved by employing likelihood-ratio quantizers (LRQ) as local decision rules under both the Bayesian criterion and Neyman-Pearson (NP) criterion even for the cases where the channels between the fusion center and local sensors are dependent and noisy. This work generalizes the previous work where independence among such channels was assumed. A person-by-person optimization (PBPO) procedure to obtain the solution is presented along with an illustrative example.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "This paper addresses three questions: how does human memory work? How could I build a memory like that? How could I use it to solve practical problems?",
    "actual_venue": "Ijcnn"
  },
  {
    "abstract": "Recent developments in games and interactive storytelling applications have seen artificially intelligent computer controlled characters being included extensively. Non-human controlled characters are starting to play an increasingly significant role in enhancing the perceived intelligence of games. Although many of them employed certain cheating techniques (e.g. allocating more resources at the start to AI opponents to make them appear more aggressive), some limited learning did appear in several games (e.g. letting AI opponents remember where human users initiated attacked in previous game). In our Virtual Singapura research project, we incorporate software agents into our virtual world to provide more complex user interactions. With intelligent software agents being infused into interactive digital media applications, there is great potential in improving the overall user experience. However, during the process of our research, we discovered that the traditional way of adding a multi-agent system into a computer game requires a large amount of investment in time and resources and a high level of expertise in Agent Oriented Software Engineering (AOSE). Moreover, game AI is usually closely coupled with other parts of the game code which makes it hard to reuse or replace. This research proposes a multi-agent development and runtime framework which not only provides ease-of-use agent design and implementation tools but also can be easily plugged into various interactive digital media applications.",
    "actual_venue": "Computers In Entertainment"
  },
  {
    "abstract": "This paper provides details on the the importance and requirements of multiple selection methods, and the creation and management of interactive objects within immersive CAVE like virtual reality systems. Various methods of selection and interactive model creation are explored, as well as the design and implementation of an application able to create interactive objects from within an immersive virtual environment in real-time. Some initial limitations of the research and potential improvements are documented in this paper. Finally, two case studies are presented that demonstrate the additional functionality and interaction gained within an immersive virtual environment from the research. Being able to create and manage interactive objects from within an immersive environment starts to bridge the gap between an immersive 3D visualisation. and an interactive, and productive environment. (C) 2010 Published by Elsevier Ltd.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "Technology progress in DNA sequencing boosts the genomic database growth at faster and faster rate. Compression, accompanied with random access capabilities, is the key to maintain those huge amounts of data. In this paper we present an LZ77-style compression scheme for relative compression of multiple genomes of the same species. While the solution bears similarity to known algorithms, it offers significantly higher compression ratios at compression speed over a order of magnitude greater. One of the new successful ideas is augmenting the reference sequence with phrases from the other sequences, making more LZ-matches available.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "For many URLLC services, mobility is a key requirement together with latency and reliability. 3GPP has defined the target of MIT as 0 ms, and a general URLLC reliability requirement as 1 - 10-5 within a latency of 1 ms for 5G. In this article, we analyzed the impact of MIT and handover failure (HOF) rate on the reliability performance. From the analysis, at 120 km/h, with MIT of 0 ms, the required...",
    "actual_venue": "Ieee Network"
  },
  {
    "abstract": "In the field of action recognition, when and where an interaction between a human and an object happens has the potential to be valid information in enhancing action recognition accuracy. Especially, in daily life where each activities are performed in longer time frame, conventional short term action recognition may fail to generalize do to the variety of shorter actions that could take place during the activity. In this paper, we propose a novel representation of human object interaction called Human-Object Maps (HOMs) for recognition of long term daily activities. HOMs are 2D probability maps that represents spatio-temporal information of human object interaction in a given scene. We analyzed the effectiveness of HOMs as well as features relating to the time of the day in daily activity recognition. Since there are no publicly available daily activity dataset that depicts daily routines needed for our task, we have created a new dataset that contains long term activities. Using this dataset, we confirm that our method enhances the prediction accuracy of the conventional 3D ResNeXt action recognition method from 86.31% to 97.89%.",
    "actual_venue": "Iapr International Conference On Machine Vision Applications"
  },
  {
    "abstract": "In this paper we present a probabilistic approach to reconstruct a document from its torn pieces which is extremely helpful for the legal system. The method investigates several previously unexplored issues and proposes probabilistic dependencies of different (available) parts (or pieces) of a document. It iteratively calculates the probability of an arrangement subject to some constraints and attempts to produce the best possible configuration (or solution) using low level image statistics. The reconstruction method also ranks different (final) arrangements in order to help decision making process. Two types of data were used in the investigation. A set of documents where documents were torn naturally (as we do it in our daily life) and the second set consisting of documents that were torn to destroy particular evidences. Evaluation shows that the method is quite robust to tackle both the problems.",
    "actual_venue": "Document Analysis And Recognition"
  },
  {
    "abstract": "The American Association for the Advancement of Science carried out a study on intellectual property and electronic journal publishing with the aim of identifying those aspects of the intellectual property regime that facilitate or constrain the effective development of electronic scientific publishing. This paper summarizes the conclusions reached, emphasizes the continuing need to maintain the legal framework offered by copyright law, but makes recommendations both for its modification and in its application to ensure that the potential benefits of electronic publishing are fully realized. The full report by AAAS is posted on the Web at www.aaas.org/spp/frl/projects/epub/finalrept.html.",
    "actual_venue": "Learned Publishing"
  },
  {
    "abstract": "XML data is explosively increasing, and a large amount of XML data, in which similar contents are described using different tag names and structures, have been emerging as a consequence. In such a situation, one cannot write a query against such XML data unless he/she knows the structure of the data. In this research, we propose a scheme to cope with this problem. Specifically, we expand XPath queries by replacing tag names with similar ones with the help of ontologies. In addition, we try to realize (structural) proximity matching of path expressions using edit similarity, which is a similarity measure based on edit distance. We also discuss application of SSJoin, which is an operator to support similarity joins in relational database systems, for speeding up the proposed scheme. We finally show the effectiveness of the proposed method by a series of experimentations.",
    "actual_venue": "Dexa"
  },
  {
    "abstract": "Wireless sensor network (WSN) has become one of the most promising network technologies for many useful applications. However, for the lack of resources, it is different but important to ensure the security of the WSNs. Key management is a corner stone on which to build secure WSNs for it has a fundamental role in confidentiality, authentication, and so on. Combinatorial design theory has been used to generate good-designed key rings for each sensor node in WSNs. A large number of combinatorial design based key management schemes have been proposed but none of them have taken key updating into consideration. In this paper, we point out the essence of key updating for the unital design based key management scheme and propose two key updating methods; then, we conduct performance analysis on the two methods from three aspects; at last, we generalize the two methods to other combinatorial design based key management schemes and enhance the second method.",
    "actual_venue": "Journal Of Sensors"
  },
  {
    "abstract": "In the past years, the scientific community has placed a special interest in remotely sensing soil moisture and vegetation parameters. Radiometry and radar techniques have been widely used for years. Global Navigation Satellite Systems opportunity signals Reflected (GNSS-R) over the earth's surface are younger, but they have already shown their potential to perform these observations. This paper presents a GNSS-R technique, based on Global Positioning System (GPS) measurements, that allows the retrieval of several geophysical parameters from land surfaces. This technique measures the power of the interference signal between the direct GPS signal and the reflected one after scattering over the land, so it is called Interference Pattern Technique (IPT). This paper presents the results obtained after applying the IPT for topography, soil moisture, and vegetation height retrievals over vegetation-covered soils.",
    "actual_venue": "Ieee Trans Geoscience And Remote Sensing"
  },
  {
    "abstract": "This paper focuses on 3D Collaborative Virtual Learning Environments, examining the state of the art in both open source and proprietary software. Issues pertaining to the use of open source Collaborative Virtual Learning Environments are discussed, rationalizing the choice of executing a collaborative learning scenario in Second Life. The scenario is then presented and evaluation results assess the appropriateness of the chosen platform with regards to its technical and pedagogical affordances. Finally, students' suggestions and reactions towards such a novel didactical approach are discussed.",
    "actual_venue": "Ijec"
  },
  {
    "abstract": "Designers require a means of designing complex free-form surfaces easily and intuitively. One general approach to designing such surfaces is to first define a curve mesh consisting of characteristic lines, such as cross sections and boundary curves, then to interpolate the curve mesh using free-form surfaces. NURBS surfaces are widely used but make the interpolation of an irregular curve mesh difficult. This has been a major limiting constraint on designers. In this paper, we propose a new surface representation that enables the smooth interpolation of an irregular curve mesh with NURBS curves and surfaces.",
    "actual_venue": "Computer Aided Geometric Design"
  },
  {
    "abstract": "The recent development in high gain DC-DC converters motivates its application on renewable energy systems with low output voltage such as photovoltaic cells resulting in the requirement of high voltage gain converters with improved static and dynamic characteristics. This study proposes an optimal reduced order linear quadratic regulator controller for a reduced order model of a voltage multiplie...",
    "actual_venue": "Iet Circuits, Devices And Systems"
  },
  {
    "abstract": "Decomposing the design of supervisory control laws for Petri nets is an efficient way to tackle its complexity. We consider legal sets which are the union of two sets. In general, this can make the control law obtained via decomposition too restrictive, i.e., it disables more transitions than necessary. Generalizing existing results, we give structural conditions under which the control law via decomposition is maximally permissive.",
    "actual_venue": "Automatic Control, Ieee Transactions"
  },
  {
    "abstract": "This paper presents a system that applies Textual Entailment recognition techniques to the AVE task. This is performed comparing representations of text snippets by means of a variety of lexical measures and syntactic structures. The representations of the question and the answer are compared determining if there is an entailment relation between them. The performed experiments over the English test corpus obtained a maximum F-score of 0.39.",
    "actual_venue": "Advances In Multilingual And Multimodal Information Retrieval"
  },
  {
    "abstract": "In recent years, the wireless local area networks (WLAN) access points (APs) are being deployed rapidly in the offices and the campuses to satisfy the quality of service (QoS) requirements such as network bandwidth. However, many empirical studies show that a large fraction of idle WLAN resources result in the significant energy losses. To reduce the energy consumption without violating the QoS requirement, we propose a QoS-aware AP energy saving mechanism using software defined network (SDN). We leverage the capability of the SDN controller to dynamically monitor the network condition information to estimate the network bandwidth requirement of the devices and then manage the network forwarding to provide seamless handover. Our experimental results show that the proposed scheme can effectively reduce the number of power-on WLAN APs while still providing QoS guarantee, such as network bandwidth and handover cost.",
    "actual_venue": "Ieee International Conference On Communications"
  },
  {
    "abstract": "Privacy is a key challenge for continued digitalization of health. The forthcoming European General Data Protection Regulation (GDPR) is transforming this challenge into regulatory directives. User consent provisioning and coordinating across data services will be the keys in addressing this challenge. We suggest a privacy-driven architecture that provides tools for providing user consent as a service. This enables managing and reusing private health information between a large amount of data sources, individuals and services, even when they are not known beforehand. The proposed architecture integrates data security and semantic descriptions into a trust query framework to provide the required interoperability and co-operation support for future health services. This approach provides benefits for all stakeholders through safer data management, cost and process savings, multi-provider services, and services based on emerging new business models.",
    "actual_venue": "Arxiv: Computers And Society"
  },
  {
    "abstract": "We present PACE, a Personalized, Automatically Calibrating Eye-tracking system that identifies and collects data unobtrusively from user interaction events on standard computing systems without the need for specialized equipment. PACE relies on eye/facial analysis of webcam data based on a set of robust geometric gaze features and a two-layer data validation mechanism to identify good training samples from daily interaction data. The design of the system is founded on an in-depth investigation of the relationship between gaze patterns and interaction cues, and takes into consideration user preferences and habits. The result is an adaptive, data-driven approach that continuously recalibrates, adapts and improves with additional use. Quantitative evaluation on 31 subjects across different interaction behaviors shows that training instances identified by the PACE data collection have higher gaze point-interaction cue consistency than those identified by conventional approaches. An in-situ study using real-life tasks on a diverse set of interactive applications demonstrates that the PACE gaze estimation achieves an average error of 2.56º, which is comparable to state-of-the-art, but without the need for explicit training or calibration. This demonstrates the effectiveness of both the gaze estimation method and the corresponding data collection mechanism.",
    "actual_venue": "CHI"
  },
  {
    "abstract": "We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.",
    "actual_venue": "Icml"
  },
  {
    "abstract": "In this contribution, the design game as a method in Participatory Design is discussed. The focus lies on the organizational design game. For using the design game relations of power, socio-technical textures and forms of work and organization are treated as concerns that need to be addressed carefully. Cases from student projects are used as illustrating examples; work environments were redesigned and design games played. It turns out that degrees of freedom are present for the choice of (gaming) method as well as the ways of using the selected method. These degrees of freedom should be used in a way that will be labeled as «interested», rather than in a way labeled as «taking for granted». It is not possible to guarantee an interested and beneficial approach; yet the paper argues on the grounds that reflective gaming practice can be supportive in this direction.",
    "actual_venue": "PDC"
  },
  {
    "abstract": "This paper proposes a novel recommendation method called RecDI. In the multi-category item recommendation domain, RecDI is designed to combine user ratings with information involving user's direct and indirect neighborhood associations. Through relevant benchmarking experiments on two real-world datasets, we show that RecDI achieves better performance than other traditional recommendation methods, which demonstrates the effectiveness of RecDI.",
    "actual_venue": "WWW (Companion Volume)"
  },
  {
    "abstract": "An extended general spacefilling curves heuristic (EGSH) is introduced as an extension to the general spacefilling curves heuristic (GSH) proposed by Bartholdi and Platzman (1988). These are generic methods directly applicable to many problems in which data is represented in a multidimensional real vector space. A mapping is established between a region of the multidimensional space and an interval of the real line, and then the problem is solved in one dimension. This becomes quite useful if the problem has an easier, faster or more reliable solution in the real line. The proposed extension allows accurate solutions to many problems not reliably solvable by the original heuristic. A successful application to function approximation is presented",
    "actual_venue": "Pattern Recognition, Proceedings Fourteenth International Conference"
  },
  {
    "abstract": "In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "Real-world time series have certain properties, such as stationarity, seasonality, linearity, among others, which determine their underlying behaviour. There is a particular class of time series called long-memory processes, characterized by a persistent temporal dependence between distant observations, that is, the time series values depend not only on recent past values but also on observations of much prior time periods. The main purpose of this research is the development, application, and evaluation of a computational intelligence method specifically tailored for long memory time series forecasting, with emphasis on many-step-ahead prediction. The method proposed here is a hybrid combining genetic programming and the fractionally integrated (long-memory) component of autoregressive fractionally integrated moving average (ARFIMA) models. Another objective of this study is the discovery of useful comprehensible novel knowledge, represented as time series predictive models. In this respect, a new evolutionary multi-objective search method is proposed to limit complexity of evolved solutions and to improve predictive quality. Using these methods allows for obtaining lower complexity (and possibly more comprehensible) models with high predictive quality, keeping run time and memory requirements low, and avoiding bloat and over-fitting. The methods are assessed on five real-world long memory time series and their performance is compared to that of statistical models reported in the literature. Experimental results show the proposed methods' advantages in long memory time series forecasting.",
    "actual_venue": "Genetic Programming And Evolvable Machines"
  },
  {
    "abstract": "Refactoring is a process of applying behavior-preserving transformations to improve the design, readability, structure, performance, abstraction, and maintainability of existing code. This paper presents an approach to genetic algorithm-driven refactoring for Java programs to automatically judge the qualities of programs based on design patterns. If a program is judged to be bad, refactoring will be further recommended so that the program can be transformed using an appropriate design pattern.",
    "actual_venue": "Software Engineering Advances"
  },
  {
    "abstract": "We consider the problem of D-hop virtual path layout in ATM (Asynchronous Transfer Mode) networks. Given a physical network and an all-to-all traffic pattern, the problem consists of designing a virtual network with a given diameter D, which can be embedded in the physical one with a minimum congestion (the congestion is the maximum load of a physical link). Here we propose a method to solve this problem when the diameter is 2. We use this method to give an asymptotically optimal solution for the 2-hop virtual path layout problem for all-to-all traffic when the physical network is a mesh, a torus or a chordal ring.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "We present and study a stabilized mixed finite element method for single-phase compressible flow through porous media. This method is based on a pressure projection stabilization method for multiple-dimensional incompressible flow problems by using the lowest equal-order pair for velocity and pressure (i.e., the P(1) - P(1) pair). An optimal error estimate in divergence norm for the velocity and suboptimal error estimates in the L(2)-norm for both velocity and pressure are obtained. Numerical results are given in support of the developed theory.",
    "actual_venue": "Journal Of Applied Mathematics"
  },
  {
    "abstract": "Creative processes are widely believed to involve the generation of multiple, discrete, well-defined possibilities followed by exploration and selection. An alternative, inspired by parallel distributed processing models of associative memory, is that creativity involves the merging and interference of memory items resulting in a single cognitive structure that is ill-defined, and can thus be said to exist in a state of potentiality. We tested this hypothesis in an experiment in which participants were interrupted midway through solving an analogy problem and asked what they were thinking in terms of a solution. Naive judges categorized their responses as AP if there was evidence of merging solution sources from memory resulting in an ill-defined idea, and SM if there was no evidence of this. Data from frequency counts and mean number of SM versus AP judgments supported the hypothesis that midway through creative processing an idea is in a potentiality state.",
    "actual_venue": "Cogsci"
  },
  {
    "abstract": "Looping is a complex dynamic process affected by many interacted factors, and is becoming more and more important in the state-of-the-art thermosonic wire bonding. To provide an insight view of loop mechanism, the looping process of standard loop was experimentally studied with a high resolution and high speed video camera. The capillary trace and loop profile evolution process were obtained from looping video with a digital image process program. A phenomenological description was used to understand the looping forming mechanism. The effect of capillary trace on loop profile was investigated, and the kinks forming mechanism were discussed. The spring back and kink up were detaily described. Experiment results show that loop profile was affected by kinks number, position on gold wire and deformation. Kinks were formed by reverse motion of capillary. From the geometry point of view, kink is the wire segment with the local maximum curvature. From the mechanical point of view, kink is the partly plastic deformed wire segment with elastic deformed core inside. This study may be useful for loop design in industry and for loop dynamic research in academic.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Among the existing hashing methods, the Self-taught hashing (STH) is regarded as the state-of-the-art work. However, it still suffers the problem of semantic loss, which mainly comes from the fact that the original optimization objective of in-sample data is NP-hard and therefore is compromised into the combination of Laplacian Eigenmaps (LE) and binarization. Obviously, the shape associated with the embedding of LE is quite dissimilar to that of binary code. As a result, binarization of the LE embedding readily leads to significant semantic loss. To overcome this drawback, we combine the constrained nonnegative sparse coding and the Support Vector Machine (SVM) to propose a new hashing method, called nonnegative sparse coding induced hashing (NSCIH). Here, nonnegative sparse coding is exploited for seeking a better intermediate representation, which can make sure that the binarization can be smoothly conducted. In addition, we build an image copy detection scheme based on the proposed hashing methods. The extensive experiments show that the NSCIH is superior to the state-of-the-art hashing methods. At the same time, this copy detection scheme can be used for performing copy detection over very large image database.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Android Honeycomb operating system is widely used for tablet devices, such as Samsung Galaxy Tab. The Android system programs are usually efficient and secure in memory management. However, there has been a few security issues reported that show Android's insufficient protection to the kernel. In this work, we reveal a new security pitfall in memory management that can cause severe errors and even system failures. Existing security software for android do not detect this pitfall, due to the private implementation of Android kernel. We then discuss two vulnerabilities introduced by this pitfall: 1) malicious programs can escalate the root-level privilege of a process, through which it can disable the security software, implant malicious codes and install rootkits in the kernel; 2) deny of service attacks can be launched. Experiments have been conducted to verify these two vulnerabilities on Samsung Galaxy Tab 10.1 with Tegra 2 CPU. To protect systems from these vulnerabilities, we proposed a patching solution, which has been adopted by Google.",
    "actual_venue": "ICC"
  },
  {
    "abstract": "This paper presents a new open-loop phase shifter and frequency synthesizer which can be implemented by small hardware. In the proposed method the differential square wave is converted to a differential ramp. Then the cross points of two ramps are detected as the middle points of high or low durations and are recovered to full digital levels, for 90° shifting operation. 4-phases in 50 MHz frequency can be generated by 3.5 mW power consumption and 60 μm × 60 μm area. All circuits have been simulated in 0.35 μm CMOS technology.",
    "actual_venue": "Iie Transactions"
  },
  {
    "abstract": "A procedure is presented for computing the friction forces required to satisfy static equilibrium, given a set of normal contact forces exerted by a three-fingered mechanical hand. The contact between the fingers and the object is modeled as \"point contact with friction. \"This means the fingertip is free to rotate about the point of contact, but sliding along its surface is resisted by a friction force. Comparing these friction forces to the maximum friction force, which can be sustained between the contacting surfaces, it must be determined whether the object slips from the grasp and further the instantaneous motion of the impending movement. Two examples are solved in detail to illustrate the procedure.",
    "actual_venue": "Robotics and Automation, IEEE Journal of  "
  },
  {
    "abstract": "Computational algorithms modeling the insertion of endovascular devices, such as coil or stents, have gained an increasing interest in recent years. This scientific enthusiasm is due to the potential impact that these techniques have to support clinicians by understanding the intravascular hemodynamics and predicting treatment outcomes. In this work, a virtual coiling technique for treating image-based aneurysm models is proposed. A dynamic path planning was used to mimic the structure and distribution of coils inside aneurysm cavities, and to reach high packing densities, which is desirable by clinicians when treating with coils. Several tests were done to evaluate the performance on idealized and image-based aneurysm models. The proposed technique was validated using clinical information of real coiled aneurysms. The virtual coiling technique reproduces the macroscopic behavior of inserted coils and properly captures the densities, shapes and coil distributions inside aneurysm cavities. A practical application was performed by assessing the local hemodynamic after coiling using computational fluid dynamics (CFD). Wall shear stress and intra-aneurysmal velocities were reduced after coiling. Additionally, CFD simulations show that coils decrease the amount of contrast entering the aneurysm and increase its residence time.",
    "actual_venue": "Ieee Trans Med Imaging"
  },
  {
    "abstract": "Modern Java Virtual Machines (JVM) commonly adopt Just-In-Time (JIT) compilation to speed up the execution of Java Bytecode. However, the effort of compiling a region of code is only worth if the code is frequently executed. Therefore, Selective Compilation is employed so that the JIT compiler is only invoked on those regions of code where most of the computation is performed (hot spots). The core task in Selective Compilation is to correctly identify the hot spots in a program. In our SeleKaffe prototype virtual machine, we introduce two heuristics aimed at detecting hot spots both statically, via bytecode analysis, and dynamically, via profiling information. Experimental results on a representative set of benchmarks show that our method selection strategy is more accurate than known strategies, and not significantly slower.",
    "actual_venue": "SAC"
  },
  {
    "abstract": "The paper gives an overview on disk drive technology and how it determines the characteristics of the related electronics. Moreover, it details the resulting design methodology with an emphasis on system modeling and simulation. For instance, it is shown how key system properties, e.g. the seek time, can be determined through mixed simulation of mechanics, electronics and firmware. In addition, the same simulation environment is used to realistically verify analog and digital circuitry as well as software",
    "actual_venue": "Iccd"
  },
  {
    "abstract": "Integrated digital signal processing in telephones allows for sound personalization algorithms directly in the de-vices. In the current study, a non-linear, frequency-dependent algorithm for adjusting telephone signals was evaluated with normal-hearing and hearing-impaired listeners under controlled acoustical conditions. Listeners adjusted the preferred output level and spectrum via a user interface on a touch screen during a phone call in quiet and in a noisy environment. Large individual differ-ences of preferred output spectra were observed. Higher output levels in noisy than in quiet environments were observed for all listeners. No clear trend was observed for the preferred spectral processing when changing from quiet to noisy environments, indicating that a user-adaptive spectral optimization is required as a user-independent solution often would not match different types for individually optimum trade-off between speech intelligibility and sound comfort.",
    "actual_venue": "Speech Communication; Itg Symposium"
  },
  {
    "abstract": "This paper addresses the problem of understanding the current set of commercially available object-oriented database systems. It proposes a classification of these systems based on their external behavior and on the target customers they are aimed at. The classification distinguishes four categories: language oriented database systems, persistent programming languages, engineering database systems and full object-oriented database systems. For each of these categories, a definition is provided, and the characteristics and benefits of the systems are given.",
    "actual_venue": "Edbt"
  },
  {
    "abstract": "We consider the problem of fingerprinting text by sets of symbols. Specifically, if S is a string, of length n, over a finite, ordered alphabet Σ, and S' is a substring of S, then the fingerprint of S' is the subset φ of Σ of precisely the symbols appearing in S'. In this paper we show efficient methods of answering various queries on fingerprint statistics. Our preprocessing is done in time O(n|Σ|log n log |Σ|) and enables answering the following queries: (1) Given an integer k, compute the number of distinct fingerprints of size k in time O(1). (2) Given a set φ ⊆ Σ, compute the total number of distinct occurrences in S of substrings with fingerprint φ in time O(|Σ|logn).",
    "actual_venue": "J Discrete Algorithms"
  },
  {
    "abstract": "The implementation of the Icon programming language is more interesting and difficult than the implementation of many other programming languages because an expression in Icon can generate a sequence of results. The implementation therefore must support control backtracking in expression evaluation. There also are several novel control structures related to generators. Because expression evaluation is limited lexically, a full coroutine mechanism is not needed and expression evaluation can be handled in a stack-like fashion.The implementation of Icon consists of a virtual machine with a stack-based architecture and an interpreter that executes the virtual machine instructions. There have been several different interpreters for Icon's virtual machine. This paper describes a new approach in which the interpreter is called recursively whenever the context for expression evaluation changes. This recursive interpreter has the advantage of being conceptually clear and flexible without sacrificing efficiency.",
    "actual_venue": "Sigplan Papers Of The Symposium On Interpreters And Interpretive Techniques"
  },
  {
    "abstract": "We designed and implemented a vision-based projected table-top interface for finger interaction. The system offers a simple and quick setup and economic design. The projection onto the tabletop provides more comfortable and direct viewing for users, and more natural, intuitive yet flexible interaction than classical or tangible interfaces. Homography calibration techniques are used to provide geometrically compensated projections on the tabletop. A robust finger tracking algorithm is proposed to enable accurate and efficient interactions using this interface. Two applications have been implemented based on this interface.",
    "actual_venue": "Iccv-Hci"
  },
  {
    "abstract": "The assessment of fetal wellbeing depends heavily on variations in fetal heart rate (FHR) patterns. The variations in FHR patterns are very complex in nature thus its reliable interpretation is very difficult and often leads to erroneous diagnosis. We propose a new method for evaluation of fetal health status based on interval type-2 fuzzy logic through fetal phonocardiography (fPCG). Type-2 fuzzy logic is a powerful tool in handling uncertainties due to extraneous variations in FHR patterns through its increased fuzziness of relations. Four FHR parameters are extracted from each fPCG signal for diagnostic decision making. The membership functions of these four inputs and one output are chosen as a range of values so as to represent the level of uncertainty. The fuzzy rules are constructed based on standard clinical guidelines on FHR parameters. Experimental clinical tests have shown very good performance of the developed system in comparison with the FHR trace simultaneously recorded through standard fetal monitor. Statistical evaluation of the developed system shows 92% accuracy. With the proposed method we hope that, long-term and continuous antenatal care will become easy, cost effective, reliable and efficient.",
    "actual_venue": "Appl Soft Comput"
  },
  {
    "abstract": "This paper presents the Model Data Passing Interface (MODPI). The approach provides fine-grained, multidirectional feedbacks between legacy environmental system models through read and write access to relevant model data during simulation using a bidirectional, event-based, publish-subscribe system with a message broker. MODPI only requires commented directives in the original code and an XML linkage file with an optional custom data conversion module. Automated code generation, compilation, and execution reduce the programming burden on the modeler. Case study results indicated that MODPI required less code modifications within each model code base both before and after automated code generation, outperforming a baseline subroutine approach. Performance overhead for MODPI was minimal for the use case, offering speedup in some cases through parallel execution. MODPI is much less invasive than other techniques, potentially encouraging adoption by the modeling community in addition to maintainability and reusability of integrated model code. Display Omitted We developed a minimally invasive model data passing interface.MODPI requires very minimal modifications to original model code bases.MODPI uses code generation to minimize the programming work.MODPI provides fine-grained, multidirectional feedback between models.We provide analysis of computational overhead concerns.",
    "actual_venue": "Environmental Modelling And Software"
  },
  {
    "abstract": "In this work-in-progress paper, we present a solution to the illumination problem that is intermediate between the conventional local and global approaches to illumination. It involves the representation of radiance on a surface as a finite element expansion in terms of wavelets. By expanding in terms of ``Nusselt coordinates'''', we show how irradiance, transport, and surface interaction can be evaluated simply and directly in terms of wavelet coefficients. We present an example of transport.",
    "actual_venue": "Comput Graph Forum"
  },
  {
    "abstract": "Introduction. The study contributes to the conceptual investigation of information use by reviewing the conceptions of epistemic work proposed by Cook and Brown, and knowing in practice proposed by Orlikowski. The above conceptions are developed in the field of organization science.Method. The key articles by Cook and Brown, and Orlikowski were scrutinized by means of in-depth conceptual analysis. The main focus was placed on the comparative analysis of the conceptions of epistemic work and knowing in practice.Results. The approach to epistemic work conceptualizes information use as the employment of tacit and explicit knowledge in the service of knowing. Knowing is understood as epistemic work that is done as an inherent part of action. Orlikowski approaches information use as construction and reconstruction of knowledgeability in and through action.Conclusions. The conceptions of epistemic work and knowing in practice provide holistic frameworks for the study of information use in context, while not probing the details of information use processes per se.",
    "actual_venue": "Information Research-An International Electronic Journal"
  },
  {
    "abstract": "This paper proposes a novel fuzzy identification approach based on an updated version of pi-sigma neural network. The proposed method has the following characteristics: 1) The consequence function of each fuzzy rule can be a nonlinear function, which makes it capable to deal with the nonlinear systems more efficiently. 2) Not only each parameter of the consequence functions but also the membership function of each fuzzy subset can be modified easily online. In this way, the fuzzy identification algorithm is greatly simplified and therefore is suitable for real-time applications. Simulation results show that the new method is effective in modeling and controlling of a large class of complex systems",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "Usual handwritten signature verification systems (HSVS) address the writer-independent (WI) approach using only bi-class robust classifiers to deal with the most challenging tasks. Indeed, writer-independent concept, reduced size of references and one-class signature verification are still open issues in practical cases. In this paper, we propose a one-class writer-independent system using feature dissimilarity measures (FDM) thresholding for classification and a reduced number of references. The proposed system involves the use of Contourlet Transform (CT) based directional code co-occurrence matrix (DCCM) feature generation method. The verification is achieved through a WI threshold which is automatically selected using a new signature stability criterion. The proposed writerindependent concept is besides addressed through the mixture of different writers’ datasets in both design and verification stages. Experimental results show the effectiveness of the proposed system in spite of the strict verification protocol using the oneclass concept, a unique threshold for accepting or rejecting a questioned signature, the reduced number of writers and the limited number of reference signatures.",
    "actual_venue": "Information Forensics And Security, Ieee Transactions"
  },
  {
    "abstract": "This paper presents an incremental learning algorithm for the hybrid RBF-BP (ILRBF-BP) network classifier. A potential function is introduced to the training sample space in space mapping stage, and an incremental learning method for the construction of RBF hidden neurons is proposed. The proposed method can incrementally generate RBF hidden neurons and effectively estimate the center and number of RBF hidden neurons by determining the density of different regions in the training sample space. A hybrid RBF-BP network architecture is designed to train the output weights. The output of the original RBF hidden layer is processed and connected with a multilayer perceptron (MLP) network; then, a back propagation (BP) algorithm is used to update the MLP weights. The RBF hidden neurons are used for nonlinear kernel mapping and the BP network is then used for nonlinear classification, which improves classification performance further. The ILRBF-BP algorithm is compared with other algorithms in artificial data sets and UCI data sets, and the experiments demonstrate the superiority of the proposed algorithm.",
    "actual_venue": "Eurasip J Adv Sig Proc"
  },
  {
    "abstract": "This paper discusses techniques for the computation of global illumination in environments with a participating medium using a Monte Carlo simulation of the particle model of light. Efficient algorithms and data structures for tracking the particles inside the volume have been developed. The necessary equation for computing the illumination along any given direction has been derived for rendering a scene with a participating medium. A major issue in any Monte Carlo simulation is the uncertainty in the final simulation results. Various steps of the algorithm have been analysed to identify major sources of uncertainty. To reduce the uncertainty, suitable modifications to the simulation algorithm have been suggested using variance reduction methods of forced collision, absorption suppression and particle divergence. Some sample scenes showing the results of applying these methods are also included.",
    "actual_venue": "Journal Of Visualization And Computer Animation"
  },
  {
    "abstract": "To fully leverage the information from different data sources and applications, an enterprise needs a generic, interoperable and flexible infrastructure to integrate and coordinate information across back-end data sources on semantic level. Through undertaking research at the intersection of the Semantic Web and Grid, the Semantic Grid expects to establish a semantic interconnection environment to effectively organize, share, cluster, fuse, and manage globally distributed versatile resources. In this context, we introduce SGII, an EII (Enterprise Information Integration) infrastructure based on Semantic Grid vision to achieve adaptive and intelligence information sharing. A survey of existent solutions is made to provide evidence of the benefits from Semantic Grid in the context of integration and interoperation of enterprise information. A primary architecture for SGII is introduced based on the analysis of realizing the vision of an infrastructure for semantic information integration on grid.",
    "actual_venue": "GCC"
  },
  {
    "abstract": "The Terrain Observation with Progressive Scans (TOPS) acquisition mode of Sentinel-1A provides a wide coverage per acquisition and features a repeat cycle of 12 days, making this acquisition mode attractive for surface subsidence monitoring. A few studies have analyzed wide-coverage surface subsidence of Wuhan based on Sentinel-1A data. In this study, we investigated wide-area surface subsidence characteristics in Wuhan using 15 Sentinel-1A TOPS Synthetic Aperture Radar (SAR) images acquired from 11 April 2015 to 29 April 2016 with the Small Baseline Subset Interferometric SAR (SBAS InSAR) technique. The Sentinel-1A SBAS InSAR results were validated by 110 leveling points at an accuracy of 6 mm/year. Based on the verified SBAS InSAR results, prominent uneven subsidence patterns were identified in Wuhan. Specifically, annual average subsidence rates ranged from -82 mm/year to 18 mm/year in Wuhan, and maximum subsidence rate was detected in Houhu areas. Surface subsidence time series presented nonlinear subsidence with pronounced seasonal variations. Comparative analysis of surface subsidence and influencing factors (i.e., urban construction, precipitation, industrial development, carbonate karstification and water level changes in Yangtze River) indicated a relatively high spatial correlation between locations of subsidence bowl and those of engineering construction and industrial areas. Seasonal variations in subsidence were correlated with water level changes and precipitation. Surface subsidence in Wuhan was mainly attributed to anthropogenic activities, compressibility of soil layer, carbonate karstification, and groundwater overexploitation. Finally, the spatial-temporal characteristics of wide-area surface subsidence and the relationship between surface subsidence and influencing factors in Wuhan were determined.",
    "actual_venue": "Remote Sensing"
  },
  {
    "abstract": "The use of crowdsourced-based network performance measurement services and technologies is set to increase continually among the National Research and Education Networks (NRENs) in the near future. This requires an understanding of the behavior of network performance issues, and their localization and verification on wireless campus networks. The approach presented in this paper is based on the end-user mobile device measurement feedback and allows the visualization of network performance in real time.",
    "actual_venue": "International Congress On Ultra Modern Telecommunications And Control Systems And Workshops"
  },
  {
    "abstract": "In bipedal walking, a trajectory is acceptable as long as it is repetitive and allows the foot to clear the ground, while allowing the biped to move forward. Since the actual trajectory followed by a biped is not as important, a biped having more than one passive joints can also meet the motion requirements. Due to physical constraints, a biped is under-actuated at the ground contact with the feet. A biped should exhibit limit cycles when moving continuously in an environment. In general, it is difficult to prove existence of limit cycles for nonlinear systems. In this work, we generate limit cycles for a class of nonlinear under-actuated bipeds using differential flatness. A specific inertia distribution renders the biped design differentially flat. Differential Flatness allows generation of a family of limit cycles amenable to numerical optimization. The results are illustrated by two DOF biped.",
    "actual_venue": "Proceedings Of Ieee International Conference On Robotics And Automation, Vols"
  },
  {
    "abstract": "In an attempt to maximize productivity within the medical imaging department, increasing importance and attention is being placed on workflow. Workflow is the process of analyzing individual steps that occur during a single event, such as the performance of an MRI exam. The primary focus of workflow optimization within the imaging department is automation and task consolidation, however, a number of other factors should be considered including the stochastic nature of the workload, availability of human resources, and the specific technologies being employed. The purpose of this paper is to determine the complex relationship that exists between information technology and the radiologic technologist, in an attempt to determine how workflow can be optimized to improve technologist productivity. This relationship takes on greater importance as more imaging departments are undergoing the transition from film-based to filmless operation. A nationwide survey was conducted to compare technologist workflow in film-based and filmless operations, for all imaging modalities. The individual tasks performed by technologists were defined, along with the amount of time allocated to these tasks. The index of workflow efficiency was determined to be the percentage of overall technologist time allocated to image acquisition, since this is the primary responsibility of the radiologic technologist. Preliminary analysis indicates technologist workflow in filmless operation is enhanced when compared with film-based operation, for all imaging modalities. The specific tasks that require less technologist time in filmless operation are accessing data and retake rates (due to both technical factors and lost exams). Surprisingly, no significant differences were reported for the task of image processing, when comparing technologist workflow in film-based and filmless operations. Additional research is planned to evaluate the potential workflow gains achievable through workflow optimization software, improved systems integration, and automation of advanced image processing techniques.",
    "actual_venue": "J Digital Imaging"
  },
  {
    "abstract": "We explore a simple, web-based method for predicting the genre of a given artist based on co-occurrence analysis, i.e. analyzing co-occurrences of artist and genre names on mu- sic-related web pages. To this end, we use the page counts provided by Google to estimate the relatedness of an arbi- trary artist to each of a set of genres. We investigatefourdif- ferent query schemes for obtaining the page counts and two different probabilistic approaches for predicting the genre of a given artist. Evaluation is performed on two test collec- tions, a large one with a quite general genre taxonomy and a quite small one with rather specific genres. Since our approach yields estimates for the relatedness of an artist to every genre of a given genre set, we can de- rive genredistributionswhich incorporateinformationabout artists that cannot be assigned a single genre. This allows us to overcome the inflexible artist-genre assignment usu- ally used in music information systems. We present a sim- ple method to visualize such genre distributions with our Traveller's Sound Player. Finally, we briefly outline how to adapt the presented approach to extract other properties of music artists from the web.",
    "actual_venue": "Ismir"
  },
  {
    "abstract": "The proposed encryption technique uses Chinese Remainder Theorem (CRT) and hash map to generate and distribute secret co-prime keys to participants. It utilizes the fact that CRT gives a unique solution for the set of congruent equations if and only if the modulus values are relatively co-prime to each other i.e., Greatest Common Divisor (GCD) of moduli is equal to 1. In this paper, we have proposed secret image sharing scheme using CRT and obtained results on grayscale images of different dimensions. The proposed technique not only increases randomness in encrypted image but also compresses the image, resulting in easy storage and fast transmission. As compression ratio is dependent on shared keys, all shared keys are essential for recovering image without any noise, absence of any key gives an image which is deviated from our original image. For r participants, r pixels are encrypted and compressed simultaneously at a time using CRT which gives one encrypted unique value corresponding to those r pixels. As this encrypted value can be greater than 255, hash map is used to store this value. The experimental results show that the encrypted image is compressed, is not disclosing any secret information and recovery of original image is loss-less.",
    "actual_venue": "Multimedia Tools And Applications"
  },
  {
    "abstract": "Decision support systems gain better performance and higher accuracy by the virtue of building multiresolutional (multigranular, multiscale) representation, and employing multiscale behavior generation subsystem (planning and control). The latter are equipped by devices for unsupervised learning that adjust their functioning to the results of self-identification. We demonstrate that planning and learning are joint processes. The author's intention is to emphasize that the concepts of multiresolutional representation (MR) and multiresolutional decision support (MR-DSS) probably have in common a general significance that crosses the boundaries of particular domains of applications and disciplines. The paper explores this phenomenon. The ubiquity of a principle that somehow persistently delivers benefits to many areas of knowledge and technology seems to be more important than a habit to follow the pigeonhole principle of paper presentation.",
    "actual_venue": "Systems, Man, And Cybernetics, : Applications And Reviews, Ieee Transactions"
  },
  {
    "abstract": "This paper proposes a new approach, interval Simultaneous Localization and Mapping (i-SLAM), which addresses the robotic mapping problem in the context of interval methods, where the robot sensor noise is assumed bounded. With no prior knowledge about the noise distribution or its probability density function, we derive and present necessary conditions to guarantee the map convergence even in the presence of nonlinear observation and motion models. These conditions may require the presence of some anchoring landmarks with known locations. The performance of i-SLAM is compared with the probabilistic counterparts in terms of accuracy and efficiency.",
    "actual_venue": "Robotics And Autonomous Systems"
  },
  {
    "abstract": "With the use of computers, paper questionnaires are being replaced by electronic questionnaires. The formats of traditional paper questionnaires have been found to affect a subject's rating. Consequently, the transition from paper to electronic format can subtly change results. The research presented begins to determine how electronic questionnaire formats change subjective ratings. For formats where subjects used a flow chart to arrive at their rating, starting at the worst and middle ratings of the flow charts were the most accurate but subjects took slightly more time to arrive at their answers. Except for the electronic paper format, starting at the worst rating was the most preferred. The paper and electronic paper versions had the worst accuracy. Therefore, for flowchart type of questionnaires, flowcharts should start at the worst rating and work their way up to better ratings.",
    "actual_venue": "HCI (1)"
  },
  {
    "abstract": "Information stored in XML documents should be protected from unauthorized access. In military or other highly secure environments, mandatory access control (MAC) policy should be enforced on the sensitive information. If we use XML documents to store or exchange information in these environments, we should also enforce MAC policy on these XML documents. In this paper, we discussed a method to enforce fine-grained MAC policy on XML documents. The model of XML document is extended to contain the security information – label. Three kinds of labels are defined to determine the labels of the nodes in XML documents. Security view of XML document under MAC policy is proposed in this paper. The operations on XML documents will be redirected to the security views which contain the proper nodes under MAC policy. Validity of the security views is also described. Four kinds of operations on XML documents are discussed in details to explain how to enforce mandatory access control. The problem of polyinstantiation caused by these operations is also discussed. At last the architecture of enforcing MAC policy on XML documents is presented.",
    "actual_venue": "Icics"
  },
  {
    "abstract": "The identification of useful structures in home video is difficult because this class of video is distinguished from other video sources by its unrestricted, non edited content and the absence of regulated storyline. In addition, home videos contain a lot of motion and erratic camera movements, with shots of the same character being captured from various angles and viewpoints. In this paper, we present a solution to the challenging problem of clustering shots and faces in home videos, based on the use of SIFT features. SIFT features have been known to be robust for object recognition; however, in dealing with the complexities of home video setting, the matching process needs to be augmented and adapted. This paper describes various techniques that can improve the number of matches returned as well as the correctness of matches. For example, existing methods for verification of matches are inadequate for cases when a small number of matches are returned, a common situation in home videos. We address this by constructing a robust classifier that works on matching sets instead of individual matches, allowing the exploitation of the geometric constraints between matches. Finally, we propose techniques for robustly extracting target clusters from individual feature matches.",
    "actual_venue": "MMM"
  },
  {
    "abstract": "In this study, we apply MOSAIC (model of syntax acquisition in children) to the simulation of the developmental patterning of children's optional infinitive (01) errors in 4 languages: English, Dutch, German, and Spanish. MOSAIC, which has already simulated this phenomenon in Dutch and English, now implements a learning mechanism that better reflects the theoretical assumptions underlying it, as well as a chunking mechanism that results in frequent phrases being treated as I unit. Using 1, identical model that learns from child-directed speech, we obtain a close quantitative fit to the data from all 4 languages despite there being considerable cross-linguistic and developmental variation in the 01 phenomenon. MOSAIC successfully simulates the difference between Spanish (a pro-drop language in which 01 errors are virtually absent) and obligatory subject languages that do display the 01 phenomenon. It also highlights differences in the 01 phenomenon across German and Dutch, 2 closely related languages whose grammar is virtually identical with respect to the relation between finiteness and verb placement. Taken together, these results suggest that (a) cross-linguistic differences in the rates at which children produce OIs are graded, quantitative differences that closely reflect the statistical properties of the input they are exposed to and (b) theories of syntax acquisition need to consider more closely the role of input characteristics as determinants of quantitative differences in the cross-linguistic patterning of phenomena in language acquisition.",
    "actual_venue": "Cognitive Science"
  },
  {
    "abstract": "This study explores how online comments influence public reactions to organizational crisis discourses based on five scenarios depicting typical online comment patterns. Through an empirical study involving 621 participants, several key findings are obtained. First, online comments have a significant effect on post-crisis reactions involving emotions, attitudes and behavioral intentions. However, except in the case of perceived comment manipulation, public reaction patterns associated with overwhelmingly positive and non-overwhelmingly positive comments are not significantly different and are friendlier to the crisis organization than the other three patterns. Moreover, reactions associated with the pattern in which there are no online comments are quite similar to those associated with the pattern of non-overwhelmingly negative comments. Second, positive emotions enhance perceived organizational integrity, whereas negative emotions reduce this perception. Third, perceived comment manipulation has a negative moderating effect on the relationship between positive emotion and perceived organizational integrity and a positive moderating effect on the relationship between negative emotion and perceived organizational integrity. Finally, emotions and perceived organizational integrity are found to be predictors of loyalty and boycott intentions. The findings of the study extend the present knowledge on crisis communication by highlighting the impact of online comments on the post-crisis reactions of the public and also provide crisis managers with guidelines on how to deploy appropriate crisis communication strategies. (C) 2013 Elsevier Ltd. All rights reserved.",
    "actual_venue": "International Journal Of Information Management"
  },
  {
    "abstract": "This paper develops energy-driven completion ratio guaranteed scheduling techniques for the implementation of embedded software on multiprocessor systems with multiple supply voltages. We leverage application's performance requirements, uncertainties in execution time, and tolerance for reasonable execution failures to scale each processor's supply voltage at run-time to reduce the multiprocessor system's total energy consumption. Specifically, we study how to trade the difference between the system's highest achievable completion ratio Qmax and the required completion ratio Q0 for energy saving. First, we propose a best-effort energy minimization algorithm (BEEM1) that achieves Qmax with the provably minimum energy consumption. We then relax its unrealistic assumption on the application's real execution time and develop algorithm BEEM2 that only requires the application's best- and worst-case execution times. Finally, we propose a hybrid offline on-line completion ratio guaranteed energy minimization algorithm (QGEM) that provides the required Q0 with further energy reduction based on the probabilistic distribution of the application's execution time. We implement the proposed algorithms and verify their energy efficiency on real-life DSP applications and the TGFF random benchmark suite. BEEM1, BEEM2, and QGEM all provide the required completion ratio with average energy reduction of 28.7, 26.4, and 35.8&percnt;, respectively.",
    "actual_venue": "Acm Trans Embedded Comput Syst"
  },
  {
    "abstract": "The use of Triple Modular Redundancy (TMR) with majority voters can guarantee 100% single fault masking coverage for a given circuit against transient faults. However, this methodology presents a minimum area overhead of 200% compared to the original circuit. In order to reduce considerably the area overhead without compromising significantly the fault coverage, TMR can use approximated logic circuits to generate redundant modules that are optimized for area, compared to the original module. In this work, we propose the use of only approximate logic modules to compose the TMR in order to reduce the area overhead close to minimal values. We use a Boolean factorization based method to compute approximate functions and to select the best composition of approximate logic. The circuits are mapped using the ABC logic synthesis tool and an academic cell library. All the tests are performed using a fault injection tool designed specifically to cope with logic gate and transistor description level. For a combinational circuit (5 inputs, 10 literals) the results have shown that it is possible to maintain the maximum protected p-n junction ratio of 98.88% with only 165% area overhead when using ATMR; and a maximum of 94.66% protected p-n junction ratio with only an 88% area when using full-ATMR. Results for a 4-bit ripple-carry adder showed a protected p-n juncion ratio of almost 97% with 168% area overhead and 93.5% ivith only 136% area overhead. (C) 2015 Elsevier Ltd. All rights reserved.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "The approach based on balanced realization theory, previously used to analyze the convergence speed of adaptive IIR filters in the identification configuration and to propose a faster algorithm (successive approximations algorithm), is now used in the inverse identification configuration. This case is of interest for applications such as channel equalization and system identification itself. We show that, while in an identification configuration the Hankel singular values of the system being identified have an important effect on convergence speed, in the inverse identification case it is the Hankel singular values of a certain system related to the system being identified that have this role. From this result, a condition for faster convergence speed is obtained as well as an inverse identification version of the successive approximations algorithm. As in the identification case, it can lead to much faster convergence with a relatively small increase in computational complexity.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Icassp-, International Conference"
  },
  {
    "abstract": "In this paper, a semi-classical one-dimensional (1D) electron fluid model is built that is based on a classical two-dimensional electron fluid theory taking into account electron–electron repulsive forces, which are significant in 1D system. We have used 1D fluid model to characterize the carbon nanotube (CNT) as interconnects, built a transmission line model and studied S-parameters and group delays. We have also compared S-parameters and group delays of CNT interconnects with the corresponding parameters of Cu interconnects. The results show that the CNT interconnects exhibit superior performance over the Cu interconnects. The results also suggest using CNT as interconnects for radio frequency (RF)-microwave applications. Copyright © 2009 John Wiley & Sons, Ltd.",
    "actual_venue": "J Circuit Theory And Applications"
  },
  {
    "abstract": "We present a set of methods to enable a cross-domain reuse of problem so- lutions via analysis patterns. First, problem-context descriptions and problem- context models as well as solution models are used to express the domain- specific problems and their assigned solutions. After that, the two-step abstrac- tion method is used to create cross-domain analysis patterns for the problem- context models as well as for the solution models. The problem-context pat- terns are used to search across domains for a solution pattern. If a solution pattern is available, it can be instantiated in the solution-seeking domain.",
    "actual_venue": "Europlop"
  },
  {
    "abstract": "High κ HfOxNy film was deposited on amorphous InGaZnO (a-IGZO) by radio-frequency reactive sputtering using an HfO2 target in nitrogen plus argon ambience, the electrical characteristics and reliability of a-IGZO metal–insulator–semiconductor (MIS) capacitors were investigated. Experimental results indicate that the nitrogen incorporation into HfO2 can produce a strong nitride interfacial barrier layer, thus lead to reducing the interface state density, suppressing the hysteresis voltage, and decreasing the gate-leakage current. Improved performance has been achieved for HfOxNy gate dielectric a-IGZO MIS capacitors, with a interface state density of 5.1×1011eV−1cm−2, a gate-leakage current density of 3.9×10−5A/cm2 at Vfb+1V, an equivalent permittivity of 24, and a hysteresis voltage of 105mV. Moreover, the enhanced reliability of Al/HfOxNy/a-IGZO MIS capacitor is observed with a small degradation of electrical characteristics after a high field stressing at 10MV/cm for 3600s.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Some of the main challenges related to wireless sensor networks implementation are low-quality communication, energy conservation, resource-constrained computation, distributed network management, data processing and the scalability of the protocols. This combination makes the implementation of software a demanding task and encourages to new approaches when thinking of software architecture.In this paper an architecture combining a low protocol stack with a cross-layer management entity is presented. One of the main ideas behind the architecture presented is to make application programming easier and to simplify the protocol stack in such a way that it would suit better for the limited resources available. The role of the cross-layer management entity is to offer a shared data structure and to take care of some sensor network specific functions, like topology management and power saving. It also provides certain services that applications and the layers in the protocol stack can use.This architecture has been created specially for needs of wireless sensor networks implementation and the special attention has been paid to modularity and testability of implementation. An implementation based on this cross-layer architecture, CiNet, is also presented in this paper. The functionality of the architecture and the CiNet network was verified by using two different protocol stacks. Wireless communication of the network is based on the 802.15.4 technology.",
    "actual_venue": "Intersense"
  },
  {
    "abstract": "The paper gives a survey of spectrum shaping codes used for digital recording systems. This class of codes belongs to the broader class of modulation codes, which are widely used in recording systems for adjusting the source characteristics to the characteristics of the recording channel. The Shannon noiseless capacities of recording channels are considered, as well as the spectra of maxentropic sequences of M-ary recording constraints. In addition, some practical encoding and decoding schemes are discussed.",
    "actual_venue": "Icassp ) Ieee International Conference"
  },
  {
    "abstract": "Latent Semantic Indexing (LSI) has been validated to be effective on many small scale text collections. However, little evidence has shown its effectiveness on unsampled large scale text corpus due to its high computational complexity. In this paper, we propose a straightforward feature selection strategy, which is named as Feature Selection for Latent Semantic Indexing (FSLSI), as a preprocessing step such that LSI can be efficiently approximated on large scale text corpus. We formulate LSI as a continuous optimization problem and propose to optimize its objective function in terms of discrete optimization, which leads to the FSLSI algorithm. We show that the closed form solution of this optimization is as simple as scoring each feature by Frobenius norm and filter out the ones with small scores. Theoretical analysis guarantees the loss of the features filtered out by FSLSI algorithm is minimized for approximating LSI. Thus we offer a general way for studying and applying LSI on large scale corpus. The large scale study on more than 1 million TREC documents shows the effectiveness of FSLSI in Information Retrieval (IR) tasks.",
    "actual_venue": "SDM"
  },
  {
    "abstract": "Virtual team members increasingly rely on virtual workspace tools to coordinate knowledge that each individual brings to the team. How the use of these tools affects knowledge coordination within virtual teams is not well understood. We distinguish between tools as features and the use of the virtual workspace as providing affordances for behaviors. Using situational awareness theory, we hypothesized two affordances of virtual workspaces that facilitate knowledge coordination. Using trading zone theory, we hypothesized two forms of trading zones created by features of virtual workspaces and the impact of these trading zones on the creation of affordances for team members. Members of 54 teams were asked about the affordances of the virtual workspace, and team leaders were asked about specific tools provided to the team. Our hypothesized model was supported: the different forms of trading zones were differentially related to the different affordances and on affordances were related to knowledge coordination satisfaction. Theoretical implications focus on the distinction between features and affordances and on the identification of specific features that affect specific affordances. Practical implications for managers and engineers supporting virtual teams include the utility of becoming knowledgeable about different forms of trading zones that virtual workspaces can provide and understanding the relationship between trading zones and different affordances.",
    "actual_venue": "Acm Trans Management Inf Syst"
  },
  {
    "abstract": "We have computationally studied para-X-substituted phenols and phenolates (X = NO, NO2, CHO, COMe, COOH, CONH2, Cl, F, H, Me, OMe, and OH) and their hydrogen-bonded complexes with B and HB (B = F and CN), respectively, at B3LYP/6-311++G** and BLYP-D/QZ4P levels of theory. Our purpose is to explore the structures and stabilities of these complexes. Moreover, to understand the emerging trends, we have analyzed the bonding mechanisms using the natural bond orbital scheme as well as KohnSham molecular orbital (MO) theory in combination with quantitative energy decomposition analyses [energy decomposition analysis (EDA), extended transition state-natural orbitals for chemical valence (ETS-NOCV)]. These quantitative analyses allow for the construction of a simple physical model that explains all computational observations. (c) 2012 Wiley Periodicals, Inc.",
    "actual_venue": "Journal Of Computational Chemistry"
  },
  {
    "abstract": "Here we describe a baseline-based binary shape coding method in which an arbitrarily shaped object is represented by the traced 1-D data from the baseline and turning point (TP). The shape coding is performed based on contour-based method in each separated shape. There are two coding modes in shape coding, i.e. the intra and inter mode as in texture coding. In inter coding mode, object identification, global shape matching, and local contour matching are used. In intra mode and residue coding, the DPCM values of TP and the 1-D distance values of the shape are encoded by a fixed arithmetic encoder. Simulation results show that the proposed method has a better coding efficiency and subjective quality compared with the block-based method, i.e. context arithmetic encoding (CAE).",
    "actual_venue": "Icip"
  },
  {
    "abstract": "When short data records are available, spectral analysis is basically an undetermined linear inverse problem. One usually considers the theoretical setting of regularization to solve such ill-posed problems. In this paper, we first show that “nonparametric” and “high resolution” are not incompatible in the field of spectral analysis. To this end, we introduce non-quadratic convex penalization functions, like in low level image processing. The spectral amplitudes estimate is then defined as the unique minimizer of a compound convex criterion. An original scheme of regularization to simultaneously retrieve narrow-band and wide-band spectral features is finally proposed",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Proceedings, Ieee International Conference"
  },
  {
    "abstract": "An interactions-oriented model is proposed for integrating assessment of individual cognitive and affective activity with distributed cognition in an attempt to identify typologies of interactions in web-based communities along three dimensions: domain content, technological competence and social interactions. Metacognition is discussed for each dimension within a pedagogical framework involving three levels of learning: acquisition, participation and mediational approaches, each level seen as satisfying the underlying psychological needs, specifically the need for competence, relatedness, affiliation or self-actualisation, respectively. Social interactions within this pedagogical framework are organised into task- and person-oriented. Task-oriented interactions include those related to gaining competence at the cognitive and metacognitive levels. Person-oriented interactions refer to typologies of social interactions that lead to impression formation, mentalising, imitation and social metacognitive activity. The use of interaction protocols is proposed as a methodology for capturing patterns of interactions that will serve as community learning profiles. Comparing such profiles will identify \"differences in patterns of interactions\". The concluding discussion outlines the pedagogical implications in relation to designing and managing web-based communities.",
    "actual_venue": "Ijwbc"
  },
  {
    "abstract": "•An integrated deep learning-based framework (DNN-I-GA) is developed for predicting A&ED patient flow under different triage levels.•A GA-based feature selection algorithm is improved by introducing the fitness-based crossover.•Manifold regularization strategies are merged into DNN, and all hyper-parameters are optimized efficiently.•Predictive values are significant indicators of patients' demand and can be used by A&ED managers to make resource planning and allocation.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "The analytic form of speech signals has been generated by combining the real-valued speech with its imaginary-valued discrete-Hilbert-transformed counter-part. Instead of using an 8th order real-valued linear prediction to analyze its spectral components, a 4th order complex-valued linear prediction is used. Furthermore, a fast and closed-form rooting procedure is adopted to extract the roots (i.e., bandwidth and frequency information of the speech formants) of the obtained 4th order polynomial of the corresponding inverse filter. The numerical complexity and convergence problems are thus avoided and the computational load is greatly reduced. Promising results in both spectral and pitch estimation are shown. Its advantages in efficient speech coding are discussed.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Ieee International Conference Icassp"
  },
  {
    "abstract": "This paper presents the textopia project, a locative media design experiment set up to explore the relationship between places and literary texts. The system allows a user to walk through a city and listen to texts that talk about the places the user is passing by, thus making possible a new way of experiencing place-bound literature in relation to place. The system also allows any user to write and share their own texts connected with places, exploring what it is to write locative literature. The sharing of old texts and usergenerated texts through the system is analysed and used to outline some categories of locative literature, unified under the concept of 'poetically augmented reality'.",
    "actual_venue": "International Journal Of Arts And Technology"
  },
  {
    "abstract": "Transmitting real-time multimedia streams over heterogeneous mobile networks is a challenging task. Variation in network and system conditions can dramatically affect application performance. When providing end-to-end quality-of-service (QoS) multiple system facets should be coordinated: orchestration of local and peer resources, reservation of network resources, adaptation of multimedia streams, etc. This paper presents an end-to-end negotiation protocol (E2ENP) for negotiating and coordinating QoS on an end-to-end basis both at application and network layer. Based on a flexible extensible markup language (XML) model and extending SDPng concepts, the protocol enables the negotiation of system capabilities and allows provider-services to effectively influence the negotiation process. The aim of the E2ENP design is to optimize the efficiency of multimedia call setup and reduce the time for QoS renegotiations, whenever vertical handovers or spontaneous network reconfigurations occur. The basic protocol is presented, together with implementation and measurement results, stemming from several studies on current and future third-generation/fourth-generation scenarios.",
    "actual_venue": "Ieee Journal On Selected Areas In Communications"
  },
  {
    "abstract": "This paper introduces an algebraic framework for a topological analysis of time-varying 2D digital binary–valued images, each of them defined as 2D arrays of pixels. Our answer is based on an algebraic-topological coding, called AT–model, for a nD (n=2,3) digital binary-valued image I consisting simply in taking I together with an algebraic object depending on it. Considering AT–models for all the 2D digital images in a time sequence, it is possible to get an AT–model for the 3D digital image consisting in concatenating the successive 2D digital images in the sequence. If the frames are represented in a quadtree format, a similar positive result can be derived.",
    "actual_venue": "Casc"
  },
  {
    "abstract": "Multi-antenna transmission with multi-antenna reception brings significant improvement in spectral efficiency for wireless communication systems. The degree of improvement is directly proportional to the correlation properties of the related MIMO (multiple-input, multiple-output) radio channel. Recently, it has been noticed that signal propagation paths appear as clusters both in the delay and spatial (azimuth) domains. Depending on the number of clusters and their directions and the corresponding angular spreading the MIMO channel characteristics, especially the correlation properties, can vary significantly. Therefore, the achievable spectrum efficiency of a MIMO channel can differ considerably from the expected maximum values. In the current study a geometry based stochastic radio channel model employing signal clustering was used as a starting point for evaluation of the spectral efficiency of MIMO channels in realistic radio environments. In order to enable double-directional radio channel modeling, uniform linear arrays were employed both at the mobile terminal and at the base station. Simulation results indicate that the degree of propagation path clustering has a significant impact on the spectral efficiency of MIMO channels",
    "actual_venue": "Vehicular Technology Conference, Vtc -Spring Ieee"
  },
  {
    "abstract": "In the field of IC technologies, there is a constant demand for energy efficiency solutions. Near-Threshold Computing (NTC) is a technique that reduces energy consumption of IC devices, but it also makes them slower. We are applying NTC to a device constructed in 130 nm CMOS technology in the purpose of designing reasonably priced low-power IC devices suitable for low performance applications. In this paper, we concentrate on how a conventional 6T SRAM cell behaves in NTC use. Memory consumes a considerable portion of area and energy of a common IC system, and therefore it is a good target for optimizing with low-energy solutions. Applying NTC to 6T SRAM is not as straightforward as merely using lower supply voltage and slower clock speed; transistor sizes inside the memory cell have to be carefully considered to make the memory reliable. Two inner NMOS transistors in 6T SRAM cell play an important role; by doubling their widths from the minimum, the reliability and the static energy consumption are improved considerably. Overall, NTC makes it possible to achieve notable savings in the energy consumption of 6T SRAM cell.",
    "actual_venue": "Circuits And Systems"
  },
  {
    "abstract": "We present a new, accurate and efficient tool for mapping short reads obtained from the Illumina Genome Analyzer following sodium bisulfite conversion. Our tool, BRAT, supports single and paired-end reads and handles input files containing reads and mates of different lengths. BRAT is faster, maps more unique paired-end reads and has higher accuracy than existing programs. The software package includes tools to end-trim low-quality bases of the reads and to report nucleotide counts for mapped reads on the reference genome.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "Context-free S grammars are introduced, for arbitrary (storage) type S, as a uniform framework for recursion-based grammars, automata, and transducers, viewed as programs. To each occurrence of a nonterminal of a context-free S grammar an object of type S is associated, that can be acted upon by tests and operations, as indicated in the rules of the grammar. Taking particular storage types gives particular formalisms, such as indexed grammars, top-down tree transducers, attribute grammars, etc. Context-free S grammars are equivalent to pushdown S automata. The context-free S languages can be obtained from the deterministic one-way S automaton languages by way of the delta operations on languages, introduced in this paper.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "This article introduces OpenCAL, a new open source computing abstraction layer for multi- and many-core computing based on the Extended Cellular Automata general formalism. OpenCAL greatly simplifies the implementation of structured grid applications, contextually making parallelism transparent to the user. Different OpenMP- and OpenCL-based implementations have been developed, together with a preliminary MPI-based distributed memory version, which is currently under development. The system software architecture is presented and underlying data structures and algorithms described. Numerical correctness and efficiency have been assessed by considering the SciddicaT Computational Fluid Dynamics landslide simulation model as reference example. Eventually, a comprehensive study has been performed to devise the best platform for execution as a function of numerical complexity and computational domain extent. Results obtained have highlighted the OpenCAL’s potential for numerical models development and their execution on the most suitable high-performance parallel computational device.",
    "actual_venue": "Journal Of Parallel And Distributed Computing"
  },
  {
    "abstract": "Let  with 1 <   | … |   be a finite abelian group, d*() =   +…+  −, and let d() denote the maximal length of a zerosum free sequence over . Then d() ≥ d*(), and the standing conjecture is that equality holds for  =  . We show that equality does not hold for   ⊕  , where  ≥ 3 is odd and  ≥ 4. This gives new information on the structure of extremal zero-sum free sequences over  .",
    "actual_venue": "Periodica Mathematica Hungarica"
  },
  {
    "abstract": "•In this paper, we propose a novel data structure for frequent itemset mining.•Based on Nodeset, we present an efficient mining algorithm named FIN.•Experiment results show that FIN outperforms state-of-the-art algorithms.",
    "actual_venue": "Expert Systems With Applications"
  },
  {
    "abstract": "Robot-assisted minimally invasive surgery (MIS) offers improved range of motion over standard laparoscopic techniques, but is characterized by a total loss of haptic feedback, requiring surgeons to rely solely on visual cues. Pneumatic tactile displays have many advantages, including low mass, low cost, compact size, and adaptability. A pneumatic haptic feedback actuator array has been developed that is suitable for mounting unto surgical robotic tools. The balloon actuators consist of spin-coated thin-film silicone membranes and molded substrates with cylindrical channels. Human perceptual tests were conducted on balloon diameters ranging from 0.75 to 2.0 mm to determine the optimal size that can be effectively detected. The control system was programmed to sequentially inflate a single balloon to one of the three levels, 100% (full hemispherical deformation), zero, 50% (half deformation), and 0% (no inflation). Blinded subjects (n=5) were asked to determine which of the two inflation levels was higher. Test results suggest that balloon diameters greater than 1.0 mm can deliver high detection accuracy. This indicates that pneumatic balloon-based actuation is a viable solution for generating haptic feedback. In addition to surgical applications, many other fields such as virtual reality-based simulators and neuroprosthetics can benefit from this technology.",
    "actual_venue": "Studies In Health Technology And Informatics"
  },
  {
    "abstract": "The paper deals with the problem of finding an equilibrium in an oligopolistic marketmodel where several subjects supply a\n single homogeneous product in a non-cooperativemanner. The problem is reduced to a nonlinear equation, some terms of which\n are determinedby solving nonlinear complementarity problems. An algorithm is presented that combinesthe Newton method steps\n with dichotomy techniques. Under certain assumptions, the algorithmis shown to be convergent at a quadratic rate. Finally,\n the algorithm is extended to thecase of nonlinear production costs, and its linear convergence is demonstrated.",
    "actual_venue": "Annals Of Operations Research"
  },
  {
    "abstract": "A method of FPGA implementation of the class of parametric digital conjunctions defined by (p)-monotone sum of basic t-norms is proposed. The paper presents the logical diagrams of parametric digital conjunctions developed by means of VHDL language in Quartus II with ModelSim software of Altera. Parametric digital conjunctions can be used in reconfigurable digital fuzzy systems where the parameter p and a sequence of basic t-norms used in definition of parametric conjunction can be changed.",
    "actual_venue": "Ieee International Conference On Fuzzy Systems"
  },
  {
    "abstract": "A formal model called database manipulating systems was introduced to model data-aware dynamic systems. Its semantics is given by an infinite labelled transition systems where a label can be an unbounded relational database. Reachability problem is undecidable over schemas consisting of either a binary relation or two unary relations. We study the reachability problem under schema restrictions and restrictions on the query language. We provide tight complexity bounds for different combinations of schema and query language, by reductions to/from standard formalism of infinite state systems such as Petri nets and counter systems. Our reductions throw light into the connections between these two seemingly unrelated models.",
    "actual_venue": "International Conference On Application Of Concurrency To System Design"
  },
  {
    "abstract": "In a recent paper, Gale has given an interesting generalization of the KKM lemma in combinatorial topology. We present a similar generalization of Sperner's well-known lemma and give a constructive proof. The argument uses the familiar idea of following simplicial paths in a triangulation. To demonstrate that the algorithm must work, orientation considerations are necessary. Gale's generalized KKM lemma is derived from the main result. A permutation-based generalization of Brouwer's fixed point theorem is also given.",
    "actual_venue": "Math Program"
  },
  {
    "abstract": "Structure is an important characteristic of documents. In the document management community it has been realized that there is a need for querying and retrieval of documents based on the structure of the documents. In this paper we propose a knowledge-based system for representation and retrieval of documents. The model on which the system is based is an extension of the description logic model of information retrieval. We discuss the advantages of our model and we show how our model can cope with many of the desirable queries involving the structure of documents.",
    "actual_venue": "Ismis"
  },
  {
    "abstract": "Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated; the association between the type of mutation strategy and whether the generated mutants were equivalent or not; the number of not killed mutants; mutation testing time; time needed for manual classification. Conclusions . The results in the DEM category are still far from perfect. Thus, the SEM and AEMG categories have been developed. The JudyDiffOp algorithm achieved good results in many areas.",
    "actual_venue": "Ieee Transactions On Software Engineering"
  },
  {
    "abstract": "Business Processes have been introduced in financial applications in order to improve productivity and efficiency, and consists in the management of a set of interrelated tasks involving different partners across different institutions. In this paper, we are concerned with the support of business processes over mobile networks. We focus on the existing limitations of the 2 phase commit protocol, which is at the basis of most popular coordination frameworks for distributed transactions, and propose an extension of it in order to address the constraints imposed by a mobile environment. We describe such extension in detail, and provide a basic algorithm that is needed in order to run it over mobile networks.",
    "actual_venue": "Comsware"
  },
  {
    "abstract": "It has long been recognized that the concept of inconsistency is a central part of com- monsense reasoning. In this issue, a number of authors have explored the idea of reasoning with maximal consistent subsets of an inconsistent stratified knowledge base. This paradigm, often called \"coherent-based reasoning\", has resulted in some interesting proposals for para- consistent reasoning, non-monotonic reasoning, and argumentation systems. Unfortunately, coherent-based reasoning is computationally very expensive. This paper harnesses the ap- proach of approximate entailment by Schaerf and Cadoli (SCH 95) to develop the concept of \"approximate coherent-based reasoning\". To this end, we begin to present a multi-modal propo- sitional logic that incorporates two dual families of modalities:2S and 3S defined for each subset S of the set of atomic propositions. The resource parameterS indicates what atoms are taken into account when evaluating formulas. Next, we define resource-bounded consolidation operations that limit and control the generation of maximal consistent subsets of a stratified knowledge base. Then, we present counterparts to existential, universal, and argumentative inference that are prominent in coherence-based approaches. By virtue of modalities2S and 3S, these inferences are approximated from below and from above, in an incremental fashion. Based on these features, we show that an anytime view of coherent-based reasoning is tenable.",
    "actual_venue": "Journal Of Applied Non-Classical Logics"
  },
  {
    "abstract": "A hybrid automaton is a mathematical model for hybrid systems, which combines, in a single formalism, automaton transitions for capturing discrete updates with differential constraints for capturing continuous flows. Formal verification of hybrid automata relies on symbolic fixpoint computation procedures that manipulate sets of states. These procedures can be implemented using boolean combinations of linear constraints over system variables, equivalently, using polyhedra, for the subclass of linear hybrid automata. In a linear hybrid automaton, the flow at each control mode is given by a rate polytope that constrains the allowed values of the first derivatives. The key property of such a flow is that, given a state-set described by a polyhedron, the set of states that can be reached as time elapses, is also a polyhedron. We call such a flow a polyhedral flow. In this paper, we study if we can generalize the syntax of linear hybrid automata for describing flows without sacrificing the polyhedral property. In particular, we consider flows described by origin-dependent rate polytopes, in which the allowed rates depend, not only on the current control mode, but also on the specific state at which the mode was entered. We identify necessary and sufficient conditions for a class of flows described by origin-dependent rate polytopes to be polyhedral. We also propose and study additional classes of flows: strongly polyhedral flows, in which the set of states that can be reached up to a given time starting from a polyhedron is guaranteed to be a polyhedron, and polyhedrally sliced flows, in which the set of states that can be reached at a given time starting from a polyhedron is guaranteed to be a polyhedron. Finally, we discuss an application of the above classes of flows to approximate exponential behaviours.",
    "actual_venue": "Formal Methods In System Design"
  },
  {
    "abstract": "An oscillator with a high-order (>2) resonator has multiple stable modes of oscillations. The stable modes for one such oscillator, having a fourth-order resonator, are found using a nonlinear analysis. By using a proper nonlinear active topology and tank component values, the fourth-order oscillator can generate either of the two distinct frequencies f1 or f2. A method is introduced to dynamicall...",
    "actual_venue": "Ieee Journal Of Solid-State Circuits"
  },
  {
    "abstract": "Optical networks have been considered for on-chip communications due to its advantages on bandwidth density, power efficiency and propagation speed over the electrical counterpart. However, the major optical device-micro ring resonator is very sensitive to manufacturing errors and temperature fluctuations, which results in the bandwidth loss or even the failure of optical link. Thus, this paper proposes a fault-tolerant and deadlock-free routing algorithm to improve the reliability of on-chip optical network without requiring additional virtual channel. In addition, a path selection mechanism taking account of the actual bandwidth of the optical link affected by fabrication and temperature variations is implemented in the routing unit. The simulation results show that compared to the conventional fault-tolerant routing methods, our routing algorithm can improve the transmission latency and throughput of the network under static and dynamic link faults by up to 51% and 22%, respectively.",
    "actual_venue": "Srds"
  },
  {
    "abstract": "Pervasive computing aims at providing services for human beings that interact with their environment (encompassing objects and people who reside in it). Pervasive computing applications must be able to take into account the context in which users evolve, for example, physical location, social or hierarchical position, current tasks as well as related information. These applications have to deal with the dynamic integration in the environment of new, and sometimes unexpected, elements (users or devices). In turn, the environment has to provide context information to newly designed applications. This requires a framework which is open, dynamic and minimal. We describe an architecture in which context information is distributed in the environment and context managers use semantic Web technologies in order to identify and characterize available resources. The components in the environment maintain their own context expressed in RDF (Resource Description Framework) and described through OWL ontologies. They may communicate this information to other components, obeying a simple protocol for identifying them and determining the information they can provide. We show how this architecture allows introducing new devices and new applications without interrupting what is working. In particular, the openness of ontology description languages makes possible the extension of context descriptions and ontology matching helps dealing with independently developed ontologies.",
    "actual_venue": "Knowledge Eng Review"
  },
  {
    "abstract": "Magnetic based strategies are promising to provide solutions for new medical robotic devices. In this paper, we derive an analytical approach for magnetic flux modeling and calculation of force/torque formulas based on a current model. This work is useful for the magnetic design and interaction analysis of medical devices, e.g. for endoscopic procedures.",
    "actual_venue": "Ieee International Conference On Biomedical Robotics And Biomechatronics"
  },
  {
    "abstract": "Combinatorial exchanges are double sided marketplaces with multiple sellers and multiple buyers trading with the help of combinatorial bids. The allocation and other associated problems in such exchanges are known to be among the hardest to solve among all economic mechanisms. In this paper, we develop computationally efficient iterative auction mechanisms for solving combinatorial exchanges. Our mechanisms satisfy Individual-rationality (IR) and budget-nonnegativity (BN) properties. We also show that our method is bounded and convergent. Our numerical experiments show that our algorithm produces good quality solutions and is computationally efficient.",
    "actual_venue": "Automation Science And Engineering"
  },
  {
    "abstract": "In multi-label learning, the use of labels correlation is crucial for the improvement of multi-label learning performance. Most of the existing methods for studying labels correlation usually do not consider the study of feature-space information. Further study is deserved about how to synchronize rich information contained in features-space and labels-space. In this paper, a multi-label learning algorithm of Non-Equilibrium Labels Completion with Mean Shift (i.e. NeLC-MS) was proposed. The aim of this research was to mine the feature hidden information by reconstructing the features space, and introduce non-equilibrium label correlation information so as to better improve the robustness of multi-label learning classification. First, the mean shift clustering method was used to reconstruct the information between features in the feature space to obtain the hidden information between features. Then, the new information entropy was used to measure the correlation between labels which gets the basic labels confidence matrix. Then the basic labels confidence matrix was improved to construct a Non-equilibrium labels completion matrix by the non-equilibrium parameters. Finally, the new training set was constructed by using the reconstructed features space and the Non-equilibrium Labels Completion matrix, and the existing linear classifier was used for predicting the new training set. The experimental results of the proposed algorithm in the opening benchmark multi-label datasets showed that the NeLC-MS algorithm would have some advantages over other comparative multi-label learning algorithms, and the effectiveness of the proposed method was further illustrated by the use of statistical hypothesis test and stability analysis.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "The applications of instance-intensive workflow are widely used in e-commerce, advanced manufacturing, etc. However, existing studies normally do not consider the problem of reducing energy consumption by utilizing the characters of instance-intensive workflow applications. This paper presents a resource usage Prediction-based Energy-Aware scheduling algorithm, named PEA. Technically, this method improves the energy efficiency of instance-intensive cloud workflow by predicting resources utilization and the strategies of batch processing and load balancing. The efficiency and effectiveness of the proposed algorithm are validated by extensive experiments.",
    "actual_venue": "Collaboratecom"
  },
  {
    "abstract": "Physical activity is often considered an important factor in handling mood regulation and depression. This paper presents a computational model of this role of physical activity in mood regulation. It is shown on the one hand how a developing depression can go hand in hand with a low level of physical activity, and on the other hand, how Exercise Therapy is able to reverse this pattern and make the depression disappear. Simulation results are presented, and properties are formally verified against these simulation runs.",
    "actual_venue": "Iconip"
  },
  {
    "abstract": "The paper reports the performance of the F32T8 lamps (full wattage and energy saver) when operated with dimming ballasts. The performance of the full wattage and energy saver lamps is very similar when operated with the same ballast. Some ballasts provide proper operating characteristics while others do not. As the lamps are dimmed, they become bluer. The dimming level has no impact on the filament Rh/Rc values on the ballasts tested. Several concerns in using dimming systems are described. Lastly, the energy consumption of using full wattage vs. energy saver lamps and fixed Light Output vs. dimming ballasts is discussed.",
    "actual_venue": "Ieee Industry Applications Society Annual Meeting"
  },
  {
    "abstract": "Consistency techniques are an efficient way of tackling constraint satisfaction problems (CSP). In particular, various arc-consistency algorithms have been designed such as the time optimal AC-4 sequential algorithm of Mohr and Henderson (1986). In this paper, we present a new distributed arc-consistency algorithm, called DisAC-4. DisAC-4 is based on AC-4, and is a coarse-grained parallel algorithm designed for distributed memory computers using message passing communication. Termination and correctness of the algorithm are proven. Theoretical complexities and experimental results are given. Both show linear speedup with respect to the number of processors. The strong point of DisAC-4 is its suitability to be implemented on very common hardware infrastructures like networks of workstations and/or PCs as well as on intensive computing parallel mainframes.",
    "actual_venue": "Science Of Computer Programming - Special On Concurrent Constraint Programming"
  },
  {
    "abstract": "A job proxy is an abstraction for provisioning CPU resources. This paper proposes an adaptive algorithm for allocating job proxies to distributed host clusters with the objective of improving large-scale job ensemble throughput. Specifically, the paper proposes a decision metric for selecting appropriate pending job proxies for migration between host clusters, and a self-synchronizing Paxos-style distributed consensus algorithm for performing the migration of these selected job proxies. The algorithm is further described in the context of a concrete application, the MyCluster system, which implements a framework for submitting, managing and adapting job proxies across distributed high performance computing (HPC) host clusters. To date, the system has been used to provision many hundreds of thousands of CPUs for computational experiments requiring high throughput on HPC infrastructures like the NSF TeraGrid. Experimental evaluation of the proposed algorithm shows significant improvement in user job throughput: an average of 8% in simulation, and 15% in a real-world experiment.",
    "actual_venue": "Tsukuba"
  },
  {
    "abstract": "This paper presents a framework for managing dynamic Service Level Specification (SLS) in Differentiated Services (DiffServ) networks. Although several works propose high-level SLS description which include time period conditions or service schedule, very few of them discuss about the mechanisms for configuring devices dynamically. Our framework is based on the IETF standards concerning DiffServ policy-based management. The PCIMe and QPIM models are extended for specifying the SLS. And the device configuration is represented in terms of a DiffServ PIB which is distributed among the managed devices using the COPS-PR protocol. The presented framework explores the COPS-PR facilities to define three possible strategies for updating the PIB information in the managed devices. This work describes, compares and discusses the performance of those strategies. In our evaluation scenario, the configuration of edge routers must be updated according to the PolicyTimePeriodConditions defined in the high-level policies. However, the discussed strategies are generic and support dynamic configuration triggered by any event.",
    "actual_venue": "LCN"
  },
  {
    "abstract": "The ever growing demand for functionally robust and error-free industrial electronics necessitates the development of techniques that will prohibit the propagation of functional errors to the final tape-out stage. This paramount requirement in the semiconductor world is imposed by the equivocal observation that functional errors slipping to silicon production introduce immense amounts of cost and jeopardize chip release dates. Functional verification and debugging are burdened with the tedious task of guaranteeing logic functionality early in the design cycle. In this paper, we present an automated method for the very first stage of functional debugging, called failure triage. Failure triage is the task of analyzing large sets of failures, grouping together those that are likely to be caused by the same design error, and then allocating those groups to the appropriate engineers for fixing. The introduced framework instruments techniques from the machine learning domain combined with the root cause analysis power of modern SAT-based debugging tools, in order to exploit information from error traces and bin the corresponding failures using clustering algorithms. Preliminary experimental results indicate an average accuracy of 93 % for the proposed failure triage engine, which corresponds to a 43 % improvement over conventional automated methods.",
    "actual_venue": "On-Line Testing Symposium"
  },
  {
    "abstract": "The impact of a single genetic locus on multiple phenotypes, or pleiotropy, is an important area of research. Biological systems are dynamic complex networks, and these networks exist within and between cells. In humans, the consideration of multiple phenotypes such as physiological traits, clinical outcomes and drug response, in the context of genetic variation, can provide ways of developing a more complete understanding of the complex relationships between genetic architecture and how biological systems function in health and disease. In this article, we describe recent studies exploring the relationships between genetic loci and more than one phenotype. We also cover methodological developments incorporating pleiotropy applied to model organisms as well as humans, and discuss how stepping beyond the analysis of a single phenotype leads to a deeper understanding of complex genetic architecture.",
    "actual_venue": "Briefings In Bioinformatics"
  },
  {
    "abstract": "In this paper, we consider the pinning exponential synchronization of complex networks via event-triggered communication. By using the combinational measurements, two classes of event triggers are designed, one depends on continuous communications between the agents, the other avoids the continuous communications. The controller updates when triggering function reaches certain threshold. For such classes of two event triggers, the exponential synchronization as well as the convergence rate of the controlled complex networks are studied, respectively, by employing the M-matrix theory, algebraic graph theory and the Lyapunov method. Two simulation examples are provided to illustrate the effectiveness of the proposed two classes of event triggering strategies. It is noteworthy that the event trigger with combinational measurements avoids decoupling the actual state of the nodes, which is more effective than the error-based event trigger.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "In this paper, a simple energy-efficient physical layer modulation scheme termed as random number modulation (RNM) is proposed. It is a class of new systems that harness the randomness of pseudo random number generators (RNGs) for efficient communication, and adds a new degree of freedom to digital communication systems. It is also highly adaptable to high-rate, low-latency, and low-rate, high-ene...",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "IT managers seem to be hesitant to adopt OSS in the absence of professional support. Previous qualitative studies have indeed suggested that the availability of external support is important for the adoption of OSS. Therefore, we feel it is interesting to gain more insight into the role of external support in the adoption process. To this end, we performed a web survey involving 95 Belgian organizations. Our data suggests a balanced picture. As expected, our results show that the majority of organizations in our sample rely on commercial support such as vendor or third party support. Even organizations that have deployed OSS to a large extent-and that are therefore likely to have some experience and familiarity with OSS-rely on commercial support. Nevertheless, a considerable proportion of organizations indicated not to rely on commercial support, which suggests that internal expertise can be sufficient for successful adoption. Finally, and most surprisingly, we have found that the OSS community is used by a large proportion of organizations. This indicates that the OSS community is a valuable source of external support for organizations. Nevertheless, it appears that it is primarily used by organizations with a rather strong background in IT.",
    "actual_venue": "International Federation For Information Processing"
  },
  {
    "abstract": "Recently, there has been an increasing concern from the evolutionary computation community on dynamic optimization problems since many real-world optimization problems are dynamic. This paper investigates a particle swarm optimization (PSO) based memetic algorithm that hybridizes PSO with a local search technique for dynamic optimization problems. Within the framework of the proposed algorithm, a local version of PSO with a ring-shape topology structure is used as the global search operator and a fuzzy cognition local search method is proposed as the local search technique. In addition, a self-organized random immigrants scheme is extended into our proposed algorithm in order to further enhance its exploration capacity for new peaks in the search space. Experimental study over the moving peaks benchmark problem shows that the proposed PSO-based memetic algorithm is robust and adaptable in dynamic environments.",
    "actual_venue": "Natural Computing"
  },
  {
    "abstract": "The work of the major EU-funded ICT DEPLOY Integrated Project (February 2008 -- April 2012) on Industrial Deployment of Advanced\\ System Engineering Methods for High Productivity and Dependability [1] was driven by the tasks of achieving and evaluating industrial takeup, initially by DEPLOY industrial partners, of DEPLOY methods and tools, together with the necessary further research on methods and tools. Our previous SEN paper [2] introduced the project. The project has been one of the most significant efforts ever focusing on understanding the issues researchers and engineers face during the deployment of formal methods. This paper briefly reports on the project legacy and provides pointers to the various sources of information produced by the project.",
    "actual_venue": "Acm Sigsoft Software Engineering Notes"
  },
  {
    "abstract": "High-speed mobile robots have many important applications in rough terrain. At high speeds, it is difficult to guarantee safe robot motion using traditional control and planning techniques. This paper presents an experimental high-speed rover system for studying reactive behavior control to avoid potentially dangerous situations. The method consists of sensor-triggered maneuvers that have been shown by a priori model-based analysis to be safe. The paper discusses vehicle and terrain models for model-based analysis. Experimental results show that behaviors based on detailed models can accurately predict the dynamics of mobile robots in rough terrain.",
    "actual_venue": "Springer Tracts In Advanced Robotics"
  },
  {
    "abstract": "Content-based image retrieval (CBIR) methods usually adopt color, texture and structure as image feature vector. Recent research indicates that since these features are not exactly associated with image semantic meaning, Query-By-One-Example (QBOE), which means to query with only one image, usually is insufficient to achieve good performance. Thus, Query-By-Multiple-Examples (QBME) methods are introduced and applied in many content-based image retrieval systems. However, how to maximize major features and minimize minor ones of these inputs while matching could influence retrieval results significantly. Some previous researchers divided input images to groups according to their relevance to desired image class. They kept important features of relevant group and discard obvious features of irrelevant group while matching. These methods did improve the retrieval results but have defects in some situations. This paper presents an improved QBME system which optimizes retrieval result. It adopts clustering method to cope with the defects of previous methods. Uniquely, to decrease the complexity of user input and reduce user-computer interaction, a modified fuzzy clustering algorithm will be introduced. It not only presents good performance but also requires no parameters from users. In this article, both the defects of previous methods and new methods to cope with them will be explained in detail.",
    "actual_venue": "Sitis"
  },
  {
    "abstract": "The main reliability issue of highly scaled floating gate NAND Flash memories is the cross-cell interference phenomenon. This is an active area of research in microelectronics engineering. In the last decade, there has been much progress and there are already proposed models for extraction of parasitic capacitive couplings within floating gate transistors. However, most of simulation-based methodologies for evaluation of the impact of cross-cell interference on the electrical behavior rely on deterministic capacitive coupling, neglecting the variability effects. This approach ignores the variable nature of the capacitive couplings caused by technological limitations such as line edge roughness (LER) in advanced technological nodes. The aim of this work is to present an alternative approach of modeling threshold voltage disturbance propagation in a raw NAND Flash memory array, sourced by variability-affected parasitic capacitive couplings.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "In a constructive way, we have found sufficient conditions under which general fuzzy systems can uniformly approximate any real continuous function on a compact domain to any degree of accuracy. More importantly, we have revealed the underlying mechanism of such function approximation: the fuzzy systems can uniformly approximate any polynomials which, according to the Weierstrass approximation theorem, can uniformly approximate any continuous function on a compact domain. These findings lead to the following practical results: (1) explicit fuzzy rules of fuzzy systems can now be easily obtained according to functions to be approximated; and (2) formulas are derived which can calculate the number of input fuzzy sets, output fuzzy sets and fuzzy rules needed in order to satisfy any given approximation accuracy. The number increases as required approximation error decreases, and as approximation error approaches zero, the number approaches ∞. These formulas also reveal that the number is minimal when the maximum number of intersection between adjacent input fuzzy sets is one. Practical implications of these results will be discussed, especially in the context of fuzzy control and fuzzy modeling. Two examples are provided to demonstrate how to design fuzzy systems to approximate given functions with required approximation accuracy.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "This paper discusses a technique whereby a logic design which is unconstrained may be replaced quasi-optimally with modules from a constrained library. Necessary and sufficient conditions for the replacement of modules within a network are developed and then reformulated as integer linear programs. These integer linear programs are solved during individual steps of a dynamic programming formulation of the network replacement problem. Execution time of the algorithm increases linearly with the network size and the algorithm produces optimal replacements of fan-out-free networks.",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "We investigate systems where it is possible to access several shared registers in one atomic step. We characterize those systems in which the consensus problem can be solved in the presence of faults and give bounds on the space required. We also describe a fast solution to the mutual exclusion problem using atomic m-register operations.",
    "actual_venue": "Distributed Computing"
  },
  {
    "abstract": "Motivated by the fact that in computer vision data samples are matrices, in this paper, we propose a matrix-variate probabilistic model for canonical correlation analysis (CCA). Unlike probabilistic CCA which converts the image samples into the vectors, our method uses the original image matrices for data representation. We show that the maximum likelihood parameter estimation of the model leads to the two-dimensional canonical correlation directions. This model helps for better understanding of two-dimensional Canonical Correlation Analysis (2DCCA), and for further extending the method into more complex probabilistic model. In addition, we show that two-dimensional Linear Discriminant Analysis (2DLDA) can be obtained as a special case of 2DCCA.",
    "actual_venue": "Eurasip J Adv Sig Proc"
  },
  {
    "abstract": "In this work we present some of our investigations concerning paraconsistent, paracomplete and non-alethic systems for knowledge\n representation in AI. Inconsistency and paracompleteness are natural phenomena in describing parts of our reality. Thus, we\n need more generical tools, i.e., formal systems for modeling contradictory and paracomplete knowledge, and in this paper we\n focus non-alethic knowledge in distributed systems.",
    "actual_venue": "Knowledge-Based Intelligent Information And Engineering Systems"
  },
  {
    "abstract": ":   Air traffic control (ATC) is currently undergoing rapid changes as new technology is being introduced. The introduction of\n new technology produces adaptations of how the system performs its computation with consequences and changes in the activity\n for both the individual and the system as a whole. Even if ATC is a highly structured and well-documented domain, the analysis\n of changes and evolution in the work environment, in view of the design of new technologies, requires the definition of an\n appropriate-level analysis. This paper discusses two important issues related to the analysis of complex domains like ATC:\n the distinction between the normative task description and the actual human activity; and the choice of the unit of analysis\n for modelling real work settings. The paper presents a case study describing the application of distributed cognition and\n discusses findings and implications of this theoretical approach for the design of new technologies for ATC.",
    "actual_venue": "Cognition, Technology And Work"
  },
  {
    "abstract": "In this note, we prove that a planar graph is 3-choosable if it contains neither cycles of length 4, 7, and 9 nor 6-cycle with one chord. In particular, every planar graph without cycles of length 4, 6, 7, or 9 is 3-choosable. Together with other known parallel results, this completes a theorem on 3-choosability of planar graphs: planar graphs without cycles of length 4, i, j, 9 with i",
    "actual_venue": "Journal Of Anqing Teachers College(Natural Science"
  },
  {
    "abstract": "We examine the sequences A that are low for dimension, i.e. those for which the effective (Hausdorff) dimension relative to A is the same as the unrelativized effective dimension. Lowness for dimension is a weakening of lowness for randomness, a central notion in effective randomness. By considering analogues of characterizations of lowness for randomness, we show that lowness for dimension can be characterized in several ways. It is equivalent to lowishness for randomness, namely, that every Martin-Lof random sequence has effective dimension 1 relative to A, and lowishness for K, namely, that the limit of K-A(n)/K(n) is 1. We show that there is a perfect Pi(0)(1)-class of low for dimension sequences. Since there are only countably many low for random sequences, many more sequences are low for dimension. Finally, we prove that every low for dimension is jump-traceable in order n(epsilon), for any epsilon > 0.",
    "actual_venue": "Journal Of Mathematical Logic"
  },
  {
    "abstract": "A human-powered user interface device sources its power from the physical effort required to operate it. This paper describes a technique by which a geared DC motor and a simple circuit can be used to enable interaction-powered rotary input devices. When turned, the circuit provides a temporary power source for an embedded device, and doubles as a sensor that provides information about the direction and rate of input. As a proof of concept, we have developed a general-purpose wireless input device -- called the Peppermill -- and illustrate its capabilities by using it as a remote control for a multimedia-browsing application.",
    "actual_venue": "Proceedings Of The Tei : Tenth International Conference On Tangible, Embedded, And Embodied Interaction"
  },
  {
    "abstract": "Web phishing is a well-known cyber-attack which is used by attackers to obtain vital information such as username, password, credit card number, social security number, and/or other credentials from Internet users via deception. A number of web phishing detection solutions have been proposed and implemented in the recent years. These solutions include the use of phishing black list, search engine, heuristics and machine learning, visual similarity techniques, DNS, access list and proactive phishing URLs detection based techniques. However, the current solutions are quite heavy in terms of their computational and communication requirements. Most of these solutions are dependent on third parties and require dedicated servers for their operation. It has been observed that search engine based solutions are the most lightweight and viable. This paper advances search engine based antiphishing research and presents the lightest possible phishing detection system, named the Lightweight Phish Detector (LPD). The LPD can run on client browsers for phishing detection. The development of LPD was done using the Google Chrome browser. Exhaustive testing has been performed to evaluate its accuracy and effectiveness. Comparisons are performed with currently available search engine based antiphishing approaches and other approaches that are currently used by popular browsers such as Chrome, Firefox, Internet Explorer, Netcraft toolbar and Cascaded Phish Detector. For testing, phishing sites reported from the Phishtank and normal sites available from Alexa ranking are used in the experiments. A true negative rate varying from 92.4% to 100% was obtained from the Alexa dataset of normal URLs while a true positive rate of 99.5% was recorded from the Phishtank URLs. Results show that the proposed scheme is very accurate. A competitive response time and intelligent action-response mechanism makes LPD a fast and intelligent antiphishing solution.",
    "actual_venue": "Computers And Security"
  },
  {
    "abstract": "Dynamic power consumption is directly related tothe number of the signal transitions in a circuit. Glitches are undesired spurious transitions caused by inputs of a gate arriving at different times, instead of arriving together, thus causing unnecessary power dissipation. Our objective in this paper is to reduce the number of glitches in a circuit to reduce dynamic power. We do so by clock skew scheduling, where different flipflopsreceive clocks at different times. We formulate thescheduling as an Integer linear Programming problem andderive vector-independent clock skew schedule to reduce glitches. We also propose linear objective functions based on timing window of gates for optimization. The proposed method was evaluated on ISCAS-89 benchmark circuits using dynamic simulation. Results show that we achieve an average reduction of ~32% in glitch power.",
    "actual_venue": "Isvlsi"
  },
  {
    "abstract": "Street criminals such as ram raids and armed robberies are hot topics. In this paper, we address a complex and difficult task of planning police patrol routes. It is necessary to consider frequencies of passages for each road and response time for emergency calls. However, existing works have focused on either the frequencies or the response time. In order to satisfy the frequency constraints and minimize the response time, it is necessary to patrol while keeping the geographical balance among patrol units. We proposed a route planning method to minimize the response time estimated by using Network Voronoi Diagram (NVD). The NVD represents the area in which each unit is dispatched. We calculate the response time in accordance with the NVD on the units' allocation. The routes are optimized using Local Search (LS) based on the average response time over each time period. In our experiments, the proposed method is compared with a random patrol method modeled as one of police patrol means in the real world. The experimental results show the response time is improved in most cases if the routes are organized using our method.",
    "actual_venue": "Icuimc"
  },
  {
    "abstract": "This paper describes a new approach to the determination of a mapping function from given coordinates of control points based on least square support vector machines (LS-SVM). An interesting property of this approach is that it constitutes a practical implementation of the structural risk minimization (SRM) principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing the mean square error over control points. Computer simulation results indicate that this new approach can remove geometric deformation and adapt to correct the errors induced by inaccuracy location of control points.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "Let there be a set J of n jobs and a set M of m parallel machines, where each job Jj takes pi,j∈Z+ time units on machine Mi and assume pi,j=∞ implies job Jj cannot be scheduled on machine Mi. In makespan minimization on unrelated parallel machines (R||Cmax), the goal is to schedule each job non-preemptively on a machine so as to minimize the makespan. A job-intersection graph GJ=(J,EJ) is an unweighted undirected graph where there is an edge {Jj,Jj′}∈EJ if there is a machine Mi such that both pi,j≠∞ and pi,j′≠∞. In this paper we consider two variants of R||Cmax where there are a small number of eligible jobs per machine. First, we prove that there are no approximation algorithms with approximation ratios less than 3/2 for R||Cmax when restricted to instances with job-intersection graphs belonging to some graph classes such as diamondless graphs and planar graphs, unless P=NP. Second, we match this lower bound by presenting a 3/2-approximation algorithm for R||Cmax restricted to instances with diamondless job-intersection graphs, and furthermore show that when GJ is triangle free R||Cmax is solvable in polynomial time. For R||Cmax restricted to instances when every machine can process at most ℓ jobs, we give an approximation algorithm with approximation ratios 3/2 and 5/3 for ℓ=3 and ℓ=4 respectively, a polynomial-time algorithm when ℓ=2, and prove that it is NP-hard to approximate the optimum solution within a factor less than 3/2 when ℓ≥3. In the special case where every pi,j∈{pj,∞}, called the restricted assignment problem, and there are only two job lengths pj∈{α,β} we present a (2−1/(ℓ−1))-approximation algorithm when ℓ≥3. In addition, we give a 2-approximation algorithm for the so-called unrelated graph balancing problem for ℓ=3 in the case where J is partitioned into b sets called bags, and no two jobs from the same bag can be scheduled on the same machine.",
    "actual_venue": "Theoretical Computer Science"
  },
  {
    "abstract": "The paper presents experiments using a radial basis function (RBF) network to tackle the unconstrained face recognition problem using low resolution video information. Input representations that mimic the effects of receptive field functions found at various stages of the human vision system were used with RBF network; that learnt to classify and generalise over different views of each person to be recognised. In particular, Difference of Gaussian (DoG) filtering and Gabor wavelet analysis are compared for face recognition from an image sequence. RBF techniques are shown to provide excellent levels of performance where the view varies and the authors discuss how to relax constraints on data capture and improve preprocessing to obtain an effective scheme for real-time, unconstrained face recognition.",
    "actual_venue": "FG"
  },
  {
    "abstract": "Seasonal fruits, like mango (Mangifera Indica L.), are harvested from gardens or farms in batches; the mangoes present in each batch are not uniformly matured, therefore, sorting of mangoes into different groups is necessary for transporting them into different locations. With this background, this paper proposes a machine vision-based system for classification of mangoes by predicting maturity level, and aimed to replace manual sorting system. The prediction of maturity level has been performed from the video signal collected by the Charge Coupled Device (CCD) camera placed on the top of the conveyer belt carrying mangoes. Extracted image frames from the video signal have been corrected and processed to extract various features, which were found to be more relevant for the prediction of maturity level. Recursive feature elimination technique in combination with support vector machine (SVM)-based classifier has been employed to identify the most relevant features among the initially chosen 27 features. Finally, the optimum set of reduced number of features have been obtained and used for classification of the mangoes into four different classes according to the maturity level. For classification, an ensemble of seven binary SVM classifiers has been combined in error correcting output code, and the minimum hamming distance-based rule has been applied in decision making phase. For the experimental study, the mangoes of five different varieties were collected from three different locations and in three different batches. The obtained experimental result found to provide an average classification accuracy up to 96%.",
    "actual_venue": "Instrumentation and Measurement, IEEE Transactions  "
  },
  {
    "abstract": "In this paper, codes with locality for four erasures are considered. An upper bound on the rate of codes with locality with sequential recovery from four erasures is derived. The rate bound derived here is field independent. An optimal construction for binary codes meeting this rate bound is also provided. The construction is based on regular graphs of girth $6$ and employs the sequential approach of locally recovering from multiple erasures. An extension of this construction that generates codes which can sequentially recover from five erasures is also presented.",
    "actual_venue": "Arxiv: Information Theory"
  },
  {
    "abstract": "Resource allocation in multicell downlink orthogonal frequency division multiple-access (OFDMA) systems is investigated, where base stations (BSs) first independently carry out subcarrier allocation and then mitigate intercell interference (InterCI) with the aid of very limited BS cooperation. Two novel InterCI mitigation algorithms are proposed. The first one is the distributed decision making as...",
    "actual_venue": "Ieee Transactions On Vehicular Technology"
  },
  {
    "abstract": "Web applications have gained increased popularity in the past decade due to the ubiquity of the web browser across platforms. With the rapid evolution of web technologies, the complexity of web applications has also grown, making maintenance tasks harder. In particular, maintaining cross-browser compliance is a challenging task for web developers, as they must test their application on a variety of browsers and platforms. Existing tools provide some support for this kind of test, but developers are still required to identify and fix cross-browser issues mainly through manual inspection. Our WEBDIFF tool addresses the limitations of existing tools by (1) automatically comparing the structural and visual characteristics of web pages when they are rendered in different browsers, and (2) reporting potential differences to developers. When used on nine real web pages, WEBDIFF automatically identified 121 issues, out of which 100 were actual problems. In this demo, we will present WEBDIFF, its underlying technology, and several examples of its use on real applications.",
    "actual_venue": "Icsm"
  },
  {
    "abstract": "With one non-trivial exception, GF( q n ) contains a primitive element of arbitrary trace over GF( q ).",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "A new approach of how to solve the 3D packing problem automatically is proposed. This problem is well known as a complete combinatorial problem. The authors attempt to realize a mechanism in which the 3D packing rule is automatically tuned, and an optimum packing solution is obtained by applying genetic algorithms which mimic the process of a natural evolution system. The 3D packing strategy is controlled by two evaluation functions which dominate the selection of a next allocation position and a box. To find the near-optimal strategy, the weighted coefficients of the evaluation functions are tuned by applying the genetic operators such as reproduction, crossover and mutation. To use the obtained tuned results as accumulated successful strategies, a 3D packing rule-base is constructed. The rules in this rule-base are composed of a `conditional part', which expresses the features of the given problem, and a `procedural part', which gives the packing strategy",
    "actual_venue": "Osaka"
  },
  {
    "abstract": "Using only position measurements, this paper presents a novel control scheme capable of solving the leader-follower and the leaderless consensus problems in networks composed of multiple Euler-Lagrange systems. Velocities are estimated using the Immersion and Invariance (I&I) observer and the control structure is similar to the Proportional plus damping (P + d) scheme. The interconnection topology...",
    "actual_venue": "Ieee Transactions On Control Of Network Systems"
  },
  {
    "abstract": "A general and systematic regularization is developed for the exact solitonic form factors of exponential operators in the (1+1)-dimensional sine-Gordon model by analytical continuation of their integral representations. The procedure is implemented in Mathematica. Test results are shown for four- and six-soliton form factors.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "Structured illumination microscopy (SIM) improves resolution by down-modulating high-frequency information of an object to fit within the passband of the optical system. Generally, the reconstruction process requires prior knowledge of the illumination patterns, which implies a well-calibrated and aberration-free system. Here, we propose a new algorithmic self-calibration strategy for SIM that does not need to know the exact patterns a priori, but only their covariance. The algorithm, termed PE-SIMS, includes a pattern-estimation (PE) step requiring the uniformity of the sum of the illumination patterns and a SIM reconstruction procedure using a statistical prior (SIMS). Additionally, we perform a pixel reassignment process (SIMS-PR) to enhance the reconstruction quality. We achieve 2 x better resolution than a conventional widefield microscope, while remaining insensitive to aberration-induced pattern distortion and robust against parameter tuning. (C) 2017 Optical Society of America",
    "actual_venue": "Biomedical Optics Express"
  },
  {
    "abstract": "Social media continues to gain prominence as an information resource. However, little is known about how people perceive trust and credibility in social media messages, particularly in terms of abstract dispositions toward organizations. The current experiment examines the role of speed of updates on a twitter feed with perceptions of trust. The experiment is also used to address the convergent validity of the RAND Public Health Disaster Trust Scale. The results do not provide evidence of a direct relationship between speed of twitter feed updates and trust, but do support a mediation model in which cognitive elaboration mediates the relationship. Further, the convergent validity of the RAND Public Health Disaster Trust Scale is discussed, along with its utility for future studies of this type.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "A double-transform method is developed for finding exact solutions to axisymmetric nonmixed time-dependent problems in elasticity in which the applied stresses are specified on the end of semi-infinite circularly cylindrical bars with stress-free lateral surfaces. The response of the bar to the sudden application of pressure to the end is determined. In another application, the method is used to calculate the reflection of a continuous train of waves off a free end of a semi-infinite cylindrical bar. For the reflection problem, an end resonance is found when the incident waves have an angular frequency in the neighborhood of omega = 1.51 V(d)/a, for a rod with Poisson's ratio of 1/3, where a is the radius of the bar and V(d) is the dilatation wave velocity. The neighborhood of the end-resonance frequency determined for the free-end reflection problem is shown to correspond to a range of frequencies of oscillation at which large deviations exist between the solution to the pure-end-condition problem and the solution to the comparative mixed-end-condition problem describing the pressure-step response.",
    "actual_venue": "Siam Journal On Applied Mathematics"
  },
  {
    "abstract": "Knowledge Creation (KC) is a critical activity inside organizations. It has been said to be a differentiating factor and an important source of competitiveness. Tacit knowledge is an important asset of any organization. Because it is not formalized is difficult to share. KC supporting systems help people inside an organization to share this tacit knowledge. This paper presents the design, and implementation of a KC system called MCKC, for Mobile Collaborative Knowledge Creation, supporting face-to-face knowledge creation and sharing in mobile scenarios, allowing people to create new knowledge and share their tacit knowledge with their co-workers, using visual metaphors, gestures and sketches to implement the human-computer interface.",
    "actual_venue": "Criwg"
  },
  {
    "abstract": "Due to the decrease of sensor and actuator prices and their ease of installation, smart homes and smart environments are more and more exploited in automation and health applications. In these applications, activity recognition has an important place. This article presents a general architecture that is responsible for adapting automation for the different users of the smart home while recognizing their activities. For that, semi-supervised learning algorithms and Markov-based models are used to determine the preferences of the user considering a combination of: (1) observations of the data that have been acquired since the start of the experiment and (2) feedback of the users on decisions that have been taken by the automation. We present preliminarily simulated experimental results regarding the determination of preferences for a user.",
    "actual_venue": "Information"
  },
  {
    "abstract": "The paper presents Ontology Definition Metamodel (ODM) that enables using Model Driven Architecture (MDA) standards in ontological engineering. Other similar metamodels are based on ontology representation languages, such as RDF(S), DAML+ OIL, etc. However, none of these other solutions uses the recent W3C effort The Web Ontology Language (OWL). In our approach, we firstly define the ODM place in the context of the MDA four- layer architecture and identify the main OWL concepts. Then, we define ODM using Meta- Object Facility (MOF). The relations between similar MOF and OWL concepts are discussed in order to show their differences (e.g. MOF or UML Class and OWL Class). The proposed ODM is a good starting point for defining an OWL- based UML profile that will enable using the well- known UML notation in ontological engineering more extensively.",
    "actual_venue": "Journal Of Object Technology"
  },
  {
    "abstract": "We present a deep, bidirectional, recurrent framework for cleaning noisy and incomplete motion capture data. It exploits temporal coherence and joint correlations to infer adaptive filters for each joint in each frame. A single model can be trained to denoise a heterogeneous mix of action types, under substantial amounts of noise. A signal that has both noise and gaps is preprocessed with a second bidirectional network that synthesizes missing frames from surrounding context. The approach handles a wide variety of noise types and long gaps, does not rely on knowledge of the noise distribution, and operates in a streaming setting. We validate our approach through extensive evaluations on noise both in joint angles and in joint positions, and show that it improves upon various alternatives.",
    "actual_venue": "Arxiv: Graphics"
  },
  {
    "abstract": "In this paper we demonstrate the power of reconfiguration by pre- senting efficient randomized algorithms for both packet routing and sorting on a reconfigurable mesh connected computer. The run times of these algorithms are better than the best achievable time bounds on a conventional mesh. Many variations of the reconfigurable mesh can be found in the literature. We define yet another variation which we call as Mr . We also make use of the standard PARBUS model.",
    "actual_venue": "Int J Found Comput Sci"
  },
  {
    "abstract": "In the last few years, the focus of data management has changed from handling relatively small amounts of data, often in relational databases, to managing large amounts of data using various different database types. In many secondary school curricula, data management is mainly considered from a \\\"database\\\" perspective. However, in contrast to the developments in computer science research and practice, the new and changing aspects of data management have hardly been discussed with respect to CS education. We suggest re-evaluating the focus and relevance of the established database syllabi, to discuss the educational value of the newly arising developments and to prevent the teaching of outdated concepts. In this paper, we will contrast current educational standards and curricula with an up-to-date characterization of data management in order to identify gaps between the principles and concepts of data management that are considered as important today from a professional point of view on the one side, and the emphasis in current CS education on the other side. The findings of this analysis will provide a basis for aligning the concepts taught in CS education with the developments in data management research and practice, as well as for re-evaluating the educational value of these concepts.",
    "actual_venue": "Wipsce"
  },
  {
    "abstract": "We investigate the differences -- in terms of bothquantitative performance and subjective preference -- between direct-touch and mouse input for unimanual andbimanual tasks on tabletop displays. The results of twoexperiments show that for bimanual tasks performed ontabletops, users benefit from direct-touch input. However,our results also indicate that mouse input may be moreappropriate for a single user working on tabletop tasksrequiring only single-point interaction.",
    "actual_venue": "CHI"
  },
  {
    "abstract": "A new invariant of contour VPIUD-virtual perimeter increment under dilation-has been proposed. Based on the contour propagation model, VPIUD is defined as the sum of secondary waves of a contour which represents the virtual perimeter under dilation minus the perimeter of the contour. It has been proved that VPIUD is invariant and binary for any component. In this paper, the invariant property of contour VPIUD has been discussed with arbitrary neighbourhood. We demonstrate that VPIUD has a clear physical meaning: a contour dilates around convex vertices and reduces around concave vertices. In 6 and 8-neighborhood, the contribution of contour pixels to VPIUD is directly linked with local curvature, therefore VPIUD is equivalent to total curvature, another invariant. VPIUD can be used to distinguish between hole and outer contours, without affecting images. The proposed algorithm has been implemented at ease and results of contour classification are presented.",
    "actual_venue": "Icip"
  },
  {
    "abstract": "Image retrieval is an important problem in the area of multimedia processing. This paper presents two new curvelet-based algorithms for texture retrieval suitable, which are suitable for use in constrained-memory devices. The developed algorithms are tested on three publicly available texture datasets: CUReT, Mondial-Marmi, and STex-fabric. Our experiments confirm the effectiveness of the proposed system. Furthermore, a weighted version of the proposed retrieval algorithm is proposed, which is shown to achieve promising results in the classification of seismic activities.",
    "actual_venue": "Signal Processing: Image Communication"
  },
  {
    "abstract": "The rapid development of industrial ecology parks is indicative of one practical pattern of theories and methods in industrial ecology. They severely affect the development of the modem society. In this paper, through adopting the social network analysis method, we could go into the interior of symbiosis networks and construct a cascading model on symbiosis networks of industrial ecology parks by using complex theory. tinder the model, we then provide some evaluation indicators to evaluate nodes' power and status and analyze quantitatively the vulnerability against cascading failures in symbiosis networks according to a new framework. All these analyses and results could support the study of eco-industrial parks in methods and technology and help us to promote the construction of symbiosis networks of industrial ecology parks, especially improve the reliability and robustness. (C) 2013 The Authors. Published by Elsevier B.V.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "We analyze the behavior of redundant robots when the joint motion is generated by inverting task velocity commands through a kinematic control scheme. Depending on the chosen inversion scheme, the robot motion is subject to differential constraints that may or may not be integrable. Accordingly, we give a classification in terms of holonomic, partially nonholonomic, and completely nonholonomic behavior, pointing out also the relationship with the so-called cyclicity property. This general classification is illustrated by means of several examples. When the kinematic control scheme is nonholonomic, the whole configuration space of the robot is accessible by a proper choice of the task input commands. Under this assumption, we address the joint reconfiguration problem, namely the design of end-effector velocity commands that drive the robot to a desired joint configuration. To solve this problem, it is possible to borrow existing methods for motion planning of nonholonomic mechanical systems, such as the sinusoidal steering technique for chained- form systems.",
    "actual_venue": "Ieee T Robotics And Automation"
  },
  {
    "abstract": "Achieving high rates of detection in low rates of embedding is still a challenging problem in many steganalysis systems. The newly proposed steganalysis system based on sparse representation classifier has shown remarkable detection rates in low embedding rate. In this paper, we propose a new steganalysis system based on double sparse representation classifier. We compare our proposed method with other steganalysis systems which use different classifier (including nearest neighbor, support vector machine, ensemble support vector machine and sparse representation). In all of our experiments, input features to the classifier are fixed and the ability of classifier is examined. Also we provide a complexity analysis in terms of execution time for different classifier. In most of experiments, our proposed method shows superior performance in terms of detection rate and complexity for low embedding rates.",
    "actual_venue": "Multimedia Tools Appl"
  },
  {
    "abstract": "Let L be chosen uniformly at random from among the latinsquares of order n ≥ 4 and let r,s bearbitrary distinct rows of L. We study the distribution ofσr,s, the permutation of the symbolsof L which maps r to s. We show that for anyconstant c 0, the following events hold withprobability 1 - o(1) as n → ∞: (i)σr,s has more than (logn)1-c cycles, (ii)σr,s has fewer than 9$\\sqrt{n}$cycles, (iii) L has fewer than $ 9\\over 2$n5-2 intercalates (latin subsquares of order 2).We also show that the probability thatσr,s is an even permutation lies inan interval $[{1\\over 4} - o(1), {3\\over 4} + o(1)]$ and theprobability that it has a single cycle lies in[2n-2,2n-2-3]. Indeed, we showthat almost all derangements have similar probability (within afactor of n3-2) of occurring asσr,s as they do if chosen uniformlyat random from among all derangements of {1,2,…,n}.We conjecture that σr,s shares theasymptotic distribution of a random derangement. Finally, we givecomputational data on the cycle structure of latin squares oforders n ≥ 11. © 2008 Wiley Periodicals, Inc. RandomStruct. Alg., 2008",
    "actual_venue": "Random Struct Algorithms"
  }
]