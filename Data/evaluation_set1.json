[
  {
    "abstract": "The edit distance is a way of quantifying how similar two strings are to one another by counting the minimum number of character insertions, deletions, and substitutions required to transform one string into the other. A simple dynamic programming computes the edit distance between two strings of length n in O(n2) time, and a more sophisticated algorithm runs in time O(n + t2) when the edit distance is t [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit of obtaining faster running time, the last couple of decades have seen a flurry of research on approximating edit distance, including polylogarithmic approximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010], and a constant-factor approximation in subquadratic time [Chakrabarty, Das, Goldenberg, Koucḱy and Saks, FOCS 2018]. We study sublinear-time algorithms for small edit distance, which was investigated extensively because of its numerous applications. Our main result is an algorithm for distinguishing whether the edit distance is at most t or at least t\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n (the quadratic gap problem) in time Õ(n/t + t\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>\n). This time bound is sublinear roughly for all t in [ω(1), o(n\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1/3</sup>\n)], which was not known before. The best previous algorithms solve this problem in sublinear time only for t = ω(n\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1/3</sup>\n) [Andoni and Onak, STOC 2009]. Our algorithm is based on a new approach that adaptively switches between uniform sampling and reading contiguous blocks of the input strings. In contrast, all previous algorithms choose which coordinates to query non-adaptively. Moreover, it can be extended to solve the t vs t\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2-ε</sup>\n gap problem in time Õ(n/t\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1-ε</sup>\n + t\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>\n).",
    "actual_venue": "Ieee Annual Symposium On Foundations Of Computer Science"
  },
  {
    "abstract": "cudaGSEA outperforms broadGSEA by around two orders-of-magnitude on a single Tesla K40c or GeForce Titan X GPU. ompGSEA provides around one order-of-magnitude speedup to broadGSEA on a standard Xeon CPU. The rapidGSEA suite is open-source software and can be downloaded at https://github.com/gravitino/cudaGSEA as standalone application or package for the R framework.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "The support for modular software and the ability to perform type checking across module boundaries are becoming the mainstay of recent high level language design. This is well illustrated by languages such as MESA and the US Department of Defence's new standard language ADA. At Bell-Northern Research, PROTEL, one of the first modular typed languages, has been used since 1975 to implement a substantial software system. The experience accumulated in building this system has given us a unique perspective. It has shown that the confidence of language designers in modular typed languages is well founded. It has also revealed some pitfalls which others will undoubtedly encounter. The purpose of this paper is to share our experience by outlining the nature of the problems and our solutions to them.",
    "actual_venue": "Icse"
  },
  {
    "abstract": "Spatial effects such as cell shape have very often been considered negligible in models of cellular pathways, and many existing simulation infrastructures do not take such effects into consideration. Recent experimental results are reversing this judgement by showing that very small spatial variations can make a big difference in the fate of a cell. This is particularly the case when considering eukaryotic cells, which have a complex physical structure and many subtle control mechanisms, but bacteria are also interesting for the huge variation in shape both between species and in different phases of their lifecycle. In this work we perform simulations that measure the effect of three common bacterial shapes on the behaviour of model cellular pathways. To perform these experiments we develop ReDi-Cell, a highly scalable GPGPU cell simulation infrastructure for the modelling of cellular pathways in spatially detailed environments. ReDi-Cell is validated against known-good simulations, prior to its use in new work. We then use ReDi-Cell to conduct novel experiments that demonstrate the effect that three common bacterial shapes (Cocci, Bacilli and Spirilli) have on the behaviour of model cellular pathways. Pathway wavefront shape, pathway concentration gradients, and chemical species distribution are measured in the three different shapes. We also quantify the impact of internal cellular clutter on the same pathways. Through this work we show that variations in the shape or configuration of these common cell shapes alter model cell behaviour. (C) 2016 The Authors. Published by Elsevier Ireland Ltd.",
    "actual_venue": "Biosystems"
  },
  {
    "abstract": "This paper presents first results of experiments in vehicular-to-roadside communication using directional antennas. With directional antennas on one side, the duration of connection to a fixed access point or a road side communication unit can be extended and on the other side the interference caused to others can be reduced. In this work results of experiments with electronical steerable directional antennas mounted on a car communicating with stationary access points are presented. The measurements show the benefit of using directional antennas in different environments typical for vehicular communications. The duration of potential 802.11b connections have been compared using directional and omnidirectional antenna patterns when driving through suburban environment. This comparison is based on passive scanning for access points in order to validate the approach in realistic scenarios. The results clearly prove a substantial potential improvement when using directional antennas.",
    "actual_venue": "Dublin"
  },
  {
    "abstract": "This paper focuses on the challenges that one encounters when building for commercial deployment an automated system for Directory Assistance (DA.) The design for an automated DA system needs to take into account constraints and requirements that arise from three distinct aspects of the application, namely, the business drivers, the user needs, and the strengths and weaknesses of voice technologies.",
    "actual_venue": "Interspeech"
  },
  {
    "abstract": "This paper presents a fully automatic system which exploits the dynamics of 3D videos and is capable of recognizing six basic facial expressions. Local video-patches of variable lengths are extracted from different locations of the training videos and represented as points on the Grass-mannian manifold. An efficient spectral clustering based algorithm is used to separately cluster points for each of the six expression classes. The resulting cluster centers are matched with the points of a test video and a voting based strategy is used to decide about the expression class of the test video. The proposed system is tested on the largest publicly available 3D video database, BU4DFE. The experimental results show that the system achieves a very high classification accuracy and outperforms the current state of the art algorithms for facial expression recognition from 3D videos.",
    "actual_venue": "Applications Of Computer Vision"
  },
  {
    "abstract": "We have discovered that 3D reconstruction can be achieved from asingle still photographic capture due to accidental motions of thephotographer, even while attempting to hold the camera still. Although these motions result in little baseline and therefore high depth uncertainty, in theory, we can combine many such measurements over the duration of the capture process (a few seconds) to achieve usable depth estimates. Wepresent a novel 3D reconstruction system tailored for this problemthat produces depth maps from short video sequences from standard cameraswithout the need for multi-lens optics, active sensors, or intentionalmotions by the photographer. This result leads to the possibilitythat depth maps of sufficient quality for RGB-D photography applications likeperspective change, simulated aperture, and object segmentation, cancome \\\"for free\\\" for a significant fraction of still photographsunder reasonable conditions.",
    "actual_venue": "Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "Most of the business decisions are based on cost and benefit considerations. Data mining techniques that make it possible for the businesses to incorporate financial considerations will be moremeaningful to the decisionmakers. Decision theoretic framework has been helpful in providing a better understanding of classification models. This study describes a semi-supervised decision theoretic rough set model. The model is based on an extension of decision theoretic model proposed by Yao. The proposal is used to model financial cost/benefit scenarios for a promotional campaign in a real-world retail store.",
    "actual_venue": "Fundam Inform"
  },
  {
    "abstract": "The brain shows complex, nonstationarity temporal dynamics, with abrupt micro- and macrostate transitions during its information processing. Detecting and characterizing these transitions in dynamical states of the brain is a critical issue in the field of neuroscience and psychiatry. In the current study, a novel method is proposed to quantify brain macrostates (e.g., sleep stages or cognitive states) from shifts of dynamical microstates or dynamical nonstationarity. A ``dynamical microstate'' is a temporal unit of the information processing in the brain with fixed dynamical parameters and specific spatial distribution. In this proposed approach, a phase-space-based dynamical dissimilarity map (DDM) is used to detect transitions between dynamically stationary microstates in the time series, and Tsallis time-dependent entropy is applied to quantify dynamical patterns of transitions in the DDM. We demonstrate that the DDM successfully detects transitions between microstates of different temporal dynamics in the simulated physiological time series against high levels of noise. Based on the assumption of nonlinear, deterministic brain dynamics, we also demonstrate that dynamical nonstationarity analysis is useful to quantify brain macrostates (sleep stages I, II, III, IV, and rapid eye movement (REM) sleep) from sleep EEGs with an overall accuracy of 77%. We suggest that dynamical nonstationarity is a useful tool to quantify macroscopic mental states (statistical integration) of the brain using dynamical transitions at the microscopic scale in physiological data.",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "Norwegian visual effects and digital design studio, Placebo Effects created a free mobile augmented reality interactive 3D coloring application that allowed users to play with the images in a narrative featuring two animated cows as part of an ongoing campaign by Norway's largest dairy producer, Tine.",
    "actual_venue": "Siggraph Asia Mobile Graphics And Interactive Applications"
  },
  {
    "abstract": "AbstractThe most commonly used image magnification techniques are interpolation based: nearest neighbor, bilinear, and bicubic. The drawbacks of these traditional methods are that images magnified by the simple nearest neighbor method often appear “blocky,” while images magnified by linear and cubic interpolations usually appear “blurry.” In this work, a new technique, which improves the performance of the traditional image magnification methods, is presented. We show how a differential image pyramid is first constructed using traditional interpolation methods, then how a vector quantizer is designed using the pyramidal data. The vector quantizer is a look-up table, termed the interresolution look-up table (IRLUT), which uses the lower resolution image vector as input to find as its output the corresponding higher resolution image vector. The improved image is produced by using the IRLUT's outputs to compensate for the image magnified by the traditional methods. Experimental results which show that images generated by the current method have sharper edges as well as lower reconstruction mean-square errors than those produced by traditional methods are presented.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "•A novel method of object representation for object tracking.•Proposed ICS-PF for overcoming sample impoverishment of PF.•Better qualitative and quantitative performance over PSO-PF and PF.•Real time application due to less computational requirement.",
    "actual_venue": "Expert Systems With Applications"
  },
  {
    "abstract": "The free movement of persons is a concept that has been settled on a European strategic level. In reality there are still a lot of steps to be followed in order to achieve this ambitious goal of mobility. Both the complexity of and the problems encountered in pan-European administrative processes interfere this aim. Interferences can be Knowledge Management (KM) specific (decentralization, implicitness, non-reusability, creation of process knowledge) as well as KM non-specific problems (linguistic, legal, document handling, cultural problems). To solve these problems both administrative processes have to be made transparent to the citizen and the knowledge about these processes has to be managed. Thus, public administrations must interact seamlessly vertically (Europe, nation, region, municipality) as well as horizontally (between countries) with each other. This implies not only the use of standards but also the interoperability of process systems. Coping with the above mentioned problems a solution requires Knowledge Management. As public administrations are in strongly heterogeneous legal environments a centralised and harmonised solution is not feasible. In this article a possible solution is described that has been developed in the European research project \"InfoCitizen\". Within InfoCitizen a Distributed Knowledge Repository and an intelligent agent-based Architecture is identified as an appropriated approach. InfoCitizen is a \"proof-of-concept\" project in which a Distributed Knowledge Repository and an agent platform are core concepts. The experiences and intermediary results of the successfully ongoing project are presented.",
    "actual_venue": "Kmgov"
  },
  {
    "abstract": "Business has begun to take advantage of these effects in many different ways. The obvious ones include new game products as well as gaming services related to existing products, e. g. for advertising or explanatory purposes. An interesting family of such products include those where mobile multimedia technologies are being combined with geographic information services, e. g. history games in city tourism, or city or company site planning.",
    "actual_venue": "Wirtschaftsinformatik"
  },
  {
    "abstract": "Despite optimistic expectations fast diffusion of mobile payments has not taken place after a decade of trials. Several explanations to this situation have been put forward using several theories and levels of analysis. Due to the complexity and dynamism of the diffusion we need multiple perspectives to account for diffusion challenge. We juxtapose three frameworks into a dynamic analysis framework. We apply the proposed framework to explain three failed introductions of mobile payments in the Swiss market. In particular, a recent ambitious trial is confronted with the proposed framework to detect roots of its failure. Our analysis suggests that market-level and behavioral facets need more attention in future in explaining mobile payment diffusion. To guide future efforts we propose several avenues for further research.",
    "actual_venue": "Hicss"
  },
  {
    "abstract": "A novel method for time-optimal pick and place motion planning based on robot dynamics is introduced. By using the elliptical trajectory with modified sine motion profile, the motion is smooth and suitable for high speed applications to avoid vibration. Since the modified sine motion profile is characterized by only two parameters, time-optimal pick and place motion planning problem is transformed into a three-variable optimization problem. For the reduction of the optimization variables, this method can rapidly obtain a solution that guarantees the smoothness of torques.",
    "actual_venue": "Robio"
  },
  {
    "abstract": "This paper presents a 2-D filtering scheme for stereo image compression using orthogonal subspace projection. To provide more candidate blocks for input data, the support region for input data is extended in the reference image. In addition, edge blocks are added to the candidate input blocks in Order to provide better compensation ability for edges and boundaries of objects. The best blocks for input data are selected one by one in order of importance to reconstruct the desired block using the Gram-Schmidt orthogonalization algorithm. Simulation results exhibit excellent performance of the proposed scheme when compared to those of the standard block-matching and least-squares(LS) based 2-D filtering schemes.",
    "actual_venue": "Proceedings Of The Ieee International Conference On Acoustics, Speech And Signal Processing, Vols"
  },
  {
    "abstract": "We describe MoM-NOCS, a Framework and a System that support communities with common interests in nature to capture and share multimedia observations of nature objects or events using mobile devices. The observations are automatically associated with contextual metadata that allow them to be visualized on top of 2D or 3D maps. The observations are managed by a multimedia management system, and annotated by the same and/or other users with common interests. Annotations made by the crowd support the knowledge distillation of the data and data provenance processes in the system. The Framework offers rich functionality for visualizing the observations made by the crowd as function of time. We have designed the MoM-NOCS to be complementary and interoperable with systems that are managed by Natural History museums like MMAT [8] and biodiversity metadata management systems like BIOCASE [4] and GBIF [6] so that they can link to interesting observations in our system, and the statistics on the observations that they manage can be visualized by our software.",
    "actual_venue": "Momm"
  },
  {
    "abstract": "Modularity is being increasingly used as an approach to solve for the information overload problem in ontologies. It eases cognitive complexity for humans, and computational complexity for machines. The current literature for modularity focuses mainly on techniques, tools, and on evaluation metrics. However, ontology developers still face difficulty in selecting the correct technique for specific applications and the current tools for modularity are not sufficient. These issues stem from a lack of theory about the modularisation process. To solve this problem, several researchers propose a framework for modularity, but alas, this has not been realised, up until now. In this article, we survey the existing literature to identify and populate dimensions of modules, experimentally evaluate and characterise 189 existing modules, and create a framework for modularity based on these results. The framework guides the ontology developer throughout the modularisation process. We evaluate the framework with a use-case for the Symptom ontology.",
    "actual_venue": "Applied Ontology"
  },
  {
    "abstract": "Several service applications have been reported by many, who proposed the use of wireless LANs (WLANs) over a wide variety of outdoor deployments. In particular, the upcoming IEEE 802.11ah WLAN protocol will enable a longer transmission range between WLAN access points (APs) and stations (STAs) up to multiple kilometers using carrier frequencies at 900MHz. However, limitations of WLAN outdoor installations have been found of the plethora of WLAN protocols in experimental studies. This article summarizes the challenges and provides a comprehensive overview of suggested improvements. As the standardization of the IEEE 802.11ah is reaching its final stage, important protocol aspects as well as new features are to be outlined. Interference problems and issues with the WLAN configuration, the physical layer (PHY), and media access control (MAC) are of paramount importance in outdoor WLAN networks; thus, are discussed in detail. Further, we examine the reported upper boundaries in throughput and link reliability of long-range WLANs in different environments, including seasurfaces, unmanned aerial vehicles (UAVs) and tunnels. At the end of this study, we reflect on the major issues regarding sub- 1GHz (S1G) WLANs and propose avenues for further research.",
    "actual_venue": "Ieee Communications Surveys And Tutorials"
  },
  {
    "abstract": "This paper proposes a new kind of k^'-means algorithms for clustering analysis with three frequency sensitive (data) discrepancy metrics in the cases that the exact number of clusters in a dataset is not pre-known. That is, by setting the number k of seed-points for learning clusters to be larger than the true number k^' of actual clusters in the dataset, i.e., kk^', these algorithms can locate the centers of k^' actual clusters by k^' converged seed-points, respectively, with the extra k-k^' seed-points corresponding to empty clusters, namely containing no winning points in the competition according to the underlying frequency sensitive discrepancy metrics. It is demonstrated by the experiments on both synthetic and real-world datasets that these three new k^'-means clustering algorithms can detect the number of actual clusters in a dataset with a classification accuracy rate as high as or higher than that of the original k^'-means algorithm. Moreover, they converge more quickly than the original one.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "The article presents a simple, practical approach for indoor localization using Received Signal Strength fingerprints from the GSM network, including an analysis of the relationship between signal strength and location, and the evolution of localization performance over time. Support Vector Machine regression applied to very high dimensional fingerprints does not reveal any smooth functional relationship between fingerprints and position. Classification using Support Vector Machines however provides very good results on discriminating different rooms in an indoor environment, albeit with performance that degrades over time. Transductive inference, introduced as a means of updating models to overcome degradation over time, provides hints that accurate indoor localization can be achieved by applying classification methods to cellular Received Signal Strength fingerprints, performance robustness being maintained via model updating and refining.",
    "actual_venue": "Instrumentation And Measurement Technology Conference"
  },
  {
    "abstract": "This book constitutes the thoroughly refereed post-conference proceedings of the 17th International Conference on Financial Cryptography and Data Security (FC 2013), held at Bankoku Shinryokan Busena Terrace Beach Resort, Okinawa, Japan, April 1-5, 2013. The 14 revised full papers and 17 short papers were carefully selected and reviewed from 125 submissions. The papers are grouped in the following topical sections: electronic payment (Bitcoin), usability aspects, secure computation, passwords, privacy primitives and non-repudiation, anonymity, hardware security, secure computation and secret sharing, authentication attacks and countermeasures, privacy of data and communication, and private data retrieval.",
    "actual_venue": "Financial Cryptography"
  },
  {
    "abstract": "We propose a novel automatic fiducial frame detection and registration method for device-to-image registration in MRI-guided prostate interventions. The proposed method does not require any manual selection of markers, and can be applied to a variety of fiducial frames, which consist of multiple cylindrical MR-visible markers placed in different orientations. The key idea is that automatic extraction of linear features using a line filter is more robust than that of bright spots by thresholding; by applying a line set registration algorithm to the detected markers, the frame can be registered to the MRI. The method was capable of registering the fiducial frame to the MRI with an accuracy of 1.00 +/- 0.73 mm and 1.41 +/- 1.06 degrees in a phantom study, and was sufficiently robust to detect the fiducial frame in 98% of images acquired in clinical cases despite the existence of anatomical structures in the field of view.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Old printed documents represent the significant part of our heritage. In order to preserve them, the digitalization is indispensable. The paper proposed a robust skew estimation method for old printed document. It is based on the connected components made by filled convex hulls around text element. The connected components are enlarged by oriented morphological operation. Then, the longest connected component is extracted. The global orientation of the document is detected by its orientation. Accordingly, document image was globally de-skewed. The algorithm is tested on synthetic and real datasets. Obtained results proved the algorithms correctness.",
    "actual_venue": "International Journal Of Computers, Communications And Control"
  },
  {
    "abstract": "One of the main goals of coverage tools is to provide the user with informative presentation of coverage information. Specifically, information on large, cohesive sets of uncovered tasks with common properties is very useful. This paper describes methods for discovering and reporting large uncovered spaces (holes) for cross-product functional coverage models. Hole analysis is a presentation method for coverage data that is both succinct and informative. Using case studies, we show how hole analysis was used to detect large uncovered spaces and improve the quality of verification.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "•We propose Mahtab, a framework for multi-core parallelization of regression testing.•We accelerate different phases of regression testing using parallelization windows.•We introduce test-prioritization based on code change relevance and confinedness.•We introduce a metric, ECC, for measuring effectiveness of change-coverage.•We propose EPSilon for quantifying effectiveness of regression test prioritization.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "Advances in microelectronics, semiconductor and information and communication technologies allow the creation and the development\n of ubiquitous systems. These intelligent systems would enable and support new dimensions of collaboration that can make business\n processes more efficient. In this paper, we analyse the possible contribution of the upcoming level of intelligence to collaborative\n innovation processes within and between enterprises as well. Considering collaborative networks and virtual organisations\n we propose a cyclic structure for innovation and a number of ubiquitous systems that have the potential to support different\n phases of the process. They would empower already existing Data Management Systems by functionalities and features of ubiquity\n and contribute to the future extension of the product lifecycle management approach (PLM).",
    "actual_venue": "J Intelligent Manufacturing"
  },
  {
    "abstract": "With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: First, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the State-Graph Manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed here. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "Research communities evolve over time, changing their interests for specific problems or research areas. Mapping the evolution of a research community, including the most frequently addressed problems, the strategies selected to propose solution for them, the venues on which results observed from applying these strategies are published, and the collaboration among distinct groups may provide lessons on actions that can positively influence the growth of research in a given field. To this end, this paper presents an analysis of the Brazilian SBSE research community. We present our major research groups focusing on the field, the software engineering problems most addressed by them, the search techniques most frequently used to solve these problems, and an analysis of our publications and collaboration. We could conclude that the Brazilian community is still expanding, both geographically and in terms of publications, and that the creation of a national workshop focusing on the research field was a keystone to allow this growth.",
    "actual_venue": "J Software Eng Rand"
  },
  {
    "abstract": "Many protocols in ad-hoc networks use dominating and connected dominating sets, for example for broadcasting and routing. For large ad hoc networks the construction of such sets should be local in the sense that each node of the network should make decisions based only on the information obtained from nodes located a constant number of hops from it. In this paper we use the location awareness of the network, i.e. the knowledge of position of nodes in the plane to provide local, constant approximation, deterministic algorithms for the construction of dominating and connected dominating sets of a Unit Disk Graph (UDG). The size of the constructed set, in the case of the dominating set, is shown to be 5 times the optimal, while for the connected dominating set 7.453 + Ɛ the optimal, for any arbitrarily small Ɛ 0. These are to our knowledge the first local algorithms whose time complexities and approximation bounds are independent of the size of the network.",
    "actual_venue": "Latin"
  },
  {
    "abstract": "In this paper, we present the new French Cloud management software called Compatible One. Compatible One is an open source cloud services broker i.e. a cloud service management software with brokering capabilities. Compatible One can provision, deploy and manage any type of cloud services, these services being provided by heterogeneous service providers selected according to Service Level Agreement (SLA). Compatible One can also federate heterogeneous resources and more precisely integrate seamlessly various cloud services. Given this approach it allows us to exploit original solutions and moreover include at design level some new paradigms like energy efficiency. This paper focuses on the current activities done in the Compatible One project for energy monitoring and energy efficient management of Clouds systems.",
    "actual_venue": "Cloud And Green Computing"
  },
  {
    "abstract": "Release planning for incremental software development assigns features to releases such that most important technical, resource, risk and budget constraints are met. The research presented in this paper is based on a three staged procedure. In addition to an existing method for (i) strategic release planning that maps requirements to subsequent releases and (ii) a more fine-grained planning that defines resource allocations for each individual release, we propose a third step, i.e., (iii) stability analysis, which analyzes proposed release plans with regards to their sensitivity to unforeseen changes. Unforeseen changes can relate to alterations in expected personnel availability and productivity, feature-specific task size (measured in terms of effort), and degree of task dependency (measured in terms of work load that can only be processed if corresponding work in predecessor tasks has been completed). The focus of this paper is on stability analysis of proposed release plans. We present the simulation model REPSIM (Release Plan Simulator) and illustrate its usefulness for stability analysis with the help of a case example.",
    "actual_venue": "Spw/Prosim"
  },
  {
    "abstract": "We explore design opportunities for using interactive technologies to enrich group fitness exercises, such as group spinning and swimming, in which an instructor guides a workout program and members synchronously perform a shared physical activity. As a case study, we investigate group fitness swimming. The design challenge is to coordinate a large group of people by considering trade-offs between social awareness and information overload. Our resulting group fitness swimming game, SwimTrain, allows a group of people to have localized synchronous interactions over a virtual space. The game uses competitive and cooperative phases to help group members acquire group-wide awareness. The results of our user study showed that SwimTrain provides socially-enriched swimming experiences, motivates swimmers to follow a training regimen and exert more intensely, and allows strategic game play dealing with skill differences among swimmers. Consequently, we propose several practical considerations for designing group fitness exergames.",
    "actual_venue": "CHI"
  },
  {
    "abstract": "This paper addresses the join selection and power assignment of a largest set of given links which can communicate successfully at the same time under the physical interference model in the simplex mode. For the special setting in which all nodes have unlimited maximum transmission power, Kesselheim [8] developed an constant approximation algorithm. For the general setting in which all nodes have bounded maximum transmission power, the existence of constant approximation algorithm remains open. In this paper, we resolve this open problem by developing a constant-approximation algorithm for the general setting in which all nodes have bounded maximum transmission power.",
    "actual_venue": "Wasa"
  },
  {
    "abstract": "We formulate the problem of 3D human pose estimation and tracking as one of inference in a graphical model. Unlike traditional kinematic tree representations, our model of the body is a collection of loosely-connected body-parts. In particular, we model the body using an undirected graphical model in which nodes correspond to parts and edges to kinematic, penetration, and temporal constraints imposed by the joints and the world. These constraints are encoded using pair-wise statistical distributions, that are learned from motion-capture training data. Human pose and motion estimation is formulated as inference in this graphical model and is solved using Particle Message Passing (PaMPas). PaMPas is a form of non-parametric belief propagation that uses a variation of particle filtering that can be applied over a general graphical model with loops. The loose-limbed model and decentralized graph structure allow us to incorporate information from \"bottom-up\" visual cues, such as limb and head detectors, into the inference process. These detectors enable automatic initialization and aid recovery from transient tracking failures. We illustrate the method by automatically tracking people in multi-view imagery using a set of calibrated cameras and present quantitative evaluation using the HumanEva dataset.",
    "actual_venue": "International Journal Of Computer Vision"
  },
  {
    "abstract": "Privacy constraints are typically enforced on shared data that contain sensitive personal attributes. However, owing to its adverse effect on the utility of the data, information loss must be minimized while sanitizing the data. Existing methods for this purpose modify the data only to the extent necessary to satisfy the privacy constraints, thereby asserting that the information loss has been minimized. However, given the subjective nature of information loss, it is often difficult to justify such an assertion. In this paper, we propose an interactive procedure to generate a data generalization scheme that optimally meets the preferences of the data publisher. A data publisher guides the sanitization process by specifying aspirations in terms of desired achievement levels in the objectives. A reference direction based methodology is used to investigate neighborhood solutions if the generated scheme is not acceptable. This approach draws its power from the constructive input received from the publisher about the suitability of a solution before finding a new one.",
    "actual_venue": "Knowledge And Data Engineering, Ieee Transactions"
  },
  {
    "abstract": "The fact that the standard probabilistic calculus does not define probabilities for sentences with embedded conditionals is\n a fundamental problem for the probabilistic theory of conditionals. Several authors have explored ways to assign probabilities\n to such sentences, but those proposals have come under criticism for making counterintuitive predictions. This paper examines\n the source of the problematic predictions and proposes an amendment which corrects them in a principled way. The account brings\n intuitions about counterfactual conditionals to bear on the interpretation of indicatives and relies on the notion of causal\n (in)dependence.",
    "actual_venue": "J Philosophical Logic"
  },
  {
    "abstract": "•Anthropomorphization of opponents was positively correlated with more preference for fairness and more strategic behavior in the ultimatum game.•Physical appearance showed no relation with preference for fairness and strategic behavior.•To improve human-robot interaction, studies should focus more on the determinants of anthropomorphism than on robot appearance.",
    "actual_venue": "International Journal Of Human Computer Studies"
  },
  {
    "abstract": "The main purpose of this study is to present a novel and useful application of a specific analytical technique to indicate the interactions and calculate the ranking of the barriers of electronic commerce (EC) in the Iran Khodro industrial group, a leading Iranian automotive company, using the combination of two techniques: interpretive structural modeling (ISM) and the fuzzy analytical network process (FANP). Based on an in-depth review of the relevant literature and interviews with managers and experts from the company, thirteen barriers and challenges to the implementation of e-commerce were determined and categorized into four main factors: technical, organizational, individual, and environmental. In the following step, the ISM technique is applied to construct a structural graph and identify inherent interactions among these barriers. The FANP is then used to quantify the relationships and weigh the significance of these barriers. The results obtained from the proposed model reveal that a “lack of awareness regarding the benefits and nature of electronic commerce” is the most important barrier to the implementation of e-commerce. This type of modeling approach can be extremely valuable for companies that wish to focus their efforts and resources on removing the most important barriers and challenges toward the successful implementation of EC.",
    "actual_venue": "Information And Management"
  },
  {
    "abstract": "In Bioinformatics, text mining and text data mining sometimes interchangeably\nused is a process to derive high-quality information from text. Perl Status\nReporter (SRr) is a data fetching tool from a flat text file and in this\nresearch paper we illustrate the use of SRr in text or data mining. SRr needs a\nflat text input file where the mining process to be performed. SRr reads input\nfile and derives the high quality information from it. Typically text mining\ntasks are text categorization, text clustering, concept and entity extraction,\nand document summarization. SRr can be utilized for any of these tasks with\nlittle or none customizing efforts. In our implementation we perform text\ncategorization mining operation on input file. The input file has two\nparameters of interest (firstKey and secondKey). The composition of these two\nparameters describes the uniqueness of entries in that file in the similar\nmanner as done by composite key in database. SRr reads the input file line by\nline and extracts the parameters of interest and form a composite key by\njoining them together. It subsequently generates an output file consisting of\nthe name as firstKey secondKey. SRr reads the input file and tracks the\ncomposite key. It further stores all that data lines, having the same composite\nkey, in output file generated by SRr based on that composite key.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "Routing metrics are critical for selecting a path with maximum throughput in wireless multi-radio multi-hop mesh networks. Due to the unique characteristics of wireless mesh networks, such as various wireless losses, data transmission rates and transmission channels, the traditional minimum hop count metric does not perform well. To address this diversity, we propose a new routing metric called Bottleneck Aware Transmission Delay (BATD). For EACH channel on a path, the BATD metric accumulates the total transmission time on the links within the same carrier sense range, and assigns a weight to the path based on the transmission delay of the channel that yields the maximal sum. The path with the least weight is preferred. As a consequence, BATD not only takes into account the diverse channel distribution when there are multiple non-overlapping channels within one path, but also considers that links on different channels can transmit data packets simultaneously. This study extensively evaluates the effectiveness of the BATD routing metric along with other popular metrics via ns-2 simulations. The experiments are performed on a controlled chain topology as well as on a randomly generated topology. The results show that the novel BATD metric outperforms other routing metrics, especially for scenarios when more than two radios are configured within each node. It achieves up to 35% throughput improvement over the current known metrics.",
    "actual_venue": "Springsim"
  },
  {
    "abstract": "This work presents the three-dimensional (3D) reconstruction of one of the most important archaeological sites in Galicia: \"Aquis Querquennis\" (Bande, Spain) using in-situ non-invasive ground-penetrating radar (GPR) and Terrestrial Light Detection and Ranging (T-LiDAR) techniques, complemented with infrared thermography. T-LiDAR is used for the recording of the 3D surface of this particular case and provides high resolution 3D digital models. GPR data processing is performed through the novel software tool \" toGPRi\", developed by the authors, which allows the creation of a 3D model of the sub-surface and the subsequent XY images or time-slices at different depths. All these products are georeferenced, in such a way that the GPR orthoimages can be combined with the orthoimages from the T-LiDAR for a complete interpretation of the site. In this way, the GPR technique allows for the detection of the structures of the barracks that are buried, and their distribution is completed with the structure measured by the T-LiDAR on the surface. In addition, the detection of buried elements made possible the identification and labelling of the structures of the surface and their uses. These structures are additionally inspected with infrared thermography (IRT) to determine their conservation condition and distinguish between original and subsequent constructions.",
    "actual_venue": "Remote Sensing"
  },
  {
    "abstract": "The increasing complexity of new digital signal processing (DSP) applications is forcing the use of floating point (FP) numbers in their hardware implementations. In this brief, we investigate the advantages of using half-unit biased (HUB) formats to implement these FP applications on field-programmable gate arrays (FPGAs). These new FP formats allow for the effective elimination of the rounding l...",
    "actual_venue": "Ieee Transactions On Circuits And Systems : Express Briefs"
  },
  {
    "abstract": "Problematic eating behaviors are a major cause of obesity. To improve our understanding of these eating behaviors, we need to be able to first reliably detect them. In this paper we use a wrist-worn sensor to test a generalized machine learning models' reliability in detecting eating episodes through data processing. We process data from a 6-axis inertial sensor. Since most eating episodes do not occur while moving, we filter out periods of physical activity, and then use an advanced motif-based time-point fusion technique to detect feeding gestures. We also cluster each of the false alarms into four categories in an effort to identify the main behaviors that confound feeding gesture detection. We tested our system on eight participants performing various activities in the wild while wearing a sensing suite: a neck- and a wrist-worn sensor, along with a wearable video camera continuously recording to capture ground truth. Trained annotators further validated the algorithms by identifying feeding gestures, and categorized the false alarms. All eating episodes were detected; however, many false alarms were also detected, yielding a 61% average F-measure in detecting feeding gestures. This result shows clear challenges in characterizing eating episodes by using a single inertial-based wrist-worn sensor.",
    "actual_venue": "Ubicomp : The Acm International Joint Conference On Pervasive And Ubiquitous Computing Maui Hawaii September"
  },
  {
    "abstract": "Modern attacks are being made against client side applications, such as web browsers, which most users use to surf and communicate on the internet. Client honeypots visit and interact with suspect web sites in order to detect and collect information about malware to protect users from malicious websites or to allow security professionals to investigate malicious content. This paper will present the idea of using web-based technology and integrating it with a client honeypot by building a low interaction client honeypot tool called Honeyware. It describes the benefits of Honeyware as well as the challenges of a low interaction client honeypot and provides some ideas for how these challenges could be overcome.",
    "actual_venue": "Software Testing, Verification And Validation Workshops"
  },
  {
    "abstract": "The efficient use of infrastructures is a hard requirement for railway companies. Thus, the scheduling of trains should aim toward optimality, which is an NP-hard problem. The paper presents a friendly and flexible computer-based decision support system for railway timetabling. It implements an efficient method, based on meta-heuristic techniques, which provides railway timetables that satisfy a realistic set of constraints and, that optimize a multi-criteria objective function.",
    "actual_venue": "Iea/Aie"
  },
  {
    "abstract": "The Internet is an interconnection of separately administered networks called Autonomous Systems or ASes. To reach entities outside the AS, the inter-domain routing protocol used today is the Border Gateway Protocol or BGP [1]. It has been approximately 15 years since BGP was deployed on the Internet. The number of ASes participating in BGP has grown to over 16; 000 today. However, this growth has been super-linear dur-ing the past few years [2]. With this sudden growth there has been concern in the re-search community about how well BGP is scaling. In particular, it has been noted that there is significant growth in the volume of BGP route announcements (or route flap-ping) [3] and in the number of BGP route entries in the routers of various ASes [2]. For every BGP routing update that is received by a router, several tasks need to be performed [4]. First, the appropriate RIB-in (routing information base) needs to be updated. Ingress filtering, as defined in the router's configuration, has to be applied to the route announcement. If it is not filtered out, the route undergoes the BGP route selection rules and it is compared against other routes. If it is selected, then it is added to the BGP routing table and the appropriate forwarding table entry is updated. Egress filtering then needs to be applied for every BGP peer (except the one that sent the original announcement). New BGP announcements need to be generated and then added to the appropriate RIB-out queues. These actions can increase the load on the router CPU. Long periods of high router CPU utilization are undesirable due to two main reasons. High utilization can poten-tially increase the amount of time a router spends processing a routing change, thereby increasing route convergence time. High route convergence times can cause packet loss by increasing the window of time during which the route for a particular destination is unavailable. Further, high router CPU utilization can disrupt other tasks, such as other protocol processing, keep alive message processing and in extreme cases, can cause the router to crash. In this work, we answer the question \"Do BGP routing table changes cause an increase in average router CPU utilization in the Sprint IP network?\". Sprint operates a \"tier-1\" IP network that connects to over 2; 000 other ASes. Thus, we believe that it is a suitable point for studying the impact of BGP on average router CPU utilization. There has been prior work [5, 6] in analyzing BGP protocol behavior during worm propagation. To the best of our knowledge, there has been no prior published work on analyzing the relationship between BGP protocol behavior and router CPU utilization.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Chirp-spread-spectrum (CSS) defined in IEEE 802.15.4a is available for ranging based on the time-of-flight of electromagnetic signal and it can be applied to localization systems. One of problems of the CSS-based localization system is the collision of signals transmitted from multiple CSS nodes. In this paper, a probabilistic arbitration algorithm is proposed to prevent the signal collision for ranging. By using the algorithm, we can adjust the probability that each node will succeed in ranging. Therefore, the proposed algorithm can assign higher priority to a node which requires frequent coordinate calculations such as a rapidly moving mobile robot. Through simulations and experimental results, we evaluate and discuss the performance of the proposed algorithm.",
    "actual_venue": "RAM"
  },
  {
    "abstract": "As more companies are beginning to adopt the e-business model, it becomes easier for buyers to compare prices at multiple sellers and choose the one that charges the best price for the same item or service. As a result, the demand for the goods of a particular seller is becoming more unstable, since other sellers are regularly offering discounts that attract large fractions of buyers. Therefore, it becomes more important for each seller to switch from static to dynamic pricing policies that take into account observable characteristics of the current demand and the state of the seller's resources. This paper presents a Reinforcement Learning algorithm that can tune parameters of a seller's dynamic pricing policy in a gradient direction (thus converging to the optimal parameter values that maximize the revenue obtained by the seller) even when the seller's environment is not fully observable. This algorithm is evaluated using a simulated Grid market environment, where customers choose a Grid Service Provider (GSP) to which they want to submit a computing job based on the posted price and expected delay information at each GSP.",
    "actual_venue": "Future Generation Comp Syst"
  },
  {
    "abstract": "We present work on a new anatomically based 3D parametric lip model for synchronized speech that also supports the lip motion required for facial expressions. The lip model is represented with a B-spline surface and high-level parameters which define the articulation of the surface. The model parameterization is muscle-based to allow for specification of a wide range of lip motion. The B-spline surface specifies not only the external portion of the lips, but the internal surface as well. This complete geometric representation replaces the original lip geometry of any facial model.We also describe a method to render the lip model using a procedural texturing paradigm to give color, lighting and surface texture for increased realism. We use our lip model in a text-to-audio-visual-speech system to achieve speech-synchronized facial animation.",
    "actual_venue": "Deform/Avatars"
  },
  {
    "abstract": "Due to the explosive growth of the Internet online reviews, we can easily collect a large amount of labeled reviews from different domains. But only some of them are beneficial for training a desired target-domain sentiment classifier. Therefore, it is important for us to identify those samples that are the most relevant to the target domain and use them as training data. To address this problem, a novel approach, based on instance selection and instance weighting via PU learning, is proposed. PU learning is used at first to learn an in-target-domain selector, which assigns an in-target-domain probability to each sample in the training set. For instance selection, the samples with higher in-target-domain probability are used as training data; For instance weighting, the calibrated in-target-domain probabilities are used as sampling weights for training an instance-weighted naive Bayes model, based on the principle of maximum weighted likelihood estimation. The experimental results prove the necessity and effectiveness of the approach, especially when the size of training data is large. It is also proved that the larger the Kullback-Leibler divergence between the training and test data is, the more effective the proposed approach will be.",
    "actual_venue": "Ijcai"
  },
  {
    "abstract": "We investigate two distinct issues related to resource allocation heuristics: robustness and failure rate. The target system consists of a number of sensors feeding a set of heterogeneous applications continuously executing on a set of heterogeneous machines connected together by high-speed heterogeneous links. There are two quality of service (QoS) constraints that must be satisfied: the maximum end-to-end latency and minimum throughput. A failure occurs if no allocation is found that allows the system to meet its QoS constraints. The system is expected to operate in an uncertain environment where the workload, i.e., the load presented by the set of sensors, is likely to change unpredictably, possibly resulting in a QoS violation. The focus of this paper is the design of a static heuristic that: (a) determines a robust resource allocation, i.e., a resource allocation that maximizes the allowable increase in workload until a run-time reallocation of resources is required to avoid a QoS violation, and (b) has a very low failure rate (i.e., the percentage of instances a heuristic fails). Two such heuristics proposed in this study are a genetic algorithm and a simulated annealing heuristic. Both were ''seeded'' by the best solution found by using a set of fast greedy heuristics.",
    "actual_venue": "J Parallel Distrib Comput"
  },
  {
    "abstract": "The numerical solution of the one-dimensional nonlinear Klein-Gordon equation on an unbounded domain is studied in this paper. Split local absorbing boundary (SLAB) conditions are obtained by the operator splitting method, then the original problem is reduced to an initial boundary value problem on a bounded computational domain, which can be solved by the finite difference method. Several numerical examples are provided to show the advantages and effectiveness of the given method, and some interesting collision behaviors are also observed.",
    "actual_venue": "J Comput Physics"
  },
  {
    "abstract": "Different types of code-reviews (Fagan-style code-inspections, Parnas-like active design reviews and walkthroughs) have been found to be very useful in improving the quality of software. In many cases reviewers use checklists to guide their analysis during review sessions. However, valuable, checklist-based code-reviews have the principal shortcoming of their high costs due to lack of supporting tools enabling at least partial automation of typical multiple appearing rules. This paper describes an approach towards semi-automation of some steps of individual review processes based on checklists. The method proposed is interactive, i.e. reviewers will be enabled to actualize, extend, and check the consistency and redundancy of their checklists. The basic idea underlying the approach is the usage of a rule-based system, adapting concepts of the compiler theory and knowledge engineering, for acquisition and representation of knowledge about the program. Redundant and conflicting knowledge about the program under study is recognized and solved by means of an embedded truth maintenance system. As a result of fault diagnosis, rules for fault classification are used. Software reliability models are applied to validate the results of each review session. The approach has shown promising preliminary results in analyses of conventional C-programs developed in the automotive industry.",
    "actual_venue": "White Plains, Ny"
  },
  {
    "abstract": "When implementing multivalued consensus using binary consensus, previous algorithms assume the availability of uniform reliable broadcast, which is not implementable in systems with fair-lossy links. In this paper, we show that with binary consensus we can implement uniform reliable broadcast directly in systems with fair-lossy links, and thus the separate assumption of the availability of uniform reliable broadcast is not necessary. We further prove that, when binary consensus instances are available only as black boxes, any implementation of uniform reliable broadcast in the fair-lossy link model requires the invocation of an infinite number of binary consensus instances even if no process ever broadcasts any messages, and this is true even when multivalued consensus is used.",
    "actual_venue": "Inf Process Lett"
  },
  {
    "abstract": "For each k ≤ 4, we give τk 0 such that a random k-CNF formula F with n variables and ⌊rkn⌋ clauses is satisfiable with high probability, but ORDERED-DLL takes exponential time on F with uniformly positive probability. Using results of [2], this can be strengthened to a high probability result for certain natural backtracking schemes and extended to many other DPLL algorithms.",
    "actual_venue": "Soda"
  },
  {
    "abstract": "Multi-core architectures introduce a new granularity at which process variations may occur, yielding asymmetry among cores that were designed---and that software expects---to be symmetric in performance. The chief source of this phenomenon are highly correlated, \"systematic\" within-die variations such as optical imperfections yielding variations across the exposure field. Per-core voltages can be used to bring all cores to the same performance level, but this compensation strategy also affects power, chiefly due to leakage power. Boosting a core's frequency may therefore boost its leakage sufficiently to engage thermal throttling. This sets up a tradeoff between static performance asymmetry due to frequency variation versus dynamic performance asymmetry due to thermal throttling. This paper explores the potential magnitude of these effects.",
    "actual_venue": "Date"
  },
  {
    "abstract": "The problem of comparison of graphs with the same number of vertices and edges by their number of spanning trees is considered. A set of operations on graphs which increase the number of their spanning trees is given. In particular, the following assertions are proved: (1) A disconnected graph is “better” (it destroys less trees if removed from the complete graph) than any connected separable graph whose blocks are components of the given graph. (2) The replacement, in a separable graph, of a block B with m edges which hangs at the unique vertex x , by the star with m edges fixed at its center in x , “worsens” the graph. (3) A chain (star) composed of identical and symmetric two-terminal networks is “better” (“worse”) than any other tree composed from the same two-terminal networks. (4) A chain (a circle) is “better” than any connected (respectively 2-connected) graph with the same number of edges. The article is concluded by a description of some operations on graphs which permit the extension of the results to a wider class of graphs.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "In this article I argue for a critical analysis and re-evaluation of the term pervasive. Due to ambiguous definitions in both the discourse on computing and gaming, this term has become theoretically entangled with others. This has led to multiple definitions, and also, in the case of gaming, to the neglect of the different perspectives in which a game can be pervasive. In the current nascent stage of research into pervasive gaming, in which attempts are being made to theorize this concept, I argue for an analytical overview of the use of the term pervasive in order to understand the different perspectives in which it is used.",
    "actual_venue": "Computers In Entertainment"
  },
  {
    "abstract": "The paper presents a new objective metrics of blocking artifacts visibility in MPEG compressed video sequences. The metric estimates a local visibility of the block grid by analyzing the discontinuity of a pixel intensity trend across the block edge and by comparing it against pixel activities within blocks. Application of the local blockiness metrics for adaptive control of low-pass filtering provides significant reduction of coding artifacts without blurring of image details. The results of our experiments show the high efficiency of the proposed approach.",
    "actual_venue": "Ieee International Conference On Multimedia And Expo , Vols And"
  },
  {
    "abstract": "The idea of weaving formal methods through computing (or software engineering) degrees is not a new one. However, there has been little success in developing and implementing such a curriculum. Formal methods continue to be taught as stand-alone modules and students, in general, fail to see how fundamental these methods are to the engineering of software. A major problem is one of motivation - how can the students be expected to enthusiasticall.y embrace a challenging subject when the learning benefits, beyond passing an exam and achieving curriculum credits, are not clear? Problem-based learning has gradually moved from being an innovative pedagogique technique, commonly used to better-motivate students. to being widely adopted in the teaching of many different disciplines: including computer science and software engineering. Our experience shows that a good problem can be re-used throughout a student's academic life. In fact, the best. computing problems be used with Children (young and old), undergraduates and postgraduates. In this paper we present a process for weaving formal methods through a university curriculum that is founded on the application of problem-based learning and a library of good software engineering problems, where students learn about formal methods without sitting a traditional formal methods module. The process of constructing good problems and integrating them into the curriculum is shown to be analagous to the process of engineering software. This approach is not, intended to replace more traditional formal methods modules: it will better prepare students for such specialised modules and ensure that all students have an understanding mid appreciation for formal methods even if they do not, go on to specialise in them.",
    "actual_venue": "Communications In Computer And Information Science"
  },
  {
    "abstract": "This paper demonstrates the use of abductive network classifier committees trained on different features for improving classification accuracy in medical diagnosis. In an earlier publication, committee members were trained on different subsets of the training set to ensure enough diversity for improved committee performance. In situations characterized by high data dimensionality, i.e. a large number of features and a relatively few training examples, it may be more advantageous to split the feature set rather than the training set. We describe a novel approach for tentatively ranking the features and forming subsets of uniform predictive quality for training individual members. The abductive network training algorithm is used to select optimum predictors from the feature set at various levels of model complexity specified by the user. Using the resulting tentative ranking, the features are grouped into mutually exclusive subsets of approximately equal predictive power for training the members. The approach is demonstrated on three standard medical diagnosis datasets (breast cancer, heart disease, and diabetes). Three-member committees trained on different feature subsets and using simple output combination methods reduce classification errors by up to 20% compared to the best single model developed with the full feature set. Results are compared with those reported previously with members trained through splitting the training set. Training abductive committee members on feature subsets of approximately equal predictive power achieves both diversity and quality for improved committee performance. Ensemble feature subset selection can be performed using GMDH-based learning algorithms. The approach should be advantageous in situations characterized by high data dimensionality.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "Although neurons function with locally imprecise analog computational units, they can collectively interact to achieve computations with high complexity, low error rate, or high precision. In this work, we show how many moderately precise integrate-and-fire analog units interact via the mechanism of a spiking `carry' to collectively add numbers to arbitrary precision. For example, in a proof-of-concept implementation that we implemented with chips built in a 0.5 μm CMOS process, we show that four 4-bit precise analog units can collectively interact to implement 16-bit-precise addition. Errors in the analog computation are minimized via novel time-based calibration techniques. Our work may lay a foundation for future collective analog computation that is arbitrarily precise just as current digital computation is today.",
    "actual_venue": "Iscas"
  },
  {
    "abstract": "In line with the progress of artificial intelligence, electronic negotiation theory has developed a great variety of promising negotiation models. These models have been evaluated in several ways. Besides formal evaluation, which could prove a lot of optimality properties, some researchers have already examined the user acceptance of these models. However, there are no studies that compare different negotiation models under identical frame conditions. This paper thus aims at contributing to IS research by filling this gap. From the perspective of temporary employment job allocation, we prototyped a pure auction negotiation system, a semi-structured argumentation-based negotiation system, and a structured argumentation-based negotiation system and evaluated these systems regarding their user acceptance with an identical test setting (application domain, tasks, and laboratory setting). The results contribute to IS-research by systematically revealing specific acceptance characteristics and differences between the negotiation approaches regarding performance, effort and usage intention.",
    "actual_venue": "E-Business Engineering"
  },
  {
    "abstract": "There has been a lot of discussion, and a lot of confusion, about the various existing and new design languages recently. SystemC, SystemVerilog, Verilog-2005, e, Vera, PSL/Sugar, UML, Analogue and Mixed-Signal versions of Verilog and VHDL make the world a veritable alphabet soup. This paper briefly looks at the evolving world of design languages from a SystemC perspective. Although a design \"language war\" may seem imminent, there are strong prospects for peaceful coexistence between languages, and flows that connect them together. And such flows give tremendous opportunities for users of languages to significantlyimprove their methodologies. In addition, the needs of advanced system and System-on-Chip (SoC) design turn up a number of interesting research opportunities for those involved in language-based design. The paper will finish by covering some of these methodology and research possibilities, including those opened up by further evolution in SystemC to include SW task and OS scheduler modelling.",
    "actual_venue": "Sbcci"
  },
  {
    "abstract": "We address the complete state estimation problem of unmanned aerial vehicles, even under high-dynamic 3-D aerobatic maneuvers, while using low-cost sensors with bias variations and higher levels of noise. In such conditions, the control demand, for a robust real-time data fusion filter with minimal lag and noise, is addressed with the efficiency of a complementary filter scheme. First, the attitude is directly estimated in Special Orthogonal Group (SO(3)) by complementing the noisy accelerometer/magnetometer vector basis with a gyro propagated vector basis. Data fusion follows a least square minimization in SO(3) (Wahba's problem) solved in an analytic nonrecursive manner. Stability of the proposed filter is shown and performance metrics are extracted, whereas the computational complexity has been minimized with an appropriate reference frame and a custom singular value decomposition algorithm. An adaptation scheme is proposed to allow unhindered operation of the filter to erroneous inputs introduced by the high dynamics of a 3-D flight. Finally, the velocity/position estimation is mainly constructed by complementary filters combining multiple sensors. In addition to the low complexity and the filtering of the noise, the proposed observer is aided through a developed vision algorithm, enabling the use of the filter in Global-Positioning-System-denied environments. Extensive experimental results and comparative studies with state-of-the-art filters, either in the laboratory or in the field using high-performance autonomous helicopters, demonstrate the efficacy of the proposed scheme in demanding conditions.",
    "actual_venue": "Ieee Trans Contr Sys Techn"
  },
  {
    "abstract": "Mitochondrial DNA (mtDNA) is widely being used for population genetics, forensic DNA fingerprinting and clinical disease association studies. The recent past has uncovered severe problems with mtDNA genotyping, not only due to the genotyping method itself, but mainly to the post-lab transcription, storage and report of mtDNA genotypes.eCOMPAGT, a system to store, administer and connect phenotype data to all kinds of genotype data is now enhanced by the possibility of storing mtDNA profiles and allowing their validation, linking to phenotypes and export as numerous formats. mtDNA profiles can be imported from different sequence evaluation programs, compared between evaluations and their haplogroup affiliations stored. Furthermore, eCOMPAGT has been improved in its sophisticated transparency (support of MySQL and Oracle), security aspects (by using database technology) and the option to import, manage and store genotypes derived from various genotyping methods (SNPlex, TaqMan, and STRs). It is a software solution designed for project management, laboratory work and the evaluation process all-in-one.The extended mtDNA version of eCOMPAGT was designed to enable error-free post-laboratory data handling of human mtDNA profiles. This software is suited for small to medium-sized human genetic, forensic and clinical genetic laboratories. The direct support of MySQL and the improved database security options render eCOMPAGT a powerful tool to build an automated workflow architecture for several genotyping methods. eCOMPAGT is freely available at http://dbis-informatik.uibk.ac.at/ecompagt.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "This paper presents a universal partially evolved parallelization of the multi-objective evolutionary algorithm based on decomposition (MOEA/D) for multi-objective optimization on message-passing clusters to reduce its computation time. The partially evolved MOEA/D (peMOEA/D) is suitable not only for the bi-objective space, but also for higher dimensional objective spaces by using a partially evolved island model. This model improves the algorithm universality and population diversity by keeping a subpopulation equal in size to the entire population, but only evolving a partial (equal to a partition size) subpopulation on each separate processor in a cluster. Furthermore, a nearest-neighbor partitioning approach, hybrid migration policy and adaptive neighbor-based topology are adopted in the peMOEA/D. The fat partitions generated by the nearest-neighbor partitioning can reduce migration traffic of elitist individuals across subpopulations. Then, hybrid migration of both elitist individuals and utopian points helps separate subpopulations to cooperate in guiding the search quickly towards a complete front. Next, the adaptive neighbor-based topology connects neighbor partitions only and achieves an excellent balance between convergence speed and migration traffic. Experimental results on benchmark multi-objective optimization problems with two or more objectives demonstrate the satisfactory overall performance of the peMOEA/D in terms of both convergence performance and speedup on message-passing clusters.",
    "actual_venue": "Soft Comput"
  },
  {
    "abstract": "The Georgia Department of Transportation (GDOT) collects and maintains an inventory of all public roads within the state. The inventory covers more than 118,000 centerline miles (188,800 km) of roads in 159 counties and over 512 municipalities. The transportation road inventory includes more than 52 items, including roadway geometry, surface type, shoulder type, speed limit signs, etc. Traditional roadway geometric properties, including number of lanes, travel lane, and shoulder widths, are measured in the field. Roadway geometric property measurement is one of the most important and, yet, the most time-consuming and riskiest component of the roadway data inventory. For the past two years, GDOT has sponsored Georgia Tech to develop a GPS/GIS-based road inventory system that re-engineers the existing paper-pencil operations. Georgia Tech has extended the research to develop video image pattern recognition algorithms and a prototype application aimed at automating the roadway geometry measurement to enhance the roadway inventory operations. A highly reliable and effective image extraction algorithm using local thresholding, predictive edge extraction, and geometric optics was developed and is presented in this paper. Preliminary results show it can effectively extract roadway features. A large-scale, experimental study on accuracy and the productivity improvement is under way.",
    "actual_venue": "Iciap"
  },
  {
    "abstract": "This paper describes an optical-flow processor core for real-time video recognition. The processor is based on the Pyramidal Lucas and Kanade (PLK) algorithm. It features a smaller chip area, higher pixel rate, and higher accuracy than conventional optical-flow processors. Introduction of search range limitation and the Carman filter to the original PLK algorithm improve the optical-flow accuracy, and reduce the processor hardware cost. Furthermore, window interleaving and window overlap methods reduces the necessary clock frequency of the processor by 70%, allowing low-power characteristics. We first verified the PLK algorithm and architecture with a proto-typed FPGA implementation. Then, we designed a VLSI processor that can handle a VGA 30-fps image sequence at a clock frequency of 332 MHz. The core size and power consumption are estimated at 3.50 x 3.00 mm(2) and 600 mW, respectively, in a 90-nm process technology.",
    "actual_venue": "Ieice Transactions On Electronics"
  },
  {
    "abstract": "Motivation: T-cell epitope identification is a critical immuno-informatic problem within vaccine design. To be an epitope, a peptide must bind an MHC protein. Results: Here, we present EpiTOP, the first server predicting MHC class II binding based on proteochemometrics, a QSAR approach for ligands binding to several related proteins. EpiTOP uses a quantitative matrix to predict binding to 12 HLA-DRB1 alleles. It identifies 89% of known epitopes within the top 20% of predicted binders, reducing laboratory labour, materials and time by 80%. EpiTOP is easy to use, gives comprehensive quantitative predictions and will be expanded and updated with new quantitative matrices over time.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "Existing algorithms for exact helical cone beam (HCB) tomographic reconstruction are computationally infeasible for clinical applications. Their computational cost is dominated by 3-D backprojection, which is generally an O(N4) operation. We present a fast hierarchical 3-D backprojection algorithm, generalizing fast 2-D parallel beam and fan beam algorithms, which reduces the overall complexity of this step to O(N3 log N), greatly accelerating the reconstruction.",
    "actual_venue": "Image Processing, Proceedings, International Conference"
  },
  {
    "abstract": "Wireless sensor networks offer a new and inexpensive way to gather data. Unfortunately, the short communication range together with distortions cause corruption in data transmission. In this paper a novel approach is proposed to deal with missing data due to transmission problems in wireless sensor networks for user activity classification. The proposed method was tested on data gathered by a sensor network consisting of two sensors and a GPS receiver. It was capable of performing classification with a 1% drop in accuracy when data from the more informative sensor was missing in 2-minute bursts. For 5-minute bursts of missing data, the drop in accuracy was 5%.",
    "actual_venue": "Mobiwac"
  },
  {
    "abstract": "In this paper, we propose to use machine learning techniques to model the vagueness of bugs for language interpreters and develop a fitness function for the language fuzzing based on genetic programming. The basic idea is that bug-triggering scripts usually contain uncommon usages which are not likely used by programmers in daily developments. We capture the uncommonness by using the probabilistic context-free grammar model and the Markov model to compute the probabilities of scripts such that bug-triggering scripts will get lower probabilities and higher fitness values. We choose the ROC (Receiver Operating Characteristic) curves to evaluate the performance of fitness functions in identifying bug-triggering scripts from normal scripts. We use a large corpus of JavaScript scripts from Github and POC test cases of bug-reports from SpiderMonkeyu0027s bugzilla for evaluations. The ROC curves from the experiments show that our method can provide better ability to rank the bug triggering scripts in the top-K elements.",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "Material selection of highly sensitive components is one of the most challenging issues in the design and development of structural elements in aerospace and nuclear industry. This work compares some of the most widely potential multi-criteria decision making models for addressing all the stages in solving a material selection problem of highly sensitive components involving conflicting as well as multiple design objectives. For the first step, the compensatory models are discussed and employed to solve a multi-criteria material selection for a thermal loaded conductor in the presence of its required multi-functional characteristics. For the next step, using different versions of the non-compensatory methods examine the outranking approach to solve the same problem. The results are compared to each other to verify the effect of compensations and non-compensations in the methods and their sensitivity to ranking stability. It is of particular interest to see how different approaches of the Multiple Attribute Decision Making (MADM) models differ from each other when criterion of cost is a critical factor in the problem. The effect of individual attributes of cost criterion has been studied to ensure the reliability of the chosen candidate material by MADM models.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "Deep learning is a popular machine learning approach which has achieved a lot of progress in all traditional machine learning areas. Internet of thing (IoT) and Smart City deployments are generating large amounts of time-series sensor data in need of analysis. Applying deep learning to these domains has been an important topic of research. The Long-Short Term Memory (LSTM) network has been proven to be well suited for dealing with and predicting important events with long intervals and delays in the time series. LTSM networks have the ability to maintain long-term memory. In an LTSM network, a stacked LSTM hidden layer also makes it possible to learn a high level temporal feature without the need of any fine tuning and preprocessing which would be required by other techniques. In this paper, we construct a long-short term memory (LSTM) recurrent neural network structure, use the normal time series training set to build the prediction model. And then we use the predicted error from the prediction model to construct a Gaussian naive Bayes model to detect whether the original sample is abnormal. This method is called LSTM-Gauss-NBayes for short. We use three real-world data sets, each of which involve long-term time-dependence or short-term time-dependence, even very weak time dependence. The experimental results show that LSTM-Gauss-NBayes is an effective and robust model.",
    "actual_venue": "Arxiv: Networking And Internet Architecture"
  },
  {
    "abstract": "The Convergence process in which European Union (EU) universities are at present involved supposes that these institutions not only have to re-structure their degree programmes, but also their teaching and learning methodologies. We study the impact of this EU process on Database teaching, comparing the practices of three different European universities and using the Convergence goals as criteria. In our discussion, we identify which aspects in each university are closer to the Convergence goals, and which are likely to help in achieving these goals.",
    "actual_venue": "Eait"
  },
  {
    "abstract": "Notions relating to computational systems exhibiting creative behaviours have been explored since the very early days of computer science, and the field of Computational Creativity research has formed in the last dozen years to scientifically explore the potential of such systems. We describe this field via a working definition; a brief history of seminal work; an exploration of the main issues, technologies and ideas; and a look towards future directions. As a society, we are jealous of our creativity: creative people and their contributions to cultural progression are highly valued. Moreover, creative behaviour in people draws on a full set of intelligent abilities, so simulating such behaviour represents a serious technical challenge for Artificial Intelligence research. As such, we believe it is fair to characterise Computational Creativity as a frontier for AI research beyond all others-maybe, even, the final frontier.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "This paper introduces a novel optimal graph construction method that is applicable to multi-dimensional, multi-surface segmentation problems. Such problems are often solved by refining an initial coarse surface within the space given by graph columns. Conventional columns are not well suited for surfaces with high curvature or complex shapes but the proposed columns, based on properly generated flow lines, which are non-intersecting, guarantee solutions that do not self-intersect and are better able to handle such surfaces. The method is applied to segment human airway walls in computed tomography images. Comparison with manual annotations on 649 cross-sectional images from 15 different subjects shows significantly smaller contour distances and larger area of overlap than are obtained with recently published graph based methods. Airway abnormality measurements obtained with the method on 480 scan pairs from a lung cancer screening trial are reproducible and correlate significantly with lung function.",
    "actual_venue": "Ipmi"
  },
  {
    "abstract": "This work focuses on finding the most discriminatory or representative features that allow to classify commercials according to negative, neutral and positive effectiveness based on the Ace Score index. For this purpose, an experiment involving forty-seven participants was carried out. In this experiment electroencephalography (EEG), electrocardiography (ECG), Galvanic Skin Response (GSR) and respiration data were acquired while subjects were watching a 30-min audiovisual content. This content was composed by a submarine documentary and nine commercials (one of them the ad under evaluation). After the signal pre-processing, four sets of features were extracted from the physiological signals using different state-of-the-art metrics. These features computed in time and frequency domains are the inputs to several basic and advanced classifiers. An average of 89.76% of the instances was correctly classified according to the Ace Score index. The best results were obtained by a classifier consisting of a combination between AdaBoost and Random Forest with automatic selection of features. The selected features were those extracted from GSR and HRV signals. These results are promising in the audiovisual content evaluation field by means of physiological signal processing.",
    "actual_venue": "Frontiers In Computational Neuroscience"
  },
  {
    "abstract": "Modern power systems are becoming increasingly decentralized, with a greater degree of observability provided through a network of sensors and local controllers in addition to existing centralized supervisory control and data acquisition platforms. However, the interconnectivity between sensors and controllers creates potential vulnerabilities which can be exploited by a cyber-attack. The majority...",
    "actual_venue": "Ieee Transactions On Smart Grid"
  },
  {
    "abstract": "In this paper we address the problem of stabilizing nonholonomic wheeled mobile robots (WMR) at an equilibrium point in the configuration space in presence of possibly modeling and non-modeling uncertainties. For general class of nonholonomic wheeled mobile robots, we demonstrate how adaptive controller's concepts can be combined with the robust methods for solving the problem of global convergence of stabilizing error. These methods are used by incorporating input augmentation in order to account for the underactuated property, without using control inputs of large magnitude. The global stability result is obtained in the Lyapunov sense. The simulation results are presented to illustrate the effectiveness of controller scheme. Real-time experimental results are also presented to demonstrate the feasibility of the proposed controller for the stabilization of a real WMR.",
    "actual_venue": "Robotics And Biomimetics"
  },
  {
    "abstract": "The Sarbanes-Oxley Act introduces a new set of requirements into software development. Corporations need to assess their internal control effectiveness for business processes to show compliance with the act. This paper proposes a conceptual framework for integrating SarbanesOxley compliance needs into software development by mapping the activities of an established framework for internal controls to the various workflows of the systems development process. Theoretical and practical contributions are discussed and future research directions are explored.",
    "actual_venue": "Communications Of The Association For Information Systems"
  },
  {
    "abstract": "Indoor localization has been an active research field for decades, where the received signal strength (RSS) fingerprinting based methodology is widely adopted and induces many important localization techniques such as the recently proposed one building the fingerprint database with crowd-sourcing. While efforts have been dedicated to improve the accuracy and efficiency of localization, the fundamental limits of RSS fingerprinting based methodology itself is still unknown in a theoretical perspective. In this paper, we present a general probabilistic model to shed light on a fundamental question: how good the RSS fingerprinting based indoor localization can achieve? Concretely, we present the probability that a user can be localized in a region with certain size, given the RSS fingerprints submitted to the system. We reveal the interaction among the localization accuracy, the reliability of location estimation and the number of measurements in the RSS fingerprinting based location determination. Moreover, we present the optimal fingerprints reporting strategy that can achieve the best accuracy for given reliability and the number of measurements, which provides a design guideline for the RSS fingerprinting based indoor localization facilitated by crowd-sourcing paradigm.",
    "actual_venue": "Ieee Conference On Computer Communications"
  },
  {
    "abstract": "Mobile social networks (MSNs) are mobile communication systems focusing not only on the behaviour but also on the social needs of the users. In a broader view all mobile systems used by people in their everyday lives can be characterized as MSNs, since all interactions taking place follow social rather than random patterns. Whether the deployed communication system takes into account the social ba...",
    "actual_venue": "Ieee Communications Surveys And Tutorials"
  },
  {
    "abstract": "Electronic commerce has a dominant role in consumer economics. and popular garnering a lot of research attention. Understanding consumer market dynamics based on product popularity is crucial for business intelligence. This work explores the temporal dynamics in online marketing. We introduce a new popularity index based on Amazon: Product Popularity based on Sales Review Volume (PPSRV). We explore and evaluate sequential deep learning models to obtain time series embedding that can predict the product popularity. We further characterize popularity competition between similar products and extend our model of popularity prediction in a competitive environment. Experimental results on large-scale reviews demonstrate the effectiveness of our approach.",
    "actual_venue": "Asonam : International Conference On Advances In Social Networks Analysis And Mining Barcelona Spain August"
  },
  {
    "abstract": "New oscillation criteria are obtained for second order forced mixed nonlinear impulsive differential equations of the form (r(t)@F\"@a(x^'))^'+q(t)@F\"@a(x)+@?k=1nq\"k(t)@F\"@b\"\"\"k(x)=e(t),t@q\"ix(@q\"i^+)=a\"ix(@q\"i),x^'(@q\"i^+)=b\"ix^'(@q\"i) where @F\"@c(s):=|s|^@c^-^1s and @b\"1@b\"2...@b\"m@a@b\"m\"+\"1...@b\"n0. If @a=1 and the impulses are dropped, then the results obtained by Sun and Wong [Y.G. Sun, J.S.W. Wong, Oscillation criteria for second order forced ordinary differential equations with mixed nonlinearities, J. Math. Anal. Appl. 334 (2007) 549-560] are recovered. Examples are given to illustrate the results.",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "In cranial neurosurgery, a common approach to matching image space to physical space in the operating room is to attach markers to the skin of the patient. By localizing these markers in image space, and subsequently in physical space before the surgical procedure, a transformation between the two spaces may be determined. However, the accuracy of this transformation is to a large extent determined by the accuracy with which the markers may be identified in each space. In this paper, we investigate the effect of zoom and slice direction on the localization error of the markers in image space. We use a dataset of three-dimensional head images taken of patients wearing skin markers. We ask a set of qualified observers to localize the centroids of the markers as accurately as possible in each image, both with and without an enhancement designed to facilitate the localization process. This enhancement is an image reformatting step to ensure that the 2-D image slices lie in planes aligned with the principal axes of the marker being localized.",
    "actual_venue": "Proceedings Of The Society Of Photo-Optical Instrumentation Engineers"
  },
  {
    "abstract": "With the popularity of BitTorrent, improving its performance has been an active research area. Super-seeding, a special upload policy for initial seeds, improves the efficiency in producing multiple seeds and reduces the uploading cost of the initial seeders. However, the overall benefit of super seeding remains a question. In this paper, we conduct an experimental study over the performance of super-seeding scheme of BitTornado. We attempt to answer the following questions: whether and how much super-seeding saves uploading cost, whether the download time of all peers is decreased by super-seeding, and in which scenario super-seeding performs worse. With varying seed bandwidth and peer behavior, we analyze the overall download time and upload cost of super seeding scheme during random period tests over 250 widely distributed PlanetLab nodes. The results show that benefits of super-seeding depend highly on the upload bandwidth of the initial seeds and the behavior of individual peers. Our work not only provides reference for the potential adoption of super-seeding in BitTorrent, but also much insights for the balance of enhancing Quality of Experience (QoE) and saving cost for a large-scale BitTorrent-like P2P commercial application.",
    "actual_venue": "Ieee International Conference On Communications, Proceedings, Vols"
  },
  {
    "abstract": "As swarms are used in increasingly more complex scenarios, further investigation is needed to determine how to give human operators the best tools to properly influence the swarm after deployment. Previous research has focused on relaying influence from the operator to the swarm, either by broadcasting commands to the entire swarm or by influencing the swarm through the teleoperation of a leader. While these methods each have their different applications, there has been a lack of research into how the influence should be propagated through the swarm in leader-based methods. This paper focuses on two simple methods of information propagation-flooding and consensus-and compares the ability of operators to maneuver the swarm to goal points using each, both with and without sensing error. Flooding involves each robot explicitly matching the speed and direction of the leader (or matching the speed and direction of the first neighboring robot that has already done so), and consensus involves each robot matching the average speed and direction of all the neighbors it senses. We discover that the flooding method is significantly more effective, yet the consensus method has some advantages at lower speeds, and in terms of overall connectivity and cohesion of the swarm.",
    "actual_venue": "Systems, Man And Cybernetics"
  },
  {
    "abstract": "This article discusses and evaluates penalized quasi-likelihood (PQL) estimation techniques for the situation where random effects are correlated, as is typical in mapping studies. This is an approximate fitting technique which uses a Laplace approximation to the integrated mixed model likelihood. It is much easier to implement than usual maximum likelihood estimation. Our results show that the PQL estimates are reasonably unbiased for analysis of mixed Poisson models when there is correlation in the random effects, except when the means are sufficiently small to yield sparse data. However, although the normal approximation to the distribution of the parameter estimates works fairly well for the parameters in the mean it does not perform as well for the variance components. In addition, when the mean mortality counts are small, the estimated standard errors of the variance components tend to become more biased than those for the mean. We illustrate our approaches by applying PQL for mapping mortality in British Columbia, Canada, over the five-year period 1985–1989.",
    "actual_venue": "Computational Statistics And Data Analysis"
  },
  {
    "abstract": "In this work we compare the performance of two algorithms, respectively based on the Extended Kalman Filter and the Unscented Kalman Filter, for the mobile robot localization and environment reconstruction problem. The proposed algorithms do not require any assumption on the robot working space: they are driven only by the measurements taken using ultrasonic sensors located onboard the robot. We also devise a switching sensors activation policy, which allows energy saving still achieving accurate tracking and reliable mapping of the workspace. The results show that the two filters work comparably well, in spite of the superior theoretical properties of the Unscented Filter.",
    "actual_venue": "Control And Automation"
  },
  {
    "abstract": "A document page may contain two or more different scripts.For Optical Character Recognition (OCR) of such adocument page, it is necessary to separate different scriptsbefore feeding them to their individual OCR system. In thispaper an automatic scheme is presented to identify text linesof different Indian scripts from a document. For theseparation task at first the scripts are grouped into a fewclasses according to script characteristics. Next featurebased on water reservoir principle, contour tracing, profileetc. are employed to identify them without any expensiveOCR-like algorithms. At present, the system has an overallaccuracy of about 97.52%.",
    "actual_venue": "Icdar"
  },
  {
    "abstract": "During its operational test and evaluation, despite top management support and significant technical achievements, a personnel assignment model implemented for the United States Navy to support assignment decisions experienced overwhelming resistance from the users, the 200 or so enlisted detailers, located at the Bureau of Naval Personnel in Washington, D.C. Our MS/OR research team had neglected to assess the negative impact of the personnel assignment model on an important detailing function: assignment negotiations or bargaining between the detailers and their customers, the service members. By involving the detailers in revising the model and making the failings of the old model the strengths of the new model, we turned certain failure into a successful program. By managing the behavioral aspects of the implementation with special emphasis on problem identification and requirements structuring, we overcame the difficulties of introducing change to a largely manual and highly decentralized decision process and we compare lessons learned with the experiences of other implementers.",
    "actual_venue": "Operations Research"
  },
  {
    "abstract": "Network coding is a promising approach for increasing performance of multicast data transmission and reducing energy costs. Of course, it is essential to consider security aspects to ensure a reliable data transmission. Particularly, pollution attacks may have serious impacts in network coding since a single attacker can jam large parts of the network. Therefore, various approaches have been introduced to secure network coding against this type of attack. However, introducing security increases costs. Even though there are some performance analysis of secure schemes, to our knowledge there are no details whether these schemes are worthwhile to replace routing under the facet of efficiency. Thus, we discuss in this paper parameters to assess the efficiency of secure network coding schemes. Using three network graphs, we evaluate parameters focusing on communication overhead for selected schemes. Our results show that there are still benefits in comparison to routing depending on the network topology.",
    "actual_venue": "Communications And Multimedia Security"
  },
  {
    "abstract": "We consider the problem of assigning reels of components to slots on feeder racks of a surface mounting machine which is used to populate printed circuit boards with surface mount technology. This reel assignment problem(RAP) is one of a series of optimization problems that should be addressed to improve the production rate. We preprocess RAP to facilitate the application of dynamic programming and then formulate it as an integer programming problem. We solve the problem by using a heuristic algorithm based on dynamic programming. We implement the algorithm as a computer program and perform computer simulations. The simulation results are compared to those of the Effective Algorithm previously proposed.",
    "actual_venue": "Ieee International Conference On Robotics And Automation, Vols"
  },
  {
    "abstract": "The medical community question answering system (MCQA) which is a new kind of medical information exchange platform is becoming more and more popular. Due to the number of patients is much more than the doctors, resulting in many patients can not get timely answers to their questions. Similar question recommendation is a common approach to solve this problem. The contributions of this paper are two-fold: (1) we propose a Siamese CNN model which measure correlation between questions and answers. (2) We first apply word2vec to learn the semantic relations between words and then construct a similar question retrieval model with answers. The study above can achieve a good performance in the real MCQA data set. It shows that our method can effectively extract similar questions recommendation list, shorten user's time to wait for an answer and improve user experience as well.",
    "actual_venue": "Neural Information Processing, Iconip , Pt"
  },
  {
    "abstract": "Designing for embodied physical interaction is just as important at a coarse level of spatial navigation as in the minutiae of object exploration. We created interactive embedded interfaces called 'Navitiles' that can be suspended in a floor to support navigation of a building. Our design uses capacitance and RFID sensors to determine users' location and LEDs to indicate possible directions. We determine whether Navitile cues could help users understand spatial relationships between points of interest. We based our study on a previous experiment that used a simulated VR maze to test whether users were able to exhibit 'shortcut' behaviour that would indicate the formation of spatial maps. Our hypothesis was that the physicality of embodied spatial navigation directed by the Navitiles in a real maze would enable users to achieve similar spatial shortcut behaviours to those found in the virtual task. We found significant evidence that sufficient spatial knowledge was acquired to enable successful shortcut performance between unexplored routes. However, further work is required to measure the effect of physical body movement on spatial skills development.",
    "actual_venue": "Tangible And Embedded Interaction"
  }
]