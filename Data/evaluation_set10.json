[
  {
    "abstract": "The paper presents an approach to translate hierarchical predicate transition nets into CC++ (a concurrent object oriented language) program skeletons. The approach consists of an overall translation architecture and a set of translation rules based on the syntax and semantics of hierarchical predicate transition nets. The results have established a link between hierarchical predicate transition nets and concurrent object-oriented programming, and provided some building blocks for a hierarchical predicate transition net based transformational software development methodology.",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "In this paper, we propose two novel measures to specify motion significance and motion complexity from human motion trajectories. Motion significance indicates the relative meaningfulness of every motion frame which is defined as a set of data points acquired at a time index from multiple motion trajectories. Motion complexity indicates the number of meaningful motion frames involved in a set of such human motions. For this, we first show that motion significance can be measured by considering both temporal entropy and spatial entropy of a motion frame, based on the analysis of Gaussian mixtures learned from human motions. Motion complexity is then calculated by measuring the averaged amount of motion significance involved in all time indexes of motion trajectories. These two measures are devised to satisfy the requirement of neural complexity measure proposed to attain small values for totally random or totally regular activities. To show that the proposed measures are consistent with our intuitive notion of motion significance and motion complexity, several human motions for drawing and pouring are analyzed by means of motion significance and motion complexity. Furthermore, our complexity measure is compared with three existing complexity measures to analyze their similarity and dissimilarity.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "In massive multiple-input multiple-output (MIMO), most precoders result in downlink signals that suffer from high peak-to-average ratio (PAR), independently of modulation order and whether single-carrier or orthogonal frequency-division multiplexing (OFDM) transmission is used. The high PAR lowers the power efficiency of the base-station amplifiers. To increase the power efficiency, low-PAR precod...",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "We present a reinforcement learning algorithm based on Dyna-Sarsa that utilizes separate representations of reward and punishment when guiding state-action value learning and action selection. The adoption of policy meta-learning optimized by a genetic algorithm is explored and results in the context of a two-armed bandit goal-navigation task in a simple grid world are presented. The findings argue for an important role for a genetic algorithm approach for constructing the foundations of autonomous reinforcement learning agents.",
    "actual_venue": "Adaptive Dynamic Programming And Reinforcement Learning"
  },
  {
    "abstract": "Traditional data repositories are typically focused on the storage and querying of crisp-precise domains of data. As a result, current commercial data repositories have no facilities for either storing or querying imprecise-approximate data. However, when considering scientific data (i.e. medical data, sensor data etc) value uncertainty is inherited to scientific measurements. In this paper we revise the context of \"value uncertainty\", and examine common models related to value uncertainty as part of the OLAP model. We present our approach for extending the OLAP model to include treatment of value uncertainty as part of a multidimensional model inhabited by flexible date and non-rigid hierarchical structures of organisation.",
    "actual_venue": "Theoretical Advances And Applications Of Fuzzy Logic And Soft Computing"
  },
  {
    "abstract": "Scaffolding, the problem of ordering and orienting contigs, typically using paired-end reads, is a crucial step in the assembly of high-quality draft genomes. Even as sequencing technologies and mate-pair protocols have improved significantly, scaffolding programs still rely on heuristics, with no guarantees on the quality of the solution. In this work, we explored the feasibility of an exact solution for scaffolding and present a first tractable solution for this problem (Opera). We also describe a graph contraction procedure that allows the solution to scale to large scaffolding problems and demonstrate this by scaffolding several large real and synthetic datasets. In comparisons with existing scaffolders, Opera simultaneously produced longer and more accurate scaffolds demonstrating the utility of an exact approach. Opera also incorporates an exact quadratic programming formulation to precisely compute gap sizes (Availability: http://sourceforge.net/projects/operasf/ ).",
    "actual_venue": "Journal Of Computational Biology"
  },
  {
    "abstract": "In this paper, a new and efficient method for solving affine sub-families included in a family of nonlinear feedback shift register (NFSR) sequences is proposed. The linear case is focused on since the affine case is an analogy. Let f(x0,x1,...,xn) = x0⊕f1(x1,...,xn-1)⊕xn be a characteristic function of an n-stage NFSR, where n is a positive integer. Let deg(f) = d > 1 and f[d] be the summation of...",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "We propose a novel approach to restoring digital document images, with the aim of improving text legibility and OCR performance. These are often compromised by the presence of artifacts in the background, derived from many kinds of degradations, such as spots, underwritings, and show-through or bleed-through effects. So far, background removal techniques have been based on local, adaptive filters and morphological-structural operators to cope with frequent low-contrast situations. For the specific problem of bleed-through/show-through, most work has been based on the comparison between the front and back pages. This, however, requires a preliminary registration of the two images. Our approach is based on viewing the problem as one of separating overlapped texts and then reformulating it as a blind source separation problem, approached through independent component analysis techniques. These methods have the advantage that no models are required for the background. In addition, we use the spectral components of the image at different bands, so that there is no need for registration. Examples of bleed-through cancellation and recovery of underwriting from palimpsests are provided.",
    "actual_venue": "Ijdar"
  },
  {
    "abstract": "Organizations require cost estimates for delivering and operating software-intensive solutions. One of the factors impacting these costs is the solution's architecture. Applying solution architecting practices helps improve the accuracy of early cost estimating. In CGI, we have started to emphasize the application of architecting practices in cost estimating, and this paper reports on the early experiences of this initiative, which we call Solution-Based Estimating. We present an architectural viewpoint designed to address the costing concern, and report some early feedback from projects that have applied the Solution-Based Estimating approach.",
    "actual_venue": "Working Ieee/Ifip Conference On Software Architecture"
  },
  {
    "abstract": "The paper is devoted to generalize a previous model of the dynamic oligopolistic market equilibrium problem allowing the presence of production excesses and assuming, in a more reasonable way that the total amounts of commodity shipments from a firm to all the demand markets be upper bounded. First, we give equilibrium conditions in terms of the well-known dynamic Cournot–Nash equilibrium principle. Then we show that such conditions can be expressed in terms of Lagrange multipliers; namely, by means of an utility function, prove that both equilibrium conditions can be equivalently expressed by a variational inequality. The variational formulation allows us to provide existence theorems and qualitative properties for equilibrium solutions. At last, a numerical example illustrates the results obtained.",
    "actual_venue": "J Optimization Theory And Applications"
  },
  {
    "abstract": "A Local Energy Management System (LEMS) is described to control Electric Vehicle charging and Energy Storage Units within built environments. To this end, the LEMS predicts the most probable half hours for a triad peak, and forecasts the electricity demand of a building facility at those times. Three operational algorithms were designed, enabling the LEMS to (i) flatten the demand profile of the building facility and reduce its peak, (ii) reduce the demand of the building facility during triad peaks in order to reduce the Transmission Network Use of System (TNUoS) charges, and (iii) enable the participation of the building manager in the grid balancing services market through demand side response. The LEMS was deployed on over a cloud-based system and demonstrated on a real building facility in Manchester, UK.",
    "actual_venue": "Icpe Companion"
  },
  {
    "abstract": "Based on the success of social software, modern information and communication systems are continuously moving from an information-centric data perspective to a more person-centric view. Easy access to federated activity streams of colleagues and other awareness information that is aggregated from different distributed intra- and extra-organizational systems become more and more important for the daily knowledge work. The increasing number of platforms every person uses requires a flexible data integration solution that keeps track of the connections between the pieces of information and the persons involved in their creation in order to create a unified and aggregated view for work groups, teams and communities. This unified data collection is especially important for social network analysis and data mining as individual profiles and activities are meanwhile typically distributed over various source systems. In this paper we present the CommunityMashup, a person-centric multi-user data integration solution for social software and similar systems that facilitates data aggregation and filtering while retaining the link to the pieces of information in the source systems. To support continuous evolution and flexible integration of frequently changing heterogeneous APIs and interfaces, we apply a model-driven development approach based on a therefore created person-centric data model. In addition to the conceptual design of the CommunityMashup, we present a reference implementation based upon open source components. Our overall goal is to build a multi-user mashup middleware for social software that offers an universal entry point in combination with unified data access for different client devices and can be used in various application scenarios with regard to individually specified service levels, e.g. continuous availability.",
    "actual_venue": "Social Netw Analys Mining"
  },
  {
    "abstract": "MPD can successfully design multiplex PCR experiments suitable for next-generation sequencing, and simplifies retooling targeted resequencing pipelines to focus on new targets as new genetic evidence emerges.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "As RFID tags become more ubiquitously available, e.g., in a supermarket, it is necessary to monitor larger-scale tag populations in a dynamic environment to get updated tag information. This paper considers the problem of monitoring a dynamic tag population, to identify both the missing tags and new tags. Traditional approach can solve the problem by collecting all tag IDs in the current population, which could be slow because it ignores the knowledge of the tag population in a previous scan. To be more efficient, this paper presents two protocols: (1) a baseline protocol with optimized length of random number bits, (2) an improved one-phase protocol with easy labor to identify only the new and missing tags in ALOHA frames by fully utilizing previous tag population knowledge. Our analysis shows that the one-phase protocol can improve the monitoring accuracy by $25\\%$ and improve the time efficiency by $55\\%$, as compared with the two-phase protocol proposed in a recent paper which also identifies population changes.",
    "actual_venue": "EUC"
  },
  {
    "abstract": "Here we provide a comparison of several projective point transformations of an elliptic curve defined over GF(2n) and rank their performance.We provide strategies to achieve improved implementations of each. Our work shows that under certain conditions, these strategies can alter the ranking of these projective point arithmetic methods.",
    "actual_venue": "Selected Areas In Cryptography"
  },
  {
    "abstract": "This paper argues that improving learning reliably and at scale requires a specific orientation toward measurement, understood broadly. Drawing on examples from a partnership between SRI International and The Carnegie Foundation for the Advancement of Teaching, this paper describes measures of student behaviors that are being used by researchers and instructors to improve learning environments at more than 50 community colleges and four-year universities for thousands of students.",
    "actual_venue": "L@S"
  },
  {
    "abstract": "This article introduces the concept of qualitative case study research as empirical inquiry. It defines and distinguishes what a case study is, the purposes, intentions, and types of case studies. It then describes how to determine if a qualitative case study is the preferred approach for conducting research. It overviews the essential steps in designing qualitative case study research, including the role of literature and theory, approaches for collecting data and analyzing it, as well as how to write up and present case study findings. It articulates how to avoid common pitfalls when engaging in qualitative case study research and concludes with the strengths and limitations associated with this form of empirical inquiry.",
    "actual_venue": "Ijavet"
  },
  {
    "abstract": "Many existing clusters use inexpensive Gigabit Ethernet and often have multiple interfaces cards to improve bandwidth and enhance fault tolerance. We investigate the use of Concurrent Multipath Transfer (CMT), an extension to the Stream Control Transmission Protocol (SCTP), to take advantage of multiple network interfaces for use with MPI programs. We evaluate the performance of our system with microbenchmarks and MPI collective routines. We also compare our method, which employs CMT at the transport layer in the operating system kernel, to existing systems that support multi-railing in the middleware. We discuss performance with respect to bandwidth, latency, congestion control and fault tolerance.",
    "actual_venue": "Pvm/Mpi"
  },
  {
    "abstract": "We study the problem of organizing a collection of objects - images, videos - into clusters, using crowdsourcing. This problem is notoriously hard for computers to do automatically, and even with crowd workers, is challenging to orchestrate: (a) workers may cluster based on different latent hierarchies or perspectives; (b) workers may cluster at different granularities even when clustering using the same perspective; and (c) workers may only see a small portion of the objects when deciding how to cluster them (and therefore have limited understanding of the \"big picture\"). We develop cost-efficient, accurate algorithms for identifying the consensus organization (i.e., the organizing perspective most workers prefer to employ), and incorporate these algorithms into a cost-effective workflow for organizing a collection of objects, termed ORCHESTRA. We compare our algorithms with other algorithms for clustering, on a variety of real-world datasets, and demonstrate that ORCHESTRA organizes items better and at significantly lower costs.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "We present a primarily speech-based user interface to a wide range of entertainment, navigation and communication applications for use in vehicles. The multimodal dialog en ables the system to uniquely identify one of 79,000 place name variants using an active vocabulary of only 3,000 words at any given time. Low confidence in speech recog nition and word-level ambiguities are compensated for in flexible clarification dialogs with the user. The underlying dialog concept was developed in the framework of the EU-project SENECa. Some recent evalua tion results of the SENECa system demonstrator are discussed in the paper",
    "actual_venue": "IUI"
  },
  {
    "abstract": "Many factors are contributing to an increased practical interest in managing software quality. By this we mean treating software quality as a key dimension of project performance, equal to cost (effort) and schedule. Corporate initiatives based on the CMM [1] and Six Sigma [2] provide two examples of forces promoting interest in quality as a management concern.This presentation describes two common approaches to measuring and modeling softwarequality across the project life cycle so that it can be made visible to management. It discussesexamples of their application in real industry settings. Both of the examples presented comefrom CMM Level 4 organizations.",
    "actual_venue": "Compsac"
  },
  {
    "abstract": "Data dependences in sequential programs limit parallelization because extracted threads cannot run independently. Although thread-level speculation can avoid the need for precise dependence analysis, communication overheads required to synchronize actual dependences counteract the benefits of parallelization. To address these challenges, we propose a lightweight architectural enhancement co-designed with a parallelizing compiler, which together can decouple communication from thread execution. Simulations of these approaches, applied to a processor with 16 Intel Atom-like cores, show an average of 6.85x performance speedup for six SPEC CINT2000 benchmarks",
    "actual_venue": "Computer Architecture"
  },
  {
    "abstract": "The case-based learning (CBL) approach has gained attention in medical education as an alternative to traditional learning methodology. However, current CBL systems do not facilitate and provide computer-based domain knowledge to medical students for solving real-world clinical cases during CBL practice. To automate CBL, clinical documents are beneficial for constructing domain knowledge. In the literature, most systems and methodologies require a knowledge engineer to construct machine-readable knowledge. Keeping in view these facts, we present a knowledge construction methodology (KCM-CD) to construct domain knowledge ontology (i.e., structured declarative knowledge) from unstructured text in a systematic way using artificial intelligence techniques, with minimum intervention from a knowledge engineer. To utilize the strength of humans and computers, and to realize the KCM-CD methodology, an interactive case-based learning system(iCBLS) was developed. Finally, the developed ontological model was evaluated to evaluate the quality of domain knowledge in terms of coherence measure. The results showed that the overall domain model has positive coherence values, indicating that all words in each branch of the domain ontology are correlated with each other and the quality of the developed model is acceptable.",
    "actual_venue": "Expert Systems"
  },
  {
    "abstract": "Innovative auction methods can be exploited to increase profits, with\nShubik's famous \"dollar auction\" perhaps being the most widely known example.\nRecently, some mainstream e-commerce web sites have apparently achieved the\nsame end on a much broader scale, by using \"pay-per-bid\" auctions to sell\nitems, from video games to bars of gold. In these auctions, bidders incur a\ncost for placing each bid in addition to (or sometimes in lieu of) the winner's\nfinal purchase cost. Thus even when a winner's purchase cost is a small\nfraction of the item's intrinsic value, the auctioneer can still profit\nhandsomely from the bid fees. Our work provides novel analyses for these\nauctions, based on both modeling and datasets derived from auctions at\nSwoopo.com, the leading pay-per-bid auction site. While previous modeling work\npredicts profit-free equilibria, we analyze the impact of information asymmetry\nbroadly, as well as Swoopo features such as bidpacks and the Swoop It Now\noption specifically, to quantify the effects of imperfect information in these\nauctions. We find that even small asymmetries across players (cheaper bids,\nbetter estimates of other players' intent, different valuations of items,\ncommitted players willing to play \"chicken\") can increase the auction duration\nwell beyond that predicted by previous work and thus skew the auctioneer's\nprofit disproportionately. Finally, we discuss our findings in the context of a\ndataset of thousands of live auctions we observed on Swoopo, which enables us\nalso to examine behavioral factors, such as the power of aggressive bidding.\nUltimately, our findings show that even with fully rational players, if players\noverlook or are unaware any of these factors, the result is outsized profits\nfor pay-per-bid auctioneers.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "This paper describes applications of molecular simulation to microelectronics processes and the subsequent development of techniques for multiscale simulation and multiscale systems engineering. The progression of the applications of simulation in the semiconductor industry from macroscopic to molecular to multiscale is reviewed. Multiscale systems are presented as an approach that incorporates molecular and multiscale simulation to design processes that control events at the molecular scale while simultaneously optimizing all length scales from the molecular to the macroscopic. It is discussed how design and control problems in microelectronics and nanotechnology, including the targeted design of processes and products at the molecular scale, can be addressed using the multiscale systems tools. This provides a framework for addressing the “grand challenge” of nanotechnology: how to move nanoscale science and technology from art to an engineering discipline.",
    "actual_venue": "Computers And Chemical Engineering"
  },
  {
    "abstract": "This paper studies synthesis and optimization of reversible circuits composed of Toffoli gates with negative and positive control lines. The proposed synthesis algorithm performs sorting among optimal implementations of certain functions - called as essential functions - to implement any reversible Boolean function. Essential functions comprise very small amount of all functions. For example, to implement 3 bit circuits, 28 essential functions out of all 40320 functions are needed. The proposed optimization algorithm considers both reversible and quantum circuit costs. First, reversible cost is reduced by considering adjacent gate pairs. Then, inner quantum structures of the gates are investigated and quantum optimization is performed. The proposed algorithms are evaluated on benchmark circuits in comparison with the results in the literature.",
    "actual_venue": "Journal Of Multiple-Valued Logic And Soft Computing"
  },
  {
    "abstract": "LetRy_{t} = u_{t}be a stochastic difference equation. Various relations between the input and output covariances and spectral densities are deduced under the hypotheses thatRis time dependent and thatu_{t}is a member of a nonstationary random process.",
    "actual_venue": "Information Theory, IEEE Transactions  "
  },
  {
    "abstract": "Most programming languages have been designed by committees or individuals. What happens if, instead, we throw open the design process and let lots of programmers weigh in on semantic choices? Will they avoid well-known mistakes like dynamic scope? What do they expect of aliasing? What kind of overloading behavior will they choose? \n\n We investigate this issue by posing questions to programmers on Amazon Mechanical Turk. We examine several language features, in each case using multiple-choice questions to explore programmer preferences. We check the responses for consensus (agreement between people) and consistency (agreement across responses from one person). In general we find low consistency and consensus, potential confusion over mainstream features, and arguably poor design choices. In short, this preliminary evidence does not argue in favor of designing languages based on programmer preference.",
    "actual_venue": "Splash : Conference On Systems, Programming, Languages, And Applications: Software For Humanity Vancouver Bc Canada October"
  },
  {
    "abstract": "In Japan, the population flow into urban areas has seen the economic power of rural areas decrease. This has made it difficult for rural areas to maintain their historical and cultural heritage. Strategies combining information technology with tourism have been applied in many local areas, but the effects have not been significant. In this study, we aim to discover local features of interest that are buried within rural areas, and publicize them on a nationwide or worldwide scale using information systems. We develop a system for discovering local features that may be broadly unknown, and discuss issues related to the activation of local areas using information technology. We then propose a strategy to activate rural areas using web advertising and social network services. The effect of web advertising and the possibility of targeting users are further examined based on the experimental results.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "In order to utilize the semantics of object methods to ensure cached object consistency, method group commutativity specifies the conditions under which a group of methods will commute. Method group commutativity is determined using a semantic specification of object methods, provided in terms of logical expressions, to create commutativity conjectures that are analyzed using the PVS theorem prover. This analysis results in the creation of a method commutativity specification (MCS), which is used by a distributed caching system to ensure the consistency of method invocations. For greater commutativity, weaker consistency requirements can be specified in the MCS. This base approach is enhanced by optimizations that consider a client's sequential execution of methods and that reduce the amount of data cached by the client. The effectiveness of method group commutativity and associated optimizations is evaluated using Java RMI application benchmarks.",
    "actual_venue": "Parallel And Distributed Systems, Ieee Transactions"
  },
  {
    "abstract": "Software systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying designs, termed design erosion. This leads to the maintenance of a system becoming increasingly difficult and time-consuming during its lifetime. Refactoring can reduce the effects of design erosion, but this process requires significant effort on the part of the maintenance programmer. Research into automated refactoring has had some success in reducing the effort involved, however source code refactoring uses refactoring steps that are too small to effect major design changes. Design-level refactoring is also possible, but these approaches operate on design models and do little to help in the subsequent refactoring of the source code. In this paper, we present a novel refactoring approach that refactors a program based both on its desired design and on its source code. The maintenance programmer first creates a desired design (a UML class model) for the software based on the current software design and their understanding of how it may be required to evolve. Then, the source code is refactored using the desired design as a target. This resulting source code has the same behavior as the original, but its design more closely correlates to the desired design. We conducted an investigation using several open source Java applications to determine how precisely it is possible to refactor program source code to a desired design. Our findings were that the original program could be refactored to the desired design with an accuracy of over 90%, hence demonstrating the viability of automated refactoring using design differencing.",
    "actual_venue": "Csmr"
  },
  {
    "abstract": "Most current conceptualizations of the use of information technology (IT) to capture, preserve and use organizational memory (OM) are based on hard data sources, such as databases and knowledge bases. Conceptualizations based on hard data sources are useful and necessary, but do not encompass the complete range of possibilities. “Stories” constitute an important component of organizational memory. Multimedia technology offers a vehicle to capture stories in a rich format. In this paper, we begin with the definitions and goals of organizational memory to indicate how the multimedia story format fits into organizational memory systems (OMS). We discuss problems associated with the story format and potential ways of mitigating the problems. Then we describe Constellations, a video analysis tool, and discuss how it can be adapted for use as one of the components of OMS",
    "actual_venue": "System Sciences, Proceedings Of The Twenty-Fifth Hawaii International Conference"
  },
  {
    "abstract": "VLSI technologists are fast developing wafer-scale integration. Rather than partitioning a silicon wafer into chips as is usually done, the idea behind wafer-scale integration is to assemble an entire system (or network of chips) on a single wafer, thus avoiding the costs and performance loss associated with individual packaging of chips. A major problem with assembling a large system of microprocessors on a single wafer, however, is that some of the processors, or cells, on the wafer are likely to be defective. In the paper, we describe practical procedures for integrating \"around\" such faults. The procedures are designed to minimize the length of the longest wire in the system, thus minimizing the communication time between cells. Although the underlying network problems are NP-complete, we prove that the procedures are reliable by assuming a probabilistic model of cell failure. We also discuss applications of the work to problems in VLSI layout theory, graph theory, fault-tolerant systems, planar geometry, and the probabilistic analysis of algorithms.",
    "actual_venue": "Sfcs Proceedings Of The Annual Symposium On Foundations Of Computer Science"
  },
  {
    "abstract": "The concept of \"large scale\" depends obviously on the phenomenon we are interested in. For example, in the field of foundation of Thermodynamics from microscopic dynamics, the spatial and time large scales are order of fraction of millimetres and microseconds, respectively, or lesser, and are defined in relation to the spatial and time scales of the microscopic systems. In large scale oceanography or global climate dynamics problems the time scales of interest are order of thousands of kilometres, for space, and many years for time, and are compared to the local and daily/monthly times scales of atmosphere and ocean dynamics. In all the cases a Zwanzig projection approach is, at least in principle, an effective tool to obtain class of universal smooth \"large scale\" dynamics for few degrees of freedom of interest, starting from the complex dynamics of the whole (usually many degrees of freedom) system. The projection approach leads to a very complex calculus with differential operators, that is drastically simplified when the basic dynamics of the system of interest is Hamiltonian, as it happens in Foundation of Thermodynamics problems. However, in geophysical Fluid Dynamics, Biology, and in most of the physical problems the building block fundamental equations of motions have a non Hamiltonian structure. Thus, to continue to apply the useful projection approach also in these cases, we exploit the generalization of the Hamiltonian formalism given by the Lie algebra of dissipative differential operators. In this way, we are able to analytically deal with the series of the differential operators stemming from the projection approach applied to these general cases. Then we shall apply this formalism to obtain some relevant results concerning the statistical properties of the El Nino Southern Oscillation (ENSO).",
    "actual_venue": "Entropy"
  },
  {
    "abstract": "The Steiner distance of a graph, introduced by Chartrand, Oellermann, Tian and Zou in 1989, is a natural generalization of the concept of classical graph distance. For a connected graph G of order at least 2 and S subset of V (G), the Steiner distance d(G)(S) among the vertices of S is the minimum size among all connected subgraphs whose vertex sets contain S. Let n, k be two integers with 2 <= k <= n. Then the Steiner k-eccentricity e(k)(v) of a vertex v of G is defined by e(k)(v) = max{d(S)vertical bar S subset of V(G), vertical bar S vertical bar = k, and v epsilon S}. Furthermore, the Steiner k-diameter of G is sdiam(k)(G) = max {e(k)(v) vertical bar v epsilon V (G)}. In 2011, Chartrand, Okamoto and Zhang showed that k - 1 <= sdiam(k)(G) <= n - 1. In this paper, graphs with sdiam(4)(G) = 3, 4, n - 1 are characterized, respectively.",
    "actual_venue": "Journal Of Interconnection Networks"
  },
  {
    "abstract": "This paper is an extension of the already published paper by Voss and Suesse [11]. In that paper, wedeveloped a new region-based fitting method using the method of normalization. There we demonstrated the zero-parametric fitting of lines, triangles, parallelograms, circles, and ellipses. In the present paper, we discuss this normalization idea for fitting of closed regions using circular segments, elliptical segments, and super-ellipses. As features, we use the area-based low order moments. We show that we have to solve only one-dimensional optimization problems in these cases.",
    "actual_venue": "Ieee Trans Pattern Anal Mach Intell"
  },
  {
    "abstract": "We report our new DRAW_SneakPeek software for DNA-seq analysis. DNA resequencing analysis workflow (DRAW) automates the workflow of processing raw sequence reads including quality control, read alignment and variant calling on high-performance computing facilities such as Amazon elastic compute cloud. SneakPeek provides an effective interface for reviewing dozens of quality metrics reported by DRAW, so users can assess the quality of data and diagnose problems in their sequencing procedures. Both DRAW and SneakPeek are freely available under the MIT license, and are available as Amazon machine images to be used directly on Amazon cloud with minimal installation.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "The throughput performance of slotted ALOHA (S-ALOHA) in a cellular system and Nakagami fading environment is studied. Based on the signal capture model, the effects of multiple access interference (MAI) from in-cell users and of cochannel interference (CCI) from cochannel cells are quantified analytically. Especially considered are the cases when asynchronism of cochannel cells is present, for which an approximation of the interference distribution is successfully applied to get a highly precise expression of the system throughput. Our study shows that, though the MAI level and capture effect determine the basic behavior of the S-ALOHA, CCI significantly reduces the throughput of S-ALOHA and asynchronous CCI introduces an especially severe impairment. Our analytical framework is also able to incorporate additional channel conditions and system parameters like lognormal shadowing and the cellular cluster size.",
    "actual_venue": "Ieee T Vehicular Technology"
  },
  {
    "abstract": "Rendering volumetric scattering in real-time is a challenge due to the complex interactions between the light and the particles in the participating media. Assuming that a ray leaving the emitter is scattered only once along its path to the sensor, we propose to represent the extinction coefficient by a Gaussian mixture model. Then the model is trained with a large number of particles colliding that ray in an online way. A low-cost updating function based on the weighted maximum likelihood estimation is derived for the weighted stepwise expectation-maximization algorithm, which is fitted into the graphics pipeline as a stage of learning. This enables all those particles to contribute to the extinction on the fly without storing and sorting them together with respect to the emitter in a geometry pass. Our approach is able to accurately reconstruct the per-pixel transmittance of the opacity map for optically thick heterogeneous media in real-time but operate in bounded memory, using the recently introduced fragment shader critical section feature of the graphics processing unit.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "•An application to train for the assessment of respiratory conditions is presented.•OMARC integrates online interactive multimedia delivery with sample case studies.•OMARC was evaluated with a pilot study and a usability & user satisfaction survey.•Users found OMARC useful and easy to use, and would recommend it to others.•OMARC significantly improved nursing students capacity to identify lung sounds.",
    "actual_venue": "International Journal Of Medical Informatics"
  },
  {
    "abstract": "In this study, a positioning algorithm which is inspired by model matching type positioning systems is presented for swarm robotics. Unlike the conventional model matching systems, the system is designed to be operated as distributed. The algorithm consists of online and offline stages. While specific data collection and positioning are computed in the offline stage, specific data exchange is performed in the online stage. In the positioning algorithm to be used, a swarm robot system where each robot receives an image from a camera located under itself and where the robots share these images with each other and perform positioning is taken as a basis. The positions computed are the positions of the robots with respect to each other. Position estimation is based on feature detection and description from images. To determine the optimal positioning algorithm, performance comparison is performed among different combinations of feature detection and description algorithms.",
    "actual_venue": "Computers And Electrical Engineering"
  },
  {
    "abstract": "We study the problem of finding an outlier-free subset of a set of points (or a probability distribution) in n-dimensional Euclidean space. A point x is defined to be a &bgr;-outlier if there exists some direction w in which its squared distance from the mean along w is greater than &bgr; times the average squared distance from the mean along w [1]. Our main theorem is that for any &egr;0, there exists a (1-&egr;) fraction of the original distribution that has no O(\\frac{n}{&egr;}(b+log \\frac{n}{&egr;))-outliers, improving on the previous bound of O(n^7b/&egr;). This bound is shown to be nearly the best possible. The theorem is constructive, and results in a \\frac{1}{1-&egr;} approximation to the following optimization problem: given a distribution &mgr; (i.e. the ability to sample from it), and a parameter &egr;0, find the minimum &bgr; for which there exists a subset of probability at least (1-&egr;) with no &bgr;-outliers.",
    "actual_venue": "Stoc"
  },
  {
    "abstract": "Phantoms are frequently used in medical imaging systems to test hardware, reconstruction algorithms, and the interpretation of data. This report describes and characterizes the use of powdered graphite as a means of adding a significant reactive component or permittivity to useful phantom media for electrical impedance imaging. The phantom materials produced have usable complex admittivity at the ...",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "This paper discusses on determination of the workspace of the body of a quadruped walking robot, called “body workspace”, and its applicability in legged locomotion. The body workspace represents the set of all valid body configurations for a next step by considering three constraints of a body position: existence of the inverse kinematic solutions, reach-ability of the next swing leg to the next desired foothold, and static equilibrium of the robot when the next swing leg is lifted. The space contains all the body positions that ensure the existence of inverse kinematic solutions, is calculated in the first. Then, a subspace inside the determined space that allows the robot to reach the next desired foothold is analyzed. Finally, the workspace is obtained by excluding all the positions inside the subspace that do not ensure the equilibrium of the robot when the next swing leg is lifted. Therefore, the workspace shows all possible solutions for choosing the next body configuration of a given static walking problem. It is significant in improving the robot’s performances since moving body takes an intrinsic role in static walking, besides swinging a leg. The algorithm runs fast in real-time because it is a pure geometric method. The body workspace of a quadruped walking robot is visualized to help the understanding of the algorithm. In addition, applications of using the body workspace in improving the robot’s ability are presented to show potential applicability of the workspace.",
    "actual_venue": "Journal Of Intelligent And Robotic Systems"
  },
  {
    "abstract": "In this paper, we propose a method for enlarging quantization steps of a QIM watermarking scheme which determines the perceptual quality and robustness of the watermarked images. In general, increasing the quantization steps leads to good robustness but poor perceptual quality of watermarked images and vice versa. However, if we choose the quantization steps considering the expected quantization results as well as the original images, we can increase both robustness and perceptual quality of the watermarked images.",
    "actual_venue": "Euc Workshops"
  },
  {
    "abstract": "This study adopted portfolio assessment as a means of deepening pre-service teachers' understanding of teaching and learning. The ultimate goal of using the portfolio was to bring the program in line with the mission of the institute, the criteria of the NCATE and INTASC, and the standards of the Ohio State License. This study discusses the challenge of implementing a year-long portfolio assessment procedure, as well as investigating how the exit portfolio assessment plays a role in facilitating pre-service teachers' professional growth in terms of knowledge, skills, and dispositions. Results indicate that pre-service teachers considered the capstone portfolio as a tool for reflection, which helped them improve critical thinking skills, self-assessment, and advancement. Also, the portfolio process helped teacher candidates develop a professional identity and promote teaching. Overall, there was growth and improvement in knowledge, skills, and dispositions toward teaching, the role of a teacher and learner, and using the web-based portfolio process.",
    "actual_venue": "International Journal Of Online Pedagogy And Course Design"
  },
  {
    "abstract": "Practical Prolog programs usually contain extralogical features like cuts, side-effects, and database manipulating predicates. It is noted that, in order to exploit implicit parallelism from real applications, a parallel logic programming system should necessarily support these features. How Prolog's extralogical features can be supported in an And-Or parallel logic programming system is discussed. It is shown that to support extralogical features an And-Or parallel logic programming system should recompute the solutions to independent goals instead of sharing them. An abstraction called the composition tree for representing And-Or parallel execution with recomputation is described. The notion of 'local-leftmostness' in the composition tree is introduced and used for deriving complete and efficient methods for supporting extralogical predicates in And-Or parallel logic programming systems based on the composition tree abstraction.",
    "actual_venue": "Spdp"
  },
  {
    "abstract": "Avatars and Artificial Actors in Virtual Environments can be controlled by speech, as an alternative to motion capture techniques. In this paper, we discuss some specific requirements for the successful implementation of speech-based control of guided actors. We describe our sublanguage approach to speech-based control and its associated parsing techniques, based on lexicalised grammars. After an introduction to the REALISM animation software, we report work in progress in the real-time processing of spoken commands based on the integration of speech processing in the REALISM control loop. We conclude by discussing the possible impact of voice-controlled artificial actors in interactive systems.",
    "actual_venue": "Captech"
  },
  {
    "abstract": "ABSTRACTBackground: An increasing research effort has devoted to just-in-time (JIT) defect prediction. A recent study by Yang et al. at FSE'16 leveraged individual change metrics to build unsupervised JIT defect prediction model. They found that many unsupervised models performed similarly to or better than the state-of-the-art supervised models in effort-aware JIT defect prediction. Goal: In Yang et al.'s study, code churn (i.e. the change size of a code change) was neglected when building unsupervised defect prediction models. In this study, we aim to investigate the effectiveness of code churn based unsupervised defect prediction model in effort-aware JIT defect prediction. Methods: Consistent with Yang et al.'s work, we first use code churn to build a code churn based unsupervised model (CCUM). Then, we evaluate the prediction performance of CCUM against the state-of-the-art supervised and unsupervised models under the following three prediction settings: cross-validation, time-wise cross-validation, and cross-project prediction. Results: In our experiment, we compare CCUM against the state-of-the-art supervised and unsupervised JIT defect prediction models. Based on six open-source projects, our experimental results show that CCUM performs better than all the prior supervised and unsupervised models. Conclusions: The result suggests that future JIT defect prediction studies should use CCUM as a baseline model for comparison when a novel model is proposed.",
    "actual_venue": "Esem"
  },
  {
    "abstract": "This paper studies algorithms for the Disjunctive Temporal Problem (DTP) a quite general temporal reasoning problem introduced in [12]. This problem involves the satisfaction of a set of constraints represented by disjunctive formulas of the form x(1)-y(1) less than or equal to r(1) boolean OR x(2) - y(2) less than or equal to r(2) boolean OR ... boolean OR x(k) - y(k) less than or equal to r(k). The paper starts sketching why DTPs are potentially very useful in plan management applications, then analyzes the current solutions to DTP, and introduces a constraint-satisfaction problem solving algorithm where different aspects of current DTP's literature are integrated. This basic algorithm is then improved exploiting the quantitative temporal information in the \"distance graph\". Using this knowledge an incremental version of the forward checking is obtained and shown to be competitive with current best results. The whole approach allows to understand pros and cons of the current algorithms for the DTP and suggests further future developments as discussed in the final part of the paper.",
    "actual_venue": "Frontiers In Artificial Intelligence And Applications"
  },
  {
    "abstract": "Several schemes for computing sparse orthogonal factorizations using static data structures have been proposed recently. One novel feature of some of these schemes is that the data structures are large enough to store both the orthogonal transformations and upper triangular factor explicitly. However, in order to make use of the static storage schemes, the orthogonal factorization has to be computed without column interchanges, which is sufficient when the observation matrix has full rank. When the least squares matrix is rank-deficient, computing the minimum-norm solution to the least squares problem requires the knowledge of the rank of the matrix, which may be difficult to get if the factorization is computed without column pivoting. In this paper an algorithm is developed that makes use of the resulting factorization to solve rank-deficient least squares problems. The new algorithm has three major features. First, it uses the original triangular factor. Second, this factor is not altered. Third, the rank decision is based on the singular value decomposition and not on the diagonal elements of the triangular factor. The techniques used are similar to those employed by Bjorck.",
    "actual_venue": "Siam Journal On Scientific Computing"
  },
  {
    "abstract": "The paper presents an abstract interpretation of Prolog that derives information about the shape and sharing of structures to which variables are bound at runtime. The purpose is to support analysis of live structures, which in turn supports compile-time memory reuse. The abstract domain consists of graphs that use shared subgraphs to represent shared substructure in the heap. This sharing in the abstract representation is not new, though we are the first to use it in a context where unification must be modeled. The principal contribution is a system of annotations that increases the precision of the sharing information and the efficiency of the analysis.",
    "actual_venue": "Plilp"
  },
  {
    "abstract": "The success of peer-to-peer overlay networks depends on cooperation among participating peers. In this paper, we investigate the degree of cooperation among individual peers required to induce globally favorable properties in an overlay network. Specifically, we consider a resource pricing problem in a market-oriented overlay network where participating peers sell own resources (e.g., CPU cycles) to earn energy which represents some money or rewards in the network. In the resource pricing model presented in this paper, each peer sets the price for own resource based on the degree of cooperation; non-cooperative peers attempt to maximize their own energy gains, while cooperative peers maximize the sum of own and neighbors' energy gains. Simulation results are presented to demonstrate that the network topology is an important factor influencing the minimum degree of cooperation required to increase the network-wide global energy gain.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Stage lighting with high intensity discharge lamp is always regarded as an important method to present artistic scenery, stage lighting changed with rhythm and/or theatricality and/or voice would bring about more visual convulsion to the audiences. A novel flicking idea for stage lighting is proposed in this paper, the operation principle for light flicking with high pressure sodium lamp is analyzed, the flicking power is deduced, the design diagram and its basic functions are discussed in detail, with these basic functions combined, many dream effects can be gotten, a sample is also made for experiment test, and the local test shows that many combined effects can be realized with remote program to make the stage more splendid and evoke the audience's feelings, it is really a new selection for design of stage lighting.",
    "actual_venue": "IAS"
  },
  {
    "abstract": "As more and more cyber security incident data ranging from systems logs to vulnerability scan results are collected, manually analyzing these collected data to detect important cyber security events become impossible. Hence, data mining techniques are becoming an essential tool for real-world cyber security applications. For example, a report from Gartner [gartner12] claims that \\\"Information security is becoming a big data analytics problem, where massive amounts of data will be correlated, analyzed and mined for meaningful patterns\\\". Of course, data mining/analytics is a means to an end where the ultimate goal is to provide cyber security analysts with prioritized actionable insights derived from big data. This raises the question, can we directly apply existing techniques to cyber security applications? One of the most important differences between data mining for cyber security and many other data mining applications is the existence of malicious adversaries that continuously adapt their behavior to hide their actions and to make the data mining models ineffective. Unfortunately, traditional data mining techniques are insufficient to handle such adversarial problems directly. The adversaries adapt to the data miner's reactions, and data mining algorithms constructed based on a training dataset degrades quickly. To address these concerns, over the last couple of years new and novel data mining techniques which is more resilient to such adversarial behavior are being developed in machine learning and data mining community. We believe that lessons learned as a part of this research direction would be beneficial for cyber security researchers who are increasingly applying machine learning and data mining techniques in practice. To give an overview of recent developments in adversarial data mining, in this three hour long tutorial, we introduce the foundations, the techniques, and the applications of adversarial data mining to cyber security applications. We first introduce various approaches proposed in the past to defend against active adversaries, such as a minimax approach to minimize the worst case error through a zero-sum game. We then discuss a game theoretic framework to model the sequential actions of the adversary and the data miner, while both parties try to maximize their utilities. We also introduce a modified support vector machine method and a relevance vector machine method to defend against active adversaries. Intrusion detection and malware detection are two important application areas for adversarial data mining models that will be discussed in details during the tutorial. Finally, we discuss some practical guidelines on how to use adversarial data mining ideas in generic cyber security applications and how to leverage existing big data management tools for building data mining algorithms for cyber security.",
    "actual_venue": "Acm Conference On Computer And Communications Security"
  },
  {
    "abstract": "We propose to approach the detection of patients affected by schizophrenia by means of dissimilarity-based classification techniques applied to brain magnetic resonance images. Instead of working with features directly, pairwise distances between expert delineated regions of interest (ROIs) are considered as representations based on which learning and classification can be performed. Experiments were carried out on a set of 64 patients and60 controls and several pairwise dissimilarity measurements have been analyzed. We demonstrate that good results are possible and especially significant improvements can be obtained when combining over different ROIs and different distance measures. The lowest error rate obtained is 0.210.",
    "actual_venue": "Int J Imaging Systems And Technology"
  },
  {
    "abstract": "Droplet-based microfluidic biochips have recently gained much attention and are expected to revolutionize the biological laboratory procedure. As biochips are adopted for the complex procedures in molecular biology, its complexity is expected to increase due to the need of multiple and concurrent assays on a chip. In this paper, we formulate the placement problem of digital microfluidic biochips with a tree-based topological representation, called T-tree. To the best knowledge of the authors, this is the first work that adopts a topological representation to solve the placement problem of digital microfluidic biochips. Experimental results demonstrate that our approach is much more efficient and effective, compared with the previous unified synthesis and placement framework.",
    "actual_venue": "DAC"
  },
  {
    "abstract": "The goal of this article is to identify criteria used in industrial practice to select members of a software project team, and to look for relationships between these criteria and project success. Initially, using semi-structured interviews for data collection and qualitative methods for data analysis and synthesis, a set of team building criteria was identified from project managers in industry. Then, using this set of criteria and a method to evaluate software project success, a survey questionnaire was created to investigate the relationship between the consistent use or application of the criteria and project success. A cross-sectional survey collected data from 48 projects in 24 software companies and the data was analyzed for internal reliability using statistical methods. The findings show that the consistent use of the set of criteria correlated significantly with project success, and the criteria related to human factors present the strongest correlations.",
    "actual_venue": "Esem"
  },
  {
    "abstract": "Security and sustainability are two critical global challenges that involve the interaction of many intelligent actors. Game theory provides a sound mathematical framework to model such interactions, and computational game theory in particular has a promising role to play in helping to address key aspects of these challenges. Indeed, in the domain of security, we have already taken some encouraging steps by successfully applying game-theoretic algorithms to real-world security problems: our algorithms are in use by agencies such as the US coast guard, the Federal Air Marshals Service, the LAX police and the Transportation Security Administration. While these applications of game-theoretic algorithms have advanced the state of the art, this paper lays out some key challenges as we continue to expand the use of these algorithms in real-world domains. One such challenge in particular is that classical game theory makes a set of assumptions of the players, which may not be consistent with real-world scenarios, especially when humans are involved. To actually model human behavior within game-theoretic framework, it is important to address the new challenges that arise due to the presence of human players: (i) human bounded rationality; (ii) limited observations and imperfect strategy execution; (iii) large action spaces. We present initial solutions to these challenges in context of security games. For sustainability, we lay out our initial efforts and plans, and key challenges related to human behavior in the loop.",
    "actual_venue": "ADT"
  },
  {
    "abstract": "Two inductorless wideband low-noise amplifiers (LNAs) fabricated in a 65-nm CMOS process are presented. By using the gain-enhanced noise-canceling technique, the gain at noise-cancelling condition is increased, while the input matching is maintained. The first work is a common-source LNA with resistive shunt feedback. It achieves a maximum power gain of 10.5 dB, a bandwidth of 10 GHz, a noise figure (NF) of 2.7-3.3 dB, and an IIP3 of -3.5 dBm. The power consumption is 13.7 mW from a 1-V supply, and the area is 0.02 mm 2. The second work is a common-gate LNA. It achieves a maximum power gain of 10.7 dB, a bandwidth of 5.2 GHz, a NF of 2.9-5.4 dB, and an IIP3 of -6 dBm. The power consumption is 7 mW from a 1-V supply, and the area is 0.03 mm 2. Experimental results demonstrate that the first LNA shows the largest bandwidth, and the second LNA has the lowest power consumption among the inductorless wideband LNAs.",
    "actual_venue": "Ieee Transactions On Circuits And Systems -Regular Papers"
  },
  {
    "abstract": "The emergence of total quality management and the ISO 9000 suite of standards has allowed a re-think of how (and why) the postimplementation evaluation of computer systems is to be carried out. Traditional performance measurement, modeling and analysis techniques - while not discredited - have been tempered with a more holistic ideology. This article recommends a sociotechnical approach to determining the quality of a computer information system. In this context, two postulates have been proposed and tested by field survey of expert systems in the insurance industry in North America. Postulate one focuses on a multidimensional concept of IS quality comprising the characteristics of task, technology, people and organization. Postulate two deals with differences in assessments of these characteristics according to stakeholder groups: managers, developers, and users. Summarizes the key findings of these postulates in the context of the TQM and ISO 9000 philosophies.",
    "actual_venue": "Industrial Management And Data Systems"
  },
  {
    "abstract": "In Software Defined Networking (SDN), the severe conflict between rule number and memory size has attracted considerable academic attention. Ternary Content Addressable Memory (TCAM), generally used to guarantee the query speed, is a scarce and expensive resource, which limits the number of rules that the switch can support. However, the table miss may increase processing burden of the controller and cause latency issues. Therefore, it is significantly important to improve the efficiency of TCAM in SDN switches. In this paper, we propose BALANCER, a traffic-aware hybrid rule allocation scheme. In BALANCER, we logically split TCAM into two parts: reactive and proactive, which can be dynamically adjusted according to network traffic behavior. Also, we propose an algorithm to generate proactive rules with high entropy in the proactive part, and for the reactive part, we provide a rule caching approach and an efficient rule replacement algorithm, Multi-Bucket. To evaluate BALANCER, we conduct comprehensive experiments with both synthetic and real-world routing policies. Compared with the reactive mode and the proactive mode, results show that BALANCER achieves the least update costs while the number of table misses is extremely close to that in the proactive mode.",
    "actual_venue": "International Conference On Computer Communications And Networks"
  },
  {
    "abstract": "As an important threat to public security, urban fire accident causes huge economic loss and catastrophic collapse. Predicting and analyzing the interior rule of urban fire accident from its appearance needed to be solved in the field. In this paper, we propose a new urban fire accident prediction approach based on XGBoost. The method determines the predictive indexes in a quantitative and qualitative way from different characteristics in various kinds of fire accidents. For screening the features we need, we adopt the feature selection algorithm based on association rules. For data cleaning, we use a method based on Box-Cox transformation that transforms the continual response variables from the feature space for removing the dependencies on unobservable errors and the predictor variable to some extent. Then we use the data to train the model based on XGBoost to obtain the best prediction accuracy. Experiments show that the method provides a feasible solution to urban fire accident prediction. The method contributes to improving the public security situation, we have added the method and related model to the City in a box™, Shenzhen Aerospace Smart City System Technology Co., Ltd.",
    "actual_venue": "International Conference On Intelligent Systems And Knowledge Engineering"
  },
  {
    "abstract": "Optimization problems can become intractable when the search space undergoes tremendous growth. Heuristic optimization methods have therefore been created that can search the very large spaces of candidate solutions. These methods, also called metaheuristics, are the general skeletons of algorithms that can be modified and extended to suit a wide range of optimization problems. Various researchers have invented a collection of metaheuristics inspired by the movements of animals and insects (e.g., firefly, cuckoos, bats and accelerated PSO) with the advantages of efficient computation and easy implementation. This paper studies a relatively new bio-inspired heuristic optimization algorithm called the Wolf Search Algorithm (WSA) that imitates the way wolves search for food and survive by avoiding their enemies. The WSA is tested quantitatively with different values of parameters and compared to other metaheuristic algorithms under a range of popular non-convex functions used as performance test problems for optimization algorithms, with superior results observed in most tests.",
    "actual_venue": "Neural Computing And Applications"
  },
  {
    "abstract": "Current fixed grid wavelength routed networks are limited in terms of spectral efficiency due to the rigid nature of wavelength assignment. We propose the Flexible Optical WDM (FWDM) network architecture for flexible grid optical networks in which the constraint on fixed spectrum allocation to channels is removed and network resources can be dynamically provisioned with an automated control plane. In this paper, we address the routing, wavelength assignment, and spectrum allocation problem (RWSA) in transparent FWDM networks with the objective of maximizing spectral efficiency. We formulate the RWSA problem using an Integer Linear Program (ILP). We also prove the NP-completeness of the RWSA problem, and propose three efficient polynomial time algorithms; namely the Greedy-Routing, Wavelength Assignment, and Spectrum Allocation algorithm (Greedy-RWSA); the K-Alternate Paths Routing, Wavelength Assignment, and Spectrum Allocation algorithm (KPaths-RWSA); the Shortest Path Routing, Wavelength Assignment, and Spectrum Allocation algorithm (SP-RWSA). We analyze the lower bound on the required spectrum for the given network topology and a set of requests. Simulation results demonstrate that FWDM networks are efficient in terms of spectrum, cost, and energy compared to fixed grid networks. The performance of the proposed algorithms is very close to the lower bound, and approaches to the lower bound as problem size increases.",
    "actual_venue": "Optical Switching And Networking"
  },
  {
    "abstract": "Exergames have proven to be a fun way of engaging in physical activity, but analyses of the energy expenditure required to play exergames indicate that most exergame play is not sufficiently vigorous to replace traditional physical activity. In this paper, we argue that it is important to design for exertion in exergames. To illustrate this idea, we present a novel game mechanic, the heart rate power-up, which can be applied to a wide range of exergame styles. A study of 20 participants found that heart-rate power-ups increase exertion levels in games -- sometimes dramatically -- while also increasing players' level of enjoyment.",
    "actual_venue": "Chi Play"
  },
  {
    "abstract": "This paper focuses on the decision-implementation complexity (DIC) of cooperative games. The complexity of a control law is a fundamental issue in practice because it is closely related to control cost. First, we formulate implementation measures of strategies as system control protocols. Then, for a class of cooperative games, a decision-implementation system model is established, and an energy-based DIC index is given as the energy expectation under Nash equilibrium strategies. A definition of DIC is presented to describe the optimal values of the DIC index. DIC is calculated by the exhaust algorithm in some specific cases, whereas the one for general cases is too complex to be calculated. In order to obtain a general calculation method, the problem is described in the form of matrices; an analytical expression describing DIC is obtained by using the properties of matrix singular values. Furthermore, when only partial information of actions is shared among players, DIC can be reduced and an improved protocol can be designed as a two-phase protocol. A numerical example is given to show that the DIC obtained in this study is the same as the one obtained by the exhaust algorithm, and that the calculation complexity of the proposed algorithm is much lower.",
    "actual_venue": "Science China-Information Sciences"
  },
  {
    "abstract": "Low level local image processing is efficiently performed by array processors operating in SIMD mode. Performing mid-level regional image processing leads to use local combinatorial operators combined with an asynchronous programmable interconnection network. However, this approach has an important hardware cost because asynchronism implies the use of combinatorial operators with many inputs. This cost should be reduced for a dense VILSI implementation. To do so, we propose to increase the connectivity level of the interconnection network as a mean to use only 2-input asynchronous combinatorial operators. Results are presented on the example of the regional sum mid-level primitive in vision chips. An extension of the methodology to higher connectivity levels is then proposed.",
    "actual_venue": "Iscas"
  },
  {
    "abstract": "Graphs are now ubiquitous in almost every field of research. Recently, new research areas devoted to the analysis of graphs and data associated to their vertices have emerged. Focusing on dynamical processes, we propose a fast, robust and scalable framework for retrieving and analyzing recurring patterns of activity on graphs. Our method relies on a novel type of multilayer graph that encodes the spreading or propagation of events between successive time steps. We demonstrate the versatility of our method by applying it on three different real-world examples. Firstly, we study how rumor spreads on a social network. Secondly, we reveal congestion patterns of pedestrians in a train station. Finally, we show how patterns of audio playlists can be used in a recommender system. In each example, relevant information previously hidden in the data is extracted in a very efficient manner, emphasizing the scalability of our method. With a parallel implementation scaling linearly with the size of the dataset, our framework easily handles millions of nodes on a single commodity server.",
    "actual_venue": "Ieee Trans Signal And Information Processing Over Networks"
  },
  {
    "abstract": "Textural features are quite effective in pattern recognition and image analysis algorithms, but their use comes with high computational cost. In this work, we present novel implementations on graphics processors (GPUs) for fast calculations of a set of basic operations for color and texture analysis. Our approach focuses on CUDA programming for exploiting the inherent parallelism and computational power of GPUs from different perspectives, as our target operations possess diverse features: streaming operators favoured on GPUs, recursive procedures easier on CPUs, memory intensive kernels with assorted access patterns (requiring considerable programming effort to fully exploit the memory hierarchy), matrix operators demanding sustainable bandwidth, and mathematical functions with remarkable arithmetic intensity on ALUs and FPUs. Experimental results are compared among different GPU strategies and versus a multicore CPU implementation, leading to varying amounts of execution time speedup up to 500x on a low-cost platform recruited for high-performance computing like the commodity GeForce 8 from Nvidia.",
    "actual_venue": "Parallel Computing: From Multicores And Gpus To Petascale"
  },
  {
    "abstract": "Generally storage devices only administrate the mapping of logical data space to local physical space while unknowing other information related to data objects. Network or metadata service failure will disable the system, although the data is still on devices intact and correct. Utilizing the redundant computing ability of storage devices, the object interface makes it possible to assist the MDS for metadata services to improve the availability of the system. Though extended attributes of the object interface and computing resource, the management of the object allocation, and local metadata service are enabled in active object storage devices (A-OSD). Based on A-OSDs, a distributed high availability storage system (HASS) characterized by two-level metadata management can be built to achieve higher availability while maintaining good performance. Experiments indicate under the same environment, compared with Lustre, HASS has better write performance and similar read performance besides the potential high availability.",
    "actual_venue": "Fcst"
  },
  {
    "abstract": "This paper presents a method for using subjective factors to evaluate project success. The method is based on collection of subjective measures with respect to project characteristics and project success indicators. The paper introduces a new classification scheme for assessing software projects. Further, it is illustrated how the method may be used to predict software success using subjective measures of project characteristics. The classification scheme is illustrated in two case studies. The results are positive and encouraging for future development of the approach.",
    "actual_venue": "Information And Software Technology"
  },
  {
    "abstract": "The objective of testing is to determine whether an implementation under test conforms to its specification. In distributed test architectures involving multiple testers, this objective can be complicated by the fact that testers may encounter problems relating to controllability and observability during the application of tests. The controllability problem manifests itself when a tester is required to send the current input and because it did not send the previous input nor did it receive the previous output it cannot determine when to send the input. The observability problem manifests itself when a tester is expecting an output in response to either the previous input or the current input and because it is not the sender of the current input, it cannot determine when to start and stop waiting for the output. Based on a distinguishing sequence, a checking sequence construction method is proposed to yield a sequence that is free from controllability and observability problems.",
    "actual_venue": "Testcom"
  },
  {
    "abstract": "Research in compiler pass phase ordering (i.e., selection of compiler analysis/transformation passes and their order of execution) has been mostly performed in the context of CPUs and, in a small number of cases, FPGAs. In this paper we present experiments regarding compiler pass phase ordering specialization of OpenCL kernels targeting NVIDIA GPUs using Clang/LLVM 3.9 and the libclc OpenCL library. More specifically, we analyze the impact of using specialized compiler phase orders on the performance of 15 PolyBench/GPU OpenCL benchmarks. In addition, we analyze the final NVIDIA PTX assembly code generated by the different compilation flows in order to identify the main reasons for the cases with significant performance improvements. Using specialized compiler phase orders, we were able to achieve performance improvements over the CUDA version and OpenCL compiled with the NVIDIA driver. Compared to CUDA, we were able to achieve geometric mean improvements of (1.54times ) (up to (5.48times )). Compared to the OpenCL driver version, we were able to achieve geometric mean improvements of (1.65times ) (up to (5.70times )).",
    "actual_venue": "Euro-Par Workshops"
  },
  {
    "abstract": "A coherent description of enterprise architecture provides insight, enables communication among stakeholders and guides complicated change processes. Unfortunately, so far no enterprise architecture description language exists that fully enables integrated enterprise modeling, because for each architectural domain, architects use their own modeling techniques and concepts, tool support, visualization techniques, etc. In this paper, we outline such an integrated language and we identify and study concepts that relate architectural domains. In our language, concepts for describing the relationships between architecture descriptions at the business, application, and technology levels play a central role, related to the ubiquitous problem of business-ICT alignment, whereas for each architectural domain we conform to existing languages or standards such as UML. In particular, usage of services offered by one layer to another plays an important role in relating the behaviour aspects of the layers. The structural aspects of the layers are linked through the interface concept, and the information aspects through realization relations.",
    "actual_venue": "International Journal Of Cooperative Information Systems"
  },
  {
    "abstract": "GPUs and CPUs have fundamentally different architectures. It is conventional wisdom that GPUs can accelerate only those applications\n that exhibit very high parallelism, especially vector parallelism such as image processing. In this paper, we explore the\n possibility of using GPUs for value prediction and speculative execution: we implement software value prediction techniques\n to accelerate programs with limited parallelism, and software speculation techniques to accelerate programs that contain runtime\n parallelism, which are hard to parallelize statically. Our experiment results show that due to the relatively high overhead,\n mapping software value prediction techniques on existing GPUs may not bring any immediate performance gain. On the other hand,\n although software speculation techniques introduce some overhead as well, mapping these techniques to existing GPUs can already\n bring some performance gain over CPU. Based on these observations, we explore the hardware implementation of speculative execution\n operations on GPU architectures to reduce the software performance overheads. The results indicate that the hardware extensions\n result in almost tenfold reduction of the control divergent sequential operations with only moderate hardware (5–8%) and power\n consumption (1–5%) overheads.",
    "actual_venue": "International Journal Of Parallel Programming"
  },
  {
    "abstract": "This paper presents a highly scalable hardware solver for Blokus Duo. Based on flat Monte Carlo method, the proposed solver contains self-contained agents whose number is configurable and only limited by FPGA capacity, which makes the proposed solver highly scalable. Data structures and tile representations are tailored to support efficient memory usage and operations. Implementation result shows that an agent can operate at up to 150MHz while requiring less than 3000 LUTs on the Altera Cyclone II EP2C70F896C6 FPGA device. Simulation result shows the proposed solver can always win level 1 Pentobi.",
    "actual_venue": "Field-Programmable Technology"
  },
  {
    "abstract": "This paper presents and evaluates preferred patterns of vibrations and active breaking techniques for the Diastolic Timed Vibrator (DTV). DTV uses low frequency mechanical vibrations applied to the chest to help in clot dissolution in pre-hospitalization treatment of acute coronary ischemia. In this work, we argue that random and ramp type vibration patterns increase the performance of the DTV method. Furthermore, we present results for various methods of vibration stopping aiming at reduction of vibration overspill into the systole of heart cycle of the patient.",
    "actual_venue": "Embc"
  },
  {
    "abstract": "The popularity of Tor as an anonymity system has made it a popular target for a variety of attacks. We focus on traffic correlation attacks, which are no longer solely in the realm of academic research with recent revelations about the NSA and GCHQ actively working to implement them in practice.   Our first contribution is an empirical study that allows us to gain a high fidelity snapshot of the threat of traffic correlation attacks in the wild. We find that up to 40% of all circuits created by Tor are vulnerable to attacks by traffic correlation from Autonomous System (AS)-level adversaries, 42% from colluding AS-level adversaries, and 85% from state-level adversaries. In addition, we find that in some regions (notably, China and Iran) there exist many cases where over 95% of all possible circuits are vulnerable to correlation attacks, emphasizing the need for AS-aware relay-selection.   To mitigate the threat of such attacks, we build Astoria--an AS-aware Tor client. Astoria leverages recent developments in network measurement to perform path-prediction and intelligent relay selection. Astoria reduces the number of vulnerable circuits to 2% against AS-level adversaries, under 5% against colluding AS-level adversaries, and 25% against state-level adversaries. In addition, Astoria load balances across the Tor network so as to not overload any set of relays.",
    "actual_venue": "Ndss"
  },
  {
    "abstract": "As wireless hotspot business becomes a tremendous financial success, users of these networks have increasing motives to misbehave in order to obtain more bandwidth at the expense of other users. Such misbehaviors threaten the performance and availability of hotspot networks and have recently attracted increasing research attention. However, the existing work so far focuses on sender-side misbehavior. Motivated by the observation that many hotspot users receive more traffic than they send, we study greedy receivers in this paper. We identify a range of greedy receiver misbehaviors, and quantify their damage using both simulation and testbed experiments. Our results show that even though greedy receivers do not directly control data transmission, they can still result in very serious damage, including completely shutting off the competing traffic. To address the issues, we further develop techniques to detect and mitigate greedy receiver misbehavior, and demonstrate their effectiveness.",
    "actual_venue": "Ieee Trans Dependable Sec Comput"
  },
  {
    "abstract": "This paper analyzes the use of link quality, based on the Signal to Noise Ratio, as a metric to improve routing in multi-hop wireless networks. We compare the behavior and performance with those of other approaches, namely a simple minimum-hop selection process, as well as the Expected Transmission Count (ETX) metric, which has recently gathered relevant attention from the scientific community. We use a simulation based analysis, but we account for a realistic channel model, able to mimic the Gray Zones effect, which has been proven to have a great impact on wireless-based communications in general and over multi-hop topologies in particular. The results show that important improvements can be brought about by making use of the proposed metric to enhance route selection procedures.",
    "actual_venue": "Ieee Vehicular Technology Conference Fall, Vols"
  },
  {
    "abstract": "This paper reports on an interview study on information security incident management that has been conducted in organizations operating industrial control systems that are highly dependent on conventional IT systems. Six distribution service operators from the power industry have participated in the study. We have investigated current practice regarding planning and preparation activities for incident management, and identified similarities and differences between the two traditions of conventional IT systems and industrial control systems. The findings show that there are differences between the IT and ICS disciplines in how they perceive an information security incident and how they plan and prepare for responding to such. The completeness of documented plans and procedures for incident management varies. Where documentation exists, this is in general not well-established throughout the organization. Training exercises with specific focus on information security are rarely performed. There is a need to create amore unified approach to information security incident management in order for the power industry to be sufficiently prepared to meet the challenges posed by Smart Grids in the near future.",
    "actual_venue": "IMF"
  },
  {
    "abstract": "Currently, peer-to-peer file sharing systems are playing a dominant role in the content distribution over Internet. Therefore understanding the impact of peer-to-peer traffic on large scale networks is significant and instrumental for the design of new systems. In this paper, we focus on Maze, one of most popular P2P systems on CERNET. We perform a systematic characterization of Maze’s [1] traffic impact on CERNET. We investigate the traffic volume and bandwidth on different spatial levels aggregation. According to our log-based analysis, we claim that current P2P systems have much room to improve in reducing backbone network consumption. Locality-aware content delivering mechanism can reduce the traffic on backbone network effectively. Moreover, a system with less free-rider[3] will further reduce the traffic consumption. Thus the designers of P2P system should pay more attention on incentive mechanism to reduce free-rider.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": "The purpose of this paper is to present a scheduling solution based on information theory principles to schedule real-time tasks. We propose the mathematical background for using information as a parameter in real-time systems as well as the relationship between information and utilization. We present a new dynamic priority scheduling solution that selects the task with the highest amount of information per studied interval. We propose the feasibility analysis of the scheduling solution and we compare its performance against the Earliest Deadline First (EDF) scheduling algorithm using as dependent variables the number of context switches and the number of preemptions. We generated 16 test files (with 100 synthetic task sets per file) using as independent variables: (a) Utilization (from 70% to 100%), (b) Number of tasks per Task set (from 2 to 5) and (c) Hyper-period (fixed at 40). The results showed that: (a) Our scheduling solution improves the performance of EDF by 1.1384% in terms of the number of context switches and 2.0428% in terms of the number of preemptions for the studied task sets; (b) The similarity ratio (number of similar schedules) between the two algorithms was 42.68%.",
    "actual_venue": "Annual Conference On Information Sciences And Systems"
  },
  {
    "abstract": "Active Antenna System (AAS) is an advanced antenna technology that features the ability of advanced beam-forming techniques to provide a great flexibility in cellular network deployment which enables improvements in network capacity and coverage. Conventionally, network dimensioning is done based on busy hour traffic leading to cost-intensive over-dimensioning for most of the time via deploying additional macro and small cells. In AAS, however, varying traffic concentrations can be flexibly handled by dynamic cell densification, e.g. by splitting a sector into smaller “sub-sectors”. Vertical sectorization is a well-known approach where a conventional sector is split vertically in to two, inner and outer sectors, resulting in 3×2 sectors per site for AAS-based tri-sectorized site. In this paper work, an alternative vertical sectorization deployment configuration is presented where the inner sectors build a so called super-cell resulting from transmitting the same cell information in all inner sectors. Investigation results show that the super-cell configuration can mitigate unwanted back and side lobe effects in close proximity of the site and, therefore, provides a significant gain for users in this coverage area.",
    "actual_venue": "Communications"
  },
  {
    "abstract": "GNSS signals affected by additive channel noise are approximately Gaussian, but often the presence of interference may change that distribution.We propose a blind detection method based on the Giannakis-Tsatsanis Gaussianity test to evaluate the presence of narrowband and wideband non-Gaussian interference. This method, resorting to the sample fourth-order autocumulants, is compared to other Gaussianity tests. The asymptotic covariance matrix of the estimated autocumulant vector is analytically determined, thus reducing the complexity of the proposed technique.",
    "actual_venue": "Ieee Trans Aerospace And Electronic Systems"
  },
  {
    "abstract": "Curve fragments, as opposed to unorganized edge elements, are of interest and use in a large number of applications such as multiview reconstructions, tracking, motion-based segmentation, and object recognition. A large number of contour grouping algorithms have been developed, but progress in this area has been hampered by the fact that current evaluation methodologies are mainly edge-based, thus ignoring how edges are grouped into contour segments. We show that edge-based evaluation schemes work poorly for the comparison of curve fragment maps, motivating two novel developments: (i) the collection of new human ground truth data whose primary representation is contour fragments and where the goal of collection is not distinguished objects but curves evident in image data, and (ii) a methodology for comparing two sets of curve fragments which takes into account the instabilities inherent in the formation of curve fragments. The approach compares two curve fragment sets by exploring deformation of one onto another while traversing discontinuous transitions. The geodesic paths in this space represent the best matching between the two sets of contour fragment. This approach is used to compare the results of edge linkers on the new contour fragment human ground truth.",
    "actual_venue": "Cvpr Workshops"
  },
  {
    "abstract": "Carriers find Network Function Virtualization (NFV) and multi-cloud computing a potent combination for deploying their network services. The resulting virtual network services (VNS) offer great flexibility and cost advantages to them. However, vesting such services with a level of performance and availability akin to traditional networks has proved to be a difficult problem for academics and practitioners alike. There are a number of reasons for this complexity. The challenging nature of management of fault and performance issues of NFV and multi-cloud based VNSs is an important reason. Rule-based techniques that are used in the traditional physical networks do not work well in the virtual environments. Fortunately, machine and deep learning techniques of Artificial Intelligence (AI) are proving to be effective in this scenario. The main objective of this tutorial is to understand how AI-based techniques can help in fault detection and localization to take such services closer to the performance and availability of the traditional networks. A case study, based on our work in this area, has been included for a better understanding of the concepts.",
    "actual_venue": "Computer Networks"
  },
  {
    "abstract": "We present an approach to parallel iterative four-atom quantum mechanics calculations in a computing environment of distributed memory nodes, each node consisting of a group of processors with a shared memory. We parallelize the action of the Hamiltonian matrix on a vector, which is the main computational bottleneck in both iterative calculations of eigenvalues and eigenvectors and the iterative determination of quantum dynamics information via, e.g., wavepacket methods. OpenMP is used to facilitate the parallel work within each node, and MPI is used to communicate information between nodes. For a realistic problem the approach is shown to scale very well up to 512 processors at the NERSC computing facility, working at up to 20% of the theoretical peak performance rate. The highest total floating point rate we achieve is 0.16 Tflops, using 768 processors. Our approach should also be applicable to quantum dynamics problems with more than four atoms.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "The frequency importance function of Mandarin Chinese (FIF-C) is estimated.Main importance concentrates in 1-2.5kHz bands for FIF-C, similar to English.FIF values are bigger for Chinese than for English in 0.16-, 1.6- and 2kHz bands.The language differences are discussed to interpret the FIF difference.The effect of speaker gender is considered and discussed. The speech intelligibility index (SII) is a widely used objective method of predicting speech intelligibility, in which the frequency importance function (FIF) is a key component. The FIF characterizes the relative contribution of different frequency bands to speech recognition. In this work, FIFs for Mandarin Chinese were derived for monosyllabic words spoken by male and female speakers. These words were phoneme balanced and selected from the word lists of a national standard, which have been used for measuring the articulation index in China since 1995. A pilot experiment was conducted to determine suitable signal-to-noise ratios (SNR) for measuring speech intelligibility. The main experiment was conducted to derive the FIFs using 288 test conditions (4 SNRs×36 filtering conditions×2 speaker genders). The noise was speech-spectrum shaped and it was generated separately for the male and female speech materials. The results show that, using 1/3 octave analysis bands: (1) The FIF averaged across genders has a peak in the frequency range between 1000 and 2500Hz, which is consistent with the FIF for English monosyllabic words; (2) The frequency bands centered at 160, 1600, and 2000Hz are slightly more important for Mandarin Chinese than for English; (3) Male speech is more intelligible than female speech, and the band centered at 160Hz is more important for female than male speech. The FIF differences between Mandarin and English and the effect of speaker gender are analyzed and discussed.",
    "actual_venue": "Speech Communication"
  },
  {
    "abstract": "Improving the safety of vulnerable road users is an international demand because still thousands of pedestrians and cyclists are injured or killed in accidents with cars, trucks and busses. Car manufacturers and several research groups work on different solutions to avoid accidents with the help of sensor based systems. Moreover, research is done in radio communication based collision avoidance systems which uses exchanged information to distinguish between dangerous and non dangerous situations. In this paper we present various filters differing by the input information used to find dangerous situations implemented in a radio based collision avoidance system simulator. The results of the filters applied to a scenario are also presented. Furthermore, we analyse the influence of different conditional parameters to transmission delay times by taking further measurements for cellular networks to determine if cellular networks can be used as a communication technology in a collision avoidance system.",
    "actual_venue": "Vehicular Technology Conference Fall"
  },
  {
    "abstract": "Structured knowledge bases are an increasingly important way for storing and retrieving information. Within such knowledge bases, an important search task is finding similar entities based on one or more example entities. We present QBEES, a novel framework for defining entity similarity based on structural features, so-called aspects and maximal aspects of the entities, that naturally model potential interest profiles of a user submitting an ambiguous query. Our approach based on maximal aspects provides natural diversity awareness and includes query-dependent and query-independent entity ranking components. We present evaluation results with a number of existing entity list completion benchmarks, comparing to several state-of-the-art baselines.",
    "actual_venue": "J Intell Inf Syst"
  },
  {
    "abstract": "This paper reviews and analyzes five parasitic capacitance cancellation methods. Critical parameters and constraints determining the cancellation frequency ranges are identified, and the effective frequency range for each cancellation method is derived based on these constraints. Due to these constraints, each method has specific advantages for certain applications. The cancellation techniques, which all make use of either mutual capacitance or mutual inductance, are applied to different applications based on their advantages, and the experiments are carried out to verify the analysis.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Abstract   Graph-based semi-supervised learning (SSL) provides a powerful framework for the modeling of manifold structures in high-dimensional spaces. Additionally, graph representation is effective for the propagation of the few initial labels existing in training data. Graph-based SSL requires robust graphs as input for an accurate data mining task, such as classification. In contrast to most graph construction methods, which ignore the labeled instances available in SSL scenarios, a previous study proposed a graph-construction method, named GBILI, to exploit the informativeness conveyed by such instances available in a semi-supervised classification domain. Here, we have improved the method proposing an optimized algorithm referred to as Robust Graph that Considers Labeled Instances (RGCLI) for the generation of more robust graphs. The contributions of this paper are threefold: i) reduction of GBILI time complexity from quadratic to     O  (  nk  log  n  )    . This enhancement allows addressing large datasets; ii) demonstration of RGCLI mathematical properties, proving the constructed graph is an optimal graph to model the smoothness assumption of SSL; and iii) evaluation of the efficacy of the proposed approach in a comprehensive semi-supervised classification scenario with several datasets, including an image segmentation task, which needs a large graph to represent the image. Such experiments show the use of labeled vertices in the graph construction process improves the graph topology, hence, the learning task in which it will be employed.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Communication between mobile agents and their users is an interesting but largely unresearched topic with important applications.\n We investigate the various types of interaction required and propose a versatile, easy-to-implement and secure WWW-based method\n to allow mobile agents to interact with arbitrary users.",
    "actual_venue": "Mobile Agents"
  },
  {
    "abstract": "Kernel representations provide a nonlinear representation, through similarities to prototypes, but require only simple linear learning algorithms given those prototypes. In a continual learning setting, with a constant stream of observations, it is critical to have an efficient mechanism for sub-selecting prototypes amongst observations. In this work, we develop an approximately submodular criterion for this setting, and an efficient online greedy submodular maximization algorithm for optimizing the criterion. We extend streaming submodular maximization algorithms to continual learning, by removing the need for multiple passes—which is infeasible—and instead introducing the idea of coverage time. We propose a general block-diagonal approximation for the greedy update with our criterion, that enables updates linear in the number of prototypes. We empirically demonstrate the effectiveness of this approximation, in terms of approximation quality, significant runtime improvements, and effective prediction performance.",
    "actual_venue": "Icml"
  },
  {
    "abstract": "In this study, a method of estimating unambiguous across-track velocity of moving targets is presented. The method can be used for ground moving target indication mode of multichannel synthetic aperture radar (SAR) systems. The basic idea is to decompose the unambiguous velocity into the fractional and the integer parts, and to estimate them individually. As such, an integrated velocity estimation scheme is proposed. In the scheme, the idea of clutter suppression interferometry is adopted to estimate the fractional velocity part via the along-track interferometry technique. For estimating the integer velocity part, the ideas behind the multilook beat frequency are adapted and applied to coarsely estimate the unambiguous velocity first, and then the coarsely estimated unambiguous velocity is combined with the fractional velocity part to obtain an estimate of the ambiguity number. Finally, the ambiguity number and the fractional velocity part are combined to otain an accurate estimate of the unambiguous velocity. The proposed algorithm is tested with real SAR data. The results show that the integrated scheme represents an accurate solution to the unambiguous across-track velocity estimation problem.",
    "actual_venue": "Iet Signal Processing"
  },
  {
    "abstract": "This paper deals with maximum likelihood (ML) classification of digital communication signals. We first propose a new approximation of the average likelihood function. Then we introduce the General Maximum Likelihood Classifier (GMLC) based on this approximation which can be applied to a wide range of classification problem involving finite mean power signals. Derivation of this classifier equations are given in the case of linear modulations with an application to the MPSK / M PSK problem. We show that the new tests are a generalization of the previous ones using ML approach, and don't need any restriction on the baseband pulse. Moreover, the GMLC provides a theoretical foundation for many empirical classification systems including those of that exploit cyclostationarity property of digital modulated signals.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "Text detection in scene image has become a hot topic in computer vision and artificial intelligence research, due to its wide range of applications and challenges. Most state-of-the-art methods for text detection based on deep learning rely on text bounding box regression. These methods can not well handle the case that if the scene text is curved. In this paper, we propose a new framework for arbitrarily oriented text detection in natural images based on fully convolutional neural networks. The main idea is to represent a text instance by two forms: text center block and word stroke region. These two elements are detected by two fully convolutional networks, respectively. Final detections are produced by the word region surrounding box algorithm. The proposed method does not need to regress the extant bounding box of the text instance, mainly because the predicted text block region itself implicitly contains position and orientation information. Besides, our method can well handle text in different languages, arbitrary orientations, curved shape and various fonts. To validate the effectiveness of the proposed method, we perform experiments on three public datasets: MSRA-TD500, USTB-SV1K and ICDAR2013, and compare it with other state-of-the-art methods. Experiment results demonstrate that the proposed method achieves competitive results. Based on VGG-16, our method achieves an F-measure of 78.84% on MSRA-TD500, 59.34% on USTB-SV1K, and 88.21% on ICDAR2013.",
    "actual_venue": "Multimedia Tools And Applications"
  },
  {
    "abstract": "The web has become a major source of information to learn about a topic. With the continuous growth of information and its high connectivity, it is hard to follow only the links that are relevant and not to get lost in hyperspace. Our aim is to support people who read documents in a highly connected information space, helping them remain on focus. Our contextually-aware in-browser text summarisation tool, IBES, does this by capturing users' current interests and providing users with contextualised summaries of linked documents, to help them decide whether the link is worth following.",
    "actual_venue": "Umap"
  }
]