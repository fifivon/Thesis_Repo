[
  {
    "abstract": "SIMPLE (Semi-Implicit Method for Pressure-Linked Equations) algorithm is important in the simulation of steady flows. As the traditional 3-D SIMPLE algorithm is time-consuming, we propose a parallel SIMPLE algorithm based on a novel tiling strategy -- alternate tiling, through replacing the original linear system and reordering the iteration space tiles. The novelty of our parallel algorithm lies in the introduction of the sequence of iteration space tiles as the sequence of execution, the time skewing technique to partition the iteration space, update operations of the grids from two directions alternately, and the improvement of the data locality. The effectiveness of the parallel algorithm and serial model of finite difference stencil algorithm are validated. Numerical experiments on distributed clusters show that the cache misses and the cost of communication and synchronization are reduced by reordering the tiles of iteration space, and the parallel SIMPLE algorithm based on alternate tiling has a good data locality and parallel efficiency in the three-dimensional incompressible pipe flow project.",
    "actual_venue": "Ccgrid"
  },
  {
    "abstract": "In this work, we give an overview of pseudoinverse learning (PIL) algorithm as well as applications. PIL algorithm is a non-gradient descent algorithm for multi-layer perception. The weight matrix of network can be exactly computed by PIL algorithm. So PIL algorithm can effectively avoid the problem of low convergence and local minima. Moreover, PIL does not require user-selected parameters, such as step size and learning rate. This algorithm has achieved good application in the fields of software reliability engineering, astronomical data analysis and so on.",
    "actual_venue": "Advances In Neural Networks - Isnn , Pt"
  },
  {
    "abstract": "•Multipath interference mitigation in GNSS receivers is formulated as a joint state and time-varying parameter estimation problem.•The proposed joint state and time-varying parameter estimation problem in the context of multipath mitigation is conducted using an expectation-maximization (EM) algorithm.•The convergence of the proposed EM-based multipath mitigation algorithm is analyzed.•The proposed localization algorithm shows a good robustness in multipath environments.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "We consider multi-objective scheduling with a single machine and multiple vehicles.The goal is to minimize vehicle delivery and total customer waiting time.We propose a PD-NSGA-II algorithm for this NP-hard problem.The performance of the algorithm is tested through random data.It is shown that the algorithm can offer high-quality solutions in reasonable time. This paper considers a class of multi-objective production-distribution scheduling problem with a single machine and multiple vehicles. The objective is to minimize the vehicle delivery cost and the total customer waiting time. It is assumed that the manufacturer's production department has a single machine to process orders. The distribution department has multiple vehicles to deliver multiple orders to multiple customers after the orders have been processed. Since each delivery involves multiple customers, it involves a vehicle routing problem. Most previous research work attempts at tackling this problem focus on single-objective optimization system. This paper builds a multi-objective mathematical model for the problem. Through deep analysis, this paper proposes that for each non-dominated solution in the Pareto solution set, the orders in the same delivery batch are processed contiguously and their processing order is immaterial. Thus we can view the orders in the same delivery batch as a block. The blocks should be processed in ascending order of the values of their average workload. All the analysis results are embedded into a non-dominated genetic algorithm with the elite strategy (PD-NSGA-II). The performance of the algorithm is tested through random data. It is shown that the proposed algorithm can offer high-quality solutions in reasonable time.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "In this study, we propose a 3D generalized micro heat transfer model in an N-carrier system with the Neumann boundary condition in spherical coordinates, which can be applied to describe the non-equilibrium heating in biological cells. Two improved unconditionally stable Crank-Nicholson schemes are then presented for solving the generalized model. In particular, we delicately adjust the location of the interior grid point that is next to the boundary so that the Neumann boundary condition can be applied directly without discretization. As such, a second-order accurate finite difference scheme without using any fictitious grid points is obtained. The convergence rates of the numerical solution are tested by an example. Results show that the convergence rates of the present schemes are about 2.0 with respect to the spatial variable r, which improves the accuracy of the Crank-Nicholson scheme coupled with the conventional first-order approximation for the Neumann boundary condition.",
    "actual_venue": "J Computational Applied Mathematics"
  },
  {
    "abstract": "In this article, a novel observation-to-generalization place model is proposed. It is shown how this model can be used to formally define the problem of finding geographically similar places. The observation-to-generalization model differentiates between observations of phenomena in the environment at a specific location and time, and generalizations about places that are inferred from these observations. A suite of operations is defined to find similar places based on the invariance of generalized place properties, and it is demonstrated how these functions can be applied to the problem of finding similar places based on the topics that people write about in place descriptions. One use for similar-place search is for exploratory research that will enable investigators to perform case–control studies on place data.",
    "actual_venue": "Journal Of Geographical Systems"
  },
  {
    "abstract": "Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a \"meta-component system\", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the \"meta-component system\" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.",
    "actual_venue": "SAC"
  },
  {
    "abstract": "Prior knowledge has been considered as valuable supplementary information in many image processing techniques. In this paper, we take the input image itself as the guidance prior and develop a novel fuzzy clustering algorithm to segment it by adding a new term to the objective function of Fuzzy C-Means. The new term comes from Guided Filter for its capability in noise suppression and edge-preserving smoothing. As a result, the memberships derived from the new objective function incorporate the guidance information from the image to be segmented. In this way, the segmentation result retains more subtle details on the boundaries of segments. According to experimental results, the proposed method shows good performance in image segmentation tasks especially for images with high noise rates.",
    "actual_venue": "International Journal Of Fuzzy Systems"
  },
  {
    "abstract": "In many online social systems, social ties between users play an important role in dictating their behavior. One of the ways this can happen is through social influence, the phenomenon that the actions of a user can induce his/her friends to behave in a similar way. In systems where social influence exists, ideas, modes of behavior, or new technologies can diffuse through the network like an epidemic. Therefore, identifying and understanding social influence is of tremendous interest from both analysis and design points of view. This is a difficult task in general, since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network. Distinguishing influence from these is essentially the problem of distinguishing correlation from causality, a notoriously hard statistical problem. In this paper we study this problem systematically. We define fairly general models that replicate the aforementioned sources of social correlation. We then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available. We give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation. We also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network (from Flickr) according to one of several models. Simulation results confirm that our test performs well on these data. Finally, we apply them to real tagging data on Flickr, exhibiting that while there is significant social correlation in tagging behavior on this system, this correlation cannot be attributed to social influence.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "Mistuning, imperfections in cyclical symmetry of bladed disks is an inevitable and perilous occurrence due to many factors including manufacturing tolerances and in-service wear and tear. It can cause some unpredictable phenomena such as mode splitting, mode localization and dramatic difference in forced vibration response. In this paper first, a method is presented which calculates the forced vibration response of a mistuned system based on an exact relationship between tuned and mistuned systems. Then, the genetic algorithm is used for solving an optimization problem to find the worst-case response of bladed-disk assembly. The second part tries to find methods to reduce the system worst-case response. Intentional mistuning which breaks the nominal symmetry of a tuned bladed disk and rearranging the bladed-disk assembly are introduced and used to reduce the system worst-case response. Finally, a two degree of freedom per blade simplified model with 56 blades is used to demonstrate the capabilities of the techniques in reducing the worst response of the bladed-disk system.",
    "actual_venue": "Mathematics And Computers In Simulation"
  },
  {
    "abstract": "This paper shows the application of a type of formal software verification technique known as lightweight model checking to a domain model in healthcare informatics in general and public health surveillance systems in particular. One of the most complex use cases of such a system is checked using assertions to verify one important system property. This use case is one of the major justifications for the complexity of the domain model. Alloy Analyzer verification tool is utilized for this purpose. Such verification work is very effective in either uncovering design flaws or in providing guarantees on certain desirable system properties in the earlier phases of the development lifecycle of any critical project.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "As today's standard screening methods frequently fail to diagnose breast cancer before metastases have developed, earlier breast cancer diagnosis is still a major challenge. Three-dimensional ultrasound computer tomography promises high-quality images of the breast, but is currently limited by a time-consuming synthetic aperture focusing technique based image reconstruction. In this work, we investigate the acceleration of the image reconstruction by a GPU, and by the FPGAs embedded in our custom data acquisition system. We compare the obtained performance results with a recent multi-core CPU and show that both platforms are able to accelerate processing. The GPU reaches the highest performance. Furthermore, we draw conclusions in terms of applicability of the accelerated reconstructions in future clinical application and highlight general principles for speed-up on GPUs and FPGAs.",
    "actual_venue": "Design And Architectures For Signal And Image Processing"
  },
  {
    "abstract": "3-D integration provides a means to overcome the difficulties in design and manufacturing of system-on-chip (SOC) and memory products. Introducing a short vertical interconnect, called through-silicon via (TSV), makes it feasible to repair and recycle bad dies by stacking. We propose a method to accomplish this using a dual-TSV hardwired switch (DTHS) in which the via-hole location is programmable. With the DTHS, we activate a spare and establish inter-die routing. The spare is nothing but a good part in another bad die. To be 3-D reparable, the design is partitioned into disjoint parts. The effort for the modification is minor in view of that a typical SOC is readily composed of modules with predefined functions and supply voltages. The DTHS is used: 1) to shut off power connections of both failed and unused parts; 2) to disconnect their signal paths; and 3) to redirect them to the selected good parts in the stacked dies. Despite the speed is degraded due to the extra load incurred by the DTHS, our simulation shows that the increase in delay time can be limited below 100 ps with an over-designed buffer which occupies 0.8% of the area of a 30 μm TSV, using a 65-nm CMOS process. The performance degradation turns out to be a necessary evil, since the increased height of the die stack leads to a thermal conductivity poorer than its 2-D counterpart. The 3-D patch die helps to shorten time-to-market and turn the irreparable dies profitable.",
    "actual_venue": "Ieee Transactions On Very Large Scale Integration Systems"
  },
  {
    "abstract": "Cellular networks (e.g., 3G) are currently facing severe traffic overload problems caused by excessive traffic demands. Offloading part of the cellular traffic through other forms of networks, such as Delay Tolerant Networks (DTNs) and WiFi hotspots, is a promising solution. However, since these networks can only provide intermittent connectivity to mobile users, utilizing them for cellular traffic offloading may result in a nonnegligible delay. As the delay increases, the users' satisfaction decreases. In this paper, we investigate the tradeoff between the amount of traffic being offloaded and the users' satisfaction. We provide a novel incentive framework to motivate users to leverage their delay tolerance for cellular traffic offloading. To minimize the incentive cost given an offloading target, users with high delay tolerance and large offloading potential should be prioritized for traffic offloading. To effectively capture the dynamic characteristics of users' delay tolerance, our incentive framework is based on reverse auction to let users proactively express their delay tolerance by submitting bids. We further illustrate how to predict the offloading potential of the users by using stochastic analysis for both DTN and WiFi cases. Extensive trace-driven simulations verify the efficiency of our incentive framework for cellular traffic offloading.",
    "actual_venue": "Ieee Transactions On Mobile Computing"
  },
  {
    "abstract": "Physicians are confronted with increasingly complex patient histories based on which they must make life-critical treatment decisions. At the same time, clinical researchers are eager to study the growing databases of patient histories to detect unknown patterns, ensure quality control, and discover surprising outcomes. Designers of Electronic Health Record systems (EHRs) have great potential to apply innovative visual methods to support clinical decision-making and research. This work surveys the state-of-the-art of information visualization systems for exploring and querying EHRs, as described in the scientific literature. We examine how systems differ in their features and highlight how these differences are related to their design and the medical scenarios they tackle. The systems are compared on a set of criteria: (1) data types covered, (2) multivariate analysis support, (3) number of patient records used (one or multiple), and (4) user intents addressed. Based on our survey and evidence gained from evaluation studies, we believe that effective information visualization can facilitate analysis of EHRs for patient treatment and clinical research. Thus, we encourage the information visualization community to study the application of their systems in health care. Our monograph is written for both scientific researchers and designers of future user interfaces for EHRs. We hope it will help them understand this vital domain and appreciate the features and virtues of existing systems, so they can create still more advanced systems. We identify potential future research topics in interactive support for data abstraction, in systems for intermittent users, such as patients, and in more detailed evaluations.",
    "actual_venue": "Foundations And Trends In Human-Computer Interaction"
  },
  {
    "abstract": "Convergence of conditional expectations is considered. The convergence results for conditional expectations are shown to be the same as when the absolutely continuous change of measure can be made directly. A filtering application is examined. If the approximating model uses noise increments with a density which must have compact support, conditions are given which ensure the expected convergence of the conditional expectations",
    "actual_venue": "Information Theory, IEEE Transactions  "
  },
  {
    "abstract": "A new noise reduction method suitable for autonomous mobile robots was proposed and applied to pre-processing of a hands-free spoken dialogue system. When a robot talks with a conversational partner in real environments, not only speech utterances by the partner but also various types of noise, such as directional noise, diffuse noise, and noise from the robot, are observed at microphones. We attempted to remove these types of noise simultaneously with small and light-weighted devices and low-computational-cost algorithms. We assumed that the conversational partner of the robot was in front of the robot. In this case, the aim of the proposed method is extracting speech signals coming from the frontal direction of the robot. The proposed noise reduction system was evaluated in the presence of various types of noise: the number of word errors was reduced by 69% as compared to the conventional methods. The proposed robot auditory system can also cope with the case in which a conversational partner (i.e., a sound source) moves from the front of the robot: the sound source was localized by face detection and tracking using facial images obtained from a camera mounted on an eye of the robot. As a result, various types of noise could be reduced in real time, irrespective of the sound source positions, by combining speech information with image information.",
    "actual_venue": "St Louis, Mo"
  },
  {
    "abstract": "With the increasing concerns of security, the secure deletion for SSDs becomes very costly due to its out-of-place update (i.e., an update is performed in a new location leaving the old data un-touched). Some previous studies used a combined erase-based and cryptography-based method to find a heuristic deletion scheme. However, the deletion overhead is still large since a key may be associated with too many pages. Therefore, how to reduce the secure deletion overhead further is becoming an interesting research issue. In this paper, a temperature-aware secure deletion scheme (TASecure) is proposed. Data with the same temperature are stored in the same region in order to reduce the overhead of secure deletion. The temperature of a data is referred to the frequency of the data is updated. Moreover, the regions based on their temperature are associated with different numbers of keys and are applied with different secure deletion schemes. Finally, the experimental results indicate that our proposed secure deletion scheme can reduce the secure deletion cost about 1.2x to 19x compared to previous studies.",
    "actual_venue": "Proceedings Of The On Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "Topology-preserving geometric deformable models (TGDMs) are used to segment objects that have a known topology. Their accuracy is inherently limited, however, by the resolution of the underlying computational grid. Although this can be overcome by using fine-resolution grids, both the computational cost and the size of the resulting surface increase dramatically. In order to maintain computational efficiency and to keep the surface mesh size manageable, we have developed a new framework, termed OTGDMs, for topology-preserving geometric deformable models on balanced octree grids (BOGs). In order to do this, definitions and concepts from digital topology on regular grids were extended to BOGs so that characterization of simple points could be made. Other issues critical to the implementation of OTGDMs are also addressed. We demonstrate the performance of the proposed method using both mathematical phantoms and real medical images.",
    "actual_venue": "Ipmi"
  },
  {
    "abstract": "We determine those triples ( m , n , k ) of integers for which there are two m -star designs on the same n -set having exactly k stars in common.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "We extend a smooth dynamical systems averaging technique to a class of hybrid systems with a limit cycle that is particularly relevant to the synthesis of stable legged gaits. After introducing a definition of hybrid averageability sufficient to recover the classical result, we illustrate its applicability by analysis of first a one-legged and then a two-legged hopping model. These abstract systems prepare the ground for the analysis of a significantly more complicated two legged model: a new template for quadrupedal running to be analyzed and implemented on a physical robot in a companion paper. We conclude with some rather more speculative remarks concerning the prospects for further extension and generalization of these ideas.",
    "actual_venue": "International Journal Of Robotic Research"
  },
  {
    "abstract": "The collection of sensory data is crucial for cyber-physical systems. Employing mobile agents (MAs) to collect data from sensors offers a new dimension to reduce and balance their energy consumption but leads to large data collection latency due to MAs' limited velocity. Most existing research effort focuses on the offline mobile data collection (MDC), where the MAs collect data from sensors based on preoptimized tours. However, the efficiency of these offline MDC solutions degrades when the data generation of sensors varies. In this paper, we investigate the on-demand MDC; that is, MAs collect data based on the real-time data collection requests from sensors. Specifically, we construct queuing models to describe the First-Come-First-Serve-based MDC with a single MA and multiple MAs, respectively, laying a theoretical foundation. We also use three examples to show how such analysis guides online MDC in practice.",
    "actual_venue": "Wireless Communications And Mobile Computing"
  },
  {
    "abstract": "Self-Organising Network (SON) use cases are seen as key enablers of efficient Long Term Evolution (LTE) system operation. Mobility Load Balancing Optimisation (LB) is a prominent SON use case defined by 3GPP with the scope of balancing the load in a given coverage area by reconfiguring the handover thresholds. When overload is detected at a given cell, the LB reduces the handover thresholds of those user equipments that can be redirected to the underutilised neighbouring cells. This mechanism improves the user plane performance in the overloaded cell. Despite the potential benefits, the available LB mechanisms have two major deficiencies, both of them causing user experience degradation: on the one hand, they tend to overestimate the load on the air interface, resulting in unnecessary handovers; on the other hand, they are either transport agnostic or have limited capability to consider the transport load during their decision process. This paper provides an overview of the LB related problems and proposes an improved solution including accurate evaluation of the radio load and a transport aware mechanism for end-to-end optimisation. The performance of the solution was evaluated by simulations.",
    "actual_venue": "Personal, Indoor And Mobile Radio Communications"
  },
  {
    "abstract": "A collection H of integers is called an affine d-cube if there exist d+1 positive integers x0,x1,…, xd so that***** Insert equation here *****We address both density and Ramsey-type questions for affine d-cubes. Regarding density results, upper bounds are found for the size of the largest subset of {1,2,…,n} not containing an affine d-cube. In 1892 Hilbert published the first Ramsey-type result for affine d-cubes by showing that, for any positive integers r and d, there exists a least number n=h(d,r) so that, for any r-colouring of {1,2,…,n}, there is a monochromatic affine d-cube. Improvements for upper and lower bounds on h(d,r) are given for d2.",
    "actual_venue": "Combinatorics Probability And Computing"
  },
  {
    "abstract": "In this paper we propose a technique to implement in a parallel fashion a turbo decoder based on an arbitrary permutation, and to expand its interleaver in order to produce a family of prunable S-random interleavers suitable for parallel implementations. We show that the spread properties of the obtained interleavers are almost optimal and we prove by simulation that they are very competitive in t...",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "We propose a framework for the verification of statecharts. We use the CSP/FDR framework to model complex systems designed in statecharts, and check for system consistency or verify special properties within the specification. We have developed an automated translation from statecharts into CSP and exploited it in both theoretical and practical senses.",
    "actual_venue": "Icfem"
  },
  {
    "abstract": "This paper presents a new approach for avoiding double-spending attacks via the concept of recipient-oriented concepts in private blockchain networks. Together with the concepts of stealth address and masternode at recipient sides, transactional privacy is assured and transaction recipients become active which receive transaction propagation unilaterally. Based on this approach, it can be shown that recipient-oriented method is suggested for preventing the double-spending attacks in this paper. In addition, the proposed method is possible to prevent double spending attacks using the verification time of the recipient and blocking time of the transaction.",
    "actual_venue": "Ieee International Conference On Sensing, Communication And Networking"
  },
  {
    "abstract": "Nearly 20–30% of all process control loops oscillate due to stiction resulting in productivity losses. Thus, detection and quantification of stiction in control valves using routine operating data is an important component of any automated controller performance monitoring application. Many techniques have been proposed for the detection and quantification of stiction. However, most of the approaches assume that the underlying process is linear; very little work is available for nonlinear processes. In this paper, Volterra model-based technique is investigated for the detection of stiction in closed-loop nonlinear systems. The advantages of the proposed method are: (i) it can be used to detect stiction in nonlinear systems and (ii) requires no prior information on whether the loop is linear or nonlinear. Results obtained from simulation and industrial case studies demonstrate the utility of the proposed methodology.",
    "actual_venue": "Computers And Chemical Engineering"
  },
  {
    "abstract": "In this paper, the real stability radii for perturbed linear differential equations of the retarded and neutral type are investigated. We present the corresponding readily computable formulae with respect to an arbitrary stability region in the complex plane. The results are further extended to the case of linear discrete time-delay systems.",
    "actual_venue": "Systems And Control Letters"
  },
  {
    "abstract": "This paper presents a methodology for delineating densely packed aggregate particles based on aggregate image classification. There is no earlier work on segmentation of aggregate particles has exploited these two building blocks for making robust object delineation. The proposed method has been tested experimentally for different kinds of densely packed aggregate images, which are difficult to detect by a normal edge detector. As tested, the studied algorithm can be applied into other applications too.",
    "actual_venue": "Pakdd"
  },
  {
    "abstract": "In new generations of microprocessors, the superscalar architecture is widely adopted to increase the number of instructions executed in one cycle. The division instruction among all of the instructions needs more cycles than the rest, e.g., addition and multiplication. This makes the division instruction an important cycles-per-instruction figure for modern microprocessors. In this paper, a radix-16/8/4/2 divisor is proposed, which uses a variety of techniques, including operand scaling, table partitioning, and, particularly, table sharing, to increase performance without the cost of increasing complexity. A physical chip using the proposed method is implemented by 0.35-/spl mu/m single poly four metal (1P4M) CMOS technology. The testing measurement shows that the chip can execute signed 64-b/32-b integer division between 3-13 cycles with a 80-MHz operating clock.",
    "actual_venue": "Ieee Trans Vlsi Syst"
  },
  {
    "abstract": "The persistent modification of synaptic efficacy as a function of the relative timing of pre- and postsynaptic spikes is a phenomenon known as spike-timing-dependent plasticity (STDP). Here we show that the modulation of STDP by a global reward signal leads to reinforcement learning. We first derive analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity, by applying a reinforcement learning algorithm to the stochastic spike response model of spiking neurons. These rules have several features common to plasticity mechanisms experimentally found in the brain. We then demonstrate in simulations of networks of integrate-and-fire neurons the efficacy of two simple learning rules involving modulated STDP. One rule is a direct extension of the standard STDP model (modulated STDP), and the other one involves an eligibility trace stored at each synapse that keeps a decaying memory of the relationships between the recent pairs of pre- and postsynaptic spike pairs (modulated STDP with eligibility trace). This latter rule permits learning even if the reward signal is delayed. The proposed rules are able to solve the XOR problem with both rate coded and temporally coded input and to learn a target output firing-rate pattern. These learning rules are biologically plausible, may be used for training generic artificial spiking neural networks, regardless of the neural model used, and suggest the experimental investigation in animals of the existence of reward-modulated STDP.",
    "actual_venue": "Neural Computation"
  },
  {
    "abstract": "A hypergraph H(V, E) with vertex set V and edge set E differs from a graph in that an edge can connect more than two vertices. An r-uniform hypergraph H(V, E) is a hypergraph with hyperedges of size r. For an r-uniform hypergraph H(V, E), an runiform clique is a subset C of V such as every subset of r elements of C belongs to E. We present hClique, an exact algorithm to find a maximum r-uniform clique for runiform graphs. In order to evidence the performance of hClique, 32 random r-graphs were solved.",
    "actual_venue": "Discrete Mathematics Algorithms And Applications"
  },
  {
    "abstract": "The job shop scheduling problem that is concerned with minimizing makespan is discussed. A new heuristic algorithm that embeds an improved shifting bottleneck procedure into the Tabu Search (TS) technique is presented. This algorithm is different from the previous procedures, because the improved shifting bottleneck procedure is a new procedure for the problem, and the two remarkable strategies of intensification and diversification of TS are modified. In addition, a new kind of neighborhood structure is defined and the method for local search is different from the previous. This algorithm has been tested on many common problem benchmarks with various sizes and levels of hardness and compared with several other algorithms. Computational experiments show that this algorithm is one of the most effective and efficient algorithms for the problem. Especially, it obtains a lower upbound for an instance with size of 50 jobs and 20 machines within a short period.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Power-line communication (PLC) has been the main enabler for modernizing the aging electrical power grid. As such, PLC systems have been the subject of intensive research in the community. One of the major aspects of PLC is the link interface, for which orthogonal frequency-division multiplexing (OFDM) has been widely adopted. In this paper, we propose the application of orthogonal poly-phase-based multicarrier code division multiple access (OPP-MC-CDMA) due to its inherent better flexibility and signal-envelope properties which can be utilized to further enhance the reliability of PLC signals. The proposed OPP-MC-CDMA system is implemented with a minimum mean square error equalizer and nonlinear preprocessing to overcome the effects of bursty noise and multipath frequency-selective fading commonly experienced in PLC channels. We study the performance of this system in terms of the output signal-to-noise ratio (SNR) and symbol error rate with various constellation sizes of OPP codes under different noise scenarios and nonlinear processor's thresholds. For comparison-sake, the performance of the OFDM scheme is included. The results reveal that the proposed approach always provides superior performance over the OFDM one with a maximum output SNR gain of up to 5.25 dB. It is also shown that the performance of the OPP-MC-CDMA technique improves when increasing the constellation size of the OPP codes, which consequently enhances the reliability of PLC.",
    "actual_venue": "Smart Grid, Ieee Transactions"
  },
  {
    "abstract": "Ubiquitous Computing becomes a popular research field, with the progress of sensor networks, automatic identification techniques etc. One of the hottest topics in ubiquitous computing is context-aware service, provided based on users' context, e.g. location and interests. Though many researches have been done in users' context-aware based services, it still a hard challenge to identify the situation of users in a systemic and precise manner to provide personalized services adapted to the users' situations. In this paper, we propose a situation-aware model to specify the situations of the users, define actions on the situations, so that when the situations really happen, defined actions will be triggered to provide services to the users. In order to realized the proposed model, we implement a ubiquitous platform called ubiquitous tiles system, by which users' identifications and location can be acquired with corresponding relation between a user and his/her location, to detect the situations.",
    "actual_venue": "Fcst"
  },
  {
    "abstract": "This paper compares the use of a recently proposed sliding mode fault detection and isolation scheme with a linear scheme based on an unknown input observer. Both methods seek to reconstruct actuator and sensor fault signals for a class of uncertain systems. Although the explicit details of the two approaches are quite different, an underlying link between them is exposed and investigated. The methods are compared using a nonlinear model of a crane system in which known faults are deliberately introduced.",
    "actual_venue": "European Journal Of Control"
  },
  {
    "abstract": "Spatial audio displays have been criticized because the use of headphones may isolate users from their real world audio environment. In this paper we study the effects of three types of audio reproduction equipment (standard headphones, bone-conductance headphones and monaural presentation using a single earphone) on time and accuracy during interaction with a deictic spatial audio display. Participants selected a target sound emitting from one of four different locations in the presence of distracters whilst wearing the different types of headphones. Target locations were marked with audio feedback. No significant differences were found for time and accuracy ratings between bone conductance and standard headphones. Monaural reproduction significantly slowed interaction. The results show that alternative reproduction equipment can be used to overcome user isolation from the natural audio environment.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "In this paper we discuss initial concepts of the development of a fully automatic guide dog system for blind users. The physical scene is scanned using a laser range device, and the three dimensional point cloud measurements are analyzed and transformed into a description of the environment that is communicated to the user via synthetic speech and/or haptic feedback allowing the user to navigate around physical space.",
    "actual_venue": "Procedia Computer Science"
  },
  {
    "abstract": "We reformulate the rendering equation to alleviate the need for explicit visibility computation, thus enabling interactive global illumination on graphics hardware. This is achieved by treating visibility implicitly and propagating an additional quantity, called antiradiance, to compensate for light transmitted extraneously. Our new algorithm shifts visibility computation to simple local iterations by maintaining additional directional antiradiance information with samples in the scene. It is easy to parallelize on a GPU. By correctly treating discretization and filtering, we can compute indirect illumination in scenes with dynamic objects much faster than traditional methods. Our results show interactive update of indirect illumination with moving characters and lights.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "Spatial parsers augment spatial hypermedia systems by letting the computer perceive the informal - but visually apparent - groupings formed by humans working with a spatial hypermedia tool. A number of research systems implementing 2D spatial parsing have been described in recent years. This paper extends spatial parsing to 3D and describes an implementation of a tailorable 3D spatial parser for the Topos system: a 3D information organization tool for use on desktops, interactive whiteboards and tables. The parser maintains a proximity graph of the heterogeneous 3D objects and applies structure experts and global repression and reinforcement techniques to this graph to find structures. A number of issues pertaining to 3D parsing as opposed to 2D parsing are discussed. The paper also presents a simple and efficient 2D parser for 3D scenes and compares it to the true 3D parser.",
    "actual_venue": "The New Review Of Hypermedia And Multimedia"
  },
  {
    "abstract": "In this paper we analyse algorithms for the geometric dual of posynomial programming problems, that make explicit use of second order information. Out of two possible approaches to the problem, it is shown that one is almost always superior. Interestingly enough, it is the second, inferior approach that has dominated the geometric programming literature.",
    "actual_venue": "Math Program"
  },
  {
    "abstract": "Changing needs in technical communications alter but are far from eliminating the personal satisfactions of the individual practitioner. They expand rather than contract opportunities.",
    "actual_venue": "Acm Journal Of Computer Documentation"
  },
  {
    "abstract": "Change detection performance is influenced by a number of factors, among which is the informativeness of targets. It has not been clarified, yet, whether the highly informative regions have a processing priority as a result of resource deployment from other tasks or whether it results from a better resource management. In this paper, we adopted a change detection paradigm in which thirty participants were randomly assigned to two groups: single (change detection task) and dual task [change detection and a simplified version of the Paced Auditory Serial Oppository Task (PASOT, Gow and Deary in J Clin Exp Neuropsychol 26:723-736, 2004), which implies a verbal effort]. Stimulus informativeness was defined as social relevance, that is, changing targets were people (high relevance) versus objects (low relevance), all other aspects (i.e., salience and position in the scene) kept constant. As hypothesized, data analyses showed a significant main effect of social relevance and task condition, i.e., better change detection performance and lower change detection times for people versus objects and for single than for dual task condition. Interestingly, the PASOT accuracy remained stable across the person versus object trials, thus implying that the better performance with socially relevant targets could not be explained by a resources withdrawal from the secondary task.",
    "actual_venue": "Cognitive Processing"
  },
  {
    "abstract": "Distributed applications are being developed that contain one or more layers of software servers. Software processes within such systems suffer contention delays both for shared hardware and at the software servers. The responsiveness of these systems is affected by the software design, the threading level and number of instances of software processes, and the allocation of processes to processors. The Method of Layers (MOL) is proposed to provide performance estimates for such systems. The MOL uses the mean value analysis (MVA) Linearizer algorithm as a subprogram to assist in predicting model performance measures.",
    "actual_venue": "Software Engineering, Ieee Transactions"
  },
  {
    "abstract": "This paper proposes to reduce the decimation factor of the multistage decimator so that its output can be fed directly to the Farrow structure for sample rate conversion, eliminating the need for another L-band filter for upsampling. Furthermore, it was found out that the programmable FIR filter can be replaced by a half-band filter placed immediately after the Farrow structure, i.e. after sample rate conversion. This significantly reduces the complexity of the proposed software radio receiver because this half-band filter, which consists of fixed filter coefficients, can be implemented efficiently without multiplication using SOPOT coefficients. As the coefficients of the multistage decimators and the subfilters in the Farrow structure are also fixed, they can also be implemented efficiently using the SOPOT coefficients. As a result, apart from the limited number of multipliers required in the Farrow structure, the entire digital IF can be implemented without any multiplication. Design example is given to demonstrate the effectiveness and feasibility of the proposed approach.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "We consider a two-agent scheduling problem on a single machine, where the objective is to minimize the total completion time of the first agent with the restriction that the number of tardy jobs of the second agent cannot exceed a given number. It is reported in the literature that the complexity of this problem is still open. We show in this paper that this problem is NP-hard under high multiplicity encoding and can be solved in pseudo-polynomial time under binary encoding. When the first agent's objective is to minimize the total weighted completion time, we show that the problem is strongly NP-hard even when the number of tardy jobs of the second agent is restricted to be zero.",
    "actual_venue": "Journal Of Combinatorial Optimization"
  },
  {
    "abstract": "Generative Adversarial Networks are powerful generative models that are able to model the manifold of natural images. We leverage this property to perform manifold regularization by approximating a variant of the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When incorporated into the semi-supervised feature-matching GAN we achieve state-of-the-art results for GAN-based semi-supervised learning on CIFAR-10 and SVHN benchmarks, with a method that is significantly easier to implement than competing methods. We also find that manifold regularization improves the quality of generated images, and is affected by the quality of the GAN used to approximate the regularizer.",
    "actual_venue": "Arxiv: Learning"
  },
  {
    "abstract": "Touch-to-track is a software feature that is used to track objects in embedded systems, such as mobile phones. The research question that is discussed in the paper is as follows: how can one design an automated test method for accurately testing object tracking algorithms? The challenge for the verification team was to design a method of test automation for testing this object tracking feature. The solution to the problem was to develop test automation algorithms using a robotic arm to accurately test this software feature. The robotic arm is used for executing test cases for tracking single or multiple objects in motion. To use the touch-to-track feature, the user selects an area in the camera preview by drawing a bounding box. Hough transformation and color segmentation are applied to accurately detect the bounding box drawn on every frame in which the object is detected. The area inside the bounding box is considered as the template. Template matching algorithms based on normalized cross-correlation, phase correlation, and speeded up robust features (SURF)-based feature extraction and matching are then applied to compare the template and original images. The tracking efficiency is calculated by dividing the total number of frames that contain the object being tracked by the total number of frames being tested. The tracking efficiencies achieved with the Hough-based bounding box detection algorithm followed by template matching algorithms based on normalized cross-correlation, phase correlation, and SURF-based feature extraction and matching are compared. These tracking efficiencies are calculated by comparing the ground truth to the tracking results for each frame. The tracking efficiency calculated for three objects is 87.7% with no colored background. The proposed object tracking solution with hardware acceleration is found to perform better than the third-party solution with respect to the latency in re-establishing tracking and the hand jitter scenario.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "Safety case development is highly recommended by some safety standards to justify the safety of a system. The Goal Structuring Notation (GSN) is a popular approach to construct a safety case. However, the content of the safety case elements, such as safety claims, is in natural language. Therefore, a common understanding of the meaning of a safety claim may be difficult to reach. Consequently, the confidence of a safety claim can be misplaced. In this paper, we propose to use an SBVRbased controlled language to support safety case development. By using the controlled language, the ambiguities caused by natural language can be mitigated. Furthermore, an SBVR editor for building a vocabulary and a GSN editor with vocabulary support are developed. Finally, a case study has been carried out to show the benefits of using the controlled language for safety case construction.",
    "actual_venue": "Communications In Computer And Information Science"
  },
  {
    "abstract": "This paper presents control schemes for efficient operation of renewable energy based islanded microgrid integrated with Energy Storage System (ESS) and diesel generator as a backup source. In the proposed control algorithm, ESS and diesel generator are operated in various control modes based on the state of charge (SOC) level of the battery ESS. The ESS system is categorized into three states as normal, warning and critical based on the SOC level. During the normal state, ESS system is operated as the main source while in the warning state it will operate along with diesel generator in peer to peer mode and in the critical state diesel generator operates as the main source to regulate the islanded microgrid. The detailed model of the islanded microgrid system is developed in MATLAB/Simulin (TM), and the simulation results of the operation of the system in various modes and transition between them are presented in the paper.",
    "actual_venue": "Ieee Industry Applications Society Annual Meeting"
  },
  {
    "abstract": "A major barrier to the human communication is attributed to the fact that there is no compatible typewriter, as in the Western World, for anynon-alphabetic language outside of the Western World. This paper will describe how a plotter can be used through programming, as a universal graphic character writer for all nonalphabetic as well as alphabetic languages in place of the typewriter. This is economically feasible since the plotter is not expensive and can be driven by a small computer on-line, or offline using a plotter and tape unit.Each character is treated as a single independent graph and decomposed into line segments within a 16 × 16 grid for non-alphabetic languages, and a 5 × 8 grid for alphabetic languages. Therefore, the graphic character can be represented by the coordinates which indicate the beginning and the ending points of the line segments.To take the most complicated Chinese language as an example, the character for \"BRAVE\" can be represented by twenty-three pairs of coordinates and packed into four 48-bit computer words. Taking this as a basis for estimation, the overwhelmingly numerous 10,000 Chinese characters can be decomposed and packed into 40K of memory. Thus, a computer with 65K memory will be able to keep them in core for direct access and processing.On the other hand, an English letter \"M\" can be represented by five pairs of coordinates and packed into one 48-bit computer word. In other words, the whole set of the English alphabet, including upper and lower cases, will need only II0 words of memory. The output on the plotter is itself a good hard copy to keep, and certainly it can be used as an original for further dupllcating, photographing and photoengraving.",
    "actual_venue": "Coling"
  },
  {
    "abstract": "This paper proposes the principles of constructive alignment as foundation for course design within Interaction Design and Children (IDC). While the field has existed for over a decade, there is still no settled curriculum for teaching it. The paper demonstrates how intended learning outcomes in combination with related work and research on teaching IDC can be used to develop a course in IDC, and exemplify this with a brief description of the development of a recently completed course. The contribution of this paper is to support anyone who intends to start teaching in this area, to stimulate discussion in the community, and contribute to an emerging curriculum for Interaction Design and Children.",
    "actual_venue": "IDC"
  },
  {
    "abstract": "In this study, we extend and combine the post-acceptance model (PAM), proposed by Bhattacherjee [Bhattacherjee, A. (2001). Understanding information systems continuance: An expectation-confirmation model. MIS Quarterly, 25(3), 351-370], with aspects of Goodhue and Thomson's theory of task-technology fit (TTF). The original PAM emphasizes cognitive beliefs and user feelings as factors that may influence a person's intention to continue to use an information system (IS). The variables added from TTF are task-technology fit and utilization. The sample consists of data that measure use and use-related aspects of an e-learning tool among university college teachers. Using structural equation modeling, results indicate that variables from TTF as well as variables from PAM explain users' IS continuance intention. As a result of these findings, we propose the existence of two different and autonomous paths from the independent variables to the dependent variable of IS continuance intention. These two paths are: a work system-centric path through utilization of the information system, and an IT-centric path through user satisfaction.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "The adoption of wireless technology for industrial wireless instrumentation requires high-quality communication performance. The use of transmission power control (TPC) can help address industrial issues concerning energy consumption, interference, and fading. This paper presents a TPC algorithm designed for industrial applications based on theoretical and empirical studies. It is shown that the proposed algorithm adapts to variations in link quality, and is hardware-independent and practical.",
    "actual_venue": "Industrial Informatics, IEEE Transactions  "
  },
  {
    "abstract": "It is widely recognized that one of the factors that are going to yield the most significant capacity increase in wireless networks is spatial reuse of radio resources through dense deployment of radio access points. This leads to the development of small cell networks where different size cells, e.g. macro cells, picocells, femtocells, relays, coexist under the same standard. Of course, dense deployment is able to unravel its potential benefits only provided that interference is properly managed. In this paper, we propose an algorithm able to perform cell association and radio resource allocation jointly, in order to maximize the sum rate in a MIMO (interference) network. Cell selection is inherently a combinatorial problem. To deal with the nonconvexity, we introduce a suitably chosen convex relaxation of the objective function and develop a fast algorithm converging to a locally optimal solution of the nonconvex problem.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "This paper presents a novel view planning method to generate suitable viewpoints for the reconstruction of the 3D shape of buildings, based on publicly available 2D map data. The proposed method first makes use of 2D map data, along with estimated height information, to generate a rough 3D model of the target building. Randomized sampling procedures are then employed to generate a set of initial candidate viewpoints for the reconstruction process. The most suitable viewpoints are selected from the candidate viewpoint set by first formulating a modified Set Covering Problem (SCP) which considers image registration constraints, as well as uncertainties present in the rough 3D model. A neighborhood greedy search algorithm is proposed to solve this SCP problem and select a series of individual viewpoints deemed most suitable for the 3D reconstruction task. The paper concludes with both computational and real-world field tests to demonstrate the overall effectiveness of the proposed method.",
    "actual_venue": "International Conference On Control, Automation, Robotics And Vision"
  },
  {
    "abstract": "In many seemingly diverse physical contexts (including, for example, certain radiation field problems, studies of crystallographic minimal surfaces, the theory of scattering of acoustic or electromagnetic waves by means of an elliptic disk, studies of elliptical crack problems in fracture mechanics, and so on), a remarkably large number of general families of elliptic-type integrals, and indeed also many definite integrals of such families with respect to their modulus (or complementary modulus), are known to arise naturally. Motivated essentially by these and many other potential avenues of their applications, we present here a systematic account of the theory of a certain family of incomplete elliptic integrals in a unified and generalized manner. By means of the familiar Riemann-Liouville fractional differintegral operators, we obtain several explicit hypergeometric representations and apply these representations with a view to deriving various associated definite integrals, not only with respect to the modulus (or complementary modulus), but also with respect to the amplitude of the incomplete elliptic integrals involved therein.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "This paper describes a novel frequency-domain approach to identification of mechanical systems with friction on the basis of their periodic steady-state motions. Most importantly, the proposed method does not require acceleration information for its implementation. An interesting feature is that any periodic excitation input with sufficiently large amplitude and/or frequency can ensure the feasibility of the proposed method. We also present some experimental results to illuminate further its practical use.",
    "actual_venue": "Ieee Trans Automat Contr"
  },
  {
    "abstract": "We present an extended version of our work on the design and implementation of a reference model of the human body, the Master Motor Map (MMM) which should serve as a unifying framework for capturing human motions, their representation in standard data structures and formats as well as their reproduction on humanoid robots. The MMM combines the definition of a comprehensive kinematics and dynamics model of the human body with 104 DoF including hands and feet with procedures and tools for unified capturing of human motions. We present online motion converters for the mapping of human and object motions to the MMM model while taking into account subject specific anthropométrie data as well as for the mapping of MMM motion to a target robot kinematics. Experimental evaluation of the approach performed on VICON motion recordings demonstrate the benefits of the MMM as an important step towards standardized human motion representation and mapping to humanoid robots.",
    "actual_venue": "Humanoid Robots"
  },
  {
    "abstract": "With the advances of wireless communication technology, using the wireless LAN as a platform to perform distributed network computing becomes feasible. We study the characteristics of end-to-end communication over wireless links. With the advantage of reduced bandwidth competition in each LAN segment separated by the wireless bridges, and with the overlap of wireless and wired communications, an analytical comparison showed that the group communications over wireless links can be more efficient than over a single segment wired LAN. We conducted experiments of running distributed applications and the results showed that with the support of threads, wireless network computing can achieve the same performance as the wired networks. Furthermore, the statistical results from our survey showed that the users cannot tell the difference between wireless and wired settings in terms of the data access speed",
    "actual_venue": "Icpads"
  },
  {
    "abstract": "Within QoE research it has become a generally acknowledged fact that user expectations can have considerable impact on subjective quality perception. In order to empirically quantify this influence we conducted a user study on expectations caused by the type of access network used. Study participants were asked to rate the QoE of various broadband services accessed via Internet connections labeled either as `wireless' or `wireline'. Our results show that for certain applications such as web-browsing, a considerable share of users rated differently in the wireless context than when (supposedly) using wireline, despite otherwise identical underlying technical network conditions. These and other findings demonstrate that dedicated studies on expectations can provide valuable information regarding the nature and impact of hitherto neglected QoE influence factors.",
    "actual_venue": "Quality Of Multimedia Experience"
  },
  {
    "abstract": "A model for opinion dynamics (Model I) has been recently introduced in which the binary opinions of the individuals are determined according to the size of their neighboring domains (population having the same opinion). The coarsening dynamics of the equivalent Ising model shows power law behavior and has been found to belong to a new universality class with the dynamic exponent z = 1.0 +/- 0.01 and persistence exponent theta similar or equal to 0.235 in one dimension. The critical behavior has been found to be robust for a large variety of annealed disorder that has been studied. Further, by mapping Model I to a system of random walkers in one dimension with a tendency to walk towards their nearest neighbour with probability epsilon, we find that for any epsilon > 0.5, the Model I dynamical behaviour is prevalentat long times.",
    "actual_venue": "Journal Of Physics Conference Series"
  },
  {
    "abstract": "Multicore architectures, especially hardware accelerator systems with heterogeneous processing elements, are being increasingly used due to the increasing processing demand of modern digital systems. However, data communication in multicore architectures is one of the main performance bottle-necks. Therefore, reducing data communication overhead is an important method to improve the speed-up of such systems. In this paper, we propose a heuristic-based approach to address the data communication bottleneck. The proposed approach uses a detailed quantitative data communication profiling to generate interconnect designs automatically that are relatively simple, low overhead and low area solutions. Experimental results show that we can gain speed-up of 3.05× for the whole application and up to 7.8× speed-up for accelerator functions in comparison with software.",
    "actual_venue": "FPT"
  },
  {
    "abstract": "This work presents a novel modeling and analysis framework for graph sequences which addresses the challenge of detecting and contextualizing anomalies in labeled, streaming graph data. We introduce a generalization of the BTER model of Seshadhri et al. by adding flexibility to community structure, and use this model to perform multi-scale graph anomaly detection. Specifically, probability models describing coarse subgraphs are built by aggregating node probabilities, and these related hierarchical models simultaneously detect deviations from expectation. This technique provides insight into the graphs' structure and helps contextualized detected event. For evaluation, two new hierarchical detectors are tested against a baseline Gaussian method on a synthetic graph sequence with seeded anomalies. We demonstrate that in a labeled setting with community structure, our graph statistics-based approach outperforms both a distribution-based detector and the baseline, accurately detecting anomalies at the node, subgraph, and graph levels.",
    "actual_venue": "Proceedings Of The Ieee/Acm International Conference On Advances In Social Networks Analysis And Mining, Asonam"
  },
  {
    "abstract": "The use of the PC and Internet for placing telephone calls will present new opportunities to capture vast amounts of un-transcribed speech for a particular speaker. This paper investigates how to best exploit this data for speaker-dependent speech recognition. Supervised and unsupervised experiments in acoustic model and language model adaptation are presented. Using one hour of automatically transcribed speech per speaker with a word error rate of 36.0%, unsupervised adaptation resulted in an absolute gain of 6.3%, equivalent to 70% of the gain from the supervised case, with additional adaptation data likely to yield further improvements. LM adaptation experiments suggested that although there seems to be a small degree of speaker idiolect, adaptation to the speaker alone, without considering the topic of the conversation, is in itself unlikely to improve transcription accuracy.",
    "actual_venue": "Icassp"
  },
  {
    "abstract": "Network simulation plays a crucial role in the field of network research, education, and industry. However, before conducting a simulation on traditional network simulators, operators need to develop a simulative behavioral model, which requires intimate knowledge of the simulator implementation. Besides, the behavioral model cannot be migrated directly into real-world devices due to its tight coupling with the simulator platform, resulting in redundant and error-prone codes rewriting. Recently, P4, a high-level domain specific language (DSL), has attracted great attention from both academia and industry for its advantages of enabling operators to define behaviors of the programmable data plane.\n\nInspired by the idea of DSL, we present NS4, a P4-driven network simulator supporting simulation of P4-enabled networks to address the problems existing in traditional simulators. Taking advantage of P4, NS4 simplifies the development of a behavioral model and bridges the gap between simulation and deployment. Furthermore, to the best of our knowledge, NS4 is the first research effort to enable simulation of a P4-enabled network, providing a useful tool for P4 research and development. In this paper, we designed and implemented NS4, consisting of data plane models integrated with ns-3, the state-of-the-art network simulator, and control plane models to interact with the P4 pipeline. Then we evaluated its effectiveness and efficiency by simulating several representative P4 programs. Results show that NS4 can simulate large-scale P4-enabled networks at a low cost.",
    "actual_venue": "Sosr : Symposium On Sdn Research Los Angeles Ca Usa March"
  },
  {
    "abstract": "In this paper, I contend that executive leaders can and should be prepared to spot timely opportunities and prevent major failures, even if it requires understanding IT technical details. An essential part of executive leadership is exception management – and exceptions occur everywhere, often surfacing first as details, nothing more. Here I present two cases that show technology is no exception. In this paper, I outline how Steelscreen.com's and RoweCom's early years can illustrate why understanding IT technical details helps explain IT-related failures and successes. In particular, I outline the differences between the two stories and argue that the show stopper for Steelscreen was at its crux a technology detail – the other shortcomings could have been addressed elsewhere. I conclude with some recommendations for executive leaders and a brief outline of a teaching approach suggestion for an MBA or a Senior Executive IT class based on the ideas presented.",
    "actual_venue": "JIT"
  },
  {
    "abstract": "This paper presents the first implementation of a new algorithm for pattern recognition in machine vision developed in our laboratory. This algorithm has been previously presented only theoretically, without practical use. In this work we applied it to the RobotCub humanoid robotics platform simulator. We used it as a base for a circular object localization within the 3D surrounding space. The algorithm is a robust and direct method for the least-square fitting of ellipses to scattered data. RobotCub is an open source platform, born to study the development of neuro-scientific and cognitive skills in human beings, especially in children. Visual pattern recognition is a basic capability of many species in nature. The skill of visually recognizing and distinguishing different objects in the surrounding environment gives rise to the development of sensory-motor maps in the brain, with the consequent capability of object manipulation. In this work we present an improvement of the RobotCub project in terms of machine vision software, by implementing the method of the least-square fitting of ellipses of Maini (EDFE), previous developed in our laboratory, in a robotics context. Moreover, we compared its performance with the Hough Tranform, and others least-square ellipse fittings techniques. We used our system to detect spherical objects by applying it to the simulated RobotCub platform. We performed several tests to prove the robustness of the algorithm within the overall system. Finally we present our results.",
    "actual_venue": "Simpar"
  },
  {
    "abstract": "Computer users have long desired a personal software agent that could execute verbal commands. Today's World Wide Web (WWW or Web), with its point and click hypertext interface, makes a tremendous amount of information readily available online. A speech interface would make the Web even more powerful, allowing us to access information by surfing the Web by voice. TI have developed Speech Aware Multimedia (SAM) with this in mind, to make information on the Web more accessible and useful. They combined an innovative speech recognition engine with the Web to let anyone browse arbitrary Web pages using only speech as the input medium. Speech brings added flexibility and power to the classical Web interface and makes information access more natural. Today's speech recognition capability is well matched to Web browsing. The Web page provides a natural, well defined context for a speech recognition application. The recognition engine does not need to recognize any and all possible phrases, but only those phrases pertaining to the specific page in view at the moment. This context imposes limits that significantly aid recognition performance. Furthermore, the visual information on a page prompts the user on what to request and how to request it by voice",
    "actual_venue": "Ieee Multimedia"
  },
  {
    "abstract": "In wireless sensor networks, each sensor node has severe resource constraints in terms of energy, computing device, and memory space. Especially, the memory space of the platform hardware is much smaller than that of the other computing systems. In this paper, we propose a OTL, which is an on-demand thread stack allocation scheme for MMU-less real-time sensor operating systems. The OTL enables to adaptively adjust the stack size by allocating stack frame based on the amount of each function's stack usage. The amount of the function's stack usage is checked at compile-time, and the adaptive adjustment of the stack occurs at run-time. Our experimental results show that the OTL significantly minimizes the spatial overhead of the threads' stacks with tolerable time overhead compared with fixed stack allocation mechanism of the existing sensor operating systems.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": "In this paper, we address the crucial issue of how to design efficient MAC protocols in autonomous wireless networks with selfish users. We model the wireless medium access control problem as a non-cooperative game in which the MAC protocol can be regarded as distributed strategy update scheme approaching the equilibrium point. Under such game theoretic framework, three MAC protocols, the aggressive, conservative and cheat-proof MAC protocol, are then proposed with tunable parameters allowing them to converge to the desired social optimal point. The first two MAC protocols require network participants to follow the rules, while the cheat-proof MAC protocol can survive the selfish environments where nodes are purely self-interested. Based on our game theoretic analysis, we provide a general methodology for designing efficient MAC protocols for autonomous wireless networks. We believe that the proposed methodology not only provides a general way of designing stable and controllable MAC protocols achieving high performance even in selfish environments, but also provides a general framework that can be extended to design efficient protocols in other non-cooperative environments.",
    "actual_venue": "Ieee Conference On Local Computer Networks"
  },
  {
    "abstract": "With the rapid development of GPS-enabled mobile devices, huge amounts of user-contributed data with location information can be collected from the Internet. With this kind of data, one promising application is travel recommendation, which has attracted a considerable number of researches recently. However, most of the previous studies only focus on one aspect of the relations among users and locations or make a coarse linear combination of the relations. Moreover, all the existing work on travel recommendation do not consider recommendation to groups, which is an important characteristic of travelers' behavior. In this paper, we present a personalized travel recommendation system named Where to Go. The novelty of the system is a 3R model which can unify user-location relation, user-user relation and location-location relation into a single framework and perform random walk with restart to analyze the model. We further extend our approach to provide recommendations for groups. To the best of our knowledge, this is the first work to use random walk with restart for group recommendation. We conduct a comprehensive performance evaluation using a real dataset collected from Flickr, which is one of the most popular online photo-sharing sites. Experimental results show that our approach provides significantly superior recommendation quality compared to other state-of-the-art travel recommendation approaches for both individuals and groups.",
    "actual_venue": "MDM"
  },
  {
    "abstract": "In this paper we show that solving the discrete logarithm problem for non-supersingular elliptic curves over finite fields\n of even characteristic is polynomial-time equivalent to solving a discrete logarithm type of problem in the infrastructure\n of a certain function field. We give an explicit correspondence between the two structures and show how to compute the equivalence.",
    "actual_venue": "Ants"
  },
  {
    "abstract": "•Current palmprint recognition methods focused on particular scenarios or devices.•Deep discriminative representation model proposed for generic palmprint recognition.•Deep convolutional networks are learned to extract deep discriminative features.•Experimented on popular contact-based (inc. multispectral) and contactless datasets.•Proposed framework achieves state-of-the-art results in EER and ARR.",
    "actual_venue": "Pattern Recognition"
  },
  {
    "abstract": "A general approach for controlling pulse-width-modulated (PWM) -type switching DC-DC converters digitally using state-feedback techniques and linear optimal control theory is reported. The methodology for redesigning the state estimator is investigated, and a method derived from the general linear-quadratic-regulator (LQR) problem, is proposed. The method is found to offer better transient responses and robustness to uncertainties in plant parameters when compared with the typical eigenvalue-assignment method. Special attention is directed to plant models with possible migrations of the open-loop zeroes across the stability boundary during operation. Results of applying these techniques to a published Cuk converter are reported to illustrate different points of interest.",
    "actual_venue": "Industrial Electronics, IEEE Transactions  "
  },
  {
    "abstract": "Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular semantic image segmentation usingfully convolutionalnetworks. In contrastto the aforementioned areas,the design choices of a fully convolutional network require several changes, rangingfrom the sort of operations that need to be used-e.g., dilated convolutions-to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searchingfor high-performance compact segmentation architectures,able to run in real-time using limited resources. To achieve that, we intentionallyover-parameterisethe architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluationphase. The design of the auxiliary cell is emitted by a controller a neural network with the fixed structure trained using reinforcement learning.More crucially,we demonstrate how to efficiently searchfor these architectureswithin limited time and computational budgets. In particular we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architecturesperforming on-par with stateof-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.corn drsieep/nas-segm-pytorch",
    "actual_venue": "Ieee/Cvf Conference On Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "This paper proposes a matrix approach for hierarchical web page clustering with two algorithms using hyperlink information among pages.One clustering algorithm clusters web pages without considering cluster overlapping.Another one takes cluster overlapping into account.These algorithms take advantage of intrinsic relationships among the pages, and are independent of the order in which the pages are presented to the algorithms.Furthermore, the proposed algorithms do not require a predefined similarity threshold for clustering.They are easy to be implemented for web applications.The primary evaluations show the effectiveness of the proposed algorithms, as well as a promising application.",
    "actual_venue": "Web Information Systems Engineering"
  },
  {
    "abstract": "Computing models of first-order or propositional logic specifications is the complementary problem of refutational theorem\n proving. A deduction system capable of producing models significantly extends the functionality of purely refutational systems\n by providing the user with useful information in case that no refutation exists.",
    "actual_venue": "Cade"
  },
  {
    "abstract": "The likelihood ratio edge detector is an efficient filter for the segmentation of synthetic aperture radar (SAR) images. We show that this filter provides biased location of the edge, when the window does not have the same orientation as the edge. A phenomenological model is proposed to characterize this bias. We then introduce an efficient technique to refine edge location: the statistical active contour. The combination of these two methods permits to achieve accurate and regularized edge location.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "A method for coherently detecting and decoding turbo coded BPSK signals transmitted over frequency-flat fading channels is discussed. Estimates of the complex channel gain and variance of the additive noise are derived first from known pilot symbols and an estimation Alter. After each iteration of turbo decoding, the channel estimates are refined using information fed back from the decoder. Both hard-decision and soft-decision feedback are considered, and simulation results compare the proposed technique with DPSK, ideal BPSK, and non-iterative PSAM based reception.",
    "actual_venue": "Wcnc: Ieee Wireless Communications And Networking Conference, Vols"
  },
  {
    "abstract": "The emergence of biometrics and it's widespread use in authentication applications made template protection in biometrics an important task to be considered in recent years. In this paper we propose a fingerprint bio-cryptosystem using fuzzy commitment scheme. We constructed Delaunay Neighbor Structures(DNS) from fingerprint minutiae points. Then a bit string representation of DNSs was developed. Bose, Chaudhuri, and Hocquenghem(BCH) error correction code is used along with bit string in the fuzzy commitment scheme to generate a secure template. During decoding phase, BCH decoding is performed to get the codeword. The EER obtained for proposed method on FVC 2002 DB1, DB2 and DB3 are 1.43%, 1.79%, and 5.89% respectively. This shows the credibility of the proposed method.",
    "actual_venue": "Advances In Signal Processing And Intelligent Recognition Systems"
  },
  {
    "abstract": "Behavioral and physiological sex differences in emotional reactivity are well documented, yet comparatively few neural differences have been identified. Here we apply quantitative activation likelihood estimation (ALE) meta-analysis across functional brain imaging studies that each reported clusters of activity differentiating men and women as they participated in emotion-evoking tasks in the visual modality. This approach requires the experimental paradigm to be balanced across the sexes, and thus may provide greater clarity than previous efforts. Results across 56 emotion-eliciting studies (n=1907) reveal distinct activation in the medial prefrontal cortex, anterior cingulate cortex, frontal pole, and mediodorsal nucleus of the thalamus in men relative to women. Women show distinct activation in bilateral amygdala, hippocampus, and regions of the dorsal midbrain including the periaqueductal gray/superior colliculus and locus coeruleus. While some clusters are consistent with prevailing perspectives on the foundations of sex differences in emotional reactivity, thalamic and brainstem regions have not previously been highlighted as sexually divergent. These data strongly support the need to include sex as a factor in functional brain imaging studies of emotion, and to extend our investigative focus beyond the cortex.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Extended Signal machines are proven able to compute any computable function in the understanding of recursive/computable analysis (CA), here type-2 Turing machines (T2-TM) with signed binary encoding. This relies on an intermediate representation of any real number as an integer (in signed binary) plus an exact value in ( *** 1,1) which allows to have only finitely many signals present outside of the computation. Extracting a (signed) bit, improving the precision by one bit and iterating the T2-TM only involve standard signal machines. For exact CA-computations, T2-TM have to deal with an infinite entry and to run through infinitely many iterations to produce an infinite output. This infinite duration can be provided by constructions emulating the black hole model of computation on an extended signal machine. Extracting/encoding an infinite sequence of bits is achieved as the limit of the approximation process with a careful handling of accumulations and singularities.",
    "actual_venue": "UC"
  },
  {
    "abstract": "The authors present a rigorous modeling and analysis of quantization effects in PDF-optimized quantizers for the M-band case. They develop a technique for nulling out the signal distortion and for minimizing quantization noise in the reconstructed output, while imposing perfect reconstruction constraints. The present analysis and compensation are important in situations requiring low average bit per channel allocation.<>",
    "actual_venue": "Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference  "
  },
  {
    "abstract": "Many web databases are hidden behind restrictive form-like interfaces which may or may not provide domain information for an attribute. When attribute domains are not available, domain discovery becomes a critical challenge facing the application of a broad range of existing techniques on third-party analytical and mash-up applications over hidden databases. In this paper, we consider the problem of domain discovery over a hidden database through its web interface. We prove that for any database schema, an achievability guarantee on domain discovery can be made based solely upon the interface design. We also develop novel techniques which provide effective guarantees on the comprehensiveness of domain discovery. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "In this paper, we base on data computing blocks (DCBs) and DCT watchdog technology to implement VLIW watchdog processor. 32-bit final DCT signature (F-DCT-S) and several 5-bit relay DCT signatures (R-DCT-S) will be computed by DCT watchdog scheme. These generated signatures are embedded into the instruction memory and then used to do the run time error checking. We use VLIW processor to simulation. In this paper, the processor degradation can be improved by doing the whole block error checking after the branch instruction, the fault detection latency is improved by doing the intermediate error checking at the R-type instruction, and the memory overhead is reduced by storing the R-DCT-S to the R-type instruction. The experimental results show that the proposed watchdog has a very high error detection coverage and shortest error detection latency to detect either single fault or multi-faults, no matter what the fault is transient or intermittent.",
    "actual_venue": "Prdc"
  },
  {
    "abstract": "Multimedia has been used creatively to entertain and educate, and can also be used for therapeutic and medical purposes. This paper addressed this issue by incorporating multimedia to design and develop an assistive device to help disabled children with speech impairments in mainstream education. The appropriate methodology for developing such an interface was investigated. Relevant multimedia, psychology, social and educational theories were taken into account. Based on this literature review, interfaces to enhance pronunciation were designed, developed and tested.",
    "actual_venue": "Universal Access In The Information Society"
  },
  {
    "abstract": "Long code Direct Sequence Spread Spectrum is a good choice for applications where security and antispoofing ability of a spread spectrum connection is concerned. Due to long period of spreading code, acquisition is a challenge for these systems because a trade-off between detection probability and acquisition time must be made. For confronting this challenge, various algorithms were proposed in the literature but almost all of them focus on expediting the coarse acquisition. In this paper, we consider the efficient dual folding algorithm for coarse acquisition level and propose 3 methods for enhancing the fine acquisition level leading to faster execution of overall acquisition algorithm. The methods are based on estimations from coarse acquisition level that are not used in conventional algorithms for fine acquisition, ie, zero padding. Theoretical expressions of 2 main comparison criteria in acquisition algorithms, ie, detection probability and mean acquisition time, are derived for conventional zero padding and each of the proposed method. Besides, a coarse estimate of resource consumption is provided by the number of floating point operations for each algorithm to make a useful comparison. Considering these 3 parameters, in comparison, the proposed methods surpass zero padding in 1, 2, or all of the 3 aspects. Simulation agrees well with analytical results.",
    "actual_venue": "International Journal Of Communication Systems"
  },
  {
    "abstract": "Human movements are inherently variable and involve some feedback mechanism. A study of the positional variance in a tapping task reveals that the variance profiles are unimodal in time. In the variance-decreasing phase, the aiming task can be modeled by a Shannon-like communication scheme where information is transmitted from a \"source\"-determined by the distance to the target at maximum variance-to a \"destination\"-the movement endpoint-over a \"channel\" with feedback perturbed by Gaussian noise. Thanks to the feedback link, we show that the variance decreases exponentially at a rate given by the channel capacity. This is confirmed on real data. The proposed information-theoretic model has promise to improve our understanding of human aimed movements.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "This paper addresses the issue of multiservice support in IEEE 802.16 or WiMAX networks. The capacity for supporting multiple service classes is indeed important for any access technology where bandwidth is limited, which is the case for IEEE 802.16. The standard currently proposes four traffic classes, and specifies that for uplink traffic, the first one (UGS) receives periodic grants whereas the other three are served via polling. Supporting two different scheduling mechanisms may have a significant impact on the complexity of Network Interface Cards, and therefore on the CAPEX for WiMAX networks. Based on this analysis, the present work investigates whether a 802.16 network that only supports the 3 polling based classes is still capable of providing the QoS levels expected for all types of applications. Both the transfer plane QoS, in terms of latency and jitter, and the command plane QoS, in terms of blocking probability are assessed. In particular, a simple, multiservice call admission control (CAC) mechanism is proposed that significantly improves on a previously proposed CAC mechanism by favouring real time traffic over non real time traffic. The outcome of this study shows that it is indeed possible to support stringent QoS with only polling based traffic classes, and fairly simple traffic engineering mechanisms fully described in the paper.",
    "actual_venue": "Ieee Wireless Communications And Networking Conference"
  },
  {
    "abstract": "Software refactoring is a well-known technique that provides transformations on software artifacts with the aim of improving their overall quality. In this paper we present a set of refactoring for Class Sheets, a modelling language that allows to specify the business logic of a spreadsheet in an object-oriented fashion. The set of refactoring that we propose allows us to improve the quality of these spreadsheet models. Moreover, it is implemented in a setting that guarantees that all model refactoring are automatically carried to all the corresponding (spreadsheet) instances, thus providing an automatic evolution of the data so it is always synchronized with the model.",
    "actual_venue": "Quality Of Information And Communications Technology"
  },
  {
    "abstract": "•This paper presents a voxelwise atlas rating approach for computer-aided diagnosis.•The method relies on multiple atlas databases, but does not require annotated images.•The method reports an accuracy of 97.3%, which is higher than other state-of-the-art methods.",
    "actual_venue": "Medical Image Analysis"
  },
  {
    "abstract": "As database management systems expand their array of analytical functionality, they become powerful research engines for biomedical data analysis and drug discovery. Databases can hold most of the data types commonly required in life sciences and consequently can be used as flexible platforms for the implementation of knowledgebases. Performing data analysis in the database simplifies data management by minimizing the movement of data from disks to memory, allowing pre-filtering and post-processing of datasets, and enabling data to remain in a secure, highly available environment. This article describes the Oracle Database 10g implementation of BLAST and Regular Expression Searches and provides case studies of their usage in bioinformatics. http://www.oracle.com/technology/software/index.html.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "The Slovenian text-to-speech engine is a modular system consisting of four independent modules (text normalization, grapheme-to-phoneme conversion, prosody generation and segmental concatenation), which are pipelined together. Each module is responsible for one portion of the problem of converting from text into speech. The first two modules comprises such tasks as end-of-sentence detection, abbreviation and number expansion, special formats conversion, morphological and contextual analysis, phonological modeling. In order to generate rules for our synthesis scheme, data was collected by analysing the readings of ten speakers, five males and five females. A two-level approach has been used for duration modelling and so-called superpositional approach at pitch modelling. The system is based on the concatenation of speech units, diphones and some frequently used polyphones, using TD-PSOLA technique.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "Recommender systems have been widely used to deal with information overload, by suggesting relevant items that match users' personal interest. One of the most popular recommendation techniques is matrix factorization (MF). The inner products of learned latent factors between users and items can estimate users' preferences for items with high accuracy, but the preferences ranking is time consuming. Thus, hashing-based fast search technologies were exploited in recommender systems. However, most previous approaches consist of two stages: continuous latent factor learning and binary quantization, but they didn't well deal with the change of inner product arising from quantization. To this end, in this paper, we propose a constraint free preference preserving hashing method, which quantizes both norm and similarity in dot product. We also design an algorithm to optimize the bit length for norm quantization. The performance of our method is evaluated on three real world datasets. The results confirm that the proposed model can improve recommendation performance by 11%-15%, as compared with the state-of-the-art hashing approaches.",
    "actual_venue": "Ieee Global Communications Conference"
  },
  {
    "abstract": "This paper describes ongoing work on selective dissemination of broadcast news. Our pipeline system includes several modules: audio preprocessing, speech recognition, and topic segmentation and indexation. The main goal of this work is to study the impact of earlier errors in the last modules. The impact of audio preprocessing errors is quite small on the speech recognition module, but quite significant in terms of topic segmentation. On the other hand, the impact of speech recognition errors on the topic segmentation and indexation modules is almost negligible. The diagnostic of the errors in these modules is a very important step for the improvement of the prototype of a media watch system described in this paper.",
    "actual_venue": "Eurasip J Adv Sig Proc"
  },
  {
    "abstract": "Blind source separation (BSS) is an increasingly popular data analysis technique with many applications. Several methods for BSS using the statistical properties of original sources have been proposed; for a famous case, non-Gaussianity, this leads to independent component analysis (ICA). In this paper, we propose a hybrid BSS method based on linear and nonlinear complexity pursuit, which combines three statistical properties of source signals: non-Gaussianity, linear predictability and nonlinear predictability. A gradient learning algorithm is presented by minimizing a loss function. Simulations verify the efficient implementation of the proposed method.",
    "actual_venue": "J Computational Applied Mathematics"
  },
  {
    "abstract": "Summary  An evaluation of Turkey's science and technology (S & T) policy in the last two decades has been made by using various indicators\n of S & T and technological innovation. National trends in inputs for research and development (R & D) activities, publication\n output and patent data have been studied for the implications of the S & T policy from 1983 to 2003. Some of the findings\n on the outcomes of policy measures in terms of inputs to R & D and publication output are as follows: (1) Total R & D expenditure,\n as percent of gross domestic product (GDP), increased from 0.32% in 1990 to 0.67% in 2002, (2) the fraction of R & D in the\n total expenditure for technological innovation increased from 6.6% in 1995-1997 to 29.2% in 1998-2000, and (3) the number\n of papers in the journals covered in the Science Citation Index (SCI) of the Institute for Scientific Information (ISI) increased\n from 464 in 1983 to 12160 in 2003 - a more than 26-fold increase in the last two decades.",
    "actual_venue": "Scientometrics"
  },
  {
    "abstract": "We report the design, prototype construction and initial testing of a small minibuoy that is aimed at use in a coordinated, wireless networked array of buoys for near-surface ocean sensing. This vehicle is designed to fill the gap between larger ocean surface vessels and/or moored buoys and subsurface gliders. The size and cost is low enough that these versatile sensor platforms can be deployed easily and in quantity. Since these minibuoys are mobile, they can keep station in currents as large as 25 cm/s or move as an adaptive, coordinated sensor array for high resolution in both time and space. The buoy is about 74 cm (29 in) long, 41 cm (16 in) wide (max) and weighs about 14.5 kg (32 lbs); hence, it can be deployed easily from small craft. Deployment times are about 1 to 2 days or more - longer with solar power. The buoy structure is fiberglass and PVC with two 2 W DC motors. Control is done with GPS and magnetic heading sensors and a PID scheme to maintain course. Communication is via a 900 MHz system with a range of 1 to 2 km and plans for a longer range HF/VHF or satellite system. The initial sensor system is designed for ocean hyperspectral observations as surface truth for airborne system calibration and validation and other ocean color applications. Acoustic, wave, air & water temperature sensors as well as GPS are included. The Mark I prototype has been successfully tested in a pool with manual control.",
    "actual_venue": "Barcelona"
  }
]