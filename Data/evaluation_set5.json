[
  {
    "abstract": "Cooperative vehicular systems are expected to improve traffic safety and efficiency through the real-time exchange of information between vehicles and infrastructure nodes. To this aim, cooperative active safety applications are being designed to extend, in space and time, the drivers' awareness of the surrounding environment in order to be able to detect potential road dangers with sufficient time for the driver to react. The strict requirements of cooperative vehicular applications and the challenging vehicular environment require that cooperative active safety applications are extensively tested under real-world conditions. In this context, this paper presents the experimental evaluation of different vehicle-to-vehicle cooperative active safety applications under real world and challenging conditions where cooperative systems need to prove their real effectiveness.",
    "actual_venue": "Vanet@Mobicom"
  },
  {
    "abstract": "This paper deals with the problem of guaranteed cost control for uncertain stochastic systems with state and input-delays. Attention is focused on the design of a state feedback controller such that the resulting closed-loop system is mean-square asymptotically stable and guarantees that a given quadratic cost function has an upper bound. In terms of a linear matrix inequality (LMI), a sufficient condition for the solvability of this problem is derived. When the LMI is satisfied, the expression of desired state feedback controller is proposed. A numerical example is given to demonstrate the feasibility of the proposed approach",
    "actual_venue": "Icicic"
  },
  {
    "abstract": "For an artifact such as a robot or a virtual agent to respond appropriately to human social touch behavior, it should be able to automatically detect and recognize touch. This paper describes the data collection of CoST: Corpus of Social Touch, a data set containing 7805 captures of 14 different social touch gestures. All touch gestures were performed in three variants: gentle, normal and rough on a pressure sensor grid wrapped around a mannequin arm. Recognition of these 14 gesture classes using various classifiers yielded accuracies up to 60 %; moreover, gentle gestures proved to be harder to classify than normal and rough gestures. We further investigated how different classifiers, interpersonal differences, gesture confusions and gesture variants affected the recognition accuracy. Finally, we present directions for further research to ensure proper transfer of the touch modality from interpersonal interaction to areas such as human–robot interaction (HRI).",
    "actual_venue": "J Multimodal User Interfaces"
  },
  {
    "abstract": "Positron emission tomography (PET) is a nuclear medical imaging technology that produces 3D images of tissue metabolic activity in human body. PET has been used in various clinical applications, such as diagnosis of tumors and diffuse brain disorders. High quality PET image plays an essential role in diagnosing diseases/disorders and assessing the response to therapy. In practice, in order to obtain the high quality PET images, standard-dose radionuclide (tracer) needs to be used and injected into the living body. As a result, it will inevitably increase the risk of radiation. In this paper, we propose a regression forest (RF) based framework for predicting standard-dose PET images using low-dose PET and corresponding magnetic resonance imaging (MRI) images instead of injecting the standard-dose radionuclide into the body. The proposed approach has been evaluated on a dataset consisting of 7 subjects using leave-one-out cross-validation. Moreover, we compare the prediction performance between sparse representation (SR) based method and our proposed method. Both qualitative and quantitative results illustrate the practicability of our proposed method.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Purpose - The purpose of this paper is to explore technology acceptance and use behavior of IS innovations by entrepreneurs. To measure the perception of IS innovations by entrepreneurs the authors review unified theory of acceptance and use of technology and the entrepreneurial potential model, empirically compare the two models, develop a new model that integrates elements from the two models, and then empirically validate the new model (technology adoption decision and use (TADU)) in a technology acceptance context.Design/methodology/approach - The data used to test the hypothesis are collected from 1,200 entrepreneurs in Malaysia. The research model was analyzed using structural equation modeling.Findings - The results indicate that perceived desirability and perceived feasibility have significant effects on entrepreneurs' intention to adopt and use innovations. Propensity to use is an important factor that has a significant effect on individual behavior. The precipitating events that happen in the time lag between intention and behavior will disrupt entrepreneurs' inertia and induce a change in their behavior, encouraging them to seek the best opportunity available.Practical implications - Understanding the individual, technological, and environmental factors that significantly affect IT adoption behavior can support policy makers in providing guidance on the adoption and usage of IT innovations by entrepreneurs.Originality/value - This study proposes a TADU model with six core determinants of intention and usage - perceived desirability, perceived feasibility, performance expectancy, effort expectancy, social influence and facilitating conditions and two new moderators, precipitating events and the propensity to act.",
    "actual_venue": "Internet Research"
  },
  {
    "abstract": "BACKGROUND: Mass spectrometry based quantification of peptides can be performed using the iTRAQ™ reagent in conjunction with mass spectrometry. This technology yields information about the relative abundance of single peptides. A method for the calculation of reliable quantification information is required in order to obtain biologically relevant data at the protein expression level. RESULTS: A method comprising sound error estimation and statistical methods is presented that allows precise abundance analysis plus error calculation at the peptide as well as at the protein level. This yields the relevant information that is required for quantitative proteomics. Comparing the performance of our method named Quant with existing approaches the error estimation is reliable and offers information for precise bioinformatic models. Quant is shown to generate results that are consistent with those produced by ProQuant™, thus validating both systems. Moreover, the results are consistent with that of Mascot™ 2.2. The MATLAB® scripts of Quant are freely available via http://www.protein-ms.de and http://sourceforge.net/projects/protms/, each under the GNU Lesser General Public License. CONCLUSION: The software Quant demonstrates improvements in protein quantification using iTRAQ™. Precise quantification data can be obtained at the protein level when using error propagation and adequate visualization. Quant integrates both and additionally provides the possibility to obtain more reliable results by calculation of wise quality measures. Peak area integration has been replaced by sum of intensities, yielding more reliable quantification results. Additionally, Quant allows the combination of quantitative information obtained by iTRAQ™ with peptide and protein identifications from popular tandem MS identification tools. Hence Quant is a useful tool for the proteomics community and may help improving analysis of proteomic experimental data. In addition, we have shown that a lognormal distribution fits the data of mass spectrometry based relative peptide quantification.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "ASummary: Cordova is an out-of-the-box solution for building and maintaining an online database of genetic variations integrated with pathogenicity prediction results from popular algorithms. Our primary motivation for developing this system is to aid researchers and clinician-scientists in determining the clinical significance of genetic variations. To achieve this goal, Cordova provides an interface to review and manually or computationally curate genetic variation data as well as share it for clinical diagnostics and the advancement of research.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "This paper proposes a hierarchical service management system for MPLS network services. Traditionally, general management systems which have been deployed in some service providers control MPLS LSPs (e.g. RSVP-TE, LDP) and services (e.g. L2VPN, L3VPN and IP) separately. If a fault occurs in an MPLS network, the dedicated management system for MPLS LSPs can detect the fault and recognize the state of MPLS LSPs. However, it cannot detect the extent of the impact due to the fault in each service. Furthermore, its own inability to identify the affected customer means it takes some time to identify the affected customers, cooperating manually with the dedicated management system for services. Therefore, this paper proposes a new automatic correlation between MPLS LSPs and each service. In particular, this paper proposes a new algorithm for a correlation between RSVP-TE LSPs and L3VPN services. Simulations are conducted to evaluate the capacity on a correlation table and the performance searching on a correlation table, and results show this system is very scalable within real MPLS production networks. This system, with the automatic correlation, could be sufficiently deployed in real MPLS production networks.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "Model-driven software development enables users to specify an application at a high level --- a level that better matches problem domain. It also promises the users with better analysis and automation. Our work embarks on two collaborating domains --- business process and human interactions --- to build an application. Business modeling expresses business operations and flows then creates business flow implementation. Human interaction modeling expresses a UI design, its relationship with business data, logic, and flow, and can generate working UI. This double modeling approach automates the production of a working system with UI and business logic connected. This paper discusses the human aspects of this modeling approach after a year long of building a procurement outsourcing contract application using the approach --- the result of which was deployed in December 2008. The paper discusses in multiple areas the happy endings and some heartache. We end with insights on how a model-driven approach could do better for humans in the process.",
    "actual_venue": "Interact"
  },
  {
    "abstract": "This paper presents a Collaborative Model for capturing and representing the engineering Design process (CoMoDe). CoMoDe is a deductive object-oriented model that, in relation to an engineering design process, is able to capture the different elements that participate in a design process in an integrated fashion. In particular, it is able to represent (i) the activities, operations, and actors that have generated each design product, (ii) the imposed requirements, and (iii) the rationale behind each decision. Furthermore, it also offers an explicit mechanism to represent and trace the different model versions that have participated in the design process. On such a basis, this proposal introduces specific procedures to handle various situations appearing in cooperative environments. They are: (i) different design teams perform independent concurrent activities on ''a priori'' independent parts of the artefact being designed and afterwards their results need to be made consistent; (ii) distinct teams concurrently work on slightly coupled parts of the artefact being designed and conflict handling must be addressed along their ''parallel'' course of actions.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "Historically, text mining methods have been enriched substantially by both statistical learning and symbolic AI. Different approaches have been extensively applied over the last 30 years to extract \"knowledge\" from text. However, in scenarios where the path from data to decisions is unclear, or where different users may be interested in different solutions, the involvement of the user or analyst in the text mining process becomes crucial. Visual Text Analytics aims at addressing these problems by incorporating concepts from Visual Analytics to text mining and natural language processing (Keim et. al, 2010).",
    "actual_venue": "Ai Matters"
  },
  {
    "abstract": "Lexical norms, normative and usually numeric, ratings of word meaning are popular tools in research domains relating to human expression and perception of language, especially with regards to emotion. In this paper we are proposing an algorithm of psycholinguistic norm expansion capable of generating high quality norms representing aspects of language beyond emotion, including language concreteness and indicators of age and gender association. Starting from small manually annotated norm lexica, continuous norms for new words are estimated using semantic similarity and a simple linear model along eleven expression-related dimensions. The model is shown to achieve state of the art level performance of word norm estimation. To investigate the potential of these norms as analysis tools of more complex phenomena we use them to investigate the differences in therapist speech in sessions conducted by practitioners adhering to the psychoanalytic and client-centered schools of therapy.",
    "actual_venue": "Annual Conference Of The International Speech Communication Association , Vols"
  },
  {
    "abstract": "The migration of popular Catch-up TV services to modern Over-The-Top (OTT) multimedia delivery infrastructures creates a wide set of scalability challenges which are commonly addressed using Content Delivery Networks (CDNs) relying on caching nodes close to users.The use of general-purpose caching nodes, tailored for generic web content, is far from optimal as it does not consider the particularities of Catch-up TV content, namely its dynamic popularity behavior, superstar effects, and relevance decay, as shown in existing scientific literature. Since caches are limited in size and are relatively small when compared to the whole catalog of available Catch-up TV content, which may contain tens of thousands of TV programs, it is crucial to make the most out of the available resources.To address these issues, this paper proposes a novel content-aware cache replacement algorithm, Most Popularly Used (MPU), capable of taking advantage of content demand forecasts built using machine learning models, to significantly outperform traditional cache replacement policies, such as Least Recently Used (LRU), Least Frequently Used (LFU), and First-In-First-Out (FIFO), and approach the optimal theoretical hit-ratio limits. MPU leverages millions of Catch-up TV request logs to validate its results under realistic conditions.",
    "actual_venue": "Ieee Symposium On Computers And Communication"
  },
  {
    "abstract": "From the constant-speed variable-output motor driving test result of an interior permanent-magnet synchronous motor for a pump drive, we observed that there was an additional loss factor, which was not previously considered. Furthermore, a 3-D finite-element method analysis was performed to investigate the cause of the additional electromagnetic loss. Accordingly, it was confirmed that there was a...",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "The Multicast Address Resolution Server architecture has been proposed as a mechanism for supporting TP multicast over ATM networks. Two basic techniques exist withing this architecture for the intra-subnet multicasting of IP packets over ATM networks. One approach makes use of a mesh of point-to-multipoint Virtual Circuits (VC Mesh) each of which is rooted at a multicast source, while the other uses a shared point-to-multipoint tree rooted at a Multicast Server (MCS). In this paper we describe the design and implementation of a MARS client and an MCS. We present a framework for comparing the VC Mesh and MCS approaches using experimental and simulation techniques. Finally we present three protocols for the usage of multiple MCSs per IP multicast group, a technique required for groups with large number of senders and group members.**",
    "actual_venue": "HPN"
  },
  {
    "abstract": "Data collected from real world are often imprecise. A few algorithms were proposed recently to compute the convex hull of maximum area when the axis-aligned squares model is used to represent imprecise input data. If squares are non-overlapping and of different sizes, the time complexity of the best known algorithm is O(n7). If squares are allowed to overlap but have the same size, the time complexity of the best known algorithm is O(n5). In this paper, we improve both bounds by a quadratic factor, i.e., to O(n5) and O(n3), respectively.",
    "actual_venue": "Cocoon"
  },
  {
    "abstract": "Today's global business environment, characterized by unprecedented competitive pressures and sophisticated customers that demand speedy solution creates a bigger set of potential suppliers to evaluate and to choose from. To deal with the complexities of the supplier selection process, an integration of Quality Function Deployment ((FD), Analytical Hierarchy Process (AHP) and Preemptive Goal Programming (PGP) techniques is proposed. A QFD matrix is used to display the degree of relationship between each pair of requirement for suppliers and supplier evaluating criterion. This paper employs the AHP first to measure the relative importance weighting for each of the requirements in the QFD process. Secondly, it is used to assess the evaluating score for each of the candidate suppliers for each particular supplier-evaluating criterion. PGP is built to deal with some suppliers' constraints such that the total value of purchase (TVP) becomes maximum and the total cost of purchase (TCP) minimum.",
    "actual_venue": "International Journal Of Information Technology And Decision Making"
  },
  {
    "abstract": "Due to the rapid development of the Internet, the energy consumption of network equipment has increased significantly. It has a considerable impact on the environment, responsible for between 2-4% of all CO2 emissions. As a consequence, much research has been done to reduce the energy consumption of networks whilst maintaining the quality of service. In this paper, a new energy-saving architecture for a backbone IP over WDM network is proposed. A key constraint of our architecture is to maintain the logical IP topology whilst saving energy by infrastructure sleeping and virtual router migration. Using the new architecture, the energy consumption saving is up to 24% in a 6 node-8 link test network and 20% in NSFNET comprising 14 nodes and 21 links. The architecture also includes a new genetic algorithm for solving the virtual router migration problem.",
    "actual_venue": "Temu"
  },
  {
    "abstract": "1/f noise was measured on lateral bipolar PNP transistors over a temperature range of 220<T<450 K. Noise power spectral density measurements were performed simultaneously across two resistors connected in series with base and collector. The equivalent base current noise source SIB has two dominant components. One is SIBE that is between the base and the emitter, in parallel with rπ. The other is SIBC coming from the surface recombination current at the neutral base, between the base and the collector. The extracted SIB exhibited a near square law dependence on base current IB. The noise remained nearly constant when the temperature was below 310 K. However, it presented strong temperature dependence when the temperature was beyond 310 K. Two different models are proposed for the noise in different temperature regions. For the high temperature region, the surface recombination velocity fluctuation model is proposed, which indicates that the noise is coming from the fluctuations in the surface recombination velocity at the neutral base surface. The tunneling assistant trapping model is responsible for the low temperature region, where the noise source is the carrier trapping–detrapping by the defects in the spacer oxide covering the surface of the depletion layer.",
    "actual_venue": "Microelectronics Reliability"
  },
  {
    "abstract": "Computer science is not that difficult but wanting to learn it is.",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "The Mobile Ad hoc Networks (MANETs) is a collection of wireless devices or nodes that communicate by dispatching packets to one another or on behalf of another device or node, without having any central network authority or infrastructure controlling data routing. In order to communicate each other, the nodes cooperatively forward data packets to other nodes in the network by using the routing protocol. However, these routing protocols are not secure hence leaving the MANET unprotected from malicious attack. Wormhole attack is a common malicious attack in MANET environment. The network consisting of 20, 60 and 100 mobile nodes uses the random model in 1000m x 1000m flat area. The sources are spread randomly over the network and only 512 bytes data packets are used. Each packet is uniformly dispersed at 180 sec, starting its journey from a random location to a random destination the objective of this paper is to evaluate the throughput performance in AODV with the existence of wormhole and Sybil attack. The simulation result has shown that there is difference performance in throughput when there is an attack.",
    "actual_venue": "Dictap"
  },
  {
    "abstract": "The number of applications that use virtual systems is growing, and one would like to use this kind of systems also for real-time applications with hard deadlines. Virtual machines with many cores are interesting since the underlying physical infrastructure usually contains many cores. We consider hard real-time tasks that execute on a virtual machine with m cores. Tasks are scheduled globally on the cores using fixed-priority preemptive scheduling. This means that a task can execute on different virtual cores at different instances in time. In order to avoid Dhall's effect, which may cause task sets with even very low utilization to miss deadlines, we classify tasks into two priority classes, namely heavy and light tasks. Heavy tasks have higher priority than light tasks. For light tasks we use rate monotonic priority assignment. In this paper we propose a utilization-based test that shows if a task set is schedulable or not. If the task set is schedulable the test provides the priority for each task. The input to the test is the task set, the number of cores in the virtual machine (m), a period for the virtual machine with m cores, and the blocking time when a virtual machine does not have access to the underlying hardware in each period.",
    "actual_venue": "Icpp Workshop"
  },
  {
    "abstract": "Social media has emerged as an effective source to investigate people's opinions in the context of a variety of topics and situations. In particular, many recent studies try to investigate social media during the crisis situations that range from natural disasters to man-made conflicts. In this paper, we investigate people's emotional responses expressed on Twitter during the 2015 Middle East Respiratory Syndrome (MERS) outbreak in South Korea. Specifically, we first present an emotion analysis method to classify fine-grained emotions in Korean Twitter posts. Then, we conduct a case study of how Korean Twitter users responded to MERS outbreak using our emotion analysis method. Experimental results on Korean benchmark dataset demonstrate the superior performance of the proposed emotion analysis approach on real-world dataset. Moreover, our analysis results on tweets related to MERS outbreak help to understand the behaviors of humans and the characteristics of sociocultural system. Further, our method can be harnessed by the media to automatically investigate public opinions as well as the authorities to gain insights for quickly deciding the assistance policies.",
    "actual_venue": "Ieee International Conference On Big Data And Smart Computing"
  },
  {
    "abstract": "Queueing systems in which the server works on primary and secondary (vacation) customers arise in many computer, communication, production and other stochastic systems. These systems can frequently be modeled as queueing systems with vacations. In this survey, we give an overview of some general decomposition results and the methodology used to obtain these results for two vacation models. We also show how other related models can be solved in terms of the results for these basic models. We attempt to provide a methodological overview with the objective of illustrating how the seemingly diverse mix of problems is closely related in structure and can be understood in a common framework.",
    "actual_venue": "Queueing Syst"
  },
  {
    "abstract": "This paper proposes a new fuzzy neural network (FNN) capable of parameter self-adapting and structure self-constructing to acquire a small number of fuzzy rules for interpreting the embedded knowledge of a system from the given training data set. The proposed FNN is inherently a modified Takagi-Sugeno-Kang (TSK)-type fuzzy-rule-based model with neural network's learning ability. There are no rules initiated at the beginning and they are created and adapted through an on-line learning processing that performs simultaneous structure and parameter identification. In the structure identification of the precondition part, the input space is partitioned in a flexible way according to the newly proposed on-line independent component analysis (ICA) mixture model. The input space is thus represented by linear combinations of independent, non-Gaussian densities. The first input training pattern is assigned to the first rule initially by the on-line ICA mixture model. Afterwards, some additional significant terms (input variables) selected by the on-line ICA mixture model will be added to the consequent part (forming a liner equation of input variables) incrementally or create a new rule in the learning processing. The combined precondition and consequent structure identification scheme can make the network grow dynamically and efficiently. In the parameter identification, the consequent parameters are tuned by the backpropagation rule and the precondition parameters are turned by the on-line ICA mixture model. Both the structure and parameter identifications are done simultaneously to form a fast learning scheme. The derived on-line ICA mixture model also provide a natural linear transformation for each input variable to enhance the knowledge representation ability of the proposed FNN and reduce the required rules and achieve higher accuracy efficiently. In order to demonstrate the performance of the proposed FNN, several experiments covering the areas of system identification, classification, and image segmentation are carried out. Our experiments show that the proposed FNN can achieve significant improvements in the convergence speed and prediction accuracy with simpler network structure.",
    "actual_venue": "Ieee Trans On Circuits And Systems"
  },
  {
    "abstract": "This paper proposes an integration between Geographical Information System (GIS) technology and constraint logic programming in order to supply the user with a declarative language that supports and improves GIS analysis. We present the language MuTACLP, where spatio-temporal and thematic information can be represented in a uniform way, and the features of constraint logic programming, such as recursion and constraint handling, can be exploited to perform sophisticated spatio-temporal reasoning. This unifying language seems also promising to address the key problem of interoperability among different GISs.",
    "actual_venue": "Computational Logic"
  },
  {
    "abstract": "Conditional Pure Literal Graphs (CPLG) characterize the set of models of a propositional formula and are introduced to help understand connections among formulas, models and autarkies. They have been applied to the SAT problem within the framework of refutation-based algorithms. Experimental results and comparisons show that the use of CPLGs is a promising direction towards efficient propositional SAT solvers based upon model elimination. In addition, they open a new perspective on hybrid search/resolution schemes.",
    "actual_venue": "Ijcar"
  },
  {
    "abstract": "We propose a technique to enhance emotional expressiveness in games and animations. Artists have used colors and painting techniques to convey emotions in their paintings for many years. Moreover, researchers have found that colors and line properties affect users' emotions. We propose using painterly rendering for character sequences in games and animations with a knowledge-based approach. This technique is especially useful for parametric facial sequences. We introduce two parametric authoring tools for animation and painterly rendering and a method to integrate them into a knowledge-based painterly rendering system. Furthermore, we present the results of a preliminary study on using this technique for facial expressions in still images. The results of the study show the effect of different color palettes on the intensity perceived for an emotion by users. The proposed technique can provide the animator with a depiction tool to enhance the emotional content of a character sequence in games and animations.",
    "actual_venue": "Int J Computer Games Technology"
  },
  {
    "abstract": "This paper presents a rigorous time-variant analysis of the 1/f MOS device noise upconversion into 1/f phase noise for two of the most popular parallel-coupled quadrature CMOS harmonic oscillators. Simple closed-form equations for the fundamental 1/f 3 phase-noise spectrum are derived and validated through SpectreRF simulations, proving that the two topologies display remarkably different sensitiv...",
    "actual_venue": "Ieee Transactions On Circuits And Systems -Regular Papers"
  },
  {
    "abstract": "Trust and Reputation management play an important role in agent-based Recommender Systems. Although several protocols and ontologies of agents using trust and reputation has been proposed, none of them has been so extensively used and implicitly accepted by research community as those from Agent Reputation and Trust (ART in advane) testbed. The motivation of this adaptation is to facilitate the use of ART principles in real distributed applications instead of a centralized testbed for experimentation. This paper presents an adaptation of the protocols proposed by ART testbed to a codification for the most popular Agent platform: JADE. This implementation follows a coherent API with the FIPA protocols included in JADE distribution for an easy use. We also complement the behaviours of corresponding initiators and responders of the protocols with an ontology formed by a collection of concepts, predicates and agent actions that may represent as the ART application domain as any other service-oriented domain. The proposal has been designed to be applied in domains where multi-agent e-commerce solutions are needed. Future work includes the integration of this ontology and protocols in context-aware scenarios such as an airport.",
    "actual_venue": "Isda"
  },
  {
    "abstract": "Recent advances in 3D shape analysis and recognition have shown that heat diffusion theory can be effectively used to describe local features of deforming and scaling surfaces. In this paper, we show how this description can be used to characterize 2D image patches, and introduce DaLI, a novel feature point descriptor with high resilience to non-rigid image transformations and illumination changes. In order to build the descriptor, 2D image patches are initially treated as 3D surfaces. Patches are then described in terms of a heat kernel signature, which captures both local and global information, and shows a high degree of invariance to non-linear image warps. In addition, by further applying a logarithmic sampling and a Fourier transform, invariance to photometric changes is achieved. Finally, the descriptor is compacted by mapping it onto a low dimensional subspace computed using Principal Component Analysis, allowing for an efficient matching. A thorough experimental validation demonstrates that DaLI is significantly more discriminative and robust to illuminations changes and image transformations than state of the art descriptors, even those specifically designed to describe non-rigid deformations.",
    "actual_venue": "International Journal Of Computer Vision"
  },
  {
    "abstract": "The letter is devoted to the introduction of a new technique to derive the cloud bottom height (CBH) from satellite measurements of the cloud reflectance in the oxygen A-band. The information on the cloud top height needed in the retrieval of the CBH must be obtained from separate measurements to insure small biases in the retrieved CBH. Such measurements can be performed by a space-based lidar.",
    "actual_venue": "Geoscience And Remote Sensing Letters, Ieee"
  },
  {
    "abstract": "Accurate mapping of impervious surface distribution is important but challenging. Integrating optical and SAR data to improve urban impervious surface estimation has recently shown promising performance. Further investigation and development on this multisensory approach are conducted in this study. A novel multiple kernel learning (MKL) framework is proposed to integrate heterogeneous features fr...",
    "actual_venue": "Ieee Journal Of Selected Topics In Applied Earth Observations And Remote Sensing"
  },
  {
    "abstract": "An efficient design of a Multi-Objective Learning Classifier System for multi-flight navigation is presented. A classifier is represented by a set of rules, which are used to simultaneously navigate all the flights in the airspace. Navigation of a flight is based on the relation of the flight with factors of the air traffic environment such as wind, storm as well as other flights. This system continually learns and refines the rules of classifiers by a multi-objective optimization algorithm - NSGAII - to discover the trade-off set of classifiers which navigate flights without any conflict, minimal distance of flying, minimal discomfort defined by storm level and the time duration of flights passing through storm areas, and minimizing total delay time of flights. We propose to detect conflicts between flights by grouping trajectory segments in 3-D (abscissa-x, ordinate-y, and time-t) boxes. The conflict detection is only implemented in a box, thus the number of conflict detection times approximates to the number of conflicts. Further, conflicts between flights are resolved using a hill climber by propagating delays in the takeoff time of conflicting flights. The advantage of the proposed system is that the classifier outputs its rules in a symbolic representation, making the overall process transparent to the user and reusable. Moreover, the system successfully discovered rules in all runs to optimize its performance.",
    "actual_venue": "Evolutionary Computation"
  },
  {
    "abstract": "Top-k dominating queries, which return the k best items with a comprehensive “goodness” criterion based on dominance, have attracted considerable attention recently due to its important role in many data mining applications including multi-criteria decision making. In the Big Data era, the modes of data storage and processing are becoming distributed, and data is incomplete commonly in some real applications. The related existing researches focus on centralized datasets, or on complete data in distributed environments, and do not involve incomplete data in distributed environments. In this work, we present the first study for processing top-k dominating queries on incomplete data in distributed environments. We show that, through detailed analysis, even though the dominance relation on incomplete data objects is non-transitive in general, the transitive dominance relation holds for some incomplete data objects with different bitmaps. We then propose an novel algorithm TKDI-MR based on MapReduce for processing TKD queries on incomplete data in distributed environments utilizing the aforementioned property. Extensive experiments with both real-world and large-scale synthetic datasets demonstrate that our approach is able to achieve good efficiency and stability.",
    "actual_venue": "Icccs"
  },
  {
    "abstract": "We present a simple, flexible, and general framework titled Partial Registration Network (PRNet), for partial-to-partial point cloud registration. Inspired by recently-proposed learning-based methods for registration, we use deep networks to tackle non-convexity of the alignment and partial correspondence problems. While previous learning-based methods assume the entire shape is visible, PRNet is suitable for partial-to-partial registration, outperforming PointNetLK, DCP, and non-learning methods on synthetic data. PRNet is self-supervised, jointly learning an appropriate geometric representation, a keypoint detector that finds points in common between partial views, and keypoint-to-keypoint correspondences. We show PRNet predicts keypoints and correspondences consistently across views and objects. Furthermore, the learned representation is transferable to classification.",
    "actual_venue": "Advances In Neural Information Processing Systems"
  },
  {
    "abstract": "In this paper, we develop new numeric modified Adomian decomposition algorithms by using the Wazwaz-El-Sayed modified decomposition recursion scheme, and investigate their practicality and efficiency for several nonlinear examples. We show how we can conveniently generate higher-order numeric algorithms at will by this new approach, including, by using examples, 12th-order and 20th-order numeric algorithms. Furthermore, we show how we can achieve a much larger effective region of convergence using these new discrete solutions. We also demonstrate the superior robustness of these numeric modified decomposition algorithms including a 4th-order numeric modified decomposition algorithm over the classic 4th-order Runge-Kutta algorithm by example. The efficiency of our subroutines is guaranteed by the inclusion of the fast algorithms and subroutines as published by Duan for generation of the Adomian polynomials to high orders.",
    "actual_venue": "Computers And Mathematics With Applications"
  },
  {
    "abstract": "Prior work on HW support for memory race recording piggybacks time stamps on coherence messages and logs the outcome of memory races using point-to-point or chunk-based approaches. These memory race recorder (MRR) techniques are effective, but they require modifications to the cache coherence protocol that can hurt performance. In addition, prior work has mostly focused on directory coherence and considered only CMP systems with single-level cache hierarchies. Most modern CMP systems shipped today, however, implement snoop coherence and feature multilevel cache hierarchies. To be practical, a MRR must target CMPs with multilevel caches, mitigate the coherence overhead due to piggybacking, and emphasize on replay speed to broaden applicability of deterministic replay. This paper contributes three new solutions for making chunk-based MRR practical for modern CMPs. We show that MRR interactions with a cache hierarchy can degrade performance and present a novel mechanism that mitigates this degradation. We propose new mechanisms for snoop-based caches that eliminate coherence traffic overhead due to piggybacking. We finally propose new techniques for improving replay speed and introduce a novel framework for evaluating the replay speed potential of MRR designs.",
    "actual_venue": "New York, Ny"
  },
  {
    "abstract": "Digital distance transformations provide helpful tools for representation and description of object shape in digital images. The resulting distance transforms should be stable under translation and rotation. To this end, the Euclidean distance is approximated. We present results for distance transforms for 3D images, where the four weights, or local distances, are used, the three weights from the 3×3×3 neighbourhood together with one weight from the outer part of the 5×5×5 neighbourhood.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "We describe the techniques of the DÉJÀ VU Scheduling Class Library to achieve a library of reusable and extendible classes for the construction of interactive scheduling systems. The constructed systems shall be efficient and user centered. We describe abstract scheduling objects, constraints between them, and potential user interactions with the system. A first scheduling system was developed for the steel plant of Böhler Kapfenberg. We demonstrate which extensions were neces- sary and show prototypical examples from the graphical user interface.",
    "actual_venue": "Apms"
  },
  {
    "abstract": "Deep space optical communications promises orders of magnitude growth in communication capacity, supporting high data rate applications such as video streaming and high bandwidth science instruments. Pulse position modulation is the modulation format of choice for deep space applications, and by inserting inter-symbol guard times between the symbols, the signal carries the timing information needed by the demodulator. Accurately extracting this timing information is crucial to demodulating and decoding this signal. In this paper we propose a low complexity maximum likelihood timing estimator for pulse position modulation with inter-symbol guard times which significantly outperforms the prior art in this domain. We show that this estimator can achieve the same performance as prior estimators with an order of magnitude less signal flux, or multiple orders of magnitude less flux-accumulation time. Further we show that this estimator achieves the Cramer-Rao bound, making it asymptotically efficient. This method does not require an explicit synchronization sequence, freeing up channel resources for data transmission.",
    "actual_venue": "Ieee Global Communications Conference"
  },
  {
    "abstract": "In this paper, we analyze the impact of process variation on the different failure mechanisms in SRAM cells. We also propose a process tolerant cache architecture suitable for high performance memory. This technique surpasses all the contemporary fault tolerant schemes such as row/column redundancy and ECC in handling failures due to process variation. Experimental results on a 64K cache show that the proposed technique can achieve 94% yield compared to its original 33% yield (standard cache) in 45nm predictive technology.",
    "actual_venue": "Cicc"
  },
  {
    "abstract": "The wideband direct-conversion receiver architecture is proposed in this paper. In order to provide a quantitative design objective, the UMTS standard is targeted. The single-ended-to-differential conversion previously handled by an inter-stage SAW filter is now performed by a balun. The balun is followed by high-P2 MP mixers driven by Cherry-Hooper LO buffers. The MP BB filter is an active-RC 3 rd-order Chebyshev architecture that drives an 8b pipelined ADC with fs=50 MHz. The AP is a scaled-down version of the MP, with the primary difference being the inclusion of an IM3 generator. As scaling reduces the breakdown voltage of CMOS devices and as system integration trends demand the further elimination of off- chip components, there arises a great need to improve the linearity of RF receivers.",
    "actual_venue": "San Francisco, Ca"
  },
  {
    "abstract": "For some special case, Huynen's decomposition cannot be used to extract a desired target from an average Kennaugh matrix. In this paper, the authors modify Huynen's method for overcoming its disadvantage, based on a simple transform of a Kennaugh matrix. Using an example, the effectiveness of the modified method is validated. © 2006 IEEE.",
    "actual_venue": "Ieee Geoscience And Remote Sensing Letters"
  },
  {
    "abstract": "With the proliferation of geo-positioning and geo-tagging techniques, spatio-textual objects that possess both a geographical location and a textual description are gaining in prevalence, and spatial keyword queries that exploit both location and textual description are gaining in prominence. However, the queries studied so far generally focus on finding individual objects that each satisfy a query rather than finding groups of objects where the objects in a group together satisfy a query. We define the problem of retrieving a group of spatio-textual objects such that the group's keywords cover the query's keywords and such that the objects are nearest to the query location and have the smallest inter-object distances. Specifically, we study three instantiations of this problem, all of which are NP-hard. We devise exact solutions as well as approximate solutions with provable approximation bounds to the problems. In addition, we solve the problems of retrieving top-k groups of three instantiations, and study a weighted version of the problem that incorporates object weights. We present empirical studies that offer insight into the efficiency of the solutions, as well as the accuracy of the approximate solutions.",
    "actual_venue": "Acm Transactions On Database Systems"
  },
  {
    "abstract": "Maintenance of situation awareness is of critical importance for the safe and productive execution of mobile field work. However, there is scarcity of research considering maintenance of situation awareness in mobile field work settings. The case study analyses information interaction as a means to maintain situation awareness. The empirical data for the study were collected from security service personnel participating in a pilot of guarding service based on NFC (near field communication) technology. NFC enables ubiquitous and location- and context-aware computing. Interviews, on-site observation and a questionnaire were conducted to define situation awareness requirements and to assess both current user experiences and future scenarios of NFC-based information support for security service work. Results of the study show that information interaction challenges were related to non-value-adding information activities when trying maintain situation awareness. Challenges were related to disturbances in information flow between clients, security service back office and field. It was found that maintaining situation awareness in circuit guarding was more challenging than in local guarding. Future NFC functionalities providing information support in particular for maintenance of short-term situation awareness were assessed as promising.",
    "actual_venue": "Cognition, Technology And Work"
  },
  {
    "abstract": "A novel numerical method is developed for solving the 3D, unsteady, incompressible Navier-Stokes equations on locally refined fully unstructured Cartesian grids in domains with arbitrarily complex immersed boundaries. Owing to the utilization of the fractional step method on an unstructured Cartesian hybrid staggered/non-staggered grid layout, flux mismatch and pressure discontinuity issues are avoided and the divergence free constraint is inherently satisfied to machine zero. Auxiliary/hanging nodes are used to facilitate the discretization of the governing equations. The second-order accuracy of the solver is ensured by using multi-dimension Lagrange interpolation operators and appropriate differencing schemes at the interface of regions with different levels of refinement. The sharp interface immersed boundary method is augmented with local near-boundary refinement to handle arbitrarily complex boundaries. The discrete momentum equation is solved with the matrix free Newton-Krylov method and the Krylov-subspace method is employed to solve the Poisson equation. The second-order accuracy of the proposed method on unstructured Cartesian grids is demonstrated by solving the Poisson equation with a known analytical solution. A number of three-dimensional laminar flow simulations of increasing complexity illustrate the ability of the method to handle flows across a range of Reynolds numbers and flow regimes. Laminar steady and unsteady flows past a sphere and the oblique vortex shedding from a circular cylinder mounted between two end walls demonstrate the accuracy, the efficiency and the smooth transition of scales and coherent structures across refinement levels. Large-eddy simulation (LES) past a miniature wind turbine rotor, parameterized using the actuator line approach, indicates the ability of the fully unstructured solver to simulate complex turbulent flows. Finally, a geometry resolving LES of turbulent flow past a complete hydrokinetic turbine illustrates the potential of the method to simulate turbulent flows past geometrically complex bodies on locally refined meshes. In all the cases, the results are found to be in very good agreement with published data and savings in computational resources are achieved.",
    "actual_venue": "J Comput Physics"
  },
  {
    "abstract": "This paper explores the application of formal analogical reasoning to incremental machine learning. The applicative context is the design of an operational assistant. The specific learning task that is focused on is the transfer from requests in natural language to commands in programming language. This work explores two questions for applying analogy in incremental learning situations: How does formal analogical reasoning behave in incremental learning situation? How do the conditions on the learning sequence influence the performance? To address these issues, an experimental setup is proposed in which multiple users are simulated. The knowledge transfer from one to the other is studied. Moreover, we discuss the influence of the order in which examples are presented to the system on the learning process.",
    "actual_venue": "Ieee International Conference On Tools With Artificial Intelligence"
  },
  {
    "abstract": "Abstract: Image-based rendering is a powerful technique for 3D object visualization and representation. Using this approach arbitrary object or scene views are generated automatically from a small set of real reference images. In this paper an algorithm for image based rendering using stereo image pairs is presented. The main goal is to produce a realistic 3D 'continuous look-around' effect along the stereo baseline. To minimize the distortion of the reconstructed pixels due to undefined disparity values, a non-linear interpolator is proposed. It uses the sparse available disparity map to generate a dense field that partially reconstructs original disparity edge information, producing a sharper intermediate view. The identification of occluded and non-occluded areas is also used to aid the view synthesis process. A special treatment for occluded image areas is also considered in the proposed technique. Several computer experiments have been conducted to assess the performance of the presented method.",
    "actual_venue": "IV"
  },
  {
    "abstract": "Summary form only given: In this panel presentation, we address regulatory challenges for the development of smart grids in Brazil, with focus on the deployment of smart metering technology and of the implementation of demand response programs. We identify particular issues of the Brazilian system that have led to the implementation of general policies and specific regulatory mechanisms that might not be adequate in a smart grid context, and discuss how those policies and mechanisms could pose barriers for rolling out necessary infrastructure and implementing specific consumer empowerment programs. Possible alternatives to overcome the identified regulatory challenges are also addressed.",
    "actual_venue": "Innovative Smart Grid Technologies"
  },
  {
    "abstract": "The Internet of Things (IoT) is an emerging classical model, envisioned as a system of billions of small interconnected devices for posing the state-of-the-art findings to real-world glitches. Over the last decade, there has been an increasing research concentration in the IoT as an essential design of the constant convergence between human behaviors and their images on Information Technology. With the development of technologies, the IoT drives the deployment of across-the-board and self-organizing wireless networks. The IoT model is progressing toward the notion of a cyber-physical world, where things can be originated, driven, intermixed, and modernized to facilitate the emergence of any feasible association. This paper provides a summary of the existing IoT research that underlines enabling technologies, such as fog computing, wireless sensor networks, data mining, context awareness, real-time analytics, virtual reality, and cellular communications. Also, we present the lessons learned after acquiring a thorough representation of the subject. Thus, by identifying numerous open research challenges, it is presumed to drag more consideration into this novel paradigm.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "One of the greatest challenges in automated driving is the ability to acquire, access and query the data pertaining to high resolution 3D objects from multiple heterogeneous sources. Specifically, the information extraction needs to be done by fusing data from both sensors and databases, and with real-time constraints. Existing structures and algorithmic approaches designed for regular maps - or even regular features in High Definition maps - are not optimal to handle the various challenges. In this paper, we review the importance and roles of high resolution 3D objects in High Definition maps being used in autonomous driving applications and summarize the characteristics of 3D objects compared to other regular map features. We also describe an end-to-end pipeline of a system targeting such problems and emphasize the challenges and feasible solutions to each part of the pipeline. Last but not least, we define the quantified evaluation metrics for each task and introduce the dataset that we built for this objective.",
    "actual_venue": "Sigspatial/Gis"
  },
  {
    "abstract": "We present a fuzzy expert system, MEDEX, for forecasting gale-force winds in the Mediterranean basin. The most successful local wind forecasting in this region is achieved by an expert human forecaster with access to numerical weather prediction products. That forecaster's knowledge is expressed as a set of 'rules-of-thumb'. Fuzzy set methodologies have proved well suited for encoding the forecaster's knowledge, and for accommodating the uncertainty inherent in the specification of rules, as well as in subjective and objective input. MEDEX uses fuzzy set theory in two ways: as a fuzzy rule base in the expert system, and for fuzzy pattern matching to select dominant wind circulation patterns as one input to the expert system. The system was developed, tuned, and verified over a two-year period, during which the weather conditions from 539 days were individually analyzed. Evaluations of MEDEX performance for both the onset and cessation of winter and summer winds are presented, and demonstrate that MEDEX has forecasting skill competitive with the US Navy's regional forecasting center in Rota, Spain.",
    "actual_venue": "Knowl Inf Syst"
  },
  {
    "abstract": "Objective In early 2010, Harvard Medical School and Boston Children's Hospital began an interoperability project with the distinctive goal of developing a platform to enable medical applications to be written once and run unmodified across different healthcare IT systems. The project was called Substitutable Medical Applications and Reusable Technologies (SMART). Methods We adopted contemporary web standards for application programming interface transport, authorization, and user interface, and standard medical terminologies for coded data. In our initial design, we created our own openly licensed clinical data models to enforce consistency and simplicity. During the second half of 2013, we updated SMART to take advantage of the clinical data models and the application-programming interface described in a new, openly licensed Health Level Seven draft standard called Fast Health Interoperability Resources (FHIR). Signaling our adoption of the emerging FHIR standard, we called the new platform SMART on FHIR. Results We introduced the SMART on FHIR platform with a demonstration that included several commercial healthcare IT vendors and app developers showcasing prototypes at the Health Information Management Systems Society conference in February 2014. This established the feasibility of SMART on FHIR, while highlighting the need for commonly accepted pragmatic constraints on the base FHIR specification. Conclusion In this paper, we describe the creation of SMART on FHIR, relate the experience of the vendors and developers who built SMART on FHIR prototypes, and discuss some challenges in going from early industry prototyping to industry-wide production use.",
    "actual_venue": "Journal Of The American Medical Informatics Association"
  },
  {
    "abstract": "Clustering technique is essential for fast retrieval in large database. In this paper, new image clustering technique is proposed for content-based image retrieval. Fuzzy-ART mechanism maps high-dimensional input features into the output neuron. Joint HSV histogram and average entropy computed from gray-level co-occurrence matrices in the localized image region is employed as input feature elements. Original Fuzzy-ART suffers unnecessary increase of the number of output neurons when the noise input is presented. Our new Fuzzy-ART mechanism resolves the problem by differently updating the committed node and uncommitted node, and checking the vigilance test again. To show the validity of our algorithm, experiment results on image clustering performance and comparison with original Fuzzy-ART are presented in terms of recall rates.",
    "actual_venue": "Iccsa"
  },
  {
    "abstract": "Many tools and techniques have been developed to address specific aspects of interacting in a virtual world. Few have been designed with an architecture that allows large numbers of entities from disparate organizations to interact in such a world, in real time, and over large geographic distances. A system architecture that does this is described. The key technologies that have made these virtual worlds possible are discussed, and it is explained and how the technologies fit into the architecture. A sample implementation of this architecture, the SIMulation NETworking (SIMNET) system, is then presented, along with various design decisions and the reasoning behind them.",
    "actual_venue": "Seattle, Wa"
  },
  {
    "abstract": "Data replication is often used to increase the availability of data in a database system. Voting schemes can be used to manage this replicated data. The authors use a simple model to study the capacity of systems using voting schemes for data management. Capacity of a system is defined as the number of operations the system can perform successfully, on an average, per unit time. The capacity of a system using voting is examined and compared with the capacity of a system using a single node. It is shown that the maximum increase in capacity by the use of majority voting is bounded by 1/p, where p is the steady-state probability of a node being alive. It is also shown that for a system employing majority voting, if the reliability of nodes is high, increasing the number of nodes to more than three gives only a marginal increase in capacity. Similar analyses are performed for three other voting schemes.",
    "actual_venue": "Software Engineering, Ieee Transactions"
  },
  {
    "abstract": "A recurrent neural network for kinematic control of redundant robot manipulators with torque minimization is presented. The proposed recurrent neural network is composed of two bidirectionally connected layers of neuron arrays. While the command signals of desired acceleration of the end-effector are fed into the input layer, the output layer generates the joint acceleration vector of the manipulator with joint torques being minimized. The proposed recurrent neural network is shown to be capable of asymptotic tracking of trajectory for the redundant manipulators with minimized joint torques",
    "actual_venue": "Systems, Man And Cybernetics"
  },
  {
    "abstract": "In this paper we consider a type system with a universal type ω where any term (whether open or closed, β-normalising or not) has type ω. We provide this type system with a realisability semantics where an atomic type is interpreted as the set of λ-terms saturated by a certain relation. The variation of the saturation relation gives a number of interpretations to each type. We show the soundness and completeness of our semantics and that for different notions of saturation (based on weak head reduction and normal β-reduction) we obtain the same interpretation for types. Since the presence of ω prevents typability and realisability from coinciding and creates extra difficulties in characterizing the interpretation of a type, we define a class U+ of the so-called positive types (where ω can only occur at specific positions). We show that if a term inhabits a positive type, then this term is β-normalisable and reduces to a closed term. In other words, positive types can be used to represent abstract data types. The completeness theorem for U+ becomes interesting indeed since it establishes a perfect equivalence between typable terms and terms that inhabit a type. In other words, typability and realisability coincide on U+. We give a number of examples to explain the intuition behind the definition of U+ and to show that this class cannot be extended while keeping its desired properties.",
    "actual_venue": "Annals Of Pure And Applied Logic"
  },
  {
    "abstract": "Data provided by sensors is always subjected to some level of uncertainty and inconsistency. Multisensor data fusion algorithms reduce the uncertainty by combining data from several sources. However, if these several sources provide inconsistent data, catastrophic fusion may occur where the performance of multisensor data fusion is significantly lower than the performance of each of the individual sensor. This paper presents an approach tomultisensor data fusion in order to decrease data uncertainty with ability to identify and handle inconsistency. The proposed approach relies on combining a modified Bayesian fusion algorithm with Kalman filtering. Three different approaches, namely, prefiltering, postfiltering and pre-postfiltering are described based on how filtering is applied to the sensor data, to the fused data or both. A case study to find the position of a mobile robot by estimating its x and y coordinates using four sensors is presented. The simulations show that combining fusion with filtering helps in handling the problem of uncertainty and inconsistency of the data.",
    "actual_venue": "Adv Artificial Intellegence"
  },
  {
    "abstract": "Despite the very well known smartphone issues such as on-off behaviour and battery/bandwidth limitations, in this paper we show that smartphones can be successfully employed in a crowdsourcing system to perform Internet AS-level topology discovery. We propose and illustrate a measurement methodology that takes these issues into account. We implemented such methodology in Portolan, our smartphone-based crowdsourcing system, and ran six months of measurements. We show that smartphones mobility allows to obtain measurements from 706 different ASes with just 200 active devices. Moreover, we show that our methodology manages to bring novelty with relatively few measurements. On average 27.75% of the AS links found by Portolan are not found by BGP measurements.",
    "actual_venue": "C2B()@Sigcomm"
  },
  {
    "abstract": "Tagging email is an important tactic for managing information overload. Machine learning methods can help the user with this task by predicting tags for incoming email messages. The natural user interface displays the predicted tags on the email message, and the user doesn't need to do anything unless those predictions are wrong (in which case, the user can delete the incorrect tags and add the missing tags). From a machine learning perspective, this means that the learning algorithm never receives confirmation that its predictions are correct---it only receives feedback when it makes a mistake. This can lead to slower learning, particularly when the predictions were not very confident, and hence, the learning algorithm would benefit from positive feedback. One could assume that if the user never changes any tag, then the predictions are correct, but users sometimes forget to correct the tags, presumably because they are focused on the content of the email messages and fail to notice incorrect and missing tags. The aim of this paper is to determine whether implicit feedback can provide useful additional training examples to the email prediction subsystem of TaskTracer, known as EP2 (Email Predictor 2). Our hypothesis is that the more time a user spends working on an email message, the more likely it is that the user will notice tag errors and correct them. If no corrections are made, then perhaps it is safe for the learning system to treat the predicted tags as being correct and train accordingly. This paper proposes three algorithms (and two baselines) for incorporating implicit feedback into the EP2 tag predictor. These algorithms are then evaluated using email interaction and tag correction events collected from 14 user-study participants as they performed email-directed tasks while using TaskTracer EP2. The results show that implicit feedback produces important increases in training feedback, and hence, significant reductions in subsequent prediction errors despite the fact that the implicit feedback is not perfect. We conclude that implicit feedback mechanisms can provide a useful performance boost for email tagging systems.",
    "actual_venue": "Uist"
  },
  {
    "abstract": "This paper advocates for the need to build a Microblogs Data Management System (MDMS) as an end-to-end data management system to support indexing, querying, and analyzing microblogs, e.g., Tweets, comments, or check-in's. We identify a set of characteristics for microblogging environments that are distinguishing from any other data management environment. Then, we propose a system architecture for the first Microblogs Data Management System, which includes indexing, querying, and recovery components. The indexing component is responsible for indexing recent data in memory, indexing older data in disk, and synchronizing the flow of data from memory to disk without affecting the query response time. The querying component is responsible for retrieving the query answer from both memory and disk storage as well as employing online selectivity estimation techniques tuned to the behavior of microblogs data. The recovery module allows for efficiently storing and processing incoming microblogs in memory without worrying about data loss.",
    "actual_venue": "Ieee International Conference On Mobile Data Management"
  },
  {
    "abstract": "With the fast evolution of digital video, research and development of new technologies are greatly needed to lower the cost of video archiving, cataloging and indexing, as well as improve the efficiency and accessibility of stored video sequences. A number of methods to respectively meet these requirements have been researched and proposed. As one of the most important research topics, video abstraction helps to enable us to quickly browse a large video database and to achieve efficient content access and representation. In this paper, a video abstraction algorithm based on the visual attention model and online clustering is proposed. First, shot boundaries are detected and key frames in each shot are extracted so that consecutive key frames in a shot have the same distance. Second, the spatial saliency map indicating the saliency value of each region of the image is generated from each key frame and regions of interest (ROI) is extracted according to the saliency map. Third, key frames, as well as their corresponding saliency map, are passed to a specific filter, and several thresholds are used so that the key frames containing less information are discarded. Finally, key frames are clustered using an online clustering method based on the features in ROIs. Experimental results demonstrate the performance and effectiveness of the proposed video abstraction algorithm.",
    "actual_venue": "Sig Proc: Image Comm"
  },
  {
    "abstract": "The nonconvex separation theorem for the Hiriart-Urruty nonlinear scalar functional (Hiriart-Urruty in Math Oper Res 4:79–97, 1979) and its sublinearity and monotonicity with respect to convex cone have played an important role in the research of vector optimization problems. By means of these special properties, this functional also can be used as a coherent measure of an investment. This work is devoted to investigating the properties of this special scalarizing function with respect to a co-radiant set, which is more general than a cone and is also a main tool in the study of approximate solutions in vector optimization problems. We first show that although this scalar functional with respect to the co-radiant set is not necessarily positively homogeneous, it satisfies some special properties, which imply it is a co-radiant function and also has subadditivity and monotonicity under the convexity assumption. Based on the subadditivity property, we calculate the classical (Fenchel) subdifferentials for the scalar functional. Finally, as the applications of our results, we give the nonlinear scalar characterizations for the approximate solutions of vector optimization problems, and establish the Lagrange multiplier rules in term of the Mordukhovich subdifferential in Asplund spaces.",
    "actual_venue": "Journal Of Global Optimization"
  },
  {
    "abstract": "Crossover is the most complicated of the standard genetic operators as well as one of the main operators in genetic algorithms. The role of the crossover operator has not been satisfactorily explained. It has not been easy to show if it is essential for the construction and exploitation of building blocks during evolution. In this paper we introduce an adaptive crossover operator for two test functions. The results show that there is a clear increase in the rate of evolution when information on the state of the population is used to select the crossover point, compared to random selection of the crossover point.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "J.-Y. Girard's original definition of proof nets for linear logic involves boxes. The box is the unit for erasing and duplicating fragments of proof nets. It imposes synchronization, limits sharing, and impedes a completely local view of computation. The authors describe an implementation of proof nets without boxes. Proof nets are translated into graphs of the sort used in optimal λ-calculus implementations; computation is performed by simple graph rewriting. This graph implementation helps in understanding optimal reductions in the λ-calculus and in the various programming languages inspired by linear logic",
    "actual_venue": "Lics"
  },
  {
    "abstract": "In this paper, a robust sliding mode learning control (SMLC) scheme is developed for steer-by-wire (SbW) systems. It is shown that an SbW system with uncertain system parameters and unknown external disturbance from the interactions between the tires and the variable road surface can be modeled as a second-order system. A sliding mode learning controller can then be designed to drive both the sliding variable and the tracking error between the steered front-wheel angle and the hand-wheel reference angle to asymptotically converge to zero. The proposed SMLC scheme exhibits many advantages over the existing schemes, including: 1) no information about vehicle parameter uncertainties and self-aligning torque variations is required for controller design; and 2) the control algorithm is capable of efficiently adjusting the closed-loop response based on the most recent history of the closed-loop stability and ensuring a robust steering performance. Both simulations and experiments are presented to show the excellent steering performance and the effectiveness of the proposed learning control methodology.",
    "actual_venue": "Ieee T Vehicular Technology"
  },
  {
    "abstract": "Real time multimedia application is a challenging issue in the mobile networks, due to high compression requirements and limited and varying channel bandwidth. H.264/AVC addresses these issues with its excellent coding efficiency, error resiliency features and provision of bit rate adaptivity, using switching (SP-) frames. To reduce the quantization errors due to multiple quantization, SP- frames are usually coded with finer quantization parameters (QP) than the rest of the sequence, resulting in an increased bit rate, sometimes exceeding to that of intra (I-) frames. This limits the basic advantages of SP-frames over I-frames and that is better coding efficiency, leading to poor utilization of the resources like bandwidth. Thus the question is \"what is the safe range of QP such that the bit rate of the switching frames does not exceed that of the I-frames\"? The aim of this paper to suggest a solution to this problem by analyzing characteristics of SP-frames and to suggest a safe range of QP for switching frames ensuring that its bit rate remains lower than that of I-frames.",
    "actual_venue": "Mobimedia"
  },
  {
    "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. ADEM (Lowe et al. 2017) formulated the automatic evaluation of dialogue systems as a learning problem and showed that such a model was able to predict responses which correlate significantly with human judgements, both at utterance and system level. Their system was shown to have beaten word-overlap metrics such as BLEU with large margins. We start with the question of whether an adversary can game the ADEM model. We design a battery of targeted attacks at the neural network based ADEM evaluation system and show that automatic evaluation of dialogue systems still has a long way to go. ADEM can get confused with a variation as simple as reversing the word order in the text! We report experiments on several such adversarial scenarios that draw out counterintuitive scores on the dialogue responses. We take a systematic look at the scoring function proposed by ADEM and connect it to linear system theory to predict the shortcomings evident in the system. We also devise an attack that can fool such a system to rate a response generation system as favorable. Finally, we allude to future research directions of using the adversarial attacks to design a truly automated dialogue evaluation system.",
    "actual_venue": "Thirty-Second Aaai Conference On Artificial Intelligence / Thirtieth Innovative Applications Of Artificial Intelligence Conference / Eighth Aaai Symposium On Educational Advances In Artificial Intelligence"
  },
  {
    "abstract": "Traditionally, web standards in general and Cascading Style Sheets (CSS) in particular take a long time from when they are defined by the W3C until they are implemented by browser vendors. This has been a limitation not only for authors, who had to wait even years before they were able to use certain CSS properties in their web pages, but also for the creators of the specification itself, who were not able to test their proposals in practice. In this paper we present ALMcss, a JavaScript prototype that implements the CSS Template Layout Module, a proposal for an addition to CSS to make it a more capable layout language. It has been developed inside the W3C CSS Working Group by two of the authors of this paper. We present the rationale of the module and an introduction to its syntax, before discussing the design of our prototype. ALMcss has served us as a proof of concept that the Template Layout Module is not only feasible, but it can be in fact implemented in current web browsers using just JavaScript and the Document Object Model (DOM). In addition, ALMcss allows web designers to start to use today the new layout capabilities of CSS that the module provides, even before it becomes an official W3C specification.",
    "actual_venue": "Acm Symposium On Document Engineering"
  },
  {
    "abstract": "This paper considers the problem of increasing the storage density in fault-tolerant VLSI systems which require only limited data retention times. To this end, the concept of storing many bits per memory cell is applied to area-efficient and fully logic-compatible gain-cell-based dynamic memories. A memory macro in 90-nm CMOS technology including multilevel write and read circuits is proposed and analyzed with respect to its read failure probability due to within-die process variations by means of Monte Carlo simulations.",
    "actual_venue": "Great Lakes Symposium On Vlsi"
  },
  {
    "abstract": "Pipelines are the main transportation means for oil and gas products across large distances. Due to the severe conditions they operate in, they are regularly inspected using conventional Pipeline Inspection Gages (PIGs) for corrosion damage. The motivation for researching a real-time distributed monitoring solution arose to mitigate costs and provide a proactive indication of potential failures. Fiber optic sensors with polymer claddings provide a means of detecting contact with hydrocarbons. By coating the fibers with a layer of metal similar in composition to that of the parent pipeline, corrosion of this coating may be detected when the polymer cladding underneath is exposed to the surrounding hydrocarbons contained within the pipeline. A Refractive Index (RI) change occurs in the polymer cladding causing a loss in intensity of a traveling light pulse due to a reduction in the fiber's modal capacity. Intensity losses may be detected using Optical Time Domain Reflectometry (OTDR) while pinpointing the spatial location of the contact via time delay calculations of the back-scattered pulses. This work presents a theoretical model for the above sensing solution to provide a design tool for the fiber optic cable in the context of hydrocarbon sensing following corrosion of an external metal coating. Results are verified against the experimental data published in the literature.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "The design and development of web-based educational systems for people with special abilities have recently attracted the attention of the research community. However, although a number of systems that claim to meet accessibility needs and preferences are proposed, most of them are typically supported by hypermedia and multimedia educational content that is specially designed for the user targeted group. Such approaches prevent their user groups (both learners and their tutors) from accessing other available resources. Therefore, it is important to be able to built generic e-learning systems that would allow the reuse of existing learning resources in different accessibility demanding applications. To this end, in this article we propose a methodology for defining an accessibility application profile that captures the accessibility properties of learning objects in a standard form and we examine its application to the IEEE Learning Object Metadata (LOM) standard.",
    "actual_venue": "The New Review Of Hypermedia And Multimedia"
  },
  {
    "abstract": "We propose a new tactile device which measures several DOF vibrations propagating along the finger. The device can be used both as a tactile sensor and human-machine interface. In this paper, as a pilot experiment, we carried out experiments on the identification of contact positions by using 2 DOF vibrations. It was shown that with a 2-axis accelerometer placed on the backside of the proximal phalanx, tapping on distal phalanx and middle phalanx can be discriminated.",
    "actual_venue": "Tsukaba"
  },
  {
    "abstract": "Many currently available powered knee prostheses (PKP) use finite state impedance control to operate a prosthetic knee joint. The desired impedance values were usually manually calibrated with trial-and-error in order to enable near-normal walking pattern. However, such a manual approach is inaccurate, time consuming, and impractical. This paper aimed to design an expert system that can tune the control impedance for powered knee prostheses automatically and quickly. The expert system was designed based on fuzzy logic inference (FLI) to match the desired knee motion and gait timing while walking. The developed system was validated on an able-bodied subject wearing a powered prosthesis. Preliminary experimental results demonstrated that the developed expert system can converge the user's knee profile and gait timing to the desired values within 2 minutes. Additionally, after the auto-tuning procedure, the user produced more symmetrical gait. These preliminary results indicate the promise of the designed expert system for quick and accuracy impedance calibration, which can significantly improve the practical value of powered lower limb prosthesis. Continuous engineering efforts are still needed to determine the calibration objectives and validate the expert system.",
    "actual_venue": "Icorr"
  },
  {
    "abstract": "In this study, whether using Augment Reality (AR)-based learning materials could benefit high school students in the process of Chinese writing was explored, along with the pros and cons of using AR for acquiring Chinese writing skills. In order to reduce the gap between the designers and practitioner teachers, Chinese instructors were invited to co-design the AR-based writing materials to achieve the integration of learners, teachers and educational system technology developers in a collaborative process. The AR-based writing support system was provided to a total of 30 twelfth-grade students who participated in the experiment. The students in the experimental group participated in the writing activity using both AR-based learning material and paper-based supports, while the control group worked with only paper-based writing support materials. The results revealed that the AR techniques helped the intermediate-level students the most in their writing performance of content control, article structure and wording. The students, especially the low-achievers, reflected that the functions of the AR system supported them to start writing the first paragraph more quickly, and enriched their ideas. A possible mode for integrating AR techniques in writing courses is proposed. This paper could serve as a reference for educators and learning technology researchers who wish to design AR-guided writing learning materials or courses with the goal of encouraging learners to experience the writing process in a variety of settings.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "We address the simple assembly line balancing problem: minimize the number of stations m for processing n partially ordered operations V={1,2,…,n} within the cycle time c. The processing time ti of operation i∈V and cycle time c are given. However, during the life cycle of the assembly line the values ti are definitely fixed only for the subset of automated operations V⧹V∼. Another subset V∼⊆V includes manual operations, for which it is impossible to fix the exact processing times during the whole life cycle of the assembly line. If j∈V∼, then operation time tj can be different for different cycles of production process. For the optimal line balance b of a paced assembly line with vector t=(t1,t2,…,tn) of the operation times, we investigate stability of its optimality with respect to possible variations of the processing times tj of the manual operations j∈V∼. In particular, we derive necessary and sufficient conditions when optimality of the line balance b is stable with respect to sufficiently small variations of the operation times tj, j∈V∼. We show how to calculate the maximal value of independent variations of the processing times of all the manual operations, which definitely keep the feasibility and optimality of the line balance b.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "To provide verifiability, cryptographic voting protocols usually require a public bulletin board for publishing the election data, so that the data can be read and verified by everyone. The basic requirements on such a board are similar for most protocols, for example that nothing can be changed or deleted from the board. Typically, when it comes to implement a voting protocol in a real system, many additional requirements arise and they can differ from protocol to protocol. This paper shows based on the protocol of UniVote what these requirements might be and what other problems may arise from an operational and organisational point of view. Based on the understanding of these problems, we propose a generic interface for the main board functionalities. This interface offers a flexible way of extending the properties of a public bulletin board to comply with all sorts of additional requirements. We give multiple examples of properties that we identified as desirable for the public bulletin board in UniVote.",
    "actual_venue": "Conference For E-Democracy And Open Government"
  },
  {
    "abstract": "Objective: In this study, we aim to investigate how users' visual performance with a small flexible display changes based on the direction (i.e., convex, concave) and the magnitude (i.e., low, high) of the display curvature. Background: Despite the wide interest in flexible display materials and deformable displays, the potential effects of nonplanar display surfaces on human perception and performance have received little attention. This study is the first to demonstrate how curving affects visual performance with an actual flexible display (4.5-in. active-matrix organic light-emitting diode). Method: In a series of three experiments, we compared the performance with a planar display to the performance with concave and convex display surfaces with low and high curvature magnitudes. Two visual search tasks were employed that required the subject to detect target letters based on their contrast (Experiments 1 and 2) and identity (Experiment 3). Performance was measured as the sensitivity of target detection (d) and threshold time of the search, respectively. Results: There were similar sensitivities for targets across the curvature variants, but the high-magnitude curvatures resulted in prolonged search times, especially for the convex form. In both of the tasks, performance was dependent on the display location, which was defined as the target's distance from the display center. Conclusion: High curvature magnitudes should be avoided, even in small displays, because large local changes in visual stimuli decrease processing speed outside the central display. Application: The findings have implications for the development of technologies, applications, and user interfaces for flexible displays and the design of visual display devices.",
    "actual_venue": "Human Factors"
  },
  {
    "abstract": "The logarithmic number system (LNS) has found appeal in digital arithmetic because it allows multiplication and division to be performed much faster and more accurately than with the widely used floating-point (FP) number formats. We review the sign/logarithmic number system and present a comparison of various techniques and architectures for performing arithmetic operations efficiently in LNS. As a case study, we describe the European logarithmic microprocessor, a device built in the framework of a research project launched in 1999. Comparison of the arithmetic performance of this microprocessor with that of a commercial superscalar pipelined FP processor leads to the conclusion that LNS can be successfully deployed in general-purpose systems.",
    "actual_venue": "Pacific Grove, Ca"
  },
  {
    "abstract": "Robust, invisible watermarking of digital images or video very often has to satisfy a set of mutually conflicting requirements. These requirements include invisibility, robustness to image processing and geometric transformations, low false positive probabilities, high payload, fast detection and embedding, etc. In this paper we show how invariance to translations can be exploited to increase the payload. This is achieved by simultaneous embedding of several shifted watermark patterns, such that the information content is hidden in the relative shifts of the patterns. The principles of this are illustrated for the case of JAWS, a spatial domain watermark method developed by Philips. The method can easily be applied to other watermark methods which are able to detect shifted versions of watermarks",
    "actual_venue": "Icmcs"
  },
  {
    "abstract": "Bent functions are actively investigated for their various applications in cryptography, coding theory and combinatorial design. As one of their generalizations, negabent functions are also quite useful, and they are originally defined via nega-Hadamard transforms for boolean functions. In this paper, we look at another equivalent definition of them. It allows us to investigate negabent functions f on F2n$\\\\mathbb {F}_{2^{n}}$, which can be written as a composition of a univariate polynomial over F2n$\\\\mathbb {F}_{2^{n}}$ and the trace mapping from F2n$\\\\mathbb {F}_{2^{n}}$ to F2$\\\\mathbb {F}_{2}$. In particular, when this polynomial is a monomial, we call f a monomial negabent function. Families of quadratic and cubic monomial negabent functions are constructed, together with several sporadic examples. To obtain more interesting negabent functions in special forms, we also look at certain negabent polynomials. We obtain several families of cubic negabent functions by using the theory of projective polynomials over finite fields.",
    "actual_venue": "Cryptography And Communications"
  },
  {
    "abstract": "In this paper, we consider the problem of optimization of a cost function on a Grassmann manifold. This problem appears in system identification in the behavioral setting, which is a structured low-rank approximation problem. We develop an optimization approach based on switching coordinate charts. This method reduces the optimization problem on the manifold to an optimization problem in a bounded domain of a Euclidean space. We compare the proposed approach with state-of-the-art methods based on data-driven local coordinates and Riemannian geometry, and show the connections between the methods. Compared to the methods based on the local coordinates, the proposed approach allows to use arbitrary optimization methods for solving the corresponding subproblems in the Euclidean space.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "We consider linear time-invariant systems ( C ,  A ,  B ) where the output map  C  is partitioned into  k  blocks  C i . We assume that the system are block right invertible, i.e. the rank of ( C ,  A ,  B ) equals the sum of the ranks of the subsystems ( C i ,  A ,  B ). We give, for the first time, a necessary and sufficient condition for the solution of the block decoupling problem using static state feedbacks of the type  u  =  Fx  +  Gv , with  G  possibly nonregular; for solving the decoupling problem we impose that the rank of the closed-loop system equals that of ( C ,  A ,  B ). This is a structural condition in terms of invariant lists of integers: the infinite zero orders, the block essential orders and Morse's list  I 2  of ( C ,  A ,  B ). The main result (Theorem 3) generalizes that of our previous work for the so called Morgan's Problem, i.e. the row by row decoupling problem.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "•We propose an automatic segmentation method for brain image segmentation.•We propose a supervised learning algorithm based on artificial neural network.•Shape context information are brought into the frame of machine learning approach.•Our method achieved competitive results based on a public image database.•Our method requires comparatively short training and testing time.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "In this paper we envision and discuss the idea of the Web of Augmented Things (WoAT). The idea is about the integration of Web of Things with Augmented Worlds, i.e. distributed software systems augmenting the physical space with virtual entities and holograms in an Augmented Reality perspective, eventually coupled with smart things that are part of the same environment.",
    "actual_venue": "Ieee International Conference On Software Architecture Workshops"
  },
  {
    "abstract": "We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single f...",
    "actual_venue": "Ieee Transactions Reliability"
  },
  {
    "abstract": "In previous work (Gough and Way 2004), we showed that our Example-Based Machine Translation (EBMT) system improved with respect to both coverage and quality when seeded with increasing amounts of training data, so that it significantly outperformed the on-line MT system Logomedia according to a wide variety of automatic evaluation metrics. While it is perhaps unsurprising that system performance is correlated with the amount of training data, we address in this paper the question of whether a large-scale, robust EBMT system such as ours can outperform a Statistical Machine Translation (SMT) system. We obtained a large English-French translation memory from Sun Microsystems from which we randomly extracted a near 4K test set. The remaining data was split into three training sets, of roughly 50K, 100K and 200K sentence-pairs in order to measure the effect of increasing the size of the training data on the performance of the two systems. Our main observation is that contrary to perceived wisdom in the field, there appears to be little substance to the claim that SMT systems are guaranteed to outperform EBMT systems when confronted with ‘enough’ training data. Our tests on a 4.8 million word bitext indicate that while SMT appears to outperform our system for French-English on a number of metrics, for English-French, on all but one automatic evaluation metric, the performance of our EBMT system is superior to the baseline SMT model.",
    "actual_venue": "Natural Language Engineering"
  },
  {
    "abstract": "A generalized decision logic in interval-set-valued informa- tion tables is introduced, which is an extension of decision logic studied by Pawlak. Each object in an interval-set-valued information table takes an interval set of values. Consequently, two types of satisfiabilities of a formula are introduced. Truth values of formulas are defined to be interval-valued, instead of single-valued. A semantics model of the pro- posed logic language is studied.",
    "actual_venue": "Rsfdgrc"
  },
  {
    "abstract": "The growing interest in ontologies is concomitant with the increasing use of agent systems in user environment. On- tologies have established themselves as schemas for encoding knowledge about a particular domain, which can be inter- preted by both humans and agents to accomplish a task in cooperation. However, construction of the domain ontolo- gies is a bottleneck, and planning towards reuse of domain ontologies is essential. Current methodologies concerned with ontology development have not dealt with explicit reuse of domain ontologies. This paper presents guidelines for systematic construction of reusable domain ontologies. A purpose-driven approach has been adopted. The guidelines have been used for constructing ontologies in the Experi- mental High-Energy Physics domain.",
    "actual_venue": "OAS"
  },
  {
    "abstract": "This study is aimed at investigating the importance of positive reputation from external experience sources for diffusion of a mobile convergent device, Smartphone. We categorized the different reputation sources into four; personal, expert, consumers, and mass media. By conducting a conjoint analysis with 53 samples from a university, we found that the prior experience of consumer group has the largest importance for the purchasing decision of the potential adopters. Moreover, early adopters and female consumers give more importance on the prior consumers' opinions. The reputation from expert and mass media was relatively lower than it from consumers and personal group. We discuss the implications for utilizing social networking service for diffusion of the innovative convergent device and the directions for further study.",
    "actual_venue": "Ictc"
  },
  {
    "abstract": "Game Design for Social Networks is a one-day workshop where the participants will learn about creating and designing game concepts for online social networking platforms, such as Facebook and Twitter. The workshop will center around exercises that enable participants to come up with new game concepts. Participants will also be able to bring existing ideas to the workshop, and have a chance to refine them through the analytical design approaches introduced throughout the day. This paper outlines the theoretical backgrounds of the workshop.",
    "actual_venue": "Mindtrek"
  },
  {
    "abstract": "This paper concerns the study of the solutions of sup-T equations such that ATR = A, where T is a triangular norm and R is a fuzzy relation.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "The computer programs which control autonomous mobile robots can make use of fuzzy processing and computing with words in order to extend their capabilities by allowing them to be less literal and exact match oriented. Both sensor data interpretation and planned activities subtleties benefit from fuzzy inexact processing.(1) The sciences of astronomy and physics have long used conceptual metaphor productively, in such terms as \"event horizon\", \"gravity well\", \"curved space\", \"time dilation\". The same technologies which provides the mechanisms for such term usage are available to robotic and geo-spatial processing.(2) Productive conceptual metaphors can be used functionally on computers.",
    "actual_venue": "Ic-Ai"
  },
  {
    "abstract": "Inter-hemispheric asynchrony within the multichannel recordings of newborn EEG is associated with abnormal functionality of the newborn brain. Mean Phase Coherence (MPC) as a bivariate phase synchrony measure is widely used for pair-wise comparisons of scalp EEG phase information. A bivariate measure, however, is unlikely to capture the key feature of asynchrony seen in the sick neonatal brain, which is characterized by a global disruption of synchrony. In this study, the concept of cointegration is employed to generalize the bivariate MPC to deal with the multivariate case. The performance of the generalized MPC (GMPC) is evaluated using two simulated signals. It is also tested on a multichannel newborn EEG dataset with asynchronous inter-hemispheric bursts. The proposed method can be used to detect and quantify the degree of inter-hemispheric asynchrony from EEG signals.",
    "actual_venue": "International Conference On Information Science, Signal Processing And Their Applications"
  },
  {
    "abstract": "Role-Based Access Control (RBAC) usually enables a higher level view of authorization. In this model, access permissions are assigned to roles and, in turn, roles are allocated to subjects. The usefulness of the RBAC model is well documented. It includes simplicity, consistency, scalability and ease of manageability. In practice, however, only limited versions of RBAC seem to have been successfully implemented, notably in applications such as databases and operating systems. The problem stems from the fact that most applications require a finer degree of authorization than what core RBAC models are able to provide. In theory, current RBAC models can be adapted to capture fine grained authorizations by dramatically increasing the number of distinct roles in these models. However, this solution comes at an unacceptably high cost of allocating low level privileges which eliminates the major benefits gained from having a high level RBAC model. This paper presents a methodology for refining abstract RBAC models into new Parameterized RBAC models which provide finer grain of authorizations. The semantics of the Parameterized RBAC model is given as a state-based core RBAC model expressed in the formal specification notation Z. By systematically applying this methodology the scope of applications of RBAC is substantially extended and the major benefits of having the core model are maintained.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "The Situation Assessment process is evolving from signal-analysis based centralized models to high-level reasoning based net-centric models, according to new paradigms of information fusion proposed by recent research. In this paper we propose a knowledge-based approach to Situation Assessment, and we apply it to maritime surveillance. A symbolic model of the world is given to an agent based framework, that use Description Logics based automatic reasoning to devise on estimate of the situation. The described approach potentially allows distributed Situation Assessment through agent collaboration. The goal is to support the understanding of the situation by relying on automatic interpretation processes, in order to provide the human operators with a synthetic vision, pointing out which are the elements on the scenario that require human intervention. The success of high level reasoning techniques is shown through experiments in a real maritime scenario, in which our approach is compared to the performances of human operators which monitor the situation without any support of an automatic reasoning system.",
    "actual_venue": "Icaart : Proceedings Of The International Conference On Agents And Artificial Intelligence"
  },
  {
    "abstract": "In this paper, the sparse matrix decompositions for DFT matrix and DCT matrix are proposed. Based on these propositions, we develop a fast hybrid DFT and DCT architecture for OFDM. In addition, we address the OFDM based on DFT or DCT in Cognitive Radio system. An adaptive OFDM based on DFT or DCT in Cognitive Radio system has the capacity to nullify individual carriers to avoid interference to the licensed users. Therefore, there could be a considerably large number of zero-valued inputs/outputs for the IDFT/DFT or IDCT/DCT on the OFDM transceiver. Hence, the standard methods of DFT and DCT are no longer efficient due to the wasted operations on zero. Based on this observation, we present a transform decomposition on two dimensional (2-D) systolic array for IDFT/DFT and IDCT/DCT, this algorithm can achieve an efficient computation for OFDM in Cognitive Radio system.",
    "actual_venue": "Fgcn"
  },
  {
    "abstract": "Recent years have seen a trend towards decentralisation - from initiatives on decentralized web to decentralized network infrastructures. In this position paper, we present an architectural vision for decentralising cloud service infrastructures. Our vision is on community cloud infrastructures on top of decentralised access infrastructures i.e. community networks, using resources pooled from the community. Our architectural vision considers some fundamental challenges of integrating the current state of the art virtualisation technologies such as Software Defined Networking (SDN) into community infrastructures which are highly unreliable. Our proposed design goal is to include lightweight network and processing virtualization with fault tolerance mechanisms to ensure sufficient level of reliability to support local services.",
    "actual_venue": "Mecc@Middleware"
  },
  {
    "abstract": "<P>With the recent focus on sustainability, firms making adjustments to their production or distribution capacity levels often have the option of investing in newer technologies with lower carbon footprints and/or energy consumption. These more sustainable technologies typically require a higher up-front investment but have lower operating (fuel or energy) costs. What complicates this decision is the fact that the projected dollar savings from the more sustainable technologies fluctuate considerably due to uncertainty in fuel prices, and the total capacity may not be utilized at 100% because of fluctuations in the demand for the product. We consider the firm's capacity adjustments over time given a portfolio of technology options when the demand and the fuel costs are stochastic and possibly dependent. Our model also allows for usage-based capacity deterioration. We provide the analytical structure of the optimal policy, which assigns different control limits for investing, staying put, and disinvesting in the capacities of the competing technology choices for each realization of demand and fuel costs at each period. We also present an application of our model to the problem of designing a delivery truck fleet for a beverage distributor.</P>",
    "actual_venue": "Andsom-Manufacturing And Service Operations Management"
  },
  {
    "abstract": "The development of tools and technologies to facilitate appropriate and effective data sharing is becoming increasingly important in many academic disciplines. In particular, the 'data explosion' problem associated with the Life Sciences has been recognised by many researchers and commented upon widely, as have the associated data management problems. In this paper we describe how a middleware framework that supports the secure sharing and aggregation of data from heterogeneous data sources--developed initially to underpin the sharing of healthcare-related data--is being used to support Systems Biology research at the University of Oxford. As well as giving an overview of the framework and its application, we attempt to set our work within the wider context of the emerging challenges associated with data sharing within the Life Sciences.",
    "actual_venue": "Dils"
  },
  {
    "abstract": "In this paper, we present the use of two evolutionary algorithms to estimate fundamental matrices. We first propose a modification of the Hybrid Taguchi Genetic Algorithm (HTGA) that employs a single objective function, either geometric or algebraic distance, for optimization. We then propose to use a multi-objective optimization algorithm, Intelligent Multi-Objective Evolutionary Algorithm (IMOEA), to optimize both geometric and algebraic distances concurrently. Our experiments show that the proposed modified HTGA (MHTGA) and IMOEA produce more accurate estimation of fundamental matrices than the traditional Genetic Algorithm (GA) and the original HTGA do.",
    "actual_venue": "Journal Of Information Science And Engineering"
  },
  {
    "abstract": "We present SwitchBlade, a platform for rapidly deploying custom protocols on programmable hardware. SwitchBlade uses a pipeline-based design that allows individual hardware modules to be enabled or disabled on the fly, integrates software exception handling, and provides support for forwarding based on custom header fields. SwitchBlade's ease of programmability and wire-speed performance enables rapid prototyping of custom data-plane functions that can be directly deployed in a production network. SwitchBlade integrates common packet-processing functions as hardware modules, enabling different protocols to use these functions without having to resynthesize hardware. SwitchBlade's customizable forwarding engine supports both longest-prefix matching in the packet header and exact matching on a hash value. SwitchBlade's software exceptions can be invoked based on either packet or flow-based rules and updated quickly at runtime, thus making it easy to integrate more flexible forwarding function into the pipeline. SwitchBlade also allows multiple custom data planes to operate in parallel on the same physical hardware, while providing complete isolation for protocols running in parallel. We implemented SwitchBlade using NetFPGA board, but SwitchBlade can be implemented with any FPGA. To demonstrate SwitchBlade's flexibility, we use SwitchBlade to implement and evaluate a variety of custom network protocols: we present instances of IPv4, IPv6, Path Splicing, and an OpenFlow switch, all running in parallel while forwarding packets at line rate.",
    "actual_venue": "Sigcomm"
  },
  {
    "abstract": "The proper functioning of any living cell relies on complex networks of gene regulation. These regulatory interactions are not static but respond to changes in the environment and evolve during the life cycle of an organism. A challenging objective in computational systems biology is to infer these time-varying gene regulatory networks from typically short time series of transcriptional profiles. While homogeneous models, like conventional dynamic Bayesian networks, lack the flexibility to succeed in this task, fully flexible models suffer from inflated inference uncertainty due to the limited amount of available data. In the present paper we explore a semi-flexible model based on a piecewise homogeneous dynamic Bayesian network regularized by gene-specific inter-segment information sharing. We explore different choices of prior distribution and information coupling and evaluate their performance on synthetic data. We apply our method to gene expression time series obtained during the life cycle of Drosophila melanogaster, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict an in vivo regulatory network of five genes in Saccharomyces cerevisiae subjected to a changing environment.",
    "actual_venue": "Machine Learning"
  },
  {
    "abstract": "If r is a natural number and H is a (finite or infinite) system of (finite or infinite) sets with pairwise r -element intersection, then H has a one-to-one choice function.",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "We present here a novel approach (and its implementation) for the automatic extraction of semantic knowledge from Java libraries.We want to match software libraries, so we need to obtain as much information as possible to use it in the matching process. For this purpose, this approach extracts information about the structure of the classes (i.e., name, fields and hierarchy), as well as information about the behavior of the classes (i.e., methods).In the literature, to our knowledge, it can be only found lightweight approaches to the extraction of this kind of information from Java object code. The approach is implemented in an automatic extraction tool (called Jar2Ontology) that has been developed as a plug-in of the Protege Ontology and Knowledge Acquisition System. Jar2Ontology extracts the semantics from Java libraries and translates it into OWL (Ontology Web Language).",
    "actual_venue": "Iceis"
  },
  {
    "abstract": "Let X i , i =1,…, n , be n = n ( N ) independent random subsets of \\s{1,2,…, N \\s}, each selected at random out of the 2 N subsets. We present some asymptotic ( N →∞) properties of \\s{ X i \\s}, e.g. if n /2 N /3 →∞ then \\s{ X i \\s} contains mutually disjoint three sets, while if n /2 N /3 → 0 then \\s{ X i \\s} contains no such three sets, almost surely.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "By determining those added assumptions sufficient to make the logical form of a natural-language sentence provable, abductive inference can be used in the interpretation of sentences to determine the information to be added to the listener's knowledge, i.e., what the listener should learn from the sentence. Some new forms of abduction are more appropriate to the task of interpreting natural language than those used in the traditional diagnostic and design synthesis applications of abduction. In one new form, least specific abduction, only literals in the logical form of the sentence can be assumed. The assignment of numeric costs to axioms and assumable literals permits specification of preferences on different abductive explanations. Least specific abduction is sometimes too restrictive. Better explanations can sometimes be found if literals obtained by backward chaining can also be assumed. Assumption costs for such literals are determined by the assumption costs of literals in the logical form and functions attached to the antecedents of the implications. There is a new Prolog-like inference system that computes minimum-cost explanations for these abductive reasoning methods.",
    "actual_venue": "Natural Language And Logic"
  },
  {
    "abstract": "A theme widely studied in the control systems and discrete-event systems communities concerns the characterization of the behavior that is achievable by a plant system via interconnection with a controller system. Many results have been found for several classes of discrete, continuous and hybrid systems. However, to the best of our knowledge, current literature does not yet consider stochastic systems. In this paper we aim at filling this gap and address this control problem for the class of Stochastic Descriptor Systems (SDS): given a plant SDS and a desired stochastic behavior, expressed as a SDS, we study conditions under which there exists a SDS controller such that the interconnection between the plant and the controller yields a system having the same stochastic external behavior of the desired SDS. Necessary and sufficient conditions are derived in terms of stochastic behavioral inclusions. When such conditions are satisfied we show that the so-called canonical controller, obtained as the interconnection of the plant SDS and the desired SDS, provides a solution to the stochastic control problem considered.",
    "actual_venue": "Ieee Annual Conference On Decision And Control"
  },
  {
    "abstract": "Most adults are overweight or obese in many western countries. Several population-level interventions on the physical, economical, political, or sociocultural environment have thus attempted to achieve a healthier weight. These interventions have involved different weight-related behaviours, such as food behaviours. Agent-based models (ABMs) have the potential to help policymakers evaluate food behaviour interventions from a systems perspective. However, fully realizing this potential involves a complex procedure starting with obtaining and analyzing data to populate the model and eventually identifying more efficient cross-sectoral policies. Current procedures for ABMs of food behaviours are mostly rooted in one technique, often ignore the food environment beyond home and work, and underutilize rich datasets. In this paper, we address some of these limitations to better support policymakers through two contributions. First, via a scoping review, we highlight readily available datasets and techniques to deal with these limitations independently. Second, we propose a three steps' process to tackle all limitations together and discuss its use to develop future models for food behaviours. We acknowledge that this integrated process is a leap forward in ABMs. However, this long-term objective is well-worth addressing as it can generate robust findings to effectively inform the design of food behaviour interventions.",
    "actual_venue": "Computational And Mathematical Methods In Medicine"
  },
  {
    "abstract": "We propose a novel semi-supervised learning method for convolutional neural networks (CNNs). CNN is one of the most popular models for deep learning and its successes among various types of applications include image and speech recognition, image captioning, and the game of 'go'. However, the requirement for a vast amount of labeled data for supervised learning in CNNs is a serious problem. Unsupervised learning, which uses the information of unlabeled data, might be key to addressing the problem, although it has not been investigated sufficiently in CNN regimes. The proposed method involves both supervised and unsupervised learning in identical feedforward networks, and enables seamless switching among them. We validated the method using an image recognition task. The results showed that learning using non-labeled data dramatically improves the efficiency of supervised learning.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "We present a novel approach to solve the problem of segmenting a sequence of animated objects into near-rigid components based on k given poses of the same non-rigid object. We model the segmentation problem as a clustering problem in dual space and find near-rigid segments with the property that segment boundaries are located at regions of large deformation. The presented approach is asymptotically faster than previous approaches that achieve the same property and does not require any user-specified parameters. However, if desired, the user may interactively change the number of segments. We demonstrate the practical value of our approach using experiments.",
    "actual_venue": "The Visual Computer"
  },
  {
    "abstract": "We propose an approach for view angle invariant recognition of 3D objects, based on modeling the variations of local feature values as function of view angle. In recognition stage we can compute the probabilities for any pixel that there is certain feature in a given pose angle. Any maximum likelihood or posterior based estimation methods can then be applied to infer the objects and their view parameters. We demonstrate the method with piecewise linear model for the pose effects, to recognize the location and pose of a head from the two eyes.",
    "actual_venue": "Scia"
  },
  {
    "abstract": "Early inflammation following status epilepticus has been implicated in the development of epilepsy and the evolution of brain injury, yet its precise role remains unclear. The development of non-invasive imaging markers of inflammation would enable researchers to test this hypothesis in vivo and study its temporal progression in relation to epileptogenic insults. In this study we have investigated the potential of a targeted magnetic resonance imaging contrast agent – vascular cell adhesion molecule 1 antibody labelled iron oxide – to image the inflammatory process following status epilepticus in the rat lithium–pilocarpine model. Intravascular administration of the targeted contrast agent was performed at approximately 1day following status epilepticus. The control group received diazepam prior to pilocarpine to prevent status epilepticus. Magnetic resonance imaging of rats was performed before and after contrast administration. Comparison with quantitative T2 measurements was also performed. At the end of the study, brains were removed for ex vivo magnetic resonance imaging and histology. Marked focal hypointensities caused by contrast agent binding were observed on in vivo magnetic resonance images in the post status epilepticus group. In particular these occurred in the periventricular organs, the hippocampus and the cerebral cortex. Relatively little contrast agent binding was observed in the control group. T2 relaxation times were not significantly increased for the hippocampus or the cerebral cortex in post status epilepticus animals. These results demonstrate the feasibility of in vivo imaging of seizure-induced inflammation in an animal model of epilepsy. The antibody targeted MRI contrast agent identified regions of acute inflammation following status epilepticus and may provide an early marker of brain injury. This technique could be used to determine the role of inflammation in models of epileptogenesis and to study the potential for anti-inflammatory therapeutic interventions.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "In this paper, we propose a cross-layer design (CLD) scheme combining rate adaptation and four types of hybrid automatic repeat request (HARQ) for multiband orthogonal frequency division multiplexing (MB-OFDM) ultra wideband (UWB) systems following the ECMA-368 standard. The time varying property is incorporated into standard UWB channel models for the purpose of investigating rate adaptation. To accurately accommodate fast time-varying channel conditions, we propose to embed the selected rate information into the acknowledgement (ACK) frames. It is shown that the proposed CLD scheme combining rate adaptation and HARQ can provide higher throughput than the HARQ-only schemes. Among the four types of HARQ schemes, HARQ Type-III has the best throughput performance while Type I has the worst.",
    "actual_venue": "ICC"
  },
  {
    "abstract": "Uncompensated friction forces compromise the positioning and tracking accuracy of motion systems. A unique tracking error known as quadrant glitch is the result of complex nonlinear friction behavior at motion reversal or near-zero velocity. Linear-feedback control strategies such as PID, cascade P/PI, or state-feedback control have to be extended with model-and nonmodel-based friction-compensation strategies to acquire sufficiently high path and tracking accuracy. This paper analyzes and validates experimentally three different friction-compensation strategies for a linear motor-based xy feed drive of a high-speed milling machine: 1) friction-model-based feedforward; 2) an inverse-model-based disturbance observer; and 3) the combination of both techniques. The friction models considered are as follows: a simple static-friction model and the recently developed generalized Maxwell-slip (GMS) model. GMS friction-model-based feedforward combined with disturbance observer almost completely eliminates the radial tracking error and quadrant glitches.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Motivation: Genome-wide functional annotation either by manual or automatic means has raised considerable concerns regarding the accuracy of assignments and the reproducibility of methodologies. In addition, a performance evaluation of automated systems that attempt to tackle sequence analyses rapidly and reproducibly is generally missing. In order to quantify the accuracy and reproducibility of function assignments on a genome-wide scale, we have re-annotated the entire genome sequence of Chlamydia trachomatis (serovar D), in a collaborative manner. Results: We have encoded all annotations in a structured format to allow further comparison and data exchange and have used a scale that records the different levels of potential annotation errors according to their propensity to propagate in the database due to transitive function assignments. We conclude that genome annotation may entail a considerable amount of errors, ranging from simple typographical errors to complex sequence analysis problems. The most surprising result of this comparative study is that automatic systems might perform as well as the teams of experts annotating genome sequences.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "We show that a unital (C^*)-algebra is commutative if and only if the gyrogroup of the set of positive invertible elements is in fact a group.",
    "actual_venue": "Periodica Mathematica Hungarica"
  },
  {
    "abstract": "Traditionally, the performance of blind SIMO channel estimates has been characterized in a deterministic fashion, by identifying those channel realizations that are not blindly identifiable. In this paper, we focus instead on the performance of Linear Equalizers for fading channels when they are based on blind channel estimates. Our analysis shows that with Zero Forcing Linear Equalizer (ZF-LE) at least one order of the diversity is lost depending on the way by which the scalar ambiguity that results from the blind channel estimation is resolved. However, in some Tx scenarios we are able to recover the diversity with MMSE-LE. Various Tx scenarios are considered in detail.",
    "actual_venue": "Ieee International Conference On Acoustics, Speech, And Signal Processing"
  },
  {
    "abstract": "The aim of this paper is to give a bivariate asymptotic expansion of the coefficient ynk = [xn]y(x)k, where y(x) = ∑ ynxn has a power series expansion with non-negative coefficients yn ⩾ 0. Such expansions are known for k/n ϵ [a, b] with a > 0. In the first part we provide two versions of full asymptotic series expansions for ynk and in the second part we show how to generalize these expansions to the case k/n ϵ [0, b] if y(x) has an algebraic singularity of the kind y(x) = g(x) - h (x)1 - x/x0. A concluding section provides extensions to multivariate asymptotic expansions and applications to multivariate generating functions. As a byproduct, we obtain a remarkable identity for Catalan numbers.",
    "actual_venue": "Eur J Comb"
  },
  {
    "abstract": "This paper presents a new gate turn-off drive circuit for GTO thyristors, which can accomplish faster turn-off switching for high-speed operation of the GTO. The switching characteristics of GTO's can be improved by use of the gate drive circuit that is able to make a very high rate of the negative gate current. The major disadvantage of the conventional gate turn-off driving technique is that it has a difficulty in realizing higher negative diG/dt due to the maximum reverse gate-cathode voltage and the stray inductances within the gate turn-off drive circuit. This paper shows that this problem can be overcome by adding another gate turn-off drive circuit to the conventional gate turn-off drive circuit. Simulation and experimental results in conjunction with chopper circuit verify the performance of the proposed gate drive circuit",
    "actual_venue": "Ieee Trans Industrial Electronics"
  },
  {
    "abstract": "The rise of smartphones equipped with various sensors has enabled personalization of various applications based on user contexts extracted from sensor readings. At the same time it has raised serious concerns about the privacy of user contexts. In this paper, we present MASKIT, a technique to filter a user context stream that provably preserves privacy. The filtered context stream can be released to applications or be used to answer their queries. Privacy is defined with respect to a set of sensitive contexts specified by the user. MASKIT limits what adversaries can learn from the filtered stream about the user being in a sensitive context - even if the adversaries are powerful and have knowledge about the filtering system and temporal correlations in the context stream. At the heart of MASKIT is a privacy check deciding whether to release or suppress the current user context. We present two novel privacy checks and explain how to choose the one with the higher utility for a user. Our experiments on real smartphone context traces of 91 users demonstrate the high utility of MASKIT.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "In this paper, we propose a statistical model-based speech enhancement technique using a multivariate polynomial regression (MPR) based on spectral difference scheme.In the analyzing step, three principal parameters, the weighting parameter in the decision-directed (DD) method, the long-term smoothing parameter for the noise estimation.The control parameter of the minimum gain value is estimated as optimal operating points technique by using to the spectral difference under various noise conditions.The MPR technique offers an effective scheme to represent complex nonlinear input-output relationship between the optimal operating points and spectral differences so that operating points can be determined according to various noise conditions in the off-line step. In this paper, we propose a statistical model-based speech enhancement technique using a multivariate polynomial regression (MPR) based on spectral difference scheme. In the analyzing step, three principal parameters, the weighting parameter in the decision-directed (DD) method, the long-term smoothing parameter for the noise estimation, and the control parameter of the minimum gain value are estimated as optimal operating points technique by using to the spectral difference under various noise conditions. These optimal operating points, which are specific according to different spectral differences, are estimated based on the composite measure, which is a relevant criterion in terms of speech quality. Thus, we apply the MPR technique by incorporating the spectral differences as independent variables in order to estimate the optimal operating points. The MPR technique offers an effective scheme to represent complex nonlinear input-output relationship between the optimal operating points and spectral differences so that operating points can be determined according to various noise conditions in the off-line step. In the on-line speech enhancement step, different parameters are chosen on a frame-by-frame basis through the regression according to the spectral difference. The performance of the proposed method is evaluated using objective and subjective speech quality measures in various noise environments. Our experimental results show that the proposed algorithm yields better performances than conventional algorithms.",
    "actual_venue": "Journal Of Systems Architecture: The Euromicro Journal"
  },
  {
    "abstract": "Abstract   This paper presents a multi-scale framework for the intensification of small scale gas-to-liquids (GTL) processes. As the process intensification tool, a radial microchannel reactor is used to facilitate the catalytic steam reforming of methane. Due to the endothermicity of this reaction, a microchannel reactor serves as a promising alternative because of its enhanced heat transfer characteristics. However, the underlying mathematical model for the microchannel reforming process is complex. Since our aim is to elucidate the optimal process topology from a plethora of alternatives through a global optimization framework, we built a surrogate mathematical model to bridge this gap. Through a rigorous model identification, parameter estimation, and cross-validation analysis we have developed an accurate mathematical model that can predict the microchannel reactor output within 0.43%. We then implemented this mathematical model into a process superstructure that considers several novel and competing process alternatives for the production of liquid fuels from natural gas. Across different case studies ranging from 500 to 5000 barrels per day of total production, we have observed that the microchannel process can improve break-even oil prices (BEOP) by as much as $10/bbl. Since the small scale GTL process aims to utilize stranded natural gas with almost zero value, a parametric analysis is performed to evaluate the BEOP at different feedstock prices. We have observed that the microchannel reforming alternative is the superior process at all the scales investigated when a natural gas price of $1/TSCF is considered. The topological findings suggest that process intensification through microchannel steam reforming is a viable approach to monetize stranded natural gas.",
    "actual_venue": "Computers And Chemical Engineering"
  },
  {
    "abstract": "A low-power, low-frequency, ad hoc networking paradigm is considered for robust communications among mobile agents in complex non-line-of-sight (NLOS) indoor and urban-type scenarios. Compared with higher frequency, the lower portion of the very high frequency (VHF) band offers improved penetration and reduced multipath in such scenarios. Low VHF is underutilized for mobile ad hoc networking due to the lack of compact low-power systems and efficient miniature antennas. We investigate the proposed approach through experiments in realistic scenarios. In order to carry out the experiments, we leverage a compact, low-power ZigBee radio operating seamlessly in the low-VHF band by introducing a bi-directional frequency converter, which translates ZigBee signals into low-VHF carriers, along with a recently developed highly miniaturized efficient antenna. The experimental low-VHF radio system and a conventional ZigBee operating at 2.4 GHz are both integrated on a compact ground robotic platform for autonomous experimentation and comparison in NLOS indoor and outdoor settings. Measurements quantify the significant advantages of the low-VHF radio system in terms of packet error rate, fading, radio signal strength, and extended spatial coverage, in a number of complex communication environments.",
    "actual_venue": "Ieee Access"
  },
  {
    "abstract": "Email communication plays an integral part of everybody's life nowadays. Especially for business emails, extracting and analysing these communication networks can reveal interesting patterns of processes and decision making within a company. Fraud detection is another application area where precise detection of communication networks is essential. In this paper we present an approach based on recurrent neural networks to untangle email threads originating from forward and reply behaviour. We further classify parts of emails into 2 or 5 zones to capture not only header and body information but also greetings and signatures. We show that our deep learning approach outperforms state-of-the-art systems based on traditional machine learning and hand-crafted rules. Besides using the well-known Enron email corpus for our experiments, we additionally created a new annotated email benchmark corpus from Apache mailing lists.",
    "actual_venue": "Advances In Information Retrieval"
  },
  {
    "abstract": "Insertion problems arise in scheduling when additional activities have to be inserted into a given schedule. This paper investigates insertion problems in a general disjunctive scheduling framework capturing a variety of job shop scheduling problems and insertion types. First, a class of scheduling problems is introduced, characterized by disjunctive graphs with the so-called short cycle property, and it is shown that in such problems, the feasible selections correspond to the stable sets of maximum cardinality in an associated conflict graph. Two types of insertion problems are then identified where the underlying disjunctive graph is through- or bi-connected. For these cases, it is shown that the short cycle property holds and the conflict graph is bipartite, allowing to derive a polyhedral characterization of all feasible insertions. An efficient method for deciding whether there exists a feasible insertion, and a lower and upper bound procedure for the minimum makespan insertion problem are developed. For bi-connected graphs, this procedure solves the insertion problem to optimality. The obtained results are applied to three extensions of the classical Job Shop, the Multi-Processor Task, Blocking and No-Wait Job Shop, and two types of insertions, job and block insertion.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "The performance of an integrated circuit depends strongly upon the power delivery system. With the introduction of ultra-small on-chip voltage regulators, novel design methodologies are needed to simultaneously determine the location of the on-chip power supplies and decoupling capacitors. In this paper, a unified design methodology is proposed to determine the optimal location of the power supplies and decoupling capacitors in high performance integrated circuits. Optimization algorithms widely used for facility location problems are applied in the proposed methodology. The effect of the number and location of the power supplies and decoupling capacitors on the power noise and response time is discussed.",
    "actual_venue": "Emerging And Selected Topics In Circuits And Systems, Ieee Journal"
  },
  {
    "abstract": "This paper presents the analysis of a two-non-identical-unit parallel system. The system stops functioning only when both the units fail. The joint distribution of the lifetimes of both units is taken to be a bivariate negative exponential. The repair times of the two units ave uncorrelated random variables, which each have a negative exponential distribution with different parameters. Using the regenerative point technique, the various reliability characteristics useful to system managers are obtained.",
    "actual_venue": "International Journal Of Systems Science"
  },
  {
    "abstract": "This paper addresses the problem of estimating abrupt changes corrupted by multiplicative noise. Two estimators are considered depending on the noise assumptions: the Maximum A Posteriori (MAP) estimator and the Least-Squares (LS) estimator. Both estimators are computed using dynamic programming. This study is then applied to edge detection in SAR images.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "In two earlier studies of lattice sums arising from the Poisson equation of mathematical physics, it was established that the lattice sum 1/ <bold> </bold>Sigma(m, nodd)cos(mx)cos(ny)/(m(2) + n(2)) = logA, where A is an algebraic number, and explicit minimal polynomials associated with A were computed for a few specific rational arguments x and y. Based on these results, one of us (Kimberley) conjectured a number-theoretic formula for the degree of A in the case x = y = 1/s for some integer s. These earlier studies were hampered by the enormous cost and complexity of the requisite computations. In this study, we address the Poisson polynomial problem with significantly more capable computational tools. As a result of this improved capability, we have confirmed that Kimberley's formula holds for all integers s up to 52 (except for s = 41, 43, 47, 49, 51, which are still too costly to test), and also for s = 60 and s = 64. As far as we are aware, these computations, which employed up to 64,000-digit precision, producing polynomials with degrees up to 512 and integer coefficients up to 10(229), constitute the largest successful integer relation computations performed to date. By examining the computed results, we found connections to a sequence of polynomials defined in a 2010 paper by Savin and Quarfoot. These investigations subsequently led to a proof, given in the Appendix, of Kimberley's formula and the fact that when s is even, the polynomial is palindromic (i.e., coefficient a(k) = a(m - k), where m is the degree).",
    "actual_venue": "Experimental Mathematics"
  },
  {
    "abstract": "Modern intrusion detection systems are comprised of three basically different approaches, host based, network based, and a third relatively recent addition called procedural based detection. The first two have been extremely popular in the commercial market for a number of years now because they are relatively simple to use, understand and maintain. However, they fall prey to a number of shortcomings such as scaling with increased traffic requirements, use of complex and false positive prone signature databases, and their inability to detect novel intrusive attempts. This intrusion detection systems represent a great leap forward over current security technologies by addressing these and other concerns. This paper presents an overview of our work in creating a true database intrusion detection system. Based on many years of Database Security Research, the proposed solution detects a wide range of specific and general forms of misuse, provides detailed reports, and has a low false-alarm rate. Traditional database security mechanisms are very limited in defending successful data attacks. Authorized but malicious transactions can make a database useless by impairing its integrity and availability. The proposed solution offers the ability to detect misuse and subversion through the direct monitoring of database operations inside the database host, providing an important complement to host-based and network-based surveillance. Suites of the proposed solution may be deployed throughout a network, and their alarms man-aged, correlated, and acted on by remote or local subscribing security services, thus helping to address issues of decentralized management. Inside the host, the proposed solution is intended to operate as a true security daemon for database systems, consuming few CPU cycles and very little memory and secondary storage. The proposed Intrusion Prevention Solution is managed by an access control system, with intrusion detection profiles, with item access rates and associating each user with profiles. Further, the method determines whether a result of a query exceeds any one of the item access rates defined in the profile associated with the user, and, in that case, notifies the access control system to alter the user authorization, thereby making the received request an unauthorized request, before the result is transmitted to the user. The method allows for a real time prevention of intrusion by letting the intrusion detection process interact directly with the access control system, and change the user authority dynamically as a result of the detected intrusion. The method is also preventing an administrator impersonating a user of a relational database, which database at least comprises a table with at least a user password, wherein the password is stored as a hash value. The method comprises the steps of: adding a trigger to the table, the trigger at least triggering an action when an administrator alters the table through the database management system (DBMS) of the database; calculating a new password hash value differing from the stored password hash value when the trigger is triggered; and replacing the stored password hash value with the new password hash value. In this paper, the design of the first MATTSSONHYBRID prototype, which is for Oracle Server 8.1.6, is discussed. MATTSSONHYBRID uses triggers and transaction profiles to keep track of the items read and written by transactions, isolates attacks by rewriting user SQL statements, and is transparent to end users. The MATTSSONHYBRID design is very general. In addition to Oracle, it can be easily adapted to support many other database application platforms such as IBM DB2, Microsoft SQL Server, Sybase, and Informix.",
    "actual_venue": "Int Conf On E-Business And Telecommunication Networks"
  },
  {
    "abstract": "We study a scheduling problem with changeover costs and capacity constraints. The problem is NP-complete, and combinatorial algorithms for solving it have not performed well. We identify a general class of facets that subsumes as special cases some known facets from the literature. We also develop a cutting-plane-based procedure and reformulation for the problem, and we obtain optimal solutions to problem instances with up to 600 integer variables without resorting to branch-and-bound procedures.",
    "actual_venue": "Operations Research"
  },
  {
    "abstract": "With the popularity of mobile communication, the importance of backbone network of the mobile networks, i.e. mobile backhaul networks, has increased significantly. With the decreasing size of mobile network system cells, it is considered next-generation mobile backhaul networks will be mesh-based. To reduce the operating expenses, i.e. OPEX, of the network, mobile network carriers are beginning to shift their circuit-switched networks to packet-switched networks. Most mobile backhaul networks are formed with microwave radios. To increase data rate, adaptive modulation and coding (AMC) is used for wireless links. As a result, the data rate of each wireless link changes over time and thus leads to unexpected packet loss or traffic degradation. This paper proposes a routing scheme and methods for estimating the transmission parameters or modes of wireless links to route bandwidth guaranteed flows over mobile backhaul networks. With estimation methods, degradation of existing flows caused by unexpected changes in the data rate of wireless links can be reduced. This paper shows using mode history of link to estimate the link quality can route bandwidth guaranteed flows efficiently by choosing more stable links for the path. We also show that link estimation methods are effective when mode distribution follows normal, uniform and Poisson distributions.",
    "actual_venue": "Budapest"
  },
  {
    "abstract": "We present a new algorithm for conversion between binary code and binary-reflected Gray code that requires approximately 2 K/3 element transfers in sequence for K elements per node, compared to K element transfers for previously known algorithms. For a binary cube of n=2 dimensions the new algorithm degenerates to yield a complexity of 2 K/+1 element a transfers, which is optimal. The new algorithm is optimal to within a multiplicative factor of 4/3 with respect to the best known 3 lower bound for any routing strategy. We show that the minimum number of element transfers for minimum path length routing is K with concurrent communication on all channels of every node of a binary cube",
    "actual_venue": "Computers, Ieee Transactions"
  },
  {
    "abstract": "Popular XML languages, like XPath, use \"tree-pattern\" queries to select nodes based on their structural characteristics. While many processing methods have already been proposed for such queries, none of them has found its way to any of the existing \"lightweight\" XML engines (i.e. engines without optimization modules). The main reason is the lack of a systematic comparison of query methods under a common storage model. In this work, we aim to fill this gap and answer two important questions: what the relative similarities and important differences among the tree-pattern query methods are, and if there is a prominent method among them in terms of effectiveness and robustness that an XML processor should support. For the first question, we propose a novel classification of the methods according to their matching process. We then describe a common storage model and demonstrate that the access pattern of each class conforms or can be adapted to conform to this model. Finally, we perform an experimental evaluation to compare their relative performance. Based on the evaluation results, we conclude that the family of holistic processing methods, which provides performance guarantees, is the most robust alternative for such an environment.",
    "actual_venue": "Vldb"
  },
  {
    "abstract": "Common Spatial Pattern (CSP) is one of the most effective feature extraction algorithm for Brain-Computer Interfaces (BCI). Despite its advantages of wide versatility and high efficiency, CSP is shown to be non-robust to noise and prone to over fitting when training sample number is limited. In order to overcome these problems, Regularized Common Spatial Pattern (RCSP) is further proposed. RCSP regularized covariance matrix estimation by two parameters, which reduces the estimation difference and improves the stationarity under small sample condition. However, RCSP does not make full use of the frequency information. In this paper, we presents a filter ensemble technique for RCSP (FERCSP) to further extract frequency information and aggregate all the RCSPs efficiently to get an ensemble-based solution. The performance of the proposed algorithm is evaluated on data set IVa of BCI Competition III against other five RCSP-based algorithms. The experimental results show that FERCSP significantly outperforms those of the existing methods in classification accuracy. The FERCSP outperforms the CSP algorithm and R-CSP-A algorithm in all five subjects with an average improvement of 6% in accuracy.",
    "actual_venue": "Proceedings Of Spie"
  },
  {
    "abstract": "The problems involved in the development of user interfaces become even more severe through the ubiquitous use of a variety of devices such as PCs, mobile phones and PDAs. Each of these devices has its own specifics that require a special user interface. Therefore, we developed and implemented an approach to generate user interfaces for multiple devices fully automatically from a high-level model. In contrast to previous approaches focusing on abstracting the user interface per se, we make use of speech act theory from the philosophy of language for the specification of desired intentions in interactions. Our new approach of using communicative acts in high-level models of user interfaces allows their creation with less technical knowledge, since such models are easier to provide than user-interface code in a usual programming language. From one such high-level model, multiple user interfaces for diverse devices are rendered fully automatically using a number of heuristics. A generated user interface for a PDA is already in real-world use and its usability was informally evaluated as good.",
    "actual_venue": "Waikoloa, Hi"
  },
  {
    "abstract": "The Massive Open Online Course (MOOC) paradigm has developed rapidly and achieved significant attention from a broad range of populations. However, many people who enroll in MOOCs do not have successful learning experiences. For example, some studies suggest that the relatively weak feelings of community and meager opportunities for collaboration may be contributing to a high dropout rate in MOOCs. In light of such problems, we are exploring new design features that could support enhanced social interactions, collaborative learning and feelings of community. We present our design ideas through a set of activity design scenarios, along with an analysis of possible benefits and negative consequences of our design.",
    "actual_venue": "L@S"
  },
  {
    "abstract": "Structural errors which arise due to poor image segmentation or clutter pose one of the main obstacles to effective relational graph matching. The aim of this paper is to provide a comparative evaluation of a number of contrasting approaches to the control of structural errors. Unique to this study is the way in which we show how a diverse family of algorithms relate to one-another using a common Bayesian framework. According to our adopted Bayesian approach relational consistency is gauged by Hamming distance. We illustrate three different ways in which this consistency measure may be used to rectify structural errors. The main conclusion of our study is that the active process of graph-editing outperforms the alternatives in terms of its ability to effectively control a large population of contaminating clutter.",
    "actual_venue": "Accv"
  },
  {
    "abstract": "Energy-efficient realization of soft-output signal detection is of great importance in emerging high-speed multiple-input multiple-output (MIMO) wireless communication systems. This paper presents three algorithm-level complexity-reduction techniques for soft-output detector design to achieve significant energy savings. To demonstrate their effectiveness, we designed a soft-output detector for 4-4 MIMO with 64-QAM using 65nm CMOS technology. While achieving near-optimum detection performance, the detector can support over 100Mbps throughput with only 0.24mm2 silicon area and 11mw power, leading to a ×10 improvement over the state of the art.",
    "actual_venue": "Islped"
  },
  {
    "abstract": "In order to cope with the rapidly changing interplay between technology and market in information industries, particularly in R&D activities in large corporations, a persistent and clarifying interplay among the participants must be lead by policy scientist as a policy process. The author of this paper argues that with emerging digital media-enabled innovations, Lasswell's (1971) concept of policy sciences and its process has become applicable to the problem of this technology innovation, if behavioral sciences view points on Technology Policy Staff are appropriately taken into consideration in terms of structure and power of the decision field. Several experiments on the perspective were made by himself in real worlds. The goals that were clarified through the persistent interplay among the decision-makers, the other participants and the author are shared and under implementation. The results of a couple of experiments in real world problems were appreciable and consequently corporate-wide application of the concept of TPSs has started in an organization",
    "actual_venue": "SMC"
  },
  {
    "abstract": "In the past years, \"group psychology\" has been fully utilized in service recommendation. The wide use of collaborative filtering recommendation and user's trust in the rating of service have proved its success. Content-based recommendation further optimizes the recommendation accuracy by strengthening the analysis on service contents. However, existing works pay little attention to the content details and their underlying semantic correlations. This paper proposes a knowledge graph based method that comprehensively considers both community effect and details of service contents. We build a knowledge graph for representing service supplydemand networks, and incorporate the community network structures of users and services into it. Further, we give a method of mining POIs (Point of Interests) of users from user reviews based on RAKE text mining algorithm and incorporate POIs into the knowledge graph. User reviews, no matter praise or criticism, always imply user preferences on specific service contents. Finally, we design a recommendation algorithm POIKG RS based on Representation Learning method of Knowledge Graph. A set of experiments demonstrate that the proposed approach can better utilize the characteristics of different group of users and their POIs, resulting in improved recommendation accuracy.",
    "actual_venue": "Ieee International Conference On Services Computing"
  },
  {
    "abstract": "Field Programmable Gate Arrays (FPGA) are widely used to accelerate parallel applications by specialized hardware. Especially for data flow intensive applications FPGAs are very well suited to design application specific data paths with a certain degree of parallelism. Since most of applications also need control flow, the most common method is to design complex state machines that are realized in hardware. However, this often leads to very high and inefficient resource utilization on the target architecture for design parts that are not performance critical nor relevant for more efficient realizations. In this paper, we propose a generic VLIW-inspired Slot Architecture (ViSA), which combines two efficient objectives, the performance of parallel hardware and the low area utilization of custom processors. Furthermore, we introduce the methodology for mapping and debugging applications on the efficient ViSA architecture.We present experimental results of two corner case applications showing that our approach is suitable for ultra low power as well as high performance computing. Using the presented co-design methodology, we will conclude that ViSA enables the realization of multi-objective design spaces for various target domains. ViSA has extreme throughput at low operating frequencies leading to significant power and energy savings over state of the art architectures.",
    "actual_venue": "System Chip"
  },
  {
    "abstract": "This paper describes the use of a new evolutionary method named Genetic Relation Algorithm (GRA) for reducing the number of class association rules extracted by other methods such as Apriori, Genetic Network Programming(GNP), etc. The purpose is to generate a small number of class association rules in order to delete irrelevant and redundant rules. A reduced rule set has advantages as it provides only useful rules and makes its analysis more efficient. Our approach is based on evaluating the distances between rules for evolving GRA and also evaluating the distances between the data in the test set and the rules for classification. Two matching criteria are presented: complete match and partial match. The classification accuracy obtained by our method is better compared to other reported results in multi-class datasets showing an impressive reduction rate.",
    "actual_venue": "Ieee Congress On Evolutionary Computation"
  },
  {
    "abstract": "Dysregulation of the hypothalamic-pituitary-adrenal (HPA) axis is a hallmark of complex and multifactorial psychiatric diseases such as anxiety and mood disorders. About 50-60% of patients with major depression show HPA axis dysfunction, i.e. hyperactivity and impaired negative feedback regulation. The neuropeptide corticotropin-releasing hormone (CRH) and its receptor type 1 (CRHR1) are key regulators of this neuroendocrine stress axis. Therefore, we analyzed CRH/CRHR1-dependent gene expression data obtained from the pituitary corticotrope cell line AtT-20, a well-established in vitro model for CRHR1-mediated signal transduction. To extract significantly regulated genes from a genome-wide microarray data set and to deduce underlying CRHR1-dependent signaling networks, we combined supervised and unsupervised algorithms.We present an efficient variable selection strategy by consecutively applying univariate as well as multivariate methods followed by graphical models. First, feature preselection was used to exclude genes not differentially regulated over time from the dataset. For multivariate variable selection a maximum likelihood (MLHD) discriminant function within GALGO, an R package based on a genetic algorithm (GA), was chosen. The topmost genes representing major nodes in the expression network were ranked to find highly separating candidate genes. By using groups of five genes (chromosome size) in the discriminant function and repeating the genetic algorithm separately four times we found eleven genes occurring at least in three of the top ranked result lists of the four repetitions. In addition, we compared the results of GA/MLHD with the alternative optimization algorithms greedy selection and simulated annealing as well as with the state-of-the-art method random forest. In every case we obtained a clear overlap of the selected genes independently confirming the results of MLHD in combination with a genetic algorithm. With two unsupervised algorithms, principal component analysis and graphical Gaussian models, putative interactions of the candidate genes were determined and reconstructed by literature mining. Differential regulation of six candidate genes was validated by qRT-PCR.The combination of supervised and unsupervised algorithms in this study allowed extracting a small subset of meaningful candidate genes from the genome-wide expression data set. Thereby, variable selection using different optimization algorithms based on linear classifiers as well as the nonlinear random forest method resulted in congruent candidate genes. The calculated interacting network connecting these new target genes was bioinformatically mapped to known CRHR1-dependent signaling pathways. Additionally, the differential expression of the identified target genes was confirmed experimentally.",
    "actual_venue": "Bmc Systems Biology"
  },
  {
    "abstract": "Trust is a ubiquitous phenomenon in social networks and people trust one another for a variety of reasons. In this paper we study the problem of trust in massively multiplayer online games (MMOs) with respect to homophily and expertise. We prose a topology of homophily in MMOs based on the literature on homophily and domain knowledge of MMOs. Our results show that while there is some mapping between homophily in MMOs and the theories of homophily in the offline world, the mapping is not complete. Only ascribed homophily and value homophily is observed in the trust network, while other types of homophilies are conspicuously absent. We observed that the trust network exhibits many properties which are not observed in most other social networks. Based on our observations we propose a generative model for trust networks in MMOs.",
    "actual_venue": "Passat) And Ieee Third Inernational Conference Social Computing"
  },
  {
    "abstract": "Linked data best practices are getting extremely popular: various companies and public institutions have started taking advantage of linked data principles for exposing their datasets, and for relating their datasets to those served by third parties. Such enthusiasm is due to the linked data promise of evolving into a Global Data Space. Linksets are sets of links relating datasets and they surely play a fundamental role in this promise. However, a stable and well-accepted notion of linkset quality has not been yet defined. This paper contributes to overcome this lack by proposing a linkset quality measure. Among the different quality dimensions that can be addressed, the proposed measure focuses on completeness. The paper formally defines novel scoring functions and proposes an interpretation of these functions when maintaining and complementing third party datasets.",
    "actual_venue": "Edbt/Icdt Workshops"
  },
  {
    "abstract": "Progress in cell type reprogramming has revived the interest in Waddington’s concept of the epigenetic landscape. Recently researchers developed the quasi-potential theory to represent the Waddington’s landscape. The Quasi-potential U(x), derived from interactions in the gene regulatory network (GRN) of a cell, quantifies the relative stability of network states, which determine the effort required for state transitions in a multi-stable dynamical system. However, quasi-potential landscapes, originally developed for continuous systems, are not suitable for discrete-valued networks which are important tools to study complex systems. In this paper, we provide a framework to quantify the landscape for discrete Boolean networks (BNs). We apply our framework to study pancreas cell differentiation where an ensemble of BN models is considered based on the structure of a minimal GRN for pancreas development. We impose biologically motivated structural constraints (corresponding to specific type of Boolean functions) and dynamical constraints (corresponding to stable attractor states) to limit the space of BN models for pancreas development. In addition, we enforce a novel functional constraint corresponding to the relative ordering of attractor states in BN models to restrict the space of BN models to the biological relevant class. We find that BNs with canalyzing/sign-compatible Boolean functions best capture the dynamics of pancreas cell differentiation. This framework can also determine the genes' influence on cell state transitions, and thus can facilitate the rational design of cell reprogramming protocols.",
    "actual_venue": "Biosystems"
  },
  {
    "abstract": "Computed tomography (CT) of the chest is a very common staging investigation for the assessment of mediastinal, hilar, and intrapulmonary lymph nodes in the context of lung cancer. In the current clinical workow, the detection and assessment of lymph nodes is usually performed manually, which can be error-prone and time- consuming. We therefore propose a method for the automatic detection of mediastinal, hilar, and intrapulmonary lymph node candidates in contrast-enhanced chest CT. Based on the segmentation of important mediastinal anatomy (bronchial tree, aortic arch) and making use of anatomical knowledge, we utilize Hessian eigenvalues to detect lymph node candidates. As lymph nodes can be characterized as blob-like structures of varying size and shape within a specic intensity interval, we can utilize these characteristics to reduce the number of false positive candidates signicantly. We applied our method to 5 cases suspected to have lung cancer. The processing time of our algorithm did not exceed 6 minutes, and we achieved an average sensitivity of 82.1% and an average precision of 13.3%.",
    "actual_venue": "Medical Imaging: Computer-Aided Diagnosis"
  },
  {
    "abstract": "Expressions involving the product of the permanent with the (n - 1)st power of the determinant of a matrix of indeterminates, and of (0,1)-matrices, are shown to be related to an extension to odd dimensions of the Alon-Tarsi Latin Square Conjecture, first stated by Zappa. These yield an alternative proof of a theorem of Drisko, stating that the extended conjecture holds for Latin squares of odd prime order. An identity involving an alternating sum of permanents of (0,1)-matrices is obtained.",
    "actual_venue": "The Electronic Journal Of Combinatorics"
  },
  {
    "abstract": "Subgraph matching, a basic SPARQL operation, is known to be NP-complete. Coupled with the rapidly increasing volumes of RDF data, it makes efficient graph query processing a very challenging problem. In this paper, we tackle the important problem of efficient processing of star-shaped subgraph matching queries, which are a core SPARQL query pattern and usually lead to a number of costly join operations. We present a novel method to encode a star-shaped subgraph into a bit string and an indexing mechanism to improve the query answering performance, called FFD-index. Our extensive evaluation shows that FFD-index and the corresponding algorithms are effective in solving star-shaped graph queries and they significantly outperform the state-of-the-art SPARQL query engine RDF-3X.",
    "actual_venue": "Database Systems For Advanced Applications, Dasfaa"
  },
  {
    "abstract": "A Gaussian mixture model (GMM)-based algorithm is proposed for video reconstruction from temporally compressed video measurements. The GMM is used to model spatio-temporal video patches, and the reconstruction can be efficiently computed based on analytic expressions. The GMM-based inversion method benefits from online adaptive learning and parallel computation. We demonstrate the efficacy of the proposed inversion method with videos reconstructed from simulated compressive video measurements, and from a real compressive video camera. We also use the GMM as a tool to investigate adaptive video compressive sensing, i.e., adaptive rate of temporal compression.",
    "actual_venue": "Ieee Transactions On Image Processing"
  },
  {
    "abstract": "Several techniques together with some partial answers are given to the questions of factoriality, type classification and fullness for amalgamated free product von Neumann algebras.",
    "actual_venue": "Journal Of The London Mathematical Society-Second Series"
  },
  {
    "abstract": "Natural gas, one of the cleanest, most efficient and useful of all energy sources, is a vital component of the world's supply of energy. To make natural gas more convenient for storage and transportation, it is refined and condensed into a liquid called liquefied natural gas (LNG). In a LNG site, safety is a long-team and critical issue. The emergency shutdown (ESD) system in the LNG receiving terminal is used to automatically stop the pumps and isolate the leakage section. Fault-tree analysis (FTA) has been widely used for providing logical functional relationships among subsystems and components of a system and identifying the root causes of the undesired failures in a system. In the conventional FTA for the ESD system, we usually assume that exact failure probabilities of events are collected. However, in most real applications, first, the FTA for the ESD system needs to be made at a early design or manufacturing stage, certain new components normally used without failure data; secondly, sometimes the environmental change in the system during the operation periods. This makes more difficult to gather past exact failures data for the FTA. To complete the FTA of the ESD system under these uncertain situations, we apply the intuitionistic fuzzy sets (IFS) theory to the FTA. We generate the intuitionistic fuzzy fault-tree interval, and the intuitionistic fuzzy reliability interval for the ESD system. We also present an algorithm to find the critical components in the system based on IFS-FTA and determine weak paths in the ESD system, where the key improvement must be made.",
    "actual_venue": "Expert Syst Appl"
  },
  {
    "abstract": "HighlightsRadigost is a purely web-based multi-agent platform, built using modern HTML5 technologies.Radigost agents and parts of the system itself are implemented in JavaScript and executed inside the client's web browser.The system is standards-compliant and interoperable, capable of interacting with third-party multi-agent solutions.Radigost is platform-independent, and supports a wide variety of hardware and software platforms.Performance evaluation results demonstrate that the runtime performance of Radigost is comparable to that of a desktop-based implementation. Recent improvements of web development technologies, commonly referred to as HTML5, have resulted in an excellent framework for developing a fully-featured, purely web-based multi-agent platform. This paper presents an architecture of such a platform, named Radigost. Radigost agents and parts of the system itself are implemented in JavaScript and executed inside the client's web browser, while an additional set of Java-based components is deployed on an enterprise application server. Radigost is platform-independent, capable of running, without any prior installation or configuration steps, on a wide variety of software and hardware configurations, including personal computers, smartphones, tablets, and modern television sets. The system is standards-compliant and fully interoperable, in the sense that its agents can transparently interact with agents in existing, third-party multi-agent solutions. Finally, performance evaluation results show that the execution speed of Radigost is comparable to that of a non web-based implementation.",
    "actual_venue": "Journal Of Systems And Software"
  },
  {
    "abstract": "First Page of the Article",
    "actual_venue": "Information Theory, IEEE Transactions  "
  },
  {
    "abstract": "Formulating and reformulating reliable textual queries have been recognized as a challenging task in Information Retrieval (IR), even for experienced users. Most existing query expansion methods, especially those based on implicit relevance feedback, utilize the user's historical interaction data, such as clicks, scrolling and viewing time on documents, to derive a refined query model. It is further expected that the user's search experience would be largely improved if we could dig out user's latent query intention, in real-time, by capturing the user's current interaction at the term level directly. In this paper, we propose a real-time eye tracking based query expansion method, which is able to: (1) automatically capture the terms that the user is viewing by utilizing eye tracking techniques; (2) derive the user's latent intent based on the eye tracking terms and by using the Latent Dirichlet Allocation (LDA) approach. A systematic user study has been carried out and the experimental results demonstrate the effectiveness of our proposed methods.",
    "actual_venue": "Acm International Conference On Information And Knowledge Management"
  },
  {
    "abstract": "In this paper, a Fuzzy Supervisory Indirect Learning Predictive Controller (FsiLPC) is proposed. The controller integrates the concepts of the conventional model based predictive control (MBPC), the controller output error method and the fuzzy rule based system. In contrast to the conventional MBPC, the design of FsiLPC is more generic in terms of model compatibility; the predictive model to be adopted in the scheme can be of any structure. The FsiLPC demonstrates a number of attractive characteristics for industrial implementation. These include flexibility of employing a predictive model, adaptive capability to the variation in the operating conditions, and simplicity of the operating mechanisms. The methodology is thus considered as a potential solution to the control of complex Multi-Input-Multi-Output (MIMO) systems. The strategy of the integration is first illustrated through an overview of the general architecture of the controller, followed by descriptions of the individual operating mechanism. Two case studies are investigated to evaluate the performances of the controller. The first case study describes the control of a simple system with non-minimum phase dynamics while the control of a multivariable industrial process of a single screw extruder is detailed in the second case study. The encouraging results stem the interest to further explore the applicability of the proposed controller for industrial uses.",
    "actual_venue": "Ieee International Conference On Fuzzy Systems"
  },
  {
    "abstract": "This paper presents Policap - a Policy Service for distributed applications that use CORBA security model. Policap was proposed for insertion in the JaCoWeb Project context, which is developing an authorization scheme for large- scale networks. This s cheme is being d eveloped in o rder to d eal with management of security policies in such networks, simplifying authorization policy implementation. The Policap policy service fills the existing gap in the use of security policies for distributed ob ject programming. The paper further presents the implementation results obtained and an evaluation of these results based on Common Criteria, ISO standard 15408.",
    "actual_venue": "SEC"
  },
  {
    "abstract": "With the emergence of enormous amount of online news, it is desirable to construct text mining methods that can extract, compare and highlight similarities of them. In this paper, we explore the research issue and methodology of correlated summarization for a pair of news articles. The algorithm aligns the (sub)topics of the two news articles and summarizes their correlation by sentence extraction. A pair of news articles are modelled with a weighted bipartite graph. A mutual reinforcement principle is applied to identify a dense subgraph of the weighted bipartite graph. Sentences corresponding to the subgraph are correlated well in textual content and convey the dominant shared topic of the pair of news articles. As a further enhancement for lengthy articles, a k-way bi-clustering algorithm can first be used to partition the bipartite graph into several clusters, each containing sentences from the two news reports. These clusters correspond to shared subtopics, and the above mutual reinforcement principle can then be applied to extract topic sentences within each subtopic group.",
    "actual_venue": "Sigkdd Explorations"
  },
  {
    "abstract": "In this paper, we exploit a new type of data redundancy in the multisource surveillance video to reduce the huge gap between the growth rate of the data and the video compression rate. Global redundancy caused by correlated appearances of moving objects in multiple videos consists of model similarity, spatial correlation and temporal consistency. Therefore, we propose a global coding scheme of moving objects to eliminate the global redundancy: a model based object reconstruction is initially employed to reconstruct the objects in the video, then a pose-based residual error prediction is developed to compensate the difference between the real video appearance and the initial reconstruction from model. The experiment with two simulated surveillance videos has proved that the proposed coding scheme can achieve better coding performance than the main profile of HEVC and surveillance profile of IEEE 1857-2013.",
    "actual_venue": "DCC"
  },
  {
    "abstract": "We present a parallel algorithm for recognizing and representing a proper interval graph in O(log^2n) time with O(m+n) processors on the CREW PRAM, where m and n are the number of edges and vertices in the graph. The algorithm uses sorting to compute a weak linear ordering of the vertices, from which an interval representation is easily obtained. It is simple, uses no complex data structures, and extends ideas from an optimal sequential algorithm for recognizing and representing a proper interval graph [X. Deng, P. Hell, J. Huang, Linear-time representation algorithms for proper circular-arc graphs and proper interval graphs, SIAM J. Comput. 25 (2) (1996) 390-403].",
    "actual_venue": "Discrete Applied Mathematics"
  },
  {
    "abstract": "Answer set programming (ASP) is nowadays one of the most popular modeling languages in the areas of Knowledge Representation and Artificial Intelligence. Hereby one represents the problem at hand in such a way that each model of the ASP program corresponds to one solution of the original problem. In recent years, several tools which support the user in developing ASP applications have been introduced. However, explicit treatment of one of the main aspects of ASP, multiple solutions, has received less attention within these tools. In this work, we present a novel system to visualize relations between answer sets of a given program. The core idea of the system is that the user specifies the concept of a relation by an ASP program itself. This yields a highly flexible system that suggests potential applications beyond development environments, e.g., applications in the field of abduction, which we will discuss in a case study.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Vehicular Ad-hoc Networks (VANETs) have recently drawn the attention of academic and industry researchers due to their potential applications in enabling various Intelligent Transportation Systems (ITS) applications for safety, entertainment, emergency response, and content sharing. Another potential application for VANETs lies in vehicle tracking, where a tracking system is used to visually track a specific vehicle or to monitor a particular area. For such applications, a large volume of information is required to be transferred between certain vehicles and a command and control centre, which can easily congest the wireless network in a VANET if not designed properly. The development of low-delay, low-overhead, and precise tracking systems in VANET is a major challenge requiring novel techniques to guarantee performance and to reduce network congestion. Among the several proposed data dissemination and management methods implemented in VANETs, clustering has been used to reduce data propagation traffic and to facilitate network management. However, clustering for target tracking in VANETs is still a challenge due to the dynamic nature of such networks. We have proposed two cluster-based algorithms for target tracking in VANETs in our previous works [1], [2]. These algorithms provide a reliable and stable platform for tracking a vehicle based on its visual features. In this paper, we have demonstrated the performance evaluation and testing results of both of our algorithms in the context of vehicular tracking under various scenarios. We have also compared the performance of both our algorithms to assess the performance of distributed algorithms as compared to centralized cluster-based target tracking algorithms. Besides, we have tested two data dissemination techniques for information delivery. Performance evaluation results demonstrate clearly that the proposed clustering schemes provide better performance for target tracking applications as compared to other cluster-based algorithms.",
    "actual_venue": "Vehicular Communications"
  },
  {
    "abstract": "The goal of this work is to investigate whether estimates of ease of part handling and part insertion can be provided by multimodal simulation using virtual environment (VE) technology, rather than by using conventional table-based methods such as Boothroyd and Dewhurst Charts. The long term goal is to extend cad systems to evaluate and compare alternative designs using Design for Assembly Analysis. A unified physically based model has been developed for modeling dynamic interactions among virtual objects and haptic interactions between the human designer and the virtual objects. This model is augmented with auditory events in a multimodal VE system called the Virtual Environment for Design for Assembly (VEDA). The designer sees a visual representation of the objects, hears collision sounds when objects hit each other and can feel and manipulate the objects through haptic interface devices with force feedback. Currently these models are 2D in order to preserve interactive update rates. Experiments were conducted with human subjects using two-dimensional peg-in-hole apparatus and a VEDA simulation of the same apparatus. The simulation duplicated as well as possible the weight, shape, size, peg-hole clearance, and frictional characteristics of the physical apparatus. The experiments showed that the Multimodal VE is able to replicate experimental results in which increased task completion times correlated with increasing task difficulty (measured as increased friction, increased handling distance combined with decreased peg-hole clearance). However, the Multimodal VE task completion times are approximately two times the physical apparatus completion times. A number of possible factors for this temporal discrepancy have been identified but their effect has not been quantified.",
    "actual_venue": "Computer-Aided Design"
  },
  {
    "abstract": "Cooperative Peer-to-Peer (P2P) information repair has been proposed to mitigate the packet loss among mobile peers during the 3G Cellular Base Station (BS) broadcast. Then network coding based P2P information exchange algorithms have been proposed to further improve the network performance, e.g., PIE algorithm and DNC-CPR algorithm. In this paper, we propose a connected dominating set (CDS) based P2P information repair (PPIR) protocol via random linear network coding, to alleviate the congestion and burden of BS's downlink channels. Our PPIR protocol consists of two phases: intra-cluster information exchange and inter-cluster information exchange. Dividing network into clusters enables our protocol to work on a sparse network environment and deeply reduces the impact of transmission collisions. Therefore, our PPIR protocol reduces the repair latency and improve the network performance. Furthermore, our protocol is capable of identifying the lost packets for the whole network and being aware of the repair process completion. Simulation results validate the effectiveness and efficiency of our PPIR protocol compared with the DNC-CPR algorithm.",
    "actual_venue": "Globecom"
  },
  {
    "abstract": "Although now well established, our information systems engineering theories and methods are applied only rarely in disciplines beyond systems development. This paper reports the application of the i* goal modelling language to describe the types of and relationships between quality of life goals of people living with dementia. Published social care frameworks to manage and improve the lives of people with dementia were reviewed to synthesize, for the first time, a comprehensive conceptual model of the types of goals of people living with dementia. Although the quality of life goal model was developed in order to construct automated reasoning capabilities in a new digital toolset that people with dementia can use for life planning, the multi-stage modelling exercise provided valuable insights into quality of life and dementia care practices of both researchers and experienced practitioners in the field.",
    "actual_venue": "Advanced Information Systems Engineering"
  },
  {
    "abstract": "A transformation of a tree decorated according to some attribute grammar may leave the tree containing attribute inconsistencies. An attribute reevaluation algorithm computes new attribute values for affected attribute instances. It has to guarantee, that never an inconsistent attribute value is accessed. Reps' algorithm performs this task in time, O(¦affected region¦). It is  as changed values trigger recomputations of attribute instances dependent on them. After each transformation, a complete update of the effected instances is performed. Reps' algorithm is compared with the data driven reevaluation scheme used in OPTRAN. It uses the same strategic information in the initial attribute evaluation and the reevaluation process. Furthermore, we present a  scheme for attribute reevaluation. It does not have the linear time complexity for each update after one transformation but, depending on the situation, often compares favourably with the data driven scheme for  of transformations. In addition, the linear time complexity of the data driven reevaluation algorithm needs fast convergence using an equality test between old and new attribute values. It is thus necessary, to keep the attribute values at (almost) all instances. The demand driven reevaluator does not need all the old attribute values. It can flexibly trade time for space. We also describe the handling of space consuming attributes, e.g. tables, lists and trees, in the reevaluation algorithm. An integrated version of data driven and demand driven reevaluation using these features has been implemented in the OPTRAN system.",
    "actual_venue": "Acta Inf"
  },
  {
    "abstract": "A large proportion of China-originated cyberattacks are allegedly aimed at extracting high-value intellectual property such as trade secrets. Senior government officials from China and the US have made frequent verbal and written allegations that their countries are victimized by cyberattacks and that their counterparts in the other country don't cooperate in fighting cybercrimes. This paper sheds light into this cyber Cold war by examining Western and Chinese allegations and counter-allegations related to cyberattacks and cyberwarfare.",
    "actual_venue": "IT Professional  "
  },
  {
    "abstract": "Document structure is an important issue not only for document analysis but for document synthesis. This paper presents a computer assisted document synthesis system based on the grammar-based structure analysis. The system is designed to accomplish the analysis and synthesis of table form documents cooperatively by a user and computer; namely, the user interprets the document meaning and gives the entry data to be filled in, while the computer detects the boxes formed by horizontal and vertical rules and determines the logical relations of adjacent boxes. First, the document is decomposed into a set of boxes and they are classified semi-automatically into four types: blank, insertion, indication and explanation. Then the box relations between the indication box and its associated entry one are analyzed based on the semantic and geometric knowledge defined in the document structure grammar. Finally, the system generates LATEX codes of the synthesized documents whose blank and insertion boxes are filled with the text and image data given by user. Experimental results have shown that the system analyzed successfully several kinds of table forms and yielded synthesized documents as expected",
    "actual_venue": "Icdar"
  },
  {
    "abstract": "Sketch-based synthesis, epitomized by the Sketch tool, lets developers synthesize software starting from a partial program, also called a sketch or template. This paper presents JSketch, a tool that brings sketch-based synthesis to Java. JSketch's input is a partial Java program that may include holes, which are unknown constants, expression generators, which range over sets of expressions, and class generators, which are partial classes. JSketch then translates the synthesis problem into a Sketch problem; this translation is complex because Sketch is not object-oriented. Finally, JSketch synthesizes an executable Java program by interpreting the output of Sketch.",
    "actual_venue": "Esec/Sigsoft Fse"
  },
  {
    "abstract": "In this paper we consider a proper generalized decomposition method to solve the steady incompressible Navier-Stokes equations with random Reynolds number and forcing term. The aim of such a technique is to compute a low-cost reduced basis approximation of the full stochastic Galerkin solution of the problem at hand. A particular algorithm, inspired by the Arnoldi method for solving eigenproblems, is proposed for an efficient greedy construction of a deterministic reduced basis approximation. This algorithm decouples the computation of the deterministic and stochastic components of the solution, thus allowing reuse of preexisting deterministic Navier-Stokes solvers. It has the remarkable property of only requiring the solution of m uncoupled deterministic problems for the construction of an m-dimensional reduced basis rather than M coupled problems of the full stochastic Galerkin approximation space, with m << M (up to one order of magnitude for the problem at hand in this work).",
    "actual_venue": "Siam Journal On Scientific Computing"
  },
  {
    "abstract": "Astrophysical applications are known to be data and computationally intensive with large amounts of images being generated by telescopes on a daily basis. To analyze these images data mining, statistical, and image processing techniques are applied on the raw data. Big data platforms such as MapReduce are ideal candidates for processing and storing astrophysical data due to their ability to process loosely coupled parallel tasks. These platforms are usually deployed in clouds, however, most astrophysical applications are legacy applications that are not optimized for cloud computing. While some work towards exploiting the benefits of Hadoop to store astrophysical data and to process the large datasets exists, not much research has been done to assess the scalability of cloud enabled astrophysical applications. In this work we analyze the data and resource scalability of MapReduce applications for astrophysical problems related to cluster detection and inter cluster spatial pattern search. The maximum level of parallelism is bounded by the number of clusters and the number of (cluster, subcluster) pairs in the pattern search. We perform scale-up tests on Google Compute Engine and Amazon EC2. We show that while data scalability is achieved, resource scalability (scale up) is bounded and moreover seems to depend on the underlying cloud platform. For future work we also plan to investigate the scale out on tens of instances with large input files of several GB.",
    "actual_venue": "Ieee/Acm International Symposium On Cluster, Cloud And Grid Computing"
  },
  {
    "abstract": "A major complaint of individuals with normal hearing and hearing impairment is a reduced ability to understand speech in a noisy environment. An adaptive LMS filter was employed to process speech in signal-to-noise ratios (S/N) varying from -8 to +12 dB. The filter configuration is commonly called noise cancellation. Intelligibility measures were obtained on normal and hearing impaired individuals using CID-W22 word lists. For the normal subjects mean intelligibility scores unprocessed were near zero for the S/N ratios -8, -4, and 0 dB and increased from 30 to 50% for the same S/N ratios when processed. For S/N ratios of +4 through +12, mean intelligibility scores prior to filtering ranged from 24 to 46%; after filtering, the scores improved from 67 to 75% i.e., within 7% of the nolse-free sound-field recordings. Preliminary data for the hearing impaired showed the same pattern with their improvement also approaching the scores obtained with no noise present in the sound field.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Ieee International Conference Icassp"
  },
  {
    "abstract": "This article describes the first participation of the Chair Media Informatics of the Chemnitz University of Technology in the Cross Language Evaluation Forum. An experimental prototype is introduced which implements several methods of optimizing search results. The configuration of the prototype is tested with the CLEF training data. The results of the Domain-Specific Monolingual German task suggest that combining the suffix stripping stemming and the decompounding approach is very useful. Also, a local document clustering (LDC) approach used to improve the query expansion (QE) based on pseudorelevance feedback (PRF) seems to be quite beneficial. Nevertheless, the evaluation of the English task using the same configuration suggests that the qualities of the results are highly speech dependent.",
    "actual_venue": "Clef"
  },
  {
    "abstract": "Security in Wireless Sensor Networks (WSNs) is highly dependent on the behavior of the base station. This happens because, if the network is left unattended, sensor nodes cannot offload data to the (secure) base station in real time and, thus, until the base station becomes available, adversaries can compromise some sensor nodes and selectively destroy data. In order to prevent such attacks, providing the so-called \"data survival\", some strategies can be employed. In this paper, we discuss and analyze different data survival strategies using cryptography. To the best of our knowledge, we provide the first implementation of such techniques, using a real sensor platform for their evaluation. As a result, we show that the main costs for the data survival process are not as high as it could be expected, and that strategies based on private keys can be used even if one considers the highly resource constrained nature of sensors.",
    "actual_venue": "LCN"
  },
  {
    "abstract": "Matching points between multiple images of a scene is a vital component of many computer vision tasks. Point matching involves creating a succinct and discriminative descriptor for each point. While current descriptors such as SIFT can find matches between features with unique local neighborhoods, these descriptors typically fail to consider global context to resolve ambiguities that can occur locally when an image has multiple similar regions. This paper presents a feature descriptor that augments SIFT with a global context vector that adds curvilinear shape information from a much larger neighborhood, thus reducing mismatches when multiple local descriptors are similar. It also provides a more robust method for handling 2D nonrigid transformations since points are more effectively matched individually at a global scale rather than constraining multiple matched points to be mapped via a planar homography. We have tested our technique on various images and compare matching accuracy between the SIFT descriptor with global context to that without.",
    "actual_venue": "Computer Vision And Pattern Recognition, Ieee Computer Society Conference"
  },
  {
    "abstract": "Internet topic detection and classification is an intelligent information access technology. It studies how to detect new events and classify sentiment of the content. Classical detection and analysis system of internet topics has low analysis efficiency and large process delay. The functions of cluster-based analysis system are internet data collection, real-time analysis and off-line data analysis. Experimental results show that the Average Job Time (AJT) and Average Waiting Time (AWT) for jobs in case of Service Cluster are comparatively lesser with respect to Physical Server, and the Service Cluster shortens the service failover time by 93.4%.",
    "actual_venue": "Iscid"
  },
  {
    "abstract": "The application of neural networks to modeling time-invariant nonlinear systems has been difficult for complicated nonstationary signals, such as speech, because the networks are unable to characterize temporal variability. This problem is addressed by proposing a network architecture, called the hidden control neural network (HCNN), for modeling signals generated by nonlinear dynamical systems with restricted time variability. The mapping implemented by a multilayered neural network is allowed to change with time as a function of an additional control input signal. The network is trained using an algorithm based on ;backpropagation' and segmentation algorithms for estimating the unknown control together with the network's parameters. Application of the network to the segmentation and modeling of a signal produced by a time-varying nonlinear system, speaker-independent recognition of spoken connected digits, and online recognition of handwritten characters demonstrates the ability of the HCNN to learn time-varying nonlinear dynamics and its potential for high-performance recognition of signals produced by time-varying sources.",
    "actual_venue": "Neural Networks, IEEE Transactions  "
  },
  {
    "abstract": "We apply the definition of chaos given by Devaney for discrete time dynamical systems to the case of elementary cellular automata, i.e., 1-dimensional binary cellular automata with radius 1. A discrete time dynamical system is chaotic according to the Devaney's definition of chaos if it is topologically transitive, is sensitive to initial conditions, and has dense periodic orbits. We enucleate an easy-to-check property of the local rule on which a cellular automaton is based which is a necessary condition for chaotic behavior. We prove that this property is also sufficient for a large class of elementary cellular automata. The main contribution of this paper is the formal proof of chaoticity for many non additive elementary cellular automata. Finally, we prove that the above mentioned property does not remain a necessary condition for chaoticity in the case of non elementary cellular automata.",
    "actual_venue": "Ciac"
  },
  {
    "abstract": "An evaluation is given of the postulate that color discrimination is achieved by the sensing of optical mode patterns excited in the retinal cones. Results are presented from analyses of the patterns and spectral responses of the optical modes. It is shown that these modes qualitatively have the proper characteristics required to satisfy the color discrimination properties of human vision. This postulate results in simple and natural explanations for the primary causes of color blindness.",
    "actual_venue": "Ieee Trans Systems Science And Cybernetics"
  },
  {
    "abstract": "This paper presents a new approach of DC motor PWM speed control, using microcomputer as controller, after discussing conventional cascade speed control. It introduces the principle of DC motor PWM speed control and the details of the realization of the approach base on AT89S51 single-chip microcomputer. Different duty cycle quadrate waves are produced by the PI of AT89S51. Different speed grades and the direction are depended on different buttons. Experiments have proved that this system is of higher performance. © 2011 IEEE.",
    "actual_venue": "Emeit"
  },
  {
    "abstract": "Recently, collaborative learning (CL) is introduced to combine active learning (AL) with semi-supervised learning (SSL), and solve the problem of limited training samples. In this paper, we proposed a novel CL framework for hyperspectral image classification, in which AL and SSL are collaboratively integrated using clustering (CLUC). CLUC attempts to obtain more diversity and higher confidence of additional training samples in both AL and SSL. Note that clustering methods, which are used separately to enhance AL or SSL, are utilized to integrate these two learning process in CLUC. First, all unlabeled samples are assigned into clusters. Based on the clustering result, clustering-based query (CBQ) for both AL and SSL, and CBQ-based pseudo-labeling (CBQPL) for SSL are designed for CLUC. Second, the most and secondary uncertain samples in each cluster are selected by CBQ for AL and SSL, respectively, to ensure their informativeness. Third, CBQPL assigns the selected secondary uncertain samples with the same label as the most uncertain one, which is manually-labeled in AL within the same cluster. CBQPL makes the confidence of pseudo-labeling rely on the clustering results. We evaluate the performance of CLUC on three real hyperspectral images. The performance of the proposed method is tested under different numbers of labeled samples and compared with several approaches. We can observe from the experimental results that CLUC have superiority in classification maps and objective metrics with limited training samples. (C) 2017 Elsevier B.V. All rights reserved.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "A methodology is proposed for selecting the best training set, the best classifier and the best set of parameters associated to the classifier. A performance indicator is defined from measures of signatures separability, accuracy between signatures in the training set and in the classified final image, and probability that a pixel belongs to the class which has been assigned. The theoretical description of the suggested methodology has been proved using a image from the Fourier time series analysis. The algorithm has been executed and the obtained results for the best and the worst classifier have been analyzed and evaluated.",
    "actual_venue": "Igarss"
  },
  {
    "abstract": "Clustering datasets is a challenging problem needed in a wide array of applications. Partition-optimization approaches, such as k-means or expectation-maximization (EM) algorithms, are sub-optimal and find solutions in the vicinity of their initialization. This paper proposes a staged approach to specifying initial values by finding a large number of local modes and then obtaining representatives from the most separated ones. Results on test experiments are excellent. We also provide a detailed comparative assessment of the suggested algorithm with many commonly-used initialization approaches in the literature. Finally, the methodology is applied to two datasets on diurnal microarray gene expressions and industrial releases of mercury.",
    "actual_venue": "Ieee/Acm Trans Comput Biology Bioinform"
  },
  {
    "abstract": "A Gals (Globally Asynchronous Locally Synchronous) system typically consists of a collection of sequential, deterministic components that execute concurrently and communicate using slow or unreliable channels. This paper proposes a general approach for modelling and verifying Gals systems using a combination of synchronous languages (for the sequential components) and process calculi (for communication channels and asynchronous concurrency). This approach is illustrated with an industrial case-study provided by Airbus: a TftpUdp communication protocol between a plane and the ground, which is modelled using the Eclipse/Topcased workbench for model-driven engineering and then analysed formally using the Cadp verification and performance evaluation toolbox.",
    "actual_venue": "Spin"
  },
  {
    "abstract": "The essence of this paper is to illustrate live data acquisition within the random access memory of a notebook trying to utilize the collected digital evidences in order to partially reconstruct previous Gmail session, which could be probative digital evidence in a court of law. The proposed framework is essentially crucial for the investigation of certain related cybercrimes on the basis of the digital breadcrumb trails being professionally disclosed and appropriately handled. Without loss of generality, the volatile data would vanish forever when the power of the computing devices is no longer sustainable. This research pinpoints the imminent threat of IT savvy cyber criminals and the corresponding counter procedures used to crack criminal cases if web-based e-mail utilities are essentially involved. This paper is focused on the prevalent e-mail utility, Gmail, as the research subject. At last, live digital evidence acquisition must be accurately fulfilled before the seizure of the computing devices in the crime scene to avoid irreversible investigation procedures which mean the digital evidences could be deleted, resulting in the loss of probative evidence. Copyright © 2012 John Wiley & Sons, Ltd.",
    "actual_venue": "Security And Communication Networks"
  },
  {
    "abstract": "In this paper we show that the inverse problems of HAMILTONIAN CYCLE and 3-D MATCHING are coNP complete. This completes the study of inverse problems of the six natural NP-complete problems from [2] and answers an open question from [1]. We classify the inverse complexity of the natural verifier for HAMILTONIAN CYCLE and 3-D MATCHING by showing coNP-completeness of the corresponding inverse problems.",
    "actual_venue": "Isaac"
  },
  {
    "abstract": "This paper presents a hybrid optimisation method based on the fusion of the clonal selection algorithm (CSA) and harmony search (HS) technique. The CSA is employed to improve the harmony memory members in the HS method. The hybrid optimisation algorithm is further used to optimise Sugeno fuzzy classification systems for the Fisher Iris data and wine data classification. Computer simulations results demonstrate the remarkable effectiveness of our new approach.",
    "actual_venue": "Ijbic"
  },
  {
    "abstract": "A noisy behavior of the time domain passivity controller during the period of low velocity is analyzed. Main reasons of the noisy behavior are investigated through a simulation with a one-DOF haptic interface model. It is shown that the PO/PC is ineffective in dissipating the produced energy when the sign of the velocity, which is numerically calculated from the measured position, is suddenly changed, and when this velocity is zero. These cases happen during the period of low velocity due to the limited resolution of the position sensor. New methods, ignoring the produced energy from the velocity sign change, and holding the control force while the velocity is zero, are proposed for removing the noisy behavior. The feasibility of the developed methods is proved with both a simulation and a real experiment.",
    "actual_venue": "Icra"
  },
  {
    "abstract": "We present a scalable robot motion planning algorithm for reach-avoid problems. We assume a discrete time, linear model of the robot dynamics and a workspace described by a set of obstacles and a target region, where both the obstacles and the region are polyhedra. Our goal is to construct a trajectory, and the associated control strategy, that steers the robot from its initial point to the target while avoiding obstacles. Differently from previous approaches, based on the discretization of the continuous state space or uniform discretization of the workspace, our approach, inspired by the lazy satisfiability modulo theory paradigm, decomposes the planning problem into smaller subproblems, which can be efficiently solved using specialized solvers. At each iteration, we use a coarse, obstacle-based discretization of the workspace to obtain candidate high-level, discrete plans that solve a set of Boolean constraints, while completely abstracting the low-level continuous dynamics. The feasibility of the proposed plans is then checked via a convex program, under constraints on both the system dynamics and the control inputs, and new candidate plans are generated until a feasible one is found. To achieve scalability, we show how to generate succinct explanations for the infeasibility of a discrete plan by exploiting a relaxation of the convex program that allows detecting the earliest possible occurrence of an infeasible transition between workspace regions. Simulation results show that our algorithm favorably compares with state-of-the-art techniques and scales well for complex systems, including robot dynamics with up to 50 continuous states.",
    "actual_venue": "Ieee Conference On Decision And Control"
  },
  {
    "abstract": "This paper proposes an efficient parallel algorithm for an important class of dynamic programming problems that includes Viterbi, Needleman--Wunsch, Smith--Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages, or wavefronts. The algorithm presented in this paper provides additional parallelism allowing multiple stages to be computed in parallel despite dependences among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation. This paper demonstrates the efficiency of the parallel algorithm by showing significant speedups on a variety of important dynamic programming problems. In particular, the parallel Viterbi decoder is up to 24× faster (with 64 processors) than a highly optimized commercial baseline.<!-- END_PAGE_1 -->",
    "actual_venue": "Commun Acm"
  },
  {
    "abstract": "An adaptive learning environment provides personalised information to the learner through self-directed study. An adaptive learning environment model can be subdivided into a learner model, domain model, instructional model and adaptive engine. Personal traits comprise part of the components in a learner model and can be identified either explicitly or implicitly in an adaptive learning environment. In such an environment, the e-learning system should adapt to a learner's needs. However, even though academic research on adaptive learning environments has increased, the field lacks a comprehensive literature analysis of learners' personal traits in these environments. This study conducts a systematic literature review to identify the most commonly used personal traits in modelling the learner and the existing techniques suitable for identifying personal traits in an adaptive learning environment. A total of 140 articles spanning the years 2010–2017 are initially reviewed, from which 78 are selected based on the inclusion and exclusion criteria relevant to this study. This study provides an overview of learners' personal traits and the techniques used to identify them to provide a basis for improving adaptive learning environments. The findings indicate that most of the previous works used a learning style from the cognition learning domain category to model individual personal traits, while the computer-based detection technique was commonly applied to identify a learner's personal traits in adaptive learning environments. This study reveals the common learner characteristics used to develop learner models and the techniques for implementing such models. The findings of this paper can guide other researchers to recognise various personal traits and the identification technique for further studies, as well as assist developers in the development of the adaptive learning system.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "As the process technology scales down, interconnects become the performance bottleneck when designing multi-core processors. 3D IC can be a good solution for reducing the interconnection delay in the multi-core processors by stacking multiple layers connected through TSVs. However, 3D technology magnifies the thermal challenges in 3D multi-core processors. For this reason, 3D multi-core architecture cannot be practical without proper solutions to the thermal problems such as efficient DFS. This paper investigates how the continuous engaged DFS technique handles the thermal problem on 3D multi-core processors in unit level. We also identify the optimal matrix to achieve best performance varying application features, cooling characteristics, and frequency levels when the thermal problem is considered. Experimental results conclude two rules for managing and balancing the temperature of the die, especially the IntReg unit: 1) To optimize the thermal profile of both dies, the die with higher cooling efficiency should be clocked at a higher frequency; and 2) To lower the temperature of IntReg unit, workload with higher thermal impact should be assigned to cores with higher cooling efficiency.",
    "actual_venue": "Acis-Icis"
  },
  {
    "abstract": "One of the main contributions of the paper is that we introduce \\\"performance as a service\\\" as a key component for future cloud storage environments. This is achieved through demonstration of the design and implementation of a multi-tier cloud storage system (CACSS), and the illustration of a linear programming model that helps to predict future data access patterns for efficient data caching management. The proposed caching algorithm aims to leverage the cloud economy by incorporating both potential performance improvement and revenue-gain into the storage systems.",
    "actual_venue": "Ieee Cloud"
  },
  {
    "abstract": "This paper argues for the importance of considering interpersonal relationships emerging among the users of multi-user applications, and demonstrates the usefulness of a multi-agent system underlying a specially designed multi-player games to investigate emerging user attitudes towards each other.",
    "actual_venue": "Aamas"
  },
  {
    "abstract": "The importance of oil reservoir simulation is self evident. Almost every device with a moving part uses oil for lubrication, most forms of transport rely on oil as a source of power and many manufacturing processes use oil as a raw material. Stocks of oil are limited and our ability to recover the available oil is dependent on our knowledge and understanding of the oil reservoir. Simulating oil reservoirs is a problem which requires large amounts of memory and computation. Models typically consist of 50 to 100 thousand grid blocks each of which could typically require 2KB of data storage during the simulation. Typically simulations may take hours or even days to run. To achieve even this each cell may have areal dimensions in the region of 100m much larger than the typical length scales in the reservoir so that rock properties have to be up-scaled from those measured using seismic and core data. Clearly, then there are good reasons why it is desirable to be able to run significantly larger models or existing models on a shorter time scale. Eclipse 100 is arguably the most widely used oil reservoir simulator. A scalable parallel version of the code was developed under the auspices of the EUROPORT2 project. The key to the parallelisation of this oil reservoir simulator was the development of a new preconditioning algorithm for the conjugate gradient method which forms the heart of the simulator. In this paper we describe briefly how the new preconditioning algorithm was developed from the original serial preconditioner and present some performance figures for 3 data sets.",
    "actual_venue": "Hpcn Europe"
  },
  {
    "abstract": "Nowadays, a substantial number of people are turning to crowdsourcing, in order to solve tasks that require human intervention. Despite a considerable amount of research done in the field of crowdsourcing, existing works fall short when it comes to classifying typically crowdsourced tasks. Understanding the dynamics of the tasks that are crowdsourced and the behaviour of workers, plays a vital role in efficient task-design. In this paper, we propose a two-level categorization scheme for tasks, based on an extensive study of 1000 workers on CrowdFlower. In addition, we present insights into certain aspects of crowd behaviour; the task affinity of workers, effort exerted by workers to complete tasks of various types, and their satisfaction with the monetary incentives.",
    "actual_venue": "HT"
  },
  {
    "abstract": "Range data is very important in human-computer interaction applications. Although less costly, range acquisition and processing still presents a speed vs data reliability tradeoff. This paper proposes a method that, given noisy and generally unreliable range data, can filter out erroneous information using range histograms for regions of interest selected in registered color data. Then, using the resulting consistent data that has passed filters, this method limits the depth search space dynamically using motion history and its current state. Experimental results demonstrate the success of the proposed algorithm. Using filtered range data, the algorithm correctly identified the hand involved in manipulation 99.85% of the time. Dynamic disparity adjustment produced a speedup of 60.17% over a static disparity range selection. An application to virtual reality navigation is also presented.",
    "actual_venue": "Digital Signal Processing"
  },
  {
    "abstract": "In this article we propose a method for linkage analysis that is based on Bayesian statistics. It is non-parametric in the sense that there is no need to specify, disease, parameters such as penetrance values. We show that the method has significantly more statistical power than existing methods on axtificially created databases. Finally, the possibility to extend the method to multi-locus diseases is discussed.",
    "actual_venue": "Icann"
  },
  {
    "abstract": "This article looks at the information security of private computer users at home and on the move, and describes the situation in Finland as observed in six surveys conducted in 2012 -- 2015 at Tampere University of Technology where students have interviewed citizens. The results are in line with other studies of information security culture in society. This supports the validity of supplementary results concerning three kinds of initiatives that have been evolving in the interviews. The surveys will continue biannually and the future surveys will have a good ground to find a suitable way to combine and modify the initiatives towards methods that can assist citizens in growing a more secure culture.",
    "actual_venue": "Trustcom/Bigdatase/Ispa"
  },
  {
    "abstract": "A wireless passive pressure measurement system for an 800 degrees C high-temperature environment is proposed and the impedance variation caused by the mutual coupling between a read antenna and a LC resonant sensor is analyzed. The system consists of a ceramic-based LC resonant sensor, a readout device for impedance phase interrogation, heat insulating material, and a composite temperature-pressure test platform. Performances of the pressure sensor are measured by the measurement system sufficiently, including pressure sensitivity at room temperature, zero drift from room temperature to 800 degrees C, and the pressure sensitivity under the 800 degrees C high temperature environment. The results show that the linearity of sensor is 0.93%, the repeatability is 6.6%, the hysteretic error is 1.67%, and the sensor sensitivity is 374 KHz/bar. The proposed measurement system, with high engineering value, demonstrates good pressure sensing performance in a high temperature environment.",
    "actual_venue": "Sensors"
  },
  {
    "abstract": "The last years have seen the development of many credit scoring models for assessing the credit-worthiness of loan applicants. Traditional credit scoring methodology has involved the use of statistical and mathematical programming techniques such as discriminant analysis, linear and logistic regression, linear and quadratic programming, or decision trees. However, the importance of credit grant decisions for financial institutions has caused growing interest in using a variety of computational intelligence techniques. This paper concentrates on evolutionary computing, which is viewed as one of the most promising paradigms of computational intelligence. Taking into account the synergistic relationship between the communities of Economics and Computer Science, the aim of this paper is to summarize the most recent developments in the application of evolutionary algorithms to credit scoring by means of a thorough review of scientific articles published during the period 2000-2012.",
    "actual_venue": "Jors"
  },
  {
    "abstract": "The multitude of scientific services and processes being developed brings about challenges for future in silico distributed experiments. Choosing the correct service from an expanding body of processes means that the the task of manually building workflows is becoming untenable. In this paper we propose a framework to tackle the future of scientific collaborative distributed computing. We introduce the notion of Networked Open Processes whereby processes are exposed, published, and linked using semantics in the same way as is done with Linked Open Data. As part of the framework we introduce several novel concepts including Process Object Identifiers, Semantic Function Templates, and TReQL, a SQL-like language for querying networked open process graphs.",
    "actual_venue": "Escience"
  },
  {
    "abstract": "Magnetotactic bacteria is a kind of polyphyletic group of prokaryotes with the characteristics of magnetotaxis that make them orient and swim along geomagnetic field lines. Its distinct biology characteristics are useful to design new optimization technology. In this paper, a new bionic optimization algorithm named Magnetotactic Bacteria Moment Migration Algorithm MBMMA is proposed. In the proposed algorithm, the moments of a chain of magnetosomes are considered as solutions. The moments of relative good solutions can migrate each other to enhance the diversity of the MBMMA. It is compared with variants of PSO on standard functions problems. The experiment results show that the MBMMA is effective in solving optimization problems. It shows better or competitive performance compared with the variants of PSO on most of the tested functions in this paper.",
    "actual_venue": "Ieee/Acm Trans Comput Biology Bioinform"
  },
  {
    "abstract": "Many different types of ranking methods based on the score and accuracy functions of intuitionistic fuzzy values (IFVs) exist in the literature. The notion of knowledge bases, as in the case of rough set theory, is very handy to show that every ranking technique produces a unique classification of IFVs with a unique order among the classes. This means these rankings give rise to unique knowledge bases. Therefore, ranking of IFVs by two or more distinct techniques may produce different results. In this study, a graphical ranking method based on the uncertainty index and entropy is proposed. This approach is tested on several numerical examples existing in the literature and shown to be intuitive and convenient for applications in real-life scenarios.",
    "actual_venue": "International Journal Of Intelligent Systems"
  },
  {
    "abstract": "The security of Wireless Sensor Networks (WSNs) has a direct reliance on secure and efficient key management. This leaves key management as a fundamental research topic in the field of WSNs security. Among the proposed key management schemes for WSNs security, LEAP (Localized Encryption and Authentication Protocol) has been regarded as an efficient protocol over the last years. LEAP supports the establishment of four types of keys. The security of these keys is under the assumption that the initial deployment phase is secure and the initial key is erased from sensor nodes after the initialization phase. However, the initial key is used again for node addition after the initialization phase whereas the new node can be compromised before erasing the key. A time-based key management scheme rethought the security of LEAP. We show the deficiency of the time-based key management scheme and proposed a key management scheme for multi-phase WSNs in this paper. The proposed scheme disperses the damage resulting from the disclosure of the initial key. We show it has better resilience and higher key connectivity probability through the analysis.",
    "actual_venue": "EUC"
  },
  {
    "abstract": "ABSTRACT Overcomplete representations are attracting interest in image processing theory, particularly due to their potential to gener- ate sparse representations of data based on their morphologi- cal diversity. We here consider a scenario of image denoising using an overcomplete dictionary of sparse linear transforms. Rather than using the basic approach where the denoised im- age is obtained by simple averaging of denoised estimates provided by each sparse transform, we here develop an ele- gant bayesian framework,to optimally combine the individual estimates. Our derivation of the optimally combined,denoiser relies on a scale mixture of gaussian (SMG) prior on the co- efficients in each representation transform. Exploiting this prior, we design a bayesian � 2-risk (mean field) nonlinear es- timator and we derive a closed-form for its expression when the SMG specializes to the Bessel K form prior. Experimental results are carried out to show the striking profits gained from exploiting sparsity of data and their morphological diversity. Index Terms— Sparsity, Morphological diversity, Bayesian combined,denoising.",
    "actual_venue": "Acoustics, Speech, And Signal Processing, Icassp-, International Conference"
  },
  {
    "abstract": "To recover the analog voltage at the input of an analog-to-digital converter (ADC) from its digital output, one needs to know at least the ADC gain and offset. For high-accuracy measurements, it is necessary to estimate the actual values of these parameters since they are usually different from the ideal values (one and zero, respectively). This estimation inevitably has an uncertainty, which contributes to the uncertainty of any measurement made with the ADC. Here, the precision of gain and offset error estimators, based on the histogram method for ADC testing is analyzed. The “terminal based” and “independently based” definitions are compared, both through simulation and experimental evaluation. Our conclusion is that, in typical conditions, the “independently based” definition is more precise.",
    "actual_venue": "Instrumentation And Measurement, Ieee Transactions"
  },
  {
    "abstract": "In case of the absence of a calibration procedure, or when there exists a color difference between direct and ambient light, standard chrominance models are not completely brightness invariant. Therefore, they cannot provide the best space for robust color modeling. Instead of using a fi xed chrominance model, our method estimates the actual dependency between color appearance and brightness. This is done by fitting a linear function to a small set of color samples. In the resulting self-calibrated ch romatic space, orthogonal to this line, the color distribution is modeled as a 2D Gaussian distribution. The method is applied to skin detection, where the face provides the initialization samples to detect the skin of hands and arm s. A comparison with fixed chrominance models shows an overall improvement and also an increased reliability of detection performance in different environments.",
    "actual_venue": "International Conference On Computer Vision Theory And Applications"
  },
  {
    "abstract": "A greedy optimization technique for minimizing the area of linear digital systems using a combination of common subexpression elimination and modification of multiplier coefficients is proposed. Since the amount of logic required by a coefficient multiplier is dependent on the value of the coefficient, the given system is transformed, using splitting of coefficients, in such a way that the overall circuit requires a smaller area. The approach explores a much larger design space as compared to previously known techniques. The approach is the first to optimize numerically intensive digital circuits by additive decomposition of multiplier coefficients. The new synthesis scheme generates functionally equivalent but structurally different circuits with a 15 to 40% reduction in area over conventional methods, for practical circuits with DSP applications.",
    "actual_venue": "Ieee Trans Vlsi Syst"
  },
  {
    "abstract": "Cloud-assisted Internet of Things (IoT) is increasingly prevalent in our society, for example in home and office environment; hence, it is also known as cloud-assisted Internet of Everything (IoE). While in such a setup, data can be easily shared and disseminated (e.g., between a device, such as Amazon Echo and the cloud, such as Amazon AWS), there are potential security considerations that need to be addressed. Thus, a number of security solutions have been proposed. For example, searchable encryption (SE) has been extensively studied due to its capability to facilitate searching of encrypted data. However, threat models in most existing SE solutions rarely consider the malicious data owner and semi-trusted cloud server at the same time, particularly in dynamic applications. In a real-world deployment, disputes between above two parties may arise as either party will accuse the other of some misbehavior. Furthermore, efficient full-update operations (e.g., data modification, data insertion, and data deletion) are not typically supported in the cloud-assisted IoE deployment. Therefore, in this paper, we present a fair and dynamic data sharing framework (FairDynDSF) in the multiowner setting. Using FairDynDSF, one can check the correctness of search results, achieve fair arbitration, multikeyword search, and dynamic update. We also prove that FairDynDSF is secure against inside keyword guessing attack and demonstrate its efficiency by evaluating its performance using various datasets.",
    "actual_venue": "Ieee Internet Of Things Journal"
  },
  {
    "abstract": "Discovery of chimeric RNAs, which are produced by chromosomal translocations as well as the joining of exons from different genes by trans-splicing, has added a new level of complexity to our study and understanding of the transcriptome. The enhanced ChiTaRS-3.1 database ( http://chitars.md.biu.ac.il) is designed to make widely accessible a wealth of mined data on chimeric RNAs, with easy-to-use analytical tools built-in. The database comprises 34 922 chimeric transcripts along with 11 714 cancer breakpoints. In this latest version, we have included multiple cross-references to GeneCards, iHop, PubMed, NCBI, Ensembl, OMIM, RefSeq and the Mitelman collection for every entry in the ' Full Collection'. In addition, for every chimera, we have added a predicted chimeric protein-protein interaction (ChiPPI) network, which allows for easy visualization of protein partners of both parental and fusion proteins for all human chimeras. The database contains a comprehensive annotation for 34 922 chimeric transcripts from eight organisms, and includes the manual annotation of 200 sense-antiSense (SaS) chimeras. The current improvements in the content and functionality to the ChiTaRS database make it a central resource for the study of chimeric transcripts and fusion proteins.",
    "actual_venue": "Nucleic Acids Research"
  },
  {
    "abstract": "Nous modélisons les processus linéaires multi-canaux non gaussiens à l'aide de méthodes directes et inverses basées sur les cumulants et utilisant des données de sortie multi-variées bruitées. Les méthodes proposées sont insensibles à un bruit gaussien additif (éventuellement coloré et de matrice de covariance inconnue) et sont garanties identifier de manière unique la matrice dy système à une post-multiplication par une matrice de permutation près. Des critès de modélisation asymptotiquement optimaux et moins coûteux en charge de calcul sont également discutés. De plus, il est prouvé que l'utilisation de cumulants d'ordre supérieur à deux permet d'estimer plus d'angles d'arrivée (ou d'harmoniques) avec moins de capteurs. Le problème de la détection du nombre de sources (ou d'entrées) à l'aide de cumulants de sortie uniquement est également abordé. Des résultats de simulation montrent que les algorithmes proposés surclassent les méthodes traditionnelles basée sur la corrélation.",
    "actual_venue": "Signal Processing"
  },
  {
    "abstract": "Assessment of locomotion through simple tests such as timed up and go (TUG) or walking trials can provide valuable information for the evaluation of treatment and the early diagnosis of people with Parkinson's disease (PD). Common methods used in clinics are either based on complex motion laboratory settings or simple timing outcomes using stop watches. The goal of this paper is to present an innovative technology based on wearable sensors on-shoe and processing algorithm, which provides outcome measures characterizing PD motor symptoms during TUG and gait tests. Our results on ten PD patients and ten age-matched elderly subjects indicate an accuracy ± precision of 2.8 ± 2.4 cm/s and 1.3 ± 3.0 cm for stride velocity and stride length estimation compared to optical motion capture, with the advantage of being practical to use in home or clinics without any discomfort for the subject. In addition, the use of novel spatio-temporal parameters, including turning, swing width, path length, and their intercycle variability, was also validated and showed interesting tendencies for discriminating patients in ON and OFF states and control subjects.",
    "actual_venue": "Biomedical Engineering, Ieee Transactions"
  },
  {
    "abstract": "Many wireless sensor network (WSN) applications require privacy of the sampled data during transmission from the source nodes to a data collecting device, say a query server. Providing an efficient data aggregation scheme with preserving data privacy is a challenging problem in WSNs. Although the secure data aggregation in WSNs has been well studied in the recent years, there exists a little work, for instance PDA (Privacy-preserving Data Aggregation), which focuses on protecting sensor data not only from adversaries but also from the participating trusted sensor nodes. However, PDA suffers from two problems. The first one is high communication cost due to unnecessary traffics in the network during data transmissions. The second one is high computation cost due to the use of expensive technique to customize sensor data. To resolve the problems, we, in this paper, propose a new private data aggregation scheme for WSNs. The proposed scheme applies the additive property of complex numbers in order to combine sensor data and preserve data privacy during transmissions to the query server. We show from our analytical performance evaluations that our scheme is more efficient than the PDA scheme in terms of both communication and computation costs.",
    "actual_venue": "Computer And Information Technology"
  },
  {
    "abstract": "It is critical to reduce the electric power consumed by servers in a cluster in order to realize eco society. In the multi-level power consumption (MLPC) model of a server with a multi-core CPU, the power consumption of the server depends on the number of active cores and active threads where at least one application process is performed. In our previous studies, we discuss the energy-aware (EA) selection algorithm to select a server for each request process. Here, a server which is expected to consume the minimum electric energy is selected in a cluster. A server consumes the basic electric power even if no process is performed. The ratio of the basic energy consumption to the total electric energy consumption is large, e.g. 40 to 50 %. In this paper, we newly propose a globally energy-aware (GEA) algorithm to select a server for each process in a cluster. Here, not only the total electric energy consumption of the servers but also the ratio of basic electric energy consumed by servers to the total energy consumption can be reduced. We evaluate the GEA algorithm and show not only the total energy consumption of the servers but also the average execution time of processes are reduced in the GEA algorithm compared with the EA, round-robin (RR), and random (RD) algorithms.",
    "actual_venue": "Nbis"
  },
  {
    "abstract": "A new t~ature evaluation index based on fuzzy set theory and a connectionist model for its evaluation are provided. A concept of flexible membership function incorporating weighting factors, is introduced which makes the modeling of the class structures more appropriate. A neuro-fuzzy algorithm is developed for determining the optimum weighting coefficients representing the feature importance. The overall importance of the features is evaluated both individually and in a group considering their dependence as well as independence. Effectiveness of the algorithms along with comparison is dem- onstrated on speech and Iris data. © 1998 Elsevier Science Inc. All rights reserved.",
    "actual_venue": "Inf Sci"
  },
  {
    "abstract": "This paper presents an overview of the current status and data applications of FORMOSAT-2, Taiwanese's first earth observation satellite mission. Highlights of its contributions to monitoring of global natural disasters and earth environmental changes will be illustrated. The FORMOSAT-2 satellite successfully complements existing high spatial resolution imaging satellites such as SPOT-5, IKONOS, and QuickBird, among others, with its unique capability of daily revisits worldwide. The FORMOSAT-2 follow-up program to ensure data continuity to the user community is briefly introduced.",
    "actual_venue": "Proceedings Of The Ieee"
  },
  {
    "abstract": "As software systems become increasingly massive, the advantages of automated transformation tools are clearly evident. These tools allow the machine to both reason about and manipulate high-level source code. They enable off-loading of mundane and laborious programming tasks from human developer to machine, thereby reducing cost and development time frames. Although there has been much work in software transformation, there still exist many hurdles in realizing this technology in a commercial domain. From our own experience, there are two significant problems that must be addressed before transformation technology can be usefully applied in a commercial setting. These are: (1) Avoiding disruption of the style (i.e., layout and commenting) of source code and the introduction of any undesired modifications that can occur as a side effect of the transformation process. (2) Correct automated handling of C preprocessing and the presentation of a semantically correct view of the program during transformation. Many existing automated transformation tools require source to be manually modified so that preprocessing constructs can be parsed. The real semantic of the program remains obscured resulting in the need for complicated analysis during transformation. Many systems also resort to pretty printing to generate transformed programs, which inherently disrupts coding style. In this paper we describe our own C/C++ transformation system, Proteus, that addresses both these issues. It has been tested on millions of lines of commercial C/C++ code and has been shown to meet the stringent criteria laid out by Lucent's own software developers.",
    "actual_venue": "Science Of Computer Programming"
  },
  {
    "abstract": "Ross Anderson reviews Gary McGraw's book, Software Security: Building Security In.",
    "actual_venue": "Ieee Security And Privacy"
  },
  {
    "abstract": "We introduce a simple procedural probabilistic programming language which is suitable for coding a wide variety of randomised algorithms and protocols. This language is interpreted over finite datatypes and has a decidable equivalence problem. We have implemented an automated equivalence checker, which we call apex, for this language, based on game semantics.We illustrate our approach with three non-trivial case studies: (i) Herman's self-stabilisation algorithm; (ii) an analysis of the average shape of binary search trees obtained by certain sequences of random insertions and deletions; and (iii) the problem of anonymity in the Dining Cryptographers protocol. In particular, we record an exponential speed-up in the latter over state-of-the-art competing approaches.",
    "actual_venue": "Tacas"
  },
  {
    "abstract": "This paper discusses the state feedback stabilization problem of a deterministic finite automaton (DFA), based on its state equation representation recently proposed by the authors. First, a linear state equation representation of the DFA is briefly explained. Next, after the notion of equilibrium points and stabilizability of the DFA are defined, a necessary and sufficient condition for the DFA to be stabilizable is derived. Then under these preparations, a characterization of all stabilizing state feedback controllers is presented. Finally, a simple example is given to show how to follow the proposed procedure.",
    "actual_venue": "Control Conference"
  },
  {
    "abstract": "Through-wall imaging using synthetic aperture array technique is studied by employing ultra-wideband antennas and for wide incidence angles. The effect of the building walls on the target image distortions is investigated by simulations and measurements. It is shown that using the idea of match filtering, the effect of the wall can be compensated for and point target response can be reconstructed. A controlled experiment within the laboratory environment is performed to verify the methods, presented. It is shown that for an ultra-wideband system operating over frequency band of 1-3 GHz highly distorted images of two point targets in close proximity of each other behind a wall can be resolved after refocusing. A dual-frequency synthetic method is also presented that can improve the cross-range resolution of the refocused image.",
    "actual_venue": "Igarss: Ieee International Geoscience And Remote Sensing Symposium, Vols -: Sensing And Understanding Our Planet"
  },
  {
    "abstract": "AbstractProtein-Protein Interactions (PPIs) play an irreplaceable role in biological activities of organisms. Although many high-throughput methods are used to identify PPIs from different kinds of organisms, they have some shortcomings, such as high cost and time-consuming. To solve the above problems, computational methods are developed to predict PPIs. Thus, in this paper, we present a method to predict PPIs using protein sequences. First, protein sequences are transformed into Position Weight Matrix (PWM), in which Scale-Invariant Feature Transform (SIFT) algorithm is used to extract features. Then Principal Component Analysis (PCA) is applied to reduce the dimension of features. At last, Weighted Extreme Learning Machine (WELM) classifier is employed to predict PPIs and a series of evaluation results are obtained. In our method, since SIFT and WELM are used to extract features and classify respectively, we called the proposed method SIFTWELM. When applying the proposed method on three well-known PPIs datasets of Yeast, Human and Helicobacter:pylori, the average accuracies of our method using five-fold cross validation are obtained as high as 94.83, 97.60 and 83.64 percent, respectively. In order to evaluate the proposed approach properly, we compare it with Support Vector Machine (SVM) classifier and other recent-developed methods in different aspects. Moreover, the training time of our method is greatly shortened, which is obviously superior to the previous methods, such as SVM, ACC, PCVMZM and so on.",
    "actual_venue": "Ieee/Acm Transactions On Computational Biology And Bioinformatics"
  },
  {
    "abstract": "In the era of big data, it is often the rare categories that are of great interest in many high-impact applications, ranging from financial fraud detection in online transaction networks to emerging trend detection in social networks, from network intrusion detection in computer networks to fault detection in manufacturing. As a result, rare category characterization becomes a fundamental learning task, which aims to accurately characterize the rare categories given limited label information. The unique challenge of rare category characterization, i.e., the non-separability nature of the rare categories from the majority classes, together with the availability of the multi-modal representation of the examples, poses a new research question: how can we learn a salient rare category oriented embedding representation such that the rare examples are well separated from the majority class examples in the embedding space, which facilitates the follow-up rare category characterization?\n\nTo address this question, inspired by the family of curriculum learning that simulates the cognitive mechanism of human beings, we propose a self-paced framework named SPARC that gradually learns the rare category oriented network representation and the characterization model in a mutually beneficial way by shifting from the 'easy' concept to the target 'difficult' one, in order to facilitate more reliable label propagation to the large number of unlabeled examples. The experimental results on various real data demonstrate that our proposed SPARC algorithm: (1) shows a significant improvement over state-of-the-art graph embedding methods on representing the rare categories that are non-separable from the majority classes; (2) outperforms the existing methods on rare category characterization tasks.",
    "actual_venue": "KDD"
  },
  {
    "abstract": "In large distribution systems, distribution centers (DC) deliver some merchandize to their retail stores in size-specific packages, also called ship-packs. These ship-packs include cases (e.g., cartons containing 24 or 48 units), inners (packages of 6 or 8 units) or caches (individual units). For each Stock Keeping Unit (SKU), a retailer can decide which of these ship-pack options to use when replenishing its retail stores. Working with a major US retailer, we have developed a cost model that balances DC handling costs, store handling costs and inventory-related costs at both the DC and the stores, and therefore can help to determine the optimum warehouse ship-pack for each SKU. We implement our model for a sample of 529 SKUs, and show that by changing ship-pack size for about 30 SKUs, the retailer can reduce its total cost by 0.3% - 0.4%. Interestingly, we find that most of the cost savings occurs at the DC level. (C) 2012 Elsevier B.V. All rights reserved.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "ICT has become a crucial element in supporting energy management. It allows for design and implementation of smart grids. In this paper we present a distributed solution for improving the efficiency for electricity distribution and for more rational use of energy, minimizing overloads and voltage variations. It is implemented by a Virtual Market that has been built according to a distributed approach over a P2P server-less overlay. In our prototype, RetroShare is used to implement a F2F (Friend-To-Friend) network where intelligent agents can broker and negotiate energy autonomously on useru0027s behalf according to high level policies are discussed.",
    "actual_venue": "Pgcic"
  },
  {
    "abstract": "This study is based on improving the visual quality of the images captured under different illumination conditions, i.e. overexposed and underexposed. This study presents a fuzzy image enhancement process based on a finite fuzzy set which is defined to optimise entropy, noise, intensity and edge information. In the proposed fuzzy approach, the overexposed and underexposed images are mapped to form...",
    "actual_venue": "Iet Image Processing"
  },
  {
    "abstract": "Face frontalization is one way to overcome the pose variation problem, which simplifies multi-view recognition into one canonical-view recognition. This paper presents a multi-task learning approach based on the generative adversarial network (GAN) that learns the emotion-preserving representations in the face frontalization framework. Taking advantage of adversarial relationship between the generator and the discriminator in GAN, the generator can frontalize input non-frontal face images into frontal face images while preserving the identity and expression characteristics; in the meantime, it can employ the learnt emotion-preserving representations to predict the expression class label from the input face. The proposed network is optimized by combining both synthesis and classification objective functions to make the learnt representations generative and discriminative simultaneously. Experimental results demonstrate that the proposed face frontalization system is very effective for expression recognition with large head pose variations.",
    "actual_venue": "Ieee International Conference On Automatic Face And Gesture Recognition"
  },
  {
    "abstract": "High-end processors typically incorporate complex branch predictors consisting of many large structures that together consume a notable fraction of total chip power (more than 10% in some cases). Depending on the applications, some of these resources may remain underused for long periods of time. We propose a methodology to reduce the energy consumption of the branch predictor by characterizing prediction demand using profiling and dynamically adjusting predictor resources accordingly. Specifically, we disable components of the hybrid direction predictor and resize the branch target buffer. Detailed simulations show that this approach reduces the energy consumption in the branch predictor by an average of 72% and up to 89% with virtually no impact on prediction accuracy and performance.",
    "actual_venue": "Islped"
  },
  {
    "abstract": "Recent technological developments in cloud computing and the ensuing commercial appeal have encouraged companies and individuals to outsource their storage and computations to powerful cloud servers. However, the challenge when outsourcing data and computation is to ensure that the cloud servers comply with their advertised policies. In this paper, we focus in particular on the scenario where a data owner wishes to (i) outsource its public database to a cloud server; (ii) enable anyone to submit multi-keyword search queries to the outsourced database; and (iii) ensure that anyone can verify the correctness of the server's responses. To meet these requirements, we propose a solution that builds upon the well-established techniques of Cuckoo hashing, polynomial-based accumulators and Merkle trees. The key idea is to (i) build an efficient index for the keywords in the database using Cuckoo hashing; (ii) authenticate the resulting index using polynomial-based accumulators and Merkle tree; (iii) and finally, use the root of the Merkle tree to verify the correctness of the server's responses. Thus, the proposed solution yields efficient search and verification and incurs a constant storage at the data owner. Furthermore, we show that it is sound under the strong bilinear Diffie-Hellman assumption and the security of Merkle trees.",
    "actual_venue": "Ieee Conference On Communications And Network Security"
  },
  {
    "abstract": "A symbol a is called active in an L system G if there is a rule a -> v with a 6 not equal v in some table of G. By Ac-X(L) we denote the number of active symbols necessary to generate L by L systems of type X. For two types X and Y of L systems such that the corresponding languages families L (X) and L (Y) satisfy L (X) subset of L (Y), we say that Y is more efficient than X, if there is a sequence of languages L-n is an element of L (X), n >= 1, such that Ac-X(L-n) >= n and AcY (L-n) <= k for some constant k. In this paper we shall show that the inclusion L (X) subset of L (Y) implies that Y is more efficient than X. Analogous results are presented for some modifications of the measure of active symbols.",
    "actual_venue": "International Journal Of Foundations Of Computer Science"
  },
  {
    "abstract": "This work has practically been motivated by an approach of modelling actor systems using algebraic graph grammars. It turned out that essential requirements on graph structures modelling computational states could nicely be expressed as conditional equations.",
    "actual_venue": "Dagstuhl Seminar On Graph Transformations In Computer Science"
  },
  {
    "abstract": "This is the first book to cover the topic of interactive computer graphics. I found the book to be both very useful and very disappointing.",
    "actual_venue": "Ieee Transactions On Computers"
  },
  {
    "abstract": "This special issue of IEEE Software explores the challenges in developing compliant software systems. Typically, organizations face conflicting objectives, with compliance policies possibly hindering innovation, slowing down the product development process, or making the whole process most costly. The goal of software engineering for compliance is to bridge the gap between the software engineering community and the compliance community. The articles in this special issue explain the nature and extent of this domain from different viewpoints, the technical challenges it poses, novel software engineering methods for supporting compliance, and the current state of the art.",
    "actual_venue": "Ieee Software"
  },
  {
    "abstract": "AbstractA critical component of both economic and perceptual decision making under uncertainty is the belief-formation process. However, most research has studied belief formation in economic and perceptual decision making in isolation. One reason for this separate treatment may be the assumption that there are distinct psychological mechanisms that underlie belief formation in economic and perceptual decisions. An alternative theory is that there exists a common mechanism that governs belief formation in both domains. Here, we test this alternative theory by combining a novel computational modeling technique with two well-known experimental paradigms. We estimate a drift-diffusion model DDM and provide an analytical method to decode prior beliefs from DDM parameters. Subjects in our experiment exhibit strong extrapolative beliefs in both paradigms. In line with the common mechanism hypothesis, we find that a single computational model explains belief formation in both tasks and that individual differences in belief formation are correlated across tasks.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2016.2453.This paper was accepted by Yuval Rottenstreich, judgment and decision making.",
    "actual_venue": "Periodicals"
  },
  {
    "abstract": "The need of a reliable drowsiness detection system is arising today, as drowsiness is considered as a major cause for accidents as much as alcohol. In this paper, we propose a real-time drowsiness detection algorithm based on a single-channel electroencephalography (EEG) for wearable devices without demanding computing and power resources. The proposed algorithm adopts a cumulative counter to extract important features from 8 different frequency bands: delta (1-3 Hz), theta (4-7 Hz), low-alpha (8-9 Hz), high-alpha (10-12 Hz), low-beta (13-17 Hz), high-beta (18-30 Hz), low-gamma (31-40 Hz), and high-gamma (41-50 Hz). These features are then processed by a support vector machine (SVM) to distinguish between drowsy and awake states. Our preliminary results demonstrate that the proposed algorithm is capable of detecting drowsiness with superior accuracy (83.36%) over the conventional method (70.62%).",
    "actual_venue": "Annual International Conference Of The Ieee Engineering In Medicine And Biology Society, Vols"
  },
  {
    "abstract": "Currently, designing low-power complex embedded systems is a main challenge for corporations in a large number of electronic domains. There are multiple motivations which lead designers to consider low-power design such as increasing lifetime, improving battery longevity, limited battery capacity, and temperature constraints. Unfortunately, there is a lack of efficient methodology and accurate too...",
    "actual_venue": "Ieee Transactions Industrial Informatics"
  },
  {
    "abstract": "This study presents the application of a wearable monitoring system for the assessment of tic events in subjects affected by Tourette Syndrome (TS). A multifactorial analysis and validation of the proposed system is carried out collecting simultaneous and synchronized recordings of data from the wearable actigraph and from two video cameras that allowed two medical doctors with different expertise to classify the motor events as tics and their related severity scale. A dedicated software implements the algorithm for automatic tic detection and to compare this assessment with the standard video recording protocol used to discriminate and classify tic events of high intensity and tic event of low intensity (facial grimacing or vocal tics). Double blind analysis on a nine subjects allowed us to compare the variability between operators and wearable device, and conclude the system has good potential but algorithms refinement is still needed before its possible application in clinical practice. Currently it still requires the integration with a video analysis protocol if the tics are mild or are vowels giving a complete clinical frame.",
    "actual_venue": "Mobihealth"
  },
  {
    "abstract": "In this paper an adaptive control is de- signed for the nonlinear boost inverter in order to cope with unknown resistive load. This adaptive control is accomplished by using a state observer to one side of the inverter and by measuring the state variables. In order to analyze the stability of the full system singular perturbation analysis is used. The resultant adaptive control is tested by means of simulations. I. Introduction The control of boost DC-AC converters is usually ac- complished tracking a reference (sinusoidal) signal. The use of this external signal makes the closed-loop control system to be non-autonomous and thus, making its anal- ysis involved. In (1), (2) a different approach was used: a control law was designed for the boost converter in order to stabilize a limit cycle corresponding to the desired behavior. No external signals were needed. Nevertheless, the use of a boost converter prevents the achievement of zero-crossing signals and, thus, AC current was not achieved. This problem was solved in (3) with the use of a double boost converter as was proposed in (4). A phase-lock loop was necessary for the correct operation of the circuit as well as for synchronization with the electrical grid. Only the case of known resistive load was considered. In this paper the previous results are extended to the case of unknown load using an adaptation mechanism. Adaptation mechanism for similar controllers were used in (1) for the case of the buck converter which can be modelled by linear equations. The fact that the boost converter model is nonlinear makes the design of the adaptation law more involved. A state observer for some of the converter variables is designed even when the state variables are measured. In order to analyze the stability of the full system singular perturbation analysis is used. For simplicity, the phase-lock system is not considered in this analysis. The resultant adaptive control is tested by means of simulations. The rest of the paper is organized as follows: in Sect. II the model of the double boost converter (boost inverter) is presented. Section III states the problem, which is solved in Sect. IV by means of the design of the adap- tation mechanism. Section V is devoted to the stability",
    "actual_venue": "Singapore"
  },
  {
    "abstract": "Progress in modern neuroscience critically depends on our ability to observe the activity of large neuronal populations with cellular spatial and high temporal resolution. However, two bottlenecks constrain efforts towards fast imaging of large populations. First, the resulting large video data is challenging to analyze. Second, there is an explicit tradeoff between imaging speed, signal-to-noise, and field of view: with current recording technology we cannot image very large neuronal populations with simultaneously high spatial and temporal resolution. Here we describe multi-scale approaches for alleviating both of these bottlenecks. First, we show that spatial and temporal decimation techniques based on simple local averaging provide order-of-magnitude speedups in spatiotemporally demixing calcium video data into estimates of single-cell neural activity. Second, once the shapes of individual neurons have been identified at fine scale (e.g., after an initial phase of conventional imaging with standard temporal and spatial resolution), we find that the spatial/temporal resolution tradeoff shifts dramatically: after demixing we can accurately recover denoised fluorescence traces and deconvolved neural activity of each individual neuron from coarse scale data that has been spatially decimated by an order of magnitude. This offers a cheap method for compressing this large video data, and also implies that it is possible to either speed up imaging significantly, or to \"zoom out\" by a corresponding factor to image order-of-magnitude larger neuronal populations with minimal loss in accuracy or temporal resolution.",
    "actual_venue": "Plos Computational Biology"
  },
  {
    "abstract": "In this paper the motivations for play in the context of single- and multi-player digital Role-Playing Games (RPGs) are examined. Survey data were drawn from respondents online and participants in a related experimental study. The results indicate that motivations for play are not simple constructs, but rather composed of multiple motivational drivers that are heavily interrelated and act in concert. Character uniqueness and Discovery & Immersion were the highest ranked motivational categories. Different levels of detail in motivations for playing single-/multi-Player RPGs were located, with mechanistic/tactical play and character-based/social play being the two overall motivational factors.",
    "actual_venue": "Futureplay"
  },
  {
    "abstract": "In the biomedical domain, the desired information of a question (query) asked by biologists usually is a list of a certain type of entities covering different aspects that are related to the question, such as genes, proteins, diseases, mutations, etc. Hence it is important for a biomedical information retrieval system to be able to provide comprehensive and diverse answers to fulfill biologists' information needs. However, traditional retrieval models assume that the relevance of a document is independent of the relevance of other documents. This assumption may result in high redundancy and low diversity in the retrieval ranked lists.In this paper, we propose a relevance-novelty combined model, named RelNov model, based on the framework of an undirected graphical model. It consists of two component models, namely the aspect-term relevance model and the aspect-term novelty model. They model the relevance of a document and the novelty of a document respectively. We show that our approach can achieve 16.4% improvement over the highest aspect level MAP reported in the TREC 2007 Genomics track, and 9.8% improvement over the highest passage level MAP reported in the TREC 2007 Genomics track.The proposed combination model which models aspects, terms, topic relevance and document novelty as potential functions is demonstrated to be effective in promoting ranking diversity as well as in improving relevance of ranked lists for genomics search. We also show that the use of aspect plays an important role in the model. Moreover, the proposed model can integrate various different relevance and novelty measures easily.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "Recent years have witnessed great success of Single Image Super-Resolution (SISR) with convolutional neural network (CNN) based models. Most existing Super-Resolution (SR) networks use bicubic upscaled images as input or directly use low-resolution images as input and do transposed convolution or sub-pixel convolution only in the reconstruction stage which do not use the hierarchical features across the network for final reconstruction. In this paper, we propose a novel stacked U-shape networks with channel-wise attention (SUSR) for SISR. In general, the proposed network consists of four parts, which are shallow feature extraction block, stacked U-shape blocks which produce high-resolution features, residual channel-wise attention blocks and reconstruction block respectively. The hierarchical high-resolution features produced by U-shape blocks have the same size with the final super-resolved image, thus different to existing methods we do upsampling operator in U-shape blocks. In order to fully exploit the different hierarchical features, we propose residual attention block (RAB) to perform feature refinement which explicitly model relationships between channels. Experiments on five public datasets show that our method can achieve much higher Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) scores than the state-of-the-art methods.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "The pilgrimage (Hajj) is an annual event that takes place in Saudi Arabia. Three major government ministries (Foreign, Internal, and Hajj) create and process Hajj data separately in their systems. Currently all data sharing between these ministries regarding Hajj is done manually. Benefits from sharing data electronically are obvious. But due to the sensitivity of some data and the common requirement of not sharing everything, a trusted environment which provides interoperability between these systems while ensuring confidentiality of shared data is needed. In order to study the possibility of establishing such an environment, data was collected regarding the security requirements of the three Saudi ministries directly from the source through interviews. There are three increasingly sophisticated security requirements: no obligation access security, multi level security, and Chinese Wall security. The paper analyzes each security requirement, builds a lattice model for it, and uses these models to specify the information flow policy for each system.",
    "actual_venue": "San Diego, Ca"
  },
  {
    "abstract": "We propose an extension to the recent approaches in topic-mixture modeling such as Latent Dirichlet Allocation and Topic Tracking Model for the purpose of unsupervised adaptation in speech recognition. Instead of using the 1-best input given by the speech recognizer, the proposed model takes confusion network as an input to alleviate recognition errors. We incorporate a selection variable which helps reweight the recognition output, thus creating a more accurate latent topic estimate. Compared to adapting based on just one recognition hypothesis, the proposed model show WER improvements on two different tasks.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "Employing the model of a layer of continuous random medium with an underlying rough surface, the bistatic scattering and backscattering coefficients are calculated. By using the reciprocity, the emissivity is then calculated. Numerical results simulate the temporal variations of complementary backscattering and emissivity of the vegetation canopies. Theoretical results are compared with the measur...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "Query recommendation is an integral part of modern search engines that helps users find their information needs. Traditional query recommendation methods usually focus on recommending users relevant queries, which attempt to find alternative queries with close search intent to the original query. Whereas the ultimate goal of query recommendation is to assist users to accomplish their search task successfully, while not just find relevant queries in spite of they can sometimes return useful search results. To better achieve the ultimate goal of query recommendation, a more reasonable way is to recommend users high utility queries, i.e., queries that can return more useful information. In this paper, we propose a novel utility query recommendation approach based on absorbing random walk on the session-flow graph, which can learn queries' utility by simultaneously modeling both users' reformulation behaviors and click behaviors. Extensively experiments were conducted on real query logs, and the results show that our method significantly outperforms the state-of-the-art methods under the evaluation metric QRR and MRD.",
    "actual_venue": "Ecir"
  },
  {
    "abstract": "Research has already outlined the enormous potential of Computer Supported Collaborative Learning (CSCL) to facilitate effective learning processes in higher education. There is still need, though, to build up a validated model able to portray the relationships between the key elements to design and carry out online collaboration methodologies. The purpose of this study is to establish a global model, with the aim to understand the key factors affecting online collaborative learning and to analyze their interrelation, examining the influence of interaction, intra group emotional support and online collaborative tools in learning in CSCL. The study was conducted with 106 students in the context of 5 university degree subjects that implied working on CSCL projects. At the end of the projects, the students filled out a questionnaire and the resulting data was analyzed using the partial least squares (PLS) technique. The research model proved to have a good predictive level, fulfilling the 6 hypotheses proposed. Results reveal the relevance of interaction, considering teacher-student interaction as well as student-student interaction in groups during the collaboration process. Emotional support linked to intragroup work reveals itself as a fundamental pillar in collaborative learning. On the other hand, online collaborative tools have proved to contribute to interaction between group members and to sustain emotional support. Consequently, in order to model cognitive presence, social presence and teaching presence during CSCL it is necessary to promote a fluent and satisfactory interaction, rooted on the learning process and on emotional support as well as on effective management of the online tools facilitating collaboration. Results also suggest the convenience of further research on other types of interaction in the context of CSCL.",
    "actual_venue": "Computers In Education"
  },
  {
    "abstract": "With millions of mobile applications available for download, and the proliferation of these types of software in our daily lives, it is becoming increasingly important to ensure the security of these applications. Previous research showed that developers have little knowledge of security and privacy regulations, and existing vulnerability detection tools have usability issues that prevent developers from using them. In my research I look at the human factor; e.g., how developers conduct security reviews of their code and what issues they face when using existing tools. In addition, I aim to develop tools and methodologies that support vulnerability detection while seamlessly integrating with the Software Development Lifecycle.",
    "actual_venue": "MUM"
  },
  {
    "abstract": "Online services can be difficult to conceive and to managedue to their inherent complexity, involving many subjectswith distinct goals (user, managers, public and privateagencies, ...), often regulated by complex norms andprocedures; moreover, they are frequently based oncomplex data and information structures usually comingfrom heterogeneous data sources (old paper archives,legacy systems, ...); finally, they must be delivered to usersthrough multiple channels (in-person, by phone, by theWeb, by kiosks, etc.). Conceptual models for Webapplications can be very useful to manage this complexity,but a new level of integration is required to simultaneouslydeal with non-trivial processes, complex data structuresand rich hypermedia communication patterns, undermultiple constraints (legal, technical, ...). DigitalGovernment is a major application domain for this topic. Itaims at improving government-citizen interactions usinginformation and communication technologies. Governmentagencies collect, store, process and share informationabout million of citizens who have different preferences,backgrounds and interaction requirements.The aim of this paper is to describe some real authors'experiences in adopting existing conceptual models todesign and to implement advanced Digital Governmentapplications and to deploy new online services. Inparticular, we discuss the learned lessons and the mainissues we have run into modeling online fiscal services forItalian local agencies.",
    "actual_venue": "MMM"
  },
  {
    "abstract": "Abdominal aortic aneurysm (AAA) is a localized dilatation of the aortic wall. Accurate measurements of its geometric characteristics are critical for a reliable estimate of AAA rupture risk. However, current imaging modalities do not provide sufficient contrast to distinguish thrombus from surrounding tissue thus making the task of segmentation quite challenging. The main objective of this paper is to address this problem and accurately extract the thrombus and outer wall boundaries from cross sections of a 3D AAA image data set (CTA). This is achieved by new geometrical methods applied to the boundary curves obtained by a Level Set Method (LSM). Such methods address the problem of leakage of a moving front into sectors of similar intensity and that of the presence of calcifications. The versatility of the methods is tested by creating artificial images which simulate the real cases. Segmentation quality is quantified by comparing the results with a manual segmentation of the slices of ten patient data sets. Sensitivity to the parameter settings and reproducibility are analyzed. This is the first work to our knowledge that utilizes the level set framework to extract both the thrombus and external AAA wall boundaries.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "Nowadays, it is a common practice to protect various types of statistical data before publishing them for different researches. For instance, when conducting extensive demographic surveys such as national census, the collected data should be at least depersonalized to guarantee proper level of privacy preservation. In practice, even more complicated methods of data protection need to be used. All these methods can be generally divided into two classes. The first ones aim at providing individual data anonymity, whereas the other ones are focused on protecting information about a group of respondents. In this paper, we propose a novel technique of providing group anonymity in statistical data using singular spectrum analysis (SSA).Also, we apply SSA to defining hidden patterns in demographic data distribution.",
    "actual_venue": "Corr"
  },
  {
    "abstract": "Presently, many communication protocols are under development which are tailored tothe efficient high-speed data transfer meeting different application-specific requirements.Our approach concentrates on a framework which facilitates the formal verification of theprotocols. The framework supplies verified and re-usable implications between predefinedprotocol and service specification components. For the verification of a specific protocol,protocol, service and medium can be modelled as...",
    "actual_venue": "Pstv"
  },
  {
    "abstract": "A goal of computing and networking systems is to limit administrative requirements for users and operators. A technical systems should be able to configure itself as much as possible to increase the usability. This leads to the design of self-organizing systems. Self-organizing systems emerge as an increasingly important area of research, not only for computer networks but also in many other fields. For analyzing properties of complex systems, a mathematical model for these systems may be useful. Whether a model with discrete time or with continuous time fits better, depends on the properties of the system and which analysis should be done in the model. In this paper we give a comparison between discrete and continuous models and we give a formal definition for modeling continuous complex systems. Then this theory is applied to model slot-synchronization in wireless networks.",
    "actual_venue": "Autonomics"
  },
  {
    "abstract": "Our previous research has shown that the collective behavior of search engine caches (e.g., Google, Yahoo, Live Search) and web archives (e.g., Internet Archive) results in the uncoordinated but large-scale refreshing and migrating of web resources. Interacting with these caches and archives, which we call the Web Infrastructure (WI), allows entire websites to be reconstructed in an approach we call lazy preservation. Unfortunately, the WI only captures the client-side view of a web resource. While this may be useful for recovering much of the content of a website, it is not helpful for restoring the scripts, web server configuration, databases, and other server-side components responsible for the construction of the website's resources. This paper proposes a novel technique for storing and recovering the server-side components of a website from the WI. Using erasure codes to embed the server-side components as HTML comments throughout the website, we can effectively reconstruct all the server components of a website when only a portion of the client-side resources have been extracted from the WI. We present the results of a preliminary study that baselines the lazy preservation of ten EPrints repositories and then examines the preservation of an EPrints repository that uses the erasure code technique to store the server-side EPrints software throughout the website. We found nearly 100% of the EPrints components were recoverable from the WI just two weeks after the repository came online, and it remained recoverable four months after it was \"lost\".",
    "actual_venue": "Jcdl"
  },
  {
    "abstract": "Password-based authenticated group key exchange allows any group of users in possession of a low-entropy secret key to establish a common session key even in the presence of adversaries. In this paper, we propose a new generic construction of password-authenticated group key exchange protocol from any two-party password-authenticated key exchange with explicit authentication. Our new construction has several advantages when compared to existing solutions. First, our construction only assumes a common reference string and does not rely on any idealized models. Second, our scheme enjoys a simple and intuitive security proof in the universally composable framework and is optimal in the sense that it allows at most one password test per user instance. Third, our scheme also achieves a strong notion of security against insiders in that the adversary cannot bias the distribution of the session key as long as one of the players involved in the protocol is honest. Finally, we show how to easily extend our protocol to the dynamic case in a way that the costs of establishing a common key between two existing groups is significantly smaller than computing a common key from scratch.",
    "actual_venue": "Ct-Rsa"
  },
  {
    "abstract": "Although the technology and applications of wireless sensor networks have greatly increased over the last years, ensuring a dependable real-time operation despite faults and temporal uncertainties is still an on-going research topic. The problems are particularly significant when considering that future applications will interact with their environment not only for supervision or monitoring, but also to directly control physical (real-time) entities, sometimes with safety-critical requirements. We believe that reasoning in terms of data validity might be a good way to approach the problem. The ability to know if sensor data flowing in the system is valid – data validity awareness –, is a first step to achieve a dependable operation. But more than that, it should be possible to ensure, given requirements for data validity throughout the operation, a dependable perception of the environment. In this paper we essentially discuss the problem, analyzing some of the issues that need to be addressed to achieve these goals. Particularly, we introduce fundamental concepts and relevant definitions, we elaborate on the main impediments to achieve data validity awareness and describe relevant means to deal with these impediments. Finally, we address the issue of ensuring a dependable perception and present some research ideas in this direction.",
    "actual_venue": "New Delhi"
  },
  {
    "abstract": "Procedural methods for animating turbulent fluid are often preferred over simulation, both for speed and for the degree of animator control. We offer an extremely simple approach to efficiently generating turbulent velocity fields based on Perlin noise, with a formula that is exactly incompressible (necessary for the characteristic look of everyday fluids), exactly respects solid boundaries (not allowing fluid to flow through arbitrarily-specified surfaces), and whose amplitude can be modulated in space as desired. In addition, we demonstrate how to combine this with procedural primitives for flow around moving rigid objects, vortices, etc.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "This paper deals with defining an interactive student modeling system (ISMS) that allows for negotiation of the student model between the system and the student. The objective of this system is to develop a more accurate student model by promoting student inspection and modification of the student model. The ISMS is based on the combination of two approaches to student modeling that begin with a basic student model template. First, machine learning techniques will be used to develop and maintain a novice-to-expert learning transition in the student model. Second, methods for externalizing the student model will be used to provide for student-system interaction. The combination of these approaches allows the student to reflect on his learning process through discussion of the student model, helping to form a deeper understanding of his learning process, which ultimately leads to better developed problem solving skills.",
    "actual_venue": "Acm Southeast Regional Conference"
  },
  {
    "abstract": "Purpose - While it is commonly recognised that Big Data have an immense potential to generate value for business organisations, appropriating value from Big Data and, in particular, Big Data-enabled analytics is still an open issue for many organisations. The purpose of this paper is to develop a maturity model to support organisations in the realisation of the value created by Big Data.Design/methodology/approach - The maturity model is developed following a qualitative approach based on literature analysis and semi-structured interviews with domain experts. The completeness and usefulness of the model is evaluated qualitatively by practitioners, whereas the applicability of the model is evaluated by Big Data maturity assessments in three real-world organisations.Findings - The proposed maturity model is considered exhaustive by domain experts and has helped the three assessed organisations to develop a more critical understanding of the next steps to take.Originality/value - The maturity model integrates existing industry-developed maturity models into one single coherent Big Data maturity model. The proposed model answers the call for research on Big Data to abstract from technical issues to focus on the business implications of Big Data initiatives.",
    "actual_venue": "Industrial Management And Data Systems"
  },
  {
    "abstract": "One of the reasons that many IEEE 802.11 WLAN infrastructures do not utilize signal encryption may be related to the general perception that encryption largely degrades performance of the system. Especially, there have been several recent research studies suggesting that encryption could noticeably degrade performance of WLAN systems by as much as 20%. Here, we note that the 2.4-GHz frequency spectrum used in the WLAN systems is unlicensed and shared by various kinds of wireless devices and applications, and essentially the high interference in this frequency band may greatly affect the experiment results. Thus, it is worthwhile to reinvestigate and verify whether encryption in WLANs actually causes that much performance degradation. In essence, encryption should only have minimal impact because the amount of overhead inserted by the encryption algorithms is normally less than 1% of the total payload. In this work, we reinvestigate the issue of encryption vs. performance of infrastructure IEEE 802.11 WLANs. A very quiet test site is selected in order to obtain reliable test results. The experiment results in this work demonstrate that the impact of encryption on the throughput performance of an infrastructure IEEE 802.11 WLAN system is negligible.",
    "actual_venue": "WTS"
  },
  {
    "abstract": "We present a soft error rate analysis (SERA) methodology for combinational and memory circuits. SERA is based on a modeling and analysis-based approach that employs a judicious mix of probability theory, circuit simulation, graph theory and fault simulation. SERA achieves five orders of magnitude speed-up over Monte Carlo based simulation approaches with less than 5% error. Dependence of soft error rate (SER) of combinational circuits on supply voltage, clock period, latching window, circuit topology, and input vector values are explicitly captured and studied for a typical 0.18 /spl mu/m CMOS process. Results show that the SER of logic is a much stronger function of timing parameters than the supply voltage. Also, an \"SER peaking\" phenomenon in multipliers is observed where the center bits have an SER that is in order of magnitude greater than that of LSBs and MSBs.",
    "actual_venue": "Iccad"
  },
  {
    "abstract": "Black-box risk scoring models permeate our lives, yet are typically proprietary and opaque. We propose a transparent model distillation approach to detect bias in such models. Model distillation was originally designed to distill knowledge from a large, complex teacher model to a faster, simpler student model without significant loss in prediction accuracy. We add a third restriction - transparency. In this paper we use data sets that contain two labels to train on: the risk score predicted by a black-box model, as well as the actual outcome the risk score was intended to predict. This allows us to compare models that predict each label. For a particular class of student models - interpretable tree additive models with pairwise interactions (GA2Ms) - we provide confidence intervals for the difference between the risk score and actual outcome models. This presents a new method for detecting bias in black-box risk scores by assessing if contributions of protected features to the risk score are statistically different from their contributions to the actual outcome.",
    "actual_venue": "Arxiv: Machine Learning"
  },
  {
    "abstract": "Skyline query processing over uncertain data streams has attracted considerable attention recently, due to its importance in helping users make intelligent decisions on complex data. Nevertheless, existing studies only focus on retrieving the skylines over data streams in a centralised environment typically with one processor, which limits the scalability and cannot meet the requirement for massive data analysis. Cloud computing provides unprecedentedly opportunities for supporting massive data management, which can be well adapted to the parallel skyline queries. In this paper, we extensively study the parallel skyline query problem over uncertain data streams in cloud computing environments. Particularly, three parallel models SPM, APM, and DPM are proposed to address the problem based on the sliding window partitioning. Additionally, an adaptive sliding granularity adjustment strategy and a load balance strategy are proposed to further optimise the queries. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposals.",
    "actual_venue": "Ijwgs"
  },
  {
    "abstract": "This paper describes a demonstration of SDN-based optical transport network virtualization and orchestration. Two scenarios are demonstrated: a dynamic setup of optical connectivity services inside a single domain as well as a multidomain service orchestration over a shared optical infrastructure using the architecture defined in the STRAUSS project.",
    "actual_venue": "Software Defined Networks"
  },
  {
    "abstract": "Word reordering is a difficult task for translation between languages with widely different word orders, such as Japanese and English. A previously proposed post-ordering method for Japanese-to-English translation first translates a Japanese sentence into a sequence of English words in a word order similar to that of Japanese, then reorders the sequence into an English word order. We employed this post-ordering framework and improved upon its reordering method. The existing post-ordering method reorders the sequence of English words via SMT, whereas our method reorders the sequence by (1) parsing the sequence using ITG to obtain syntactic structures which are similar to Japanese syntactic structures, and (2) transferring the obtained syntactic structures into English syntactic structures according to the ITG. The experiments using Japanese-to-English patent translation demonstrated the effectiveness of our method and showed that both the RIBES and BLEU scores were improved over compared methods.",
    "actual_venue": "Acm Trans Asian Lang Inf Process"
  },
  {
    "abstract": "Cloud computing ushers in an era of consolidated information technology infrastructure that is elastic, available and scalable. Virtualization is a critical building block in this evolution and enables centralized, consistent, and policy-driven administration of the underlying computing resources and their protection. This paper presents a cloud-based application whitelisting system called CLAW, which leverages this centralized management flexibility to guarantee that only application binaries in a pre-approved set are allowed to run in each virtual machine under its management. In addition, by applying virtual machine introspection technology, CLAW performs this security policy enforcement without installing any agents inside the managed VMs. We describe the key techniques in the design and implementation of CLAW and compare them with previous hypervisor-based application whitelisting systems. Empirical measurements on a Xen-based CLAW prototype for Windows-based virtual machines show that the run-time performance overhead of out-of-VM application whitelisting is under 10%.",
    "actual_venue": "Ieee Cloud"
  },
  {
    "abstract": "The main purpose of the research and development activities presented in this paper is related to the content development techniques for the ITV. It deals with the development of a real-time immersive and interactive TV show based on a DVB-MHP technology. It is basically a TV quiz that can be played from home thanks to virtual avatars that simulate the behaviors of the players. In fact, the immersive and interactive dimensions require a new gaming concept that renders the viewers as active and participative actors. This work has been achieved under RTI-TVS ESA (European Spatial Agency) project.",
    "actual_venue": "Montreal, Que, Canada"
  },
  {
    "abstract": "Scale-out approach, in contrast to scale-up approach (exploring increasing performance by utilizing more powerful shared-memory servers), refers to deployment of applications on a large number of small, inexpensive, but tightly packaged and tightly interconnected servers. The purpose of this study is to understand the performance of scale-out architectures with a typical enterprise workload, IBM Trade Performance Benchmark Sample for WebSphere Application Server (a.k.a. Trade). We describe a performance evaluation methodology that gives accurate predictions of application performance and system utilization by utilizing experimental data driven model development. Through experiments and extrapolation from the derived model, we show that for such workload, WebSphere Application Server packages for distributed environments scale well while the possible bottleneck of the application deployment is the database tier. Index Terms-- web-services, scale-out, queueing network, parameter inference, experiments",
    "actual_venue": "Mascots"
  },
  {
    "abstract": "This paper presents a HSPICE macromodel of a phase change memory (PCM) cell. The model simulates not only the resistance change by phase (as corresponding to the two states, amorphous and crystalline), but also the temperature profile, the crystalline fraction during the programming and the drift behavior in resistance and threshold voltage. The proposed macromodel (consisting of two models) generates the I-V and R-I plots of a PCM cell at a very small error compared with experimental data. The electrical-based modeling by HSPICE allows to fully characterize the holding voltage and the continuous behavior of the PCM resistance, while assessing the impact of the programming time. Furthermore, the proposed model takes into account the drift behavior of few parameters when the PCM is not been programmed or read, making the model more realistic. Selection of the parameters is based on operational features, so the electrical characterization of the PCM is simple, easy to simulate and intuitive.",
    "actual_venue": "Nanoarch: Proceedings Of The Ieee/Acm International Symposium On Nanoscale Architectures"
  },
  {
    "abstract": "Middleboxes are being rearchitected to be service oriented, composable, extensible, and elastic. Yet system-level support for high availability (HA) continues to introduce significant performance overhead. In this paper, we propose Pico Replication (PR), a system-level framework for middleboxes that exploits their flow-centric structure to achieve low overhead, fully customizable HA. Unlike generic (virtual machine level) techniques, PR operates at the flow level. Individual flows can be checkpointed at very high frequencies while the middlebox continues to process other flows. Furthermore, each flow can have its own checkpoint frequency, output buffer and target for backup, enabling rich and diverse policies that balance---per-flow---performance and utilization. PR leverages OpenFlow to provide near instant flow-level failure recovery, by dynamically rerouting a flow's packets to its replication target. We have implemented PR and a flow-based HA policy. In controlled experiments, PR sustains checkpoint frequencies of 1000Hz, an order of magnitude improvement over current VM replication solutions. As a result, PR drastically reduces the overhead on end-to-end latency from 280% to 15.5% and throughput overhead from 99.5% to 3.2%.",
    "actual_venue": "Socc"
  },
  {
    "abstract": "In online character recognition based on elastic matching, such as dynamic programming matching, many of mis-recognitions are often caused by overfitting, which is the phenomenon that the distance between reference pattern of an incorrect category and an input pattern is under-estimated by unnatural matching. In this paper, a new recognition technique is proposed where category-specific deformations, called eigen-deformations, are utilized to suppress those misrecognitions. Generally, matching results at overfitting are not consistent with the eigen-deformations. Thus, the overfitting can be detected and penalized by a posterior evaluation of this inconsistency. The result of a recognition experiment showed the usefulness of the proposed technique.",
    "actual_venue": "Iwfhr"
  },
  {
    "abstract": "This paper focuses on the development of solutions able to enhance the peer selection semantics supported by P2P based applications. The proposed framework relies on the concept of a flexible P2P tracker able to support multiple versatile peer selection procedures that might be activated and configured during the application lifetime. The devised flexible tracker may also resort to additional cross-layer informations provided by the network level or other external entities. Using a BitTorrent-like P2P approach as a case study, the proposed solution fosters the capabilities of P2P applications to differentiate, benefit or penalize specific peers, also giving ground for the development of advanced applications based on the P2P paradigm.",
    "actual_venue": "Sixth International Conference On Broadband Communications, Networks, And Systems"
  },
  {
    "abstract": "A method for robot obstacle avoidance and path planning is proposed. The algorithm is based on a camera image feedback loop utilizing a neural network for image processing. The method can successfully generate collision-free paths in a 2D robot workspace containing randomly-placed polygonal obstacles. The control algorithm is simple and robust and has low computational requirements. Controller simulation implemented on a personal computer can generate collision-free robot paths in real time, requiring approximately 1 sec. per robot move",
    "actual_venue": "Osaka"
  },
  {
    "abstract": "The current work aims to study within a nutrigenetics context the multifactorial trait beneath obesity. To this end, the use of parallel Multifactor Dimensionality Reduction (pMDR) is investigated towards the identification of i) factors that have an impact to obesity onset solely or interacting with each other and ii) rules that describe the interactions among them. Data have been obtained from a large scale nutrigenetics study and each subject, characterized as normal or overweight based on Body Mass Index (BMI), is featured a 63-dimensional vector describing his/her genetic variations and nutritional habits. pMDR method was used to reduce the initial set of factors into subsets that can classify a subject into either normal or overweight with a certain accuracy and are further used by corresponding prediction models. Results showed that pMDR selected factors associated to obesity and constructed predictive models showing a good generalization ability. Rules describing interactions of the selected factors were extracted, thus enlightening the classification mechanism of the constructed model.",
    "actual_venue": "Setn"
  },
  {
    "abstract": "Two humanoid robots are used to play table tennis with each other. For each humanoid robot, three cameras and a computer are equipped to form a stereovision system and a monocular vision system. The stereovision system consisting of two smart cameras and a computer measures the 3-dimensional position of the table tennis ball. It adopts parallel processing mode in order to realize hundred frames level measurement per second. A high-speed digital camera and the computer compose the monocular vision system, which measures the pose of the robot relative to the table via a color mark attached on the robot. The two smart cameras in each stereovision system are synchronized via I/O signals. The vision systems for the two robots are synchronized by time verification. Experimental results verify the effectiveness of the designed vision system and the proposed methods.",
    "actual_venue": "Case"
  },
  {
    "abstract": "We prove an exponent bound for relative difference sets corresponding to symmetric nets. We discuss the case λ = 1 which gives some restrictions on possible automorphism groups of projective planes.",
    "actual_venue": "J Comb Theory, Ser A"
  },
  {
    "abstract": "We present evidence for a close analogy between the nonlinear behaviour of a pulsed microwave-driven Josephson junction at low temperature and the experimentally observed behaviour of Josephson systems operated below the quantum transition temperature under similar conditions. We specifically address observations of Ramsey-type fringe oscillations, which can be understood in classical nonlinear dynamics as results of slow transient oscillations in a pulsed microwave environment. Simulations are conducted to mimic experimental measurements by recording the statistics of microwave-induced escape events from the anharmonic potential well of a zero-voltage state. Observations consistent with experimentally obtained Ramsey-type oscillations are found in the classical model.",
    "actual_venue": "Open Syst Inform Dynam"
  },
  {
    "abstract": "For making software systems autonomic, it is important to understand and model software-management tasks. Each such task contains typically many interactions between the administrator and the software system. We propose to model software-management interactions and tasks in the form of a discourse between the administrator and the software system. Such discourse models are based on insights from theories of human communication. This should make them \"natural\" for humans to define and understand. While it may be obvious that such discourse models cover software-management interactions, we found that they may also represent major parts of the related tasks. Our well-defined models of interactions and tasks as well as their operationalization should facilitate their execution and automation.",
    "actual_venue": "Gosier"
  },
  {
    "abstract": "Protein fold recognition is the prediction of protein's tertiary structure (Fold) given the protein's sequence without relying on sequence similarity. Using machine learning techniques for protein fold recognition, most of the state-of-the-art research has focused on more traditional algorithms such as Support Vector Machines (SVM), K-Nearest Neighbor (KNN) and Neural Networks (NN). In this paper, we present an empirical study of two variants of Boosting algorithms - AdaBoost and LogitBoost for the problem of fold recognition. Prediction accuracy is measured on a dataset with proteins from 27 most populated folds from the SCOP database, and is compared with results from other literature using SVM, KNN and NN algorithms on the same dataset. Overall, Boosting methods achieve 60\\%\\ fold recognition accuracy on an independent test protein dataset which is the highest prediction achieved when compared with the accuracy values obtained with other methods proposed in the literature. Boosting algorithms have the potential to build efficient classification models in a very fast manner.",
    "actual_venue": "Philadelphia, Pa"
  },
  {
    "abstract": "This article describes a multilayer model-based approach for text compression. It uses linguistic information to develop a multilayer decomposition model of the text in order to achieve better compression. This new approach is illustrated for the case of the Arabic language, where the majority of words are generated according to the Semitic root-and-pattern scheme. Text is split into three linguistically homogeneous layers representing the three categories of words: derivative, non-derivative and functional words. A fourth layer, called the Mask, is introduced to aid with the reconstruction of the original text from the three layers in the decoding side. Suitable compression techniques are then applied to the different layers in order to maximize the compression ratio. The proposed method has been evaluated in terms of the rate of compression it provides and its time efficiency. Results are shown along with real texts to illustrate the performance of the new approach. The novelties of the compression technique presented in this article are that (1) the morphological structure of words may be used to support better compression and to improve the performances of traditional compression techniques; (2) search for words can be done on the compressed text directly through the appropriate one of its layers; and (3) applications such as text mining and document classification can be performed directly on the compressed texts.",
    "actual_venue": "International Arab Journal Of Information Technology"
  },
  {
    "abstract": "Success in online services cannot promise the success in corresponding mobile services. To understand the mobile service adoption behavior under the context of online service transition, this study, taking mobile payment as an example, from a trust transfer perspective, examines users' acceptance of mobile payment using the TAM (Technology acceptance model). A field survey with 220 mobile payment student users is conducted to test the research model and hypotheses. The key findings include: trust of online payment and structural assurance play the crucial role in the initial trust of mobile payments; perceived ease of use and perceived usefulness positively influent trust in mobile payment. Limitations, theoretical and practical implications are also discussed.",
    "actual_venue": "Fifteenth Wuhan International Conference On E-Business"
  },
  {
    "abstract": "In this paper we extend the Curry-Howard correspondence to intuitionistic sequent calculus with explicit structural rules of weakening and contraction. We present a linear term calculus derived from the calculus of Espírito Santo, which captures the computational content of the intuitionistic sequent logic, by adding explicit operators for weakening and contraction. For the proposed calculus we introduce the type assignment system with simple types and prove some operational properties, including the subject reduction and strong normalisation property. We then relate the proposed linear type calculus to the simply typed intuitionistic calculus of Kesner and Lengrand, which handles explicit operators of weakening and contraction in the natural deduction framework.",
    "actual_venue": "Tbillc"
  },
  {
    "abstract": "Virtual teams use communication technologies to interact and accomplish work without physical presence. Prior research has shown that virtual teams can encounter difficulties in making decision. However, never ICTs have enabled computer-based simulated environments that look real and in which avatars can interact with each other. In comparison with research on decision making in face-to-face teams, literature on virtual teams in virtual worlds is just emerging. Our objective is to investigate how virtual world technology impacts team decision making, and to compare with face-to-face teams. We collected data on teams operating in both environments and used an analytical hierarchy process model to compare their decision quality. Virtual teams in the virtual world took longer to reach a decision, but their decision accuracy was better than face-to-face teams. The results were surprising and suggest that virtual world technology can be an effective team operating environment.",
    "actual_venue": "Journal Of Decision Systems"
  },
  {
    "abstract": "Many sensor network studies assume that the energy cost for sensing is negligible compared with the cost of communications or computing. Opportunities exist to deploy sensor networks utilizing active sensors with a high energy cost such as radar. For a node utilizing radar as its primary sensor, the actual sensing procedure is the main power consumer. In the worst case almost 50% of the power is consumed by the sensing procedure, while only 3% is used for communication, the remainder is consumed by the computing platform. In this paper we examine a wireless sensor network composed of short range radars used to monitor rainfall. These short-range radar nodes are designed to be deployed as part of an ad-hoc network and to limit their reliance on existing infrastructure. We refer to these networks as \"off-the-grid\" (OTG) weather radar networks. Independence of the wired infrastructure (power or communications) allows OTG networks to be deployed in specific regions where sensing needs are greatest, such as mountain valleys prone to flash-flooding, geographic regions where the infrastructure is susceptible to failure, and underdeveloped regions lacking urban infrastructure. We present a simulation based investigation of such an OTG sensor network. We focus on power management and energy harvesting for the network. We use these simulations to demonstrate how geographic location, battery capacity, optimization of power consumption, and node density have an impact on the performance and operational lifetime of such a sensor network. In addition to these simulations, we present the design and implementation of an OTG prototype sensor node. Experiences and data gained from the operation of this node are used as input parameters for the simulations.",
    "actual_venue": "San Francisco, Ca"
  },
  {
    "abstract": "Single antenna vector OFDM (V-OFDM) system has been proposed and investigated in the past. It contains the conventional OFDM and the single carrier frequency domain equalizer (SC-FDE) as two special cases and is flexible to choose any number of symbols in intersymbol interference (ISI) by choosing a proper vector size. In this paper, we develop cyclic delay diversity (CDD) transmission for V-OFDM when there are multiple transmit antennas (CDD-V-OFDM). Similar to CDD-OFDM systems, CDD-V-OFDM can also collect both spatial and multipath diversities. Since V-OFDM first converts a single input single output (SISO) ISI channel to a multi-input and multi-output (MIMO) ISI channel of order/length $K$ times less, where $K$ is the vector size, for a given bandwidth, the CDD-V-OFDM can accommodate $K$ times more transmit antennas than the CDD-OFDM does to collect all the spatial and multipath diversities. This property will specially benefit a massive MIMO system. We show that with the linear MMSE equalizer at each subcarrier, the CDD-V-OFDM achieves diversity order $d_{\\text{CDD-V-OFDM}}^{\\text{MMSE}} = \\min \\{ \\lfloor 2^{-R}K \\rfloor, N_t L \\} +1$, where $R$ is the transmission rate, $N_t$ is the number of transmit antennas, and $L$ is the ISI channel length between each transmit and receive antenna pair. Simulations are presented to illustrate our theory.",
    "actual_venue": "Arxiv: Information Theory"
  },
  {
    "abstract": "We address power minimization of earliest deadline first and rate monotonic schedules by voltage and frequency scaling. We prove that the problems are NP-hard, and present (1+ϵ) fully polynomial time approximation techniques that generate solutions which are guaranteed to be within a specified quality bound (QB= ϵ) (say within 1% of the optimal). We demonstrate that our techniques can match optimal solutions when QB is set at 1%, out perform existing approaches even when QB is set at 10%, generate solutions that are quite close to optimal (<; 5%) even when QB is set at higher values (25%), and execute in a fraction of a second (with QB > 5%) for large 100 node task sets.",
    "actual_venue": "Islped"
  },
  {
    "abstract": "When searching for good scheme, a good solution can be destroyed by an inappropriate choice of crossover points. Furthermore, because of the randomicity of crossover, mutation and selection, a better solution can hardly reach in last stage in EA, and the solution always traps in local optimal. Faced to \"exploding\" solution space, it is tough to find high quality solution just by increasing the population size, diversity of searching, and the number of iteration. In this paper, we design the immunity operator to improve the crossover result by utilizing the immunity theory. As the \"guided mutation operator\", the immunity operator substituted the \"blind mutation operator\" in normal EA, to restrain the degenerate phenomenon during the evolutionary process. We examine the algorithm with examples of TSP and gain promising result.",
    "actual_venue": "International Conference On Computational Science"
  },
  {
    "abstract": "Reconfigurable software has been applied for a long time. Reconfigurable technology also provides possibility for reconfiguring hardware but this has not been much exploited so far. In this paper, a flexible processor architecture is proposed that allows for variable resolution in data variables at run-time. Experiments are undertaken for an image processing task where the results show that the approach is beneficial.",
    "actual_venue": "AHS"
  },
  {
    "abstract": "This paper describes a method for mobile robot position estimation based on feature tracking through the sequence of images provided by a conventional video camera. The block-based correspondence algorithms selects candidates through grey level matching and local coherence analysis; a DSP-based multiprocessing image processing system is used to achieve real time performance. Point-to-point correspondences are used to estimate the motion parameters of a mobile robot. Motion-based procedures to eliminate false matching have been implemented. The method has been applied to a mobile robot in non-structured and outdoor environments",
    "actual_venue": "Iros - Proceedings Of The Ieee/Rsj International Conference On Intelligent Robot And Systems: Innovative Robotics For Real-World Applications, Vols"
  },
  {
    "abstract": "The paper aims at presenting the use of a collection based knowledge representation to improve a geo-political risks and crisis management system. Initially, the system was built onto classic ontological knowledge representation for the use in a multi-agent system. Events occurring were classified using object matching with model events. After comparisons with type scenarios, the system could predict if a crisis can happen. A better understanding of crisis appeared and the system was completely working but, only if the main agent which has the task of questioning the user was replaced with a human because an ontological model is not well adapted for such work. This gives us motive to reconsider the role and the design of the system, stressing the need of a better knowledge management. A collection based knowledge representation seemed to be a good alternative to gather and organize critical information without losing time and information in a type matching process. It is one of the first times this concept of collection, well known in the artistic domain is used in an IT system. We will explain this particular knowledge representation through the successful example of the CHEOPS Project.",
    "actual_venue": "Sainte Luce"
  },
  {
    "abstract": "Walther's estimation calculus was designed to prove the termination of functional programs, and can also be used to solve the similar problem of proving the well-foundedness of induction rules. However, there are certain features of the goal formulae which are more common to the problem of induction rule well-foundedness than the problem of termination, and which the calculus cannot handle. We present a sound extension of the calculus that is capable of dealing with these features. The extension develops Walther's concept of an argument bounded function in two ways: firstly, so that the function may be bounded below by its cirgument, and secondly, so that a bound may exist between two arguments of a predicate. Our calculus enables automatic proofs of the well-foundedness of a large class of induction rules not captiued by the original calculus.",
    "actual_venue": "Lpar"
  },
  {
    "abstract": "In contrast to applications relying on specialized and expensive highly-available infrastructure, the basic approach of microservice architectures to achieve fault tolerance - and finally high availability - is to modularize the software system into small, self-contained services that are connected via implementation-independent interfaces. Microservices and all dependencies are deployed into self-contained environments called containers that are executed as multiple redundant instances. If a service fails, other instances will often still work and take over. Due to the possibility of failing infrastructure, these services have to be deployed on several physical systems. This horizontal scaling of redundant service instances can also be used for load-balancing. Decoupling the service communication using asynchronous message queues can increase fault tolerance, too. The Deutsche Bahn AG (German railway company) uses as system called EPA for seat reservations for inter-urban rail services. Despite its high availability, the EPA system in its current state has several disadvantages such as high operational cost, need for special hardware, technological dependencies, and expensive and time-consuming updates. With the help of a prototype, we evaluate the general properties of a microservice architecture and its dependability with reference to the legacy system. We focus on requirements for an equivalent microservice-based system and the migration process; services and data, containerization, communication via message queues; and achieving similar fault tolerance and high availability with the help of replication inside the resulting architecture.",
    "actual_venue": "Ieee International Conference On Software Quality, Reliability And Security - Companion"
  },
  {
    "abstract": "•We construct the projected transition function of the given STS by projecting out the unobservable events, which maps a projected predicate to another via an observable event.•For a specification represented as a predicate, a supremal normal subpredicate that requires only the controllable-observable events enabled/disabled, is computed based on STS.•By considering the projected transition function, the supremal weakly controllable, coreachable, and normal subpredicate is obtained that describes the controlled behavior of the STS under partial observation.•The proposed approach based on STS provides the possibility to supervise controllable events under partial observation in large-scale systems with the state explosion problem managed.•The control functions of controllable events are also obtained to solve the supervisory control problem under partial observation.",
    "actual_venue": "Information Sciences"
  },
  {
    "abstract": "Objective: The purpose of this paper was to study the applicability of paradigms with motion forms for use in a brain-computer interface (BCI). We examined the performances of different paradigms and evaluated the stimulus effects. Methods: We designed four novel stimulus paradigms based on basic motion modes: swing, rotation, spiral, and radial contraction-expansion. Canonical correlation analysi...",
    "actual_venue": "Ieee Transactions On Bio-Medical Engineering"
  },
  {
    "abstract": "This paper considers the multiple access of mobile users to a common wireless channel. The channel is slotted and the binary feedback (empty slot/nonempty slot) is sent to all accessing users. If a slot was not empty and only one user transmitted in it, the transmission is considered successful. Only the user, which had the successful transmission, receives information about its success. In the Introduction, the paper gives a review of known multiple-access algorithms for such a channel. Then our algorithm is constructed that has none of the weaknesses of the algorithms discussed in the Introduction. The algorithm is stable, in contrast to the ALOHA algorithm. It can work in a channel with capture and multiple reception. Without them, the algorithm has a throughput of 0.2891. It is shown how capture and multiple reception can increase the algorithm throughput to 0.6548 and decrease the packet delay for some fading models. The average packet delay and variance are found for two fading models. The models are Rayleigh fading with incoherent and coherent combining of joint interference power. The accessing traffic is Poisson.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "Intrinsic cortical dynamics are thought to underlie trial-to-trial variability of visually evoked responses in animal models. Understanding their function in the context of sensory processing and representation is a major current challenge. Here we report that intrinsic cortical dynamics strongly affect the representational geometry of a brain region, as reflected in response-pattern dissimilarities, and exaggerate the similarity of representations between brain regions. We characterized the representations in several human visual areas by representational dissimilarity matrices (RDMs) constructed from fMRI response-patterns for natural image stimuli. The RDMs of different visual areas were highly similar when the response-patterns were estimated on the basis of the same trials (sharing intrinsic cortical dynamics), and quite distinct when patterns were estimated on the basis of separate trials (sharing only the stimulus-driven component). We show that the greater similarity of the representational geometries can be explained by coherent fluctuations of regional-mean activation within visual cortex, reflecting intrinsic dynamics. Using separate trials to study stimulus-driven representations revealed clearer distinctions between the representational geometries: a Gabor wavelet pyramid model explained representational geometry in visual areas V1–3 and a categorical animate–inanimate model in the object-responsive lateral occipital cortex.",
    "actual_venue": "Neuroimage"
  },
  {
    "abstract": "Traffic between an edge network and the rest of the Internet can be represented as a dynamic loop-free graph. Understanding in depth the dynamics in time and space (spatial structure, topological breadth, destination persistency, traffic dominating paths) of this graph provides significant insight on the Internet internal architecture and capabilities. This paper analyzes inter-domain traffic from a large campus network based on one month by way of Netflow measurements. Our analysis reveals the topological properties and structure of the traffic graph (breadth, depth, volume), the stability of contacted destinations and the relationship between their popularity and their path length. Based on the observed traffic, we explore the suitability of a simple mathematical model to describe the structure of the outgoing traffic graph.",
    "actual_venue": "Networking"
  },
  {
    "abstract": "We give experimental evidence for the benefits of order-preserving compression in sorting algorithms. While, in general, any algorithm might benefit from compressed data because of reduced paging requirements, we identified two natural candidates that would further benefit from order-preserving compression, namely string-oriented sorting algorithms and word-RAM algorithms for keys of bounded length. The word-RAM model has some of the fastest known sorting algorithms in practice. These algorithms are designed for keys of bounded length, usually 32 or 64 bits, which limits their direct applicability for strings. One possibility is to use an order-preserving compression scheme, so that a bounded-key-length algorithm can be applied. For the case of standard algorithms, we took what is considered to be the among the fastest nonword RAM string sorting algorithms, Fast MKQSort, and measured its performance on compressed data. The Fast MKQSort algorithm of Bentley and Sedgewick is optimized to handle text strings. Our experiments show that order-compression techniques results in savings of approximately 15&percnt; over the same algorithm on noncompressed data. For the word-RAM, we modified Andersson's sorting algorithm to handle variable-length keys. The resulting algorithm is faster than the standard Unix sort by a factor of 1.5X. Last, we used an order-preserving scheme that is within a constant additive term of the optimal Hu--Tucker, but requires linear time rather than O(mlog m), where m &equals; &verbar;Σ&verbar; is the size of the alphabet.",
    "actual_venue": "Journal Of Experimental Algorithmics"
  },
  {
    "abstract": "Ultra-reliable low latency communication (URLLC) is an important new feature brought by 5G, with a potential to support a vast set of applications that rely on mission-critical links. In this article, we first discuss the principles for supporting URLLC from the perspective of the traditional assumptions and models applied in communication/information theory. We then discuss how these principles are applied in various elements of the system design, such as use of various diversity sources, design of packets and access protocols. The important messages are that there is a need to optimize the transmission of signaling information, as well as a need for a lean use of various sources of diversity.",
    "actual_venue": "Arxiv: Information Theory"
  },
  {
    "abstract": "This paper presents an approach to using task models in both the design and the evaluation phases of interactive safety-critical applications. We explain how it is possible to use information contained in task models to support the design and development of effective user interfaces. Moreover, we show how task models can also support a systematic inspection-based usability assessment by examining possible deviations that can occur while users interact with the system, an important issue especially when coping with the peculiar requirements of safety-critical applications. Such evaluation provides useful technical documentation to help users achieve an in-depth understanding of the system and its design rationale. Lastly, a description of the application of our approach to a real case study in the air-traffic control domain will illustrate the main features of the proposed method. In particular, we discuss examples taken from an application for air-traffic controllers in an aerodrome supported by graphical user interfaces for data-link communications with pilots.",
    "actual_venue": "International Journal Of Systems Science"
  },
  {
    "abstract": "Frequent sensor calibration is essential in sensor networks with low-cost sensors. We exploit the fact that temporally and spatially close measurements of different sensors measuring the same phenomenon are similar. Hence, when calibrating a sensor, we adjust its calibration parameters to minimize the differences between co-located measurements of previously calibrated sensors. In turn, freshly calibrated sensors can now be used to calibrate other sensors in the network, referred to as multi-hop calibration. We are the first to study multi-hop calibration with respect to a reference signal (micro-calibration) in detail. We show that ordinary least squares regression---commonly used to calibrate noisy sensors---suffers from significant error accumulation over multiple hops. In this paper, we propose a novel multi-hop calibration algorithm using geometric mean regression, which (i) highly reduces error propagation in the network, (ii) distinctly outperforms ordinary least squares in the multi-hop scenario, and (iii) requires considerably fewer ground truth measurements compared to existing network calibration algorithms. The proposed algorithm is especially valuable when calibrating large networks of heterogeneous sensors with different noise characteristics. We provide theoretical justifications for our claims. Then, we conduct a detailed analysis with artificial data to study calibration accuracy under various settings and to identify different error sources. Finally, we use our algorithm to accurately calibrate 13 million temperature, ground ozone (O3), and carbon monoxide (CO) measurements gathered by our mobile air pollution monitoring network.",
    "actual_venue": "Ipsn"
  },
  {
    "abstract": "Many optimal control problems include a continuous nonlinear dynamic system, state, and control constraints, and final state constraints. When using dynamic programming to solve such a problem, the solution space typically needs to be discretized and interpolation is used to evaluate the cost-to-go function between the grid points. When implementing such an algorithm, it is important to treat numerical issues appropriately. Otherwise, the accuracy of the found solution will deteriorate and global optimality can be restored only by increasing the level of discretization. Unfortunately, this will also increase the computational effort needed to calculate the solution. A known problem is the treatment of states in the time-state space from which the final state constraint cannot be met within the given final time. In this brief, a novel method to handle this problem is presented. The new method guarantees global optimality of the found solution, while it is not restricted to a specific class of problems. Opposed to that, previously proposed methods either sacrifice global optimality or are applicable to a specific class of problems only. Compared to the basic implementation, the proposed method allows the use of a substantially lower level of discretization while achieving the same accuracy. As an example, an academic optimal control problem is analyzed. With the new method, the evaluation time was reduced by a factor of about 300, while the accuracy of the solution was maintained.",
    "actual_venue": "Ieee Transactions Control Systems Technology"
  },
  {
    "abstract": "An artificial neural network model is proposed that combines several aspects taken from physiological observations (oscillations, synchronizations) with a visual latency mechanism in order to achieve an improved analysis of visual scenes. The network consists of two parts. In the lower layers that contain no lateral connections the propagation velocity of the activity of the units depends on the contrast of the individual objects in the scene. In the upper layers lateral connections are used to achieve synchronization between corresponding image parts. This architecture assures that the activity that arises in response to a scene containing objects with different contrast is spread out over several layers in the network. Thereby adjacent objects with different contrast will be separated and synchronization occurs in the upper layers without mutual disturbance between different objects. A comparison with a one-layer network shows that synchronization occurs in the upper layers without mutual disturbance between different objects. A comparison with a one-layer network shows that synchronization in the latency dependent multilayer net is indeed achieved much faster as soon as more than five objects have to be recognized. In addition, it is shown that the network is highly robust against noise in the stimuli or variations in the propagation delays (latencies), respectively. For a consistent analysis of a visual scene the different features of an individual object have to be recognized as belonging together and separated from other objects. This study shows that temporal differences, naturally introduced by stimulus latencies in every biological sensory system, can strongly improve the performance and allow for an analysis of more complex scenes.",
    "actual_venue": "Neural Computation"
  },
  {
    "abstract": "The automatic classification of patent applications into a particular patent classification system remains a challenge with many practical applications. From a computer science point of view, the task is a multi-label hierarchical classification problem, i.e. each patent application might belong to multiple classes within the class hierarchy. The problem is still especially difficult for purely text-based classifiers because patents and patent applications are often formulated in a rather generic way. Thus, additional sources of information should be used to improve class prediction. In our approach, we propose the use of location information contained in the meta data of a patent application in combination with text-based patent classification. We argue that certain technological areas often cluster in geographic regions. For example, space travel technology is often collocated at Houston, Texas due to the NASA facilities in this area. In many cases, the addresses of the inventors are correlated to the technological area of a given patent. Thus, the addresses can be exploited to provide additional information about the technological area. We present a geo-enriched classifier joining established methods for text-based classification with location-based topic prediction. Since the location-based prediction is not applicable to all cases, we provide a method to regulate the impact of the spatial predictor for these cases. Our experiments indicate that spatial prediction is applicable to a considerable amount of patent applications and that the combination of spatial prediction and text-based classification significantly improves the prediction accuracy.",
    "actual_venue": "Georich@Sigmod"
  },
  {
    "abstract": "When developing SO mechanisms, mapping requirements to actual designs and implementations demands a lot of expertise. Among other things, it is important to define the right degree of freedom for the system that allows for self-organization. Back-to-back testing supports this hard engineering task by an adequate testing method helping to reveal failures in this design and implementation procedure. Within this paper we propose a model-based approach for back-to-back testing. The approach is built on top of the S# framework and integrated into the Visual Studio development environment, enabling the creation of executable test models with comprehensive tooling support for model debugging. By applying the concepts to a self-organizing production cell, we show how it is used to fully automatically reveal faults of a SO mechanism.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Customer returns are defective parts that pass all functional and parametric tests, but fail in the field. To prevent customer returns, this paper analyzes wafer probe test data and tries to understand what it takes to screen them out during testing. Because these parts pass all tests, analyzing their signatures based on the original test perspective does not make sense. In this work, we search for a novel test perspective where the test signatures from parametric measurements can be used to separate the returned parts from the rest of population. Our study shows that in order to effectively screen customer returns during wafer test, a multivariate screening methodology is desired. This study is based on analyzing over 1000 parametric wafer probe tests and dies from seven lots, each lot containing one returned part. We demonstrate that analyzing customer returns from a multivariate test perspective leads to robust and conservative results.",
    "actual_venue": "Ieee Vlsi Test Symposium"
  },
  {
    "abstract": "Security metrics are usually defined informally and, therefore, the rigourous analysis of these metrics is a hard task. This analysis is required to identify the existing relations between the security metrics, which try to quantify the same quality: security. Risk, computed as Annualised Loss Expectancy, is often used in order to give the overall assessment of security as a whole. Risk and security metrics are usually defined separately and the relation between these indicators have not been considered thoroughly. In this work we fill this gap by providing a formal definition of risk and formal analysis of relations between security metrics and risk.",
    "actual_venue": "Wistp"
  },
  {
    "abstract": "Chordal Axis (CA) is a new representation of planar shapes introduced by Prasad in [1], useful for skeleton computation, shape analysis, characterization and recognition The CA is a subset of chord and center of discs tangent to the contour of a shape, derivated from Medial Axis (MA) Originally presented in a computational geometry approach, the CA was extracted on a constrained Delaunay triangulation of a discretely sampled contour of a shape Since discrete distance transformations allow to efficiently compute the center of distance balls and detect discrete MA, we propose in this paper to redefine the CA in the discrete space, to extract on distance transforms in the case of chamfer norms, for which the geometry of balls is well-known, and to compare with MA.",
    "actual_venue": "Dgci"
  },
  {
    "abstract": "As more and more e-retailers promise their customers that online experiences will be satisfying ones, understanding what creates a satisfying customer experience becomes crucial. Even though this understanding appears crucial, no studies have comprehensively examined the factors that make consumers satisfied with their e-retailing experiences. To partly fill this void, the author examines the role that consumer perceptions of online convenience, merchandising, service-ability, site design, and financial security play in esatisfaction assessment. And finds that convenience, site design, and security are the dominant factors in consumer assessments of e-satisfaction.",
    "actual_venue": "Iciw"
  },
  {
    "abstract": "One of the challenging issues in dynamic resource management scheme of wireless communications is to achieve the instantaneous channel gain for transmission pairs. When designing the resource allocation algorithms, the previous works usually rely on the statistical information of the channel distribution and the harsh assumption of uncertain channel, and they are with much conservatism for the dynamic channel gain. To overcome the limitations of the existing methods, the Fuzzy Logic System (FLS) is adopted in this paper to estimate the instantaneous channel gain for the dynamic communication environments. Based on the estimated channel gains, the power allocation scheme is proposed for the orthogonal frequency division multiple access (OFDMA) femtocell network. The proposed scheme attempts to maximize the total rate of the communication system and to guarantee the quality of service of each user. The original optimization problem is a nonconvex optimization problem which is difficult to solve. The successive convex approximation (SCA) is adopted to transform the nonconvex original problem into a tractable one. The distributed iterative algorithm is developed to integrate with the proposed scheme in real-time dynamic communication environments. The effectiveness of the proposed scheme is demonstrated through the simulations, and the results show that the sum rate and energy efficiency of users can be improved by the proposed scheme, compared to the existing schemes.",
    "actual_venue": "Computer Networks"
  },
  {
    "abstract": "There is a large and growing body of literature concerning the solutions of geometric problems on mesh-connected arrays of\n processors. Most of these algorithms are optimal (i.e., run in timeO(n\n 1/d\n ) on ad-dimensionaln-processor array), and they all assume that the parallel machine is trying to solve a problem of sizen on ann-processor array. Here we investigate the situation where we have a mesh of sizep and we are interested in using it to solve a problem of sizen >p. The goal we seek is to achieve, when solving a problem of sizen >p, the same speed up as when solving a problem of sizep. We show that for many geometric problems, the same speedup can be achieved when solving a problem of sizen >p as when solving a problem of sizep.",
    "actual_venue": "Algorithmica"
  },
  {
    "abstract": "In 2003, the International Patient Decision Aid Standards (IPDAS) Collaboration was established to enhance the quality and effectiveness of patient decision aids by establishing an evidence-informed framework for improving their content, development, implementation, and evaluation. Over this 10 year period, the Collaboration has established: a) the background document on 12 core dimensions to inform the original modified Delphi process to establish the IPDAS checklist (74 items); b) the valid and reliable IPDAS instrument (47 items); and c) the IPDAS qualifying (6 items), certifying (6 items + 4 items for screening), and quality criteria (28 items). The objective of this paper is to describe the evolution of the IPDAS Collaboration and discuss the standardized process used to update the background documents on the theoretical rationales, evidence and emerging issues underlying the 12 core dimensions for assessing the quality of patient decision aids.",
    "actual_venue": "Bmc Med Inf And Decision Making"
  },
  {
    "abstract": "The growing deployment of renewable power generation creates a growing opportunity in \"stranded power\", power that is generated a close to zero-cost, but not usable by the power grid. We propose to use this stranded power to create a \"zero carbon\" high-performance computing resource, exploiting the batch computing model to exploit the volatile power efficiently.",
    "actual_venue": "Cit/Iucc/Dasc/Picom Ieee International Conference On Computer And Information Technology - Ubiquitous Computing And Communications - Dependable, Autonomic And Secure Computing - Pervasive Intelligence And Computing"
  },
  {
    "abstract": "This paper considers the problem of channel coding over Gaussian intersymbol interference (ISI) channels with a given decoding rule. Specifically, it is assumed that the mismatched decoder has an incorrect assumption on the channel impulse response. The mismatch capacity is the highest achievable rate for a given decoding rule. The existing achievable rates for channels and decoding metrics with memory (as in our model) are currently available only in the form of multi-letter expressions that cannot be calculated. Consequently, they provide little insight on the mismatch problem. In this paper, we derive the computable formulas of achievable rates and discuss some implications of our results. Our achievable rates are based on two ensembles: the ensemble of codewords generated by an autoregressive process and the ensemble of codewords drawn uniformly over a “type class” of real-valued sequences. We provide a few numerical results of our achievable rates, as functions of the mismatched ISI parameters. Finally, we compare our results with universal decoders which are designed \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">outside</italic>\n the true class of channels that we consider in this paper.",
    "actual_venue": "Ieee Transactions On Information Theory"
  },
  {
    "abstract": "Regression testing is applied after modifications are performed to large software systems in order to verify that the changes made do not unintentionally disrupt other existing components. When employing regression testing it is often desirable to reduce the number of test cases executed in order to achieve a certain objective; a process known as test suite minimisation. We use multi-objective optimisation to analyse the trade-off between code coverage and execution time for the test suite of Mockito, a popular framework used to create mock objects for unit tests in Java. We show that a large reduction can be made in terms of execution time at the expense of only a small reduction in code coverage and discuss how the described methods can be easily applied to many projects that utilise regression testing.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "No abstract available.",
    "actual_venue": "Acm Sigsoft Software Engineering Notes"
  },
  {
    "abstract": "This paper describes experiences with several architectural frameworks. An \"architectural framework\" specifies what is included in the description of an architecture, independent of the specific system being described. The three frameworks are the U.S. DoD C4ISR Architecture Framework, the associated Core Architecture Data Model and the emerging IEEE Recommended Practice on Architecture Description. From these experiences, we speculate on the further evolution of architecture frameworks and architectural descriptions.",
    "actual_venue": "Ada-Europe"
  },
  {
    "abstract": "Narrowband source localization gets extremely challenging in strong reverberation. When the room is perfectly known, some dictionary-based methods have recently been proposed, allowing source localization with few measurements. In this paper, we first show that, for these methods, the choice of frequencies is important as they fail to localize sources that emit at a frequency near the modal frequencies of the room. A more difficult case, but also important in practice, is when the room geometry and boundary conditions are unknown. In this setup, we introduce a new model for the acoustic soundfield, based on the Vekua theory, that allows a separation of the field into its reverberant and direct source contributions, at the cost of more measurements. This can be used for the design of a dereverberation pre-processing step, suitable for a large variety of standard source localization techniques. We discuss the spatial sampling strategies for the sound field, in order to successfully recover acoustic sources, and the influence of parameters such as number of measurements and model order. This is validated in numerical and experimental tests, that show that this method significantly improves localization in strong reverberant conditions.",
    "actual_venue": "Selected Topics in Signal Processing, IEEE Journal of  "
  },
  {
    "abstract": "The work reported in this paper is motivated towards the development of a mathematical model for swarm systems based on macroscopic primitives. A pattern formation and transformation model is proposed. The pattern transformation model comprises two general methods for pattern transformation, namely a macroscopic transformation method and a mathematical transformation method. The problem of transformation is formally expressed and four special cases of transformation are considered. Simulations to confirm the feasibility of the proposed models and transformation methods are presented. Comparison between the two transformation methods is also reported.",
    "actual_venue": "Robotics And Autonomous Systems"
  },
  {
    "abstract": "This paper addresses the problem of decoding and precoding in the K-user MIMO interference channels. At the receiver side, a joint decoding of the interference and the desired signal is able to improve the receive diversity order. At the transmitter side, we introduce a joint linear precoding design that maximizes the joint cut-off rate, known as a tight lower bound on the joint mutual information for high signal-to-noise ratio (SNR). We also derive a closed-form solution of the precoding matrices that maximizes the mutual information when the SNR is close to zero. This solution is characterized by its low computational complexity, and only requires a local channel state information knowledge at the transmitters. Our simulation results show that decoding interference jointly with the desired signal results in a significant improvement of the receive diversity order. Also a substantial bit error rate and sum-rate improvements are illustrated using the proposed precoding designs.",
    "actual_venue": "Ieee International Symposium On Personal, Indoor And Mobile Radio Communications"
  },
  {
    "abstract": "Sustaining participation in design is difficult, especially in neighborhoods that seek change consistent with their cultural values. Participatory Design offers several approaches (e.g. infrastructuring) that help to balance the sensibilities of urban planning with the immediacies of design. This paper investigates the distinctive value of a \\\"constellation\\\" of participatory activities that sustained engagement throughout the design of an urban plaza, combining physical and digital flows. Based on a three-year collaboration in a historically black South LA neighborhood, this study analyzes the reinvention of urban furniture -- payphones, bus benches, newspaper boxes, planters, and public displays -- into community interactions. After reviewing the concrete vision of this constellation, the City of Los Angeles decided to fund the implementation of a pedestrian plaza. This paper articulates our methods of infrastructuring, providing techniques that sustain participation over time around physical urban objects that become touch points for fluid groups of designers. More than any one design, the constellation approach provides a platform for horizontal iteration, maintaining focus and participation in imagining a neighborhood's socio-technical future.",
    "actual_venue": "PDC"
  },
  {
    "abstract": "Quality consultants are used in ISO 9000 implementation projects especially by many small and medium-sized enterprises. Clients do not always appreciate differences between quality consultants. This paper aims to provide an analytical tool to select the best quality consultant providing the most customer satisfaction. The clients of three Turkish quality consultancy firms were interviewed and the most important criteria taken into account by the clients while they were selecting their consultancy firms were determined by a designed questionnaire. The fuzzy analytic hierarchy process was applied to compare these consultancy firms. The means of the triangular fuzzy numbers produced by the customers and experts for each comparison were successfully used in the pair wise comparison matrices.",
    "actual_venue": "International Journal Of Information Technology And Decision Making"
  },
  {
    "abstract": "In mobile computing, factors such as add-on hardware components and heterogeneous networks result in an environment of changing resource constraints. An application in such a constrained environment must adapt to these changes so that available resources are properly utilized. We propose an architecture for exporting awareness of the mobile computing environment to an application. In this architecture, a change in the environment is modeled as an asynchronous event that includes information related to the change. Events are typed and are organized as an extensible class hierarchy so that they can be handled at different levels of abstraction according to the requirement of each application. We also compare two approaches to structure an adaptive application. One addresses the problem of incorporating adaptiveness into legacy applications, while the other considers the design of an application with adaptiveness in mind.",
    "actual_venue": "Ieee Trans Software Eng"
  },
  {
    "abstract": "•Focusing on determining the activity sequence with minimum total feedback time in a DSM.•NP-complete optimization problem.•A new formulation of the optimization problem is proposed, which allows obtaining optimal solutions in a reasonable amount of time for problems up to 40 coupled activities.•Two simple rules for reducing the total feedback time, which can be conveniently used by management.•A heuristic approach that is able to provide good solutions for large instances.•The usefulness of the approach has been validated through case study, computer experiments and benchmark analysis.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Salient object detection is a fundamental task in computer vision and pattern recognition. And it has been investigated by many researchers in many fields for a long time. Numerous salient object detection models based on deep learning have been designed in recent years. However, the saliency maps extracted by most of the existing models are blurry or have irregular edges. To alleviate these problems, we propose a novel approach named SalNet to detect the salient objects accurately in this paper. The architecture of the SalNet is an U-Net which can combine the features of the shallow and deep layers. Moreover, a new objective function based on the image convolution is further proposed to refine the edges of saliency maps by using a constraint on the L1 distance between edge information of the ground-truth and the saliency maps. Finally, we evaluate our proposed SalNet on benchmark datasets and compare it with the state-of-the-art algorithms. Experimental results demonstrate that the SalNet is effective and outperforms several representative methods in salient object detection task.",
    "actual_venue": "Prcv"
  },
  {
    "abstract": "Tire-road friction characteristics are deeply interlaced with all vehicle safety oriented control systems, as road conditions strongly affect the control schemes behavior. In this work we focus on estimating the peak-value of the tire-road friction curve, as this is the stability boundary for the wheel braking dynamics. Moreover, we show how such algorithm can be employed as a supervisory control to enhance safety properties and performance of current anti-lock braking systems. The proposed strategy is analyzed and tested for different sensors' configurations, i.e., with and without longitudinal wheel slip measurement available and its validity is assessed on experimental data collected on a test vehicle",
    "actual_venue": "Itsc"
  },
  {
    "abstract": "Approach of automatic selection of parallel sparse linear solver and tuning its parameters is discussed. Performance of linear solver is considered as function of sparse matrix features, selected sparse solver and its settings, hardware platform. Supervised learning is used to create this function and is based on gathering of empirical performance data and the following use of the support vector regression. After creating complexity function, a genetic algorithm is used to find its minimum across the solver types and settings to find locally optimal solver. Results of experiments with PETSc linear solvers are presented for several classes of sparse problems from different applications.",
    "actual_venue": "Parallel Computing: From Multicores And Gpus To Petascale"
  },
  {
    "abstract": "Frequency reuse distance is a crucial parameter in cellular system design that determines the service quality, which is usually measured in terms of the signal-to-interference ratio (SIR). The instantaneous SIR is a random variable; theoretically, its characterization is completely provided by the outage probability. In cellular engineering, however, frequency reuse planning is usually based on a simple static design, without taking into account the dynamic propagation effects. The static method has the advantage of simplicity, but failing to indicate the system dynamic performance. In this paper, we determine a simple mathematical expression between the static and dynamic SIRs whereby a new cell planning technique is developed. The new technique makes it possible to incorporate dynamic considerations into a static design thereby bridging the gap between the two methods. We also use the new results to study the system capacity, revealing the dependence of the system capacity on the minimum required outage probability",
    "actual_venue": "Vehicular Technology, Ieee Transactions"
  },
  {
    "abstract": "We extend the notion of natural extension, that gives the least committal extension of a given assessment, from the theory of sets of desirable gambles to that of choice functions. We give an expression of this natural extension and characterise its existence by means of a property called avoiding complete rejection. We prove that our notion reduces indeed to the standard one in the case of choice functions determined by binary comparisons, and that these are not general enough to determine all coherent choice function. Finally, we investigate the compatibility of the notion of natural extension with the structural assessment of indifference between a set of options.",
    "actual_venue": "Communications In Computer And Information Science"
  },
  {
    "abstract": "Graphene which is a single atom layer of carbon film with the interesting properties of high carrier mobility, high carrier concentration, high thermal conductivity, high velocity saturation, and reduced short channel effects, is emerging as a replacement of the ubiquitous silicon. This is particularly true for high-speed analog and radio-frequency electronics due to low Ion/Ioff ratio. In this paper, design exploration of a graphene FET (GFET) based LC-VCO is performed with wireless (WLAN) as the target application. Verilog-A based GFET modeling is performed. The model is used in design simulation, characterization, and sensitivity analysis of a cross-coupled version of an LC-VCO. In order to analyze the effects of nanoscale process variations, statistical process variation analysis of the GFET based LC-VCO is performed for selected figures of merit through exhaustive Monte Carlo simulations. Power dissipation and quality factor are also analyzed and their characteristic data are illustrated to obtain a comprehensive description of the circuit. Frequency and phase noise are observed to be within the nominal design range having standard deviation 0.06 GHz and 7.78 dBc/Hz corresponding to 2.35% and 9.03% of the mean, respectively, for the total statistical variation of the parameters.",
    "actual_venue": "Quality Electronic Design"
  },
  {
    "abstract": "As parallel systems from traditional mainframe vendors join offerings from specialist parallel computer manufacturers in the commercial marketplace, this paper examines the issues facing a business end-user considering migration to parallel systems. The results of a 1992 survey of major UK companies are presented, highlighting a number of factors which are perceived as inhibiting the wider uptake of parallel computing in commerce.",
    "actual_venue": "Parco"
  },
  {
    "abstract": "We consider the traffic allocation problem: arriving customers have to be assigned to one of a group of servers. The aim is to optimize system performance measures, such as mean waiting time of a customer or total number of customers in the system, under a given static allocation policy. Two static policies are considered: probabilistic assignment and allocation according to a fixed pattern. For these two policies, general properties as well as optimization aspects are discussed.",
    "actual_venue": "Theor Comput Sci"
  },
  {
    "abstract": "The discrete resource allocation problem (RAP) examines the problem of allocation of discrete and indivisible scarce resources in a multiple agent environment from the point of view of a coordinator. The basis for resource allocation is preference information of each agent regarding the discrete resources. Such preference information is obtained by the coordinator through a sequential experimental process that entails interrogating each of the agents about their preference profiles. In this environment, where information is incomplete and has finite cost, two kinds of experimental schemes are investigated: one consists of pairwise comparisons between resource bundles about which there is incomplete  a priori  preference information, and the other consists of experiments that rank resource bundles about which there is incomplete a priori preference information. The signals that result from each such experiment are used by the coordinator to systematically eliminate all except the one preference ordering that describes the agent's preference profile. The paper carries out the investigation in terms of entropy, a general nonparametric measure.",
    "actual_venue": "Automatica"
  },
  {
    "abstract": "In this paper, we survey algorithms that allocate a parallel program represented by an edge-weighted directed acyclic graph (DAG), also called a task graph or macro-dataflow graph, to a set of homogeneous processors, with the objective of minimizing the completion time. We analyze 21 such algorithms and classify them into four groups. The first group includes algorithms that schedule the DAG to a bounded number of processors directly. These algorithms are called the bounded number of processors (BNP) scheduling algorithms. The algorithms in the second group schedule the DAG to an unbounded number of clusters and are called the unbounded number of clusters (UNC) scheduling algorithms. The algorithms in the third group schedule the DAG using task duplication and are called the task duplication based (TDB) scheduling algorithms. The algorithms in the fourth group perform allocation and mapping on arbitrary processor network topologies. These algorithms are called the arbitrary processor network (APN) scheduling algorithms. The design philosophies and principles behind these algorithms are discussed, and the performance of all of the algorithms is evaluated and compared against each other on a unified basis by using various scheduling parameters.",
    "actual_venue": "Ispan"
  },
  {
    "abstract": "Research on stress at work often involves the analysis of urinary adrenaline and noradrenaline. It is usually assumed that samples have to be cooled quickly and stored at refrigerator temperature before freezing. This is often difficult to achieve in field studies. This experimental study therefore tests the robustness of results when samples are not cooled immediately. Samples of 9 men and women, collected at 3 points in time, were immediately frozen, kept for a variable delay in a warm room, or stored in a refrigerator before freezing. Two indices were calculated: (a) the ratio of hormones to liquid volume, period of excretion, and body weight; and (b) the ratio of hormones to urinary creatinine. The reliability of high performance liquid chromatography analysis was satisfactory, as was the comparability of the 2 indices. Unfavorable storage up to 24 hr did not cause bacteria-driven decreases of catecholamines, regardless of storage temperature or sampling time. Results suggest high stability for at least 24 hr without cooling, provided the samples are immediately acidified. Cooling may therefore be handled less restrictively than has been assumed. The application of this research is to facilitate research in settings where samples are collected at different places, such as participants' homes or different workplaces.",
    "actual_venue": "Human Factors"
  },
  {
    "abstract": "The graphs associated with regular and semi-regular plane tessellations are characterized in terms of certain algebraic structures associated with them. Each of the graphs has associated with it a rosetta, a configuration of lattice points and colored edges which is repeated throughout the plane.",
    "actual_venue": "Pattern Recognition Letters"
  },
  {
    "abstract": "Symbolic regression via genetic programming is a flexible approach to machine learning that does not require up-front specification of model structure. However, traditional approaches to symbolic regression require the use of protected operators, which can lead to perverse model characteristics and poor generalisation. In this paper, we revisit interval arithmetic as one possible solution to allow genetic programming to perform regression using unprotected operators. Using standard benchmarks, we show that using interval arithmetic within model evaluation does not prevent invalid solutions from entering the population, meaning that search performance remains compromised. We extend the basic interval arithmetic concept with `safeu0027 search operators that integrate interval information into their process, thereby greatly reducing the number of invalid solutions produced during search. The resulting algorithms are able to more effectively identify good models that generalise well to unseen data. We conclude with an analysis of the sensitivity of interval arithmetic-based operators with respect to the accuracy of the supplied input feature intervals.",
    "actual_venue": "Arxiv: Neural And Evolutionary Computing"
  },
  {
    "abstract": "In this contribution we propose a hybrid genetic programming approach for evolving a decision making system in the domain of RoboCup Soccer (Simulation League). Genetic programming has been rarely used in this domain in the past, due to the difficulties and restrictions of the soccer simulation. The real-time requirements of robot soccer and the lengthy evaluation time even for simulated games provide a formidable obstacle to the application of evolutionary approaches. Our new method uses two evolutionary phases, each of which compensating for restrictions and limitations of the other. The first phase produces some evolved GP individuals applying an off-game evaluation system which can be trained on snapshots of game situations as they actually happened in earlier games, and corresponding decisions tagged as correct or wrong. The second phase uses the best individuals of the first phase as input to run another GP system to evolve players in a real game environment where the quality of decisions is evaluated through winning or losing during real-time runs of the simulator. We benchmark the new system against a baseline system used by most simulation league teams, as well as against winning systems of the 2016 tournament.",
    "actual_venue": "Gecco"
  },
  {
    "abstract": "In this paper, a framework for collaborative face recognition from video sequences in a multi-camera environment is proposed. Collaboration between cameras allows for higher recognition performance in both the common and non-common field-of-view (FOV) cases. For the latter, the appearance of an object in a nearby camera is predicted using the last tracked position of the object paired with a time-of-arrival model between camera pairs. An experiment using four cameras in an office environment confirms the applicability and performance gains of the proposed framework.",
    "actual_venue": "Ieee International Conference On Advanced Video And Signal Based Surveillance"
  },
  {
    "abstract": "We introduce probabilistic (modular) embedding as an extension to the well-known notion of modular embedding [5] conceived to capture the expressiveness of stochastic systems, focussing here on tuple-based probabilistic languages.",
    "actual_venue": "SAC"
  },
  {
    "abstract": "This study examines whether adolescent motivations for social media use, social comparison tendencies and gender are related to online aggression victimization and/or perpetration. Results from a national cross-sectional survey of adolescents (N = 340) reveal that social media use, romantic motivations, social belongingness motivations and greater social comparison tendencies are associated with online aggression victimization (R2 = 0.38). Information motivations and entertainment motivations are negatively associated with online aggression perpetration, but romantic motivations, social comparison, and social media use were positive predictors (R2 = 0.34). Further examination of interactions and indirect effects suggests that romantic motivations for social media use are an important predictor of involvement in online aggression among adolescents.",
    "actual_venue": "Computers In Human Behavior"
  },
  {
    "abstract": "In this paper, we focus on visual venue category prediction, which can facilitate various applications for location-based service and personalization. Considering the complementarity of different media platforms, it is reasonable to leverage venue-relevant media data from different platforms to boost the prediction performance. Intuitively, recognizing one venue category involves multiple semantic...",
    "actual_venue": "Ieee Transactions On Multimedia"
  },
  {
    "abstract": "Motivation The number of available membrane protein structures has markedly increased in the last years and, in parallel, the reliability of the methods to detect transmembrane (TM) segments. In the present report, we characterized inter-residue interactions in alpha-helical membrane proteins using a dataset of 3462 TM helices from 430 proteins. This is by far the largest analysis published to date. Results Our analysis of residue-residue interactions in TM segments of membrane proteins shows that almost all interactions involve aliphatic residues and Phe. There is lack of polar-polar, polar-charged and charged-charged interactions except for those between Thr or Ser sidechains and the backbone carbonyl of aliphatic and Phe residues. The results are discussed in the context of the preferences of amino acids to be in the protein core or exposed to the lipid bilayer and to occupy specific positions along the TM segment. Comparison to datasets of beta-barrel membrane proteins and of alpha-helical globular proteins unveils the specific patterns of interactions and residue composition characteristic of alpha-helical membrane proteins that are the clue to understanding their structure. Availability and implementation Results data and datasets used are available at http://lmc.uab.cat/TMalphaDB/interactions.php. Supplementary information Supplementary data are available at Bioinformatics online.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "The hippocampus is a subcortical structure which is involved in memory function. There is a considerable amount of evidence available which indicates that the hippocampal system is necessary for effective spatial learning in rodents and short-term topographical memory in human. Recordings of neural activities from the hippocampus of behaving animals can help us to understand how spatial information is encoded and processed by the hippocampus. In this work, we designed a triple-region microelectrode array (MEA) which took into concern the anatomical structures of the rat hippocampus. The array was composed of 16 stainless steel wires which were arranged into three groups that differed in length. Each group targeted one subregion of the hippocampus. The array was chronically implanted into the rat hippocampus through craniotomy. Neural activities were monitored both during the implantation and after recovery. The triple-region MEA was capable of recording unitary activities from multiple subregions of the rat hippocampus and the spatial distribution of firing rates were analyzed while the animal freely explored in the environment.",
    "actual_venue": "Embc"
  },
  {
    "abstract": "The organization and processes of today's health and social care are becoming ever more complex as a consequence of societal trends, including an aging population and an increased reliance on care at home. One aspect of the increased complexity is that a single patient may receive care from several care providers, which easily results in situations with potentially incoherent, uncoordinated, and interfering care processes. In order to describe and analyze such situations, the article introduces the notion of a process conglomeration. This is defined as a set of patient-care processes that all concern the same patient, that are overlapping in time, and that all are sharing the overall goal of improving or maintaining the health and social well-being of the patient. Problems and challenges of process conglomerations are investigated using coordination theory and models for continuous process improvement. In order to address the challenges, a solution is proposed in the form of a Coordination Hub, being an integrated software service that offers a number of information services for coordinating the activities of the processes in a process conglomeration.",
    "actual_venue": "Health Informatics Journal"
  },
  {
    "abstract": "We study the well known LCP (Largest Common Point-Set) under Bottleneck Distance Problem. Given two proteins a and b (as sequences of points in 3D space) and a distance cutoff &#x03C3;, the goal is to find a spatial superposition and an alignment that maximizes the number of pairs of points from a and b that can be fit under the distance &#x03C3; from each other. The best to date algorithms for approximate and exact solution to this problem run in time O(n^8) and O(n^32), respectively, where n represents the protein length. This work improves the runtime of the approximation algorithm and the algorithm for absolute optimum for both order-dependent and order-independent alignments. More specifically, our algorithms for near-optimal and optimal sequential alignments run in time O(^7 log n) and O(n^14 log n), respectively. For non-sequential alignments, corresponding running times are O(n^7.5) and O(n^14.5).",
    "actual_venue": "Ieee/Acm Trans Comput Biology Bioinform"
  },
  {
    "abstract": "Automatic Speech Recognition has reached almost human performance in some controlled scenarios. However, recognition of impaired speech is a difficult task for two main reasons: data is (i) scarce and (ii) heterogeneous. In this work we train different architectures on a database of dysarthric speech. A comparison between architectures shows that, even with a small database, hybrid DNN-HMM models outperform classical GMM-HMM according to word error rate measures. A DNN is able to improve the recognition word error rate a 13% for subjects with dysarthria with respect to the best classical architecture. This improvement is higher than the one given by other deep neural networks such as CNNs, TDNNs and LSTMs. All the experiments have been done with the Kaldi toolkit for speech recognition for which we have adapted several recipes to deal with dysarthric speech and work on the TORGO database. These recipes are publicly available.",
    "actual_venue": "Advances In Speech And Language Technologies For Iberian Languages, Iberspeech"
  },
  {
    "abstract": "Memory testing commonly faces two issues: the characterization of detailed and realistic fault models, and the definition of time-efficient test algorithms to detect them. March tests have proven to be a fast, simple and regularly structured class of memory test algorithms. This paper proposes a new polynomial algorithm to automatically generate march tests. The formal model adopted to represent memory faults allows the definition of a general methodology to deal with both static, dynamic and linked faults.",
    "actual_venue": "Ieee Trans Computers"
  },
  {
    "abstract": "In this paper, we present a reservation-based medium access control (MAC) protocol with multicast support for wavelength-division multiplexing networks. Our system is based on the single-hop, passive optical star architecture. Of the available wavelengths (channels), one channel is designated as a control channel, and the remaining channels are used for data transmission. Each node is equipped with a pair of fixed transceiver to access the control channel, and a fixed transmitter and a tunable receiver to access data channels. For easy implementation of the protocol in hardware and for precisely computing the protocol's processing overhead, we give a register-transfer model of the protocol. We simulate the protocol to study its throughput behavior, and present its analytic model. For a node to be able to send data packets in successive data slots with no time gap between them, in spite of the situation that the protocol's execution time may be longer than data transmission time, we propose the idea of multiple MAC units at each node. Unicast throughput of our protocol reaches the theoretically possible maximum throughput for MAC protocols with distributed control, and the multicast throughput is at least as good as, and even better than, those delivered by existing MAC protocols with distributed control.",
    "actual_venue": "Ieee Journal On Selected Areas In Communications"
  },
  {
    "abstract": "Currently, multimedia objects can be easily created, stored, (re)-transmitted, and edited for good or bad. In this sense, there has been an increasing interest in finding the structure of temporal evolution within a set of documents and how documents are related to one another overtime. This process, also known in the literature as Multimedia Phylogeny, aims at finding the phylogeny tree(s) that best explains the creation process of a set of near-duplicate documents (e.g., images/videos) and their ancestry relationships. Solutions to this problem have direct applications in forensics, security, copyright enforcement, news tracking services and other areas. In this paper, we explore one heuristic and one optimum branching algorithm for reconstructing the evolutionary tree associated with a set of image documents. This can be useful for aiding experts to track the source of child pornography image broadcasting or the chain of image distribution in time, for instance. We compare the algorithms with the state-of-the-art solution considering 350,000 test cases and discuss advantages and disadvantages of each one in a real scenario.",
    "actual_venue": "J Visual Communication And Image Representation"
  },
  {
    "abstract": "There have been some rapid advances on the design of full duplex (FD) transceivers in recent years. Although the benefits of FD have been studied for single-hop wireless communications, its potential on throughput performance in a multi-hop wireless network remains unclear. As for multi-hop networks, a fundamental problem is to compute the achievable end-to-end throughput for one or multiple communication sessions. The goal of this paper is to offer some fundamental understanding on end-to-end throughput performance limits of FD in a multi-hop wireless network. We show that through a rigorous mathematical formulation, we can cast the multi-hop throughput performance problem into a formal optimization problem. Through numerical results, we show that in many cases, the end-to-end session throughput in a FD network can exceed $2 \\\\times$ of that in a half duplex (HD) network. Our finding can be explained by the much larger design space for scheduling that is offered by removing HD constraints in throughput maximization problem. The results in this paper offer some new understandings on the potential benefits of FD for end-to-end session throughput in a multi-hop wireless network.",
    "actual_venue": "Ieee Trans Mob Comput"
  },
  {
    "abstract": "The on-chip communication architecture is a primary determinant of overall performance in complex system-on-chip (SoC) designs. Since the communication requirements of SoC components can vary significantly over time, communication architectures that dynamically detect and adapt to such variations can substantially improve system performance. In this paper, we propose Flexbus, a new architecture that can efficiently adapt the logical connectivity of the communication architecture and the components connected to it. Flexbus achieves this by dynamically controlling both the communication architecture topology, as well as the mapping of SoC components to the communication architecture. This is achieved through new dynamic bridge by-pass, and component remapping techniques. In this paper, we introduce these techniques, describe how they can be realized within modern on-chip buses, and discuss policies for run-time reconfiguration of Flexbus-based architectures.The techniques underlying Flexbus are general, and are applicable to a variety of bus standards. We have implemented Flexbus as an extension of the popular AMBA AHB bus, and have evaluated it using a commercial design flow. We report on experiments conducted to analyze its area, timing, and performance under a wide variety of system-level traffic profiles. We have applied Flexbus to two example SoC designs: 1) an IEEE 802.11 MAC processor and 2) an UMTS turbo decoder. Our results show that Flexbus provides gains of up to 34.55 % in application data-rates over conventional architectures, with negligible area overhead and a 3.2% penalty in clock period.",
    "actual_venue": "Vlsi) Systems, Ieee Transactions"
  },
  {
    "abstract": "The storage subsystem has undergone tremendous innovation in order to keep up with the ever-increasing demand for throughput. Non Volatile Memory Express (NVMe) based solid state devices are the latest development in this domain, delivering unprecedented performance in terms of latency and peak bandwidth. NVMe drives are expected to be particularly beneficial for I/O intensive applications, with databases being one of the prominent use-cases. This paper provides the first, in-depth performance analysis of NVMe drives. Combining driver instrumentation with system monitoring tools, we present a breakdown of access times for I/O requests throughout the entire system. Furthermore, we present a detailed, quantitative analysis of all the factors contributing to the low-latency, high-throughput characteristics of NVMe drives, including the system software stack. Lastly, we characterize the performance of multiple cloud databases (both relational and NoSQL) on state-of-the-art NVMe drives, and compare that to their performance on enterprise-class SATA-based SSDs. We show that NVMe-backed database applications deliver up to 8× superior client-side performance over enterprise-class, SATA-based SSDs.",
    "actual_venue": "Systor"
  },
  {
    "abstract": "The Team Orienteering Problem (TOP) is a variant of the vehicle routing problem. Given a set of vertices, each one associated with a score, the goal of TOP is to maximize the sum of the scores collected by a fixed number of vehicles within a certain prescribed time limit. More particularly, the Team Orienteering Problem with Time Windows (TOPTW) imposes the period of time of customer availability as a constraint to assimilate the real world situations. In this paper, we present a memetic algorithm for TOPTW based on the application of split strategy to evaluate an individual. The effectiveness of the proposed MA is shown by many experiments conducted on benchmark problem instances available in the literature. The computational results indicate that the proposed algorithm competes with the heuristic approaches present in the literature and improves best known solutions in 101 instances.",
    "actual_venue": "Artificial Evolution, Ea"
  },
  {
    "abstract": "This paper explores the role that incremental water charging (temporary levies on water use) can play in reducing the environmental costs that arise during drought events. The paper combines a multi-attribute Revealed Preference Model calibrated at a local level (Agricultural District) and a regionally-calibrated Computable General Equilibrium model to estimate the impacts of incremental water charging on water use, water withdrawals and market income. The methodology is applied to the particular case of Italy's Emilia Romagna Region. Results provide a basis for the assessment of tradeoffs in water conservation.",
    "actual_venue": "Environmental Modelling And Software"
  },
  {
    "abstract": "Many Small and Medium Enterprises (SMEs) tend to gradually adopt Web based business applications to enhance their business processes. To support this gradual adoption we need a framework that supports iterative development. Further processes that have been supported by web based business applications can change and evolve requiring applications to be changed accordingly. To support these needs we have extended the Component Based E Application Development and Deployment Shell; CBEADS©. We analysed many business applications and derived a meta-model. We implemented this meta-model with in CBEADS© and developed a set of Smart Tools to take the instance values of the meta-model and generate the web based business applications. When a new business application is required, a business analyst can create a new instance of the meta-model. To change an implemented business application the appropriate values of the meta-model instance that corresponds to the particular application can be changed.",
    "actual_venue": "Icwe"
  },
  {
    "abstract": "Multimedia documents are different from traditional text documents, because they may contain encodings of raw sensorical data. This fact has severe consequences for the efficient indexing and retrieval of information from documents in large unstructured collections (e.g. WWW), because it is very difficult to automatically identify generic meanings from visual or audible objects. A novel method for image retrieval from large collections is proposed in this paper. The method is based on color co-occurrence descriptors that utilize compact representations of essential information of the visual image content. The set of descriptor elements represents “elementary” color segments, their borders, and their mutual spatial distribution on the image frame. Such representation is flexible enough to describe image scenes ranging from simple combinations of color segments to high frequency color textures equally well. At the retrieval stage the comparison between a given query descriptor and the database descriptors is performed by a similarity measure. Image descriptors are robust versus affine transformations and several other image distortions. The consideration of the descriptors as sets of elements allows the combination of several images or subimages into a single query. Basic properties of the method are demonstrated experimentally on an image database containing 20000 images",
    "actual_venue": "Lausanne"
  },
  {
    "abstract": "Web-based information systems provide to their users the ability to interleave querying and browsing during their information discovery efforts. The MIX system provides an API called QDOM (Querible Document Object Model) that supports the interleaved querying and browsing of virtual XML views, specified in an XQuery-like language.QDOM is based on the DOM standard. It allows the client applications to navigate into the view using standard DOM navigation commands. Then the application can use any visited node as the root for a query that creates a new view.The query/navigation processing algorithms of MIX perform decontextualization, i.e., they translate a query that has been issued from within the context of other queries and navigations into efficient queries that are understood by the source outside of the context of previous operations. In addition, MIX provides a navigation-driven query evaluation model, where source data are retrieved only as needed by the subsequent navigations.",
    "actual_venue": "Icde"
  },
  {
    "abstract": "We present a proof for the probabilistic completeness of RRT-based algorithms when planning with constraints on end-effector pose. Pose constraints can induce lower-dimensional constraint manifolds in the configuration space of the robot, making rejection sampling techniques infeasible. RRT-based algorithms can overcome this problem by using the sample-project method: sampling coupled with a projection operator to move configuration space samples onto the constraint manifold. Until now it was not known whether the sample-project method produces adequate coverage of the constraint manifold to guarantee probabilistic completeness. The proof presented in this paper guarantees probabilistic completeness for a class of RRT-based algorithms given an appropriate projection operator. This proof is valid for constraint manifolds of any fixed dimensionality.",
    "actual_venue": "Robotics And Automation"
  },
  {
    "abstract": "An integral formulation for the solution of a class of second order boundary value problems which are described by the equation d 2 yy/dx 2 + P(x,y, dy/dx, d 2 yy/dx 2 )) = 0, x ε (0,a), is presented. The resulting integral equations are then solved by expressing the dependent variable y as a power series which made the computation of various integrals possible. The proposed method is tested through some examples to show the applicability of the method to solve a wide range of second order differential equations including the nonlinear ones.",
    "actual_venue": "Applied Mathematics And Computation"
  },
  {
    "abstract": "The development of open electricity markets has led to a decoupling between the market clearing procedure that defines the power dispatch and the security analysis that enforces predefined stability margins. This gap results in market inefficiencies introduced by corrections to the market solution to accommodate stability requirements. In this paper we present an optimal power flow formulation that aims to close this gap. First, we show that the pseudospectral abscissa can be used as a unifying stability measure to characterize both poorly damped oscillations and voltage stability margins. This leads to two novel optimization problems that can find operation points which minimize oscillations or maximize voltage stability margins, and make apparent the implicit tradeoff between these two stability requirements. Finally, we combine these optimization problems to generate a dynamics-aware optimal power flow formulation that provides voltage as well as small signal stability guarantees.",
    "actual_venue": "Decision And Control"
  },
  {
    "abstract": "We define a subset krtUML of UML which is rich enough to express all behavioural. modelling entities of UML used for real-time applications, covering such aspects as active objects, dynamic object creation and destruction, dynamically changing communication topologies in inter-object communication, asynchronous signal based communication, synchronous communication using operation calls, and shared memory communication through global attributes. We define a formal interleaving semantics for this kernel language by associating with each model M epsilon krtUML a symbolic transition system STS(M). We outline how to compile industrial real-time UML models making use of generalisation hierarchies, weak- and strong aggregation, and hierarchical state-machines into krtUML, and propose modelling guidelines for real-time applications of UML. This work provides the semantical foundation for formal verification of real-time UML models described in the companion paper [11].",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Despite the intense research focused on the investigation of the functioning settings of Particle Swarm Optimization, the particles initialization functions - determining the initial positions in the search space - are generally ignored, especially in the case of real-world applications. As a matter of fact, almost all works exploit uniform distributions to randomly generate the particles coordinates. In this article, we analyze the impact on the optimization performances of alternative initialization functions based on logarithmic, normal, and lognormal distributions. Our results show how different initialization strategies can affect - and in some cases largely improve - the convergence speed, both in the case of benchmark functions and in the optimization of the kinetic constants of biochemical systems.",
    "actual_venue": "Ieee Conference On Computational Intelligence In Bioinformatics And Computational Biology"
  },
  {
    "abstract": "Organizations have to comply with geo-location policies that prescribe geographical locations at which personal data may be stored or processed. When using cloud services, checking data geo-location policies during design-time is no longer possible - data geo-location policies need to be checked during run-time. Cloud elasticity mechanisms dynamically replicate and migrate virtual machines and services among data centers, thereby affecting the geo-location of data. Due to the dynamic nature of such replications and migrations, the actual, concrete changes to the deployment of cloud services and thus to the data geo-locations are not known. We propose a policy checking approach utilizing runtime models that reflect the deployment and interaction structure of cloud services and components. By expressing privacy policy checks as an st-connectivity problem, potential data transfers that violate the geo-location policies can be rapidly determined. We experimentally evaluate our approach with respect to applicability and performance using an SOA-version of the CoCoME case study.",
    "actual_venue": "Lecture Notes In Computer Science"
  },
  {
    "abstract": "Discriminative subgraphs are widely used to define the feature space for graph classification in large graph databases. Several scalable approaches have been proposed to mine discriminative subgraphs. However, their intensive computation needs prevent them from mining large databases. We propose an efficient method GAIA for mining discriminative subgraphs for graph classification in large databases. Our method employs a novel subgraph encoding approach to support an arbitrary subgraph pattern exploration order and explores the subgraph pattern space in a process resembling biological evolution. In this manner, GAIA is able to find discriminative subgraph patterns much faster than other algorithms. Additionally, we take advantage of parallel computing to further improve the quality of resulting patterns. In the end, we employ sequential coverage to generate association rules as graph classifiers using patterns mined by GAIA. Extensive experiments have been performed to analyze the performance of GAIA and to compare it with two other state-of-the-art approaches. GAIA outperforms the other approaches both in terms of classification accuracy and runtime efficiency.",
    "actual_venue": "Sigmod Conference"
  },
  {
    "abstract": "Early fire detection is crucial to minimise damage and save lives. Video surveillance smoke detectors do not suffer from transport delays and can cover large areas. The smoke detection on images is, however, a difficult problem due the variability of smoke density, lighting conditions, background clutter, and unstable patterns. In order to solve this problem, we propose a novel unsupervised object classifier. Single visual features are classified using a model that simultaneously creates a codebook and categorises the smoke using a bag-of-words paradigm based on LDA model. Our algorithm can also tell the amount of smoke present on the image. Multiple image sequences from different cameras are used to show the viability of the proposed approach. Our experiments show that the model generalises well for different cameras, perspectives and scales.",
    "actual_venue": "International Conference On Robotics And Automation"
  },
  {
    "abstract": "This paper investigates the potential of exploiting interference as a source of green signal energy in parallel transmissions of co-existing radio systems assisted by a cognitive relay. Assuming a cognitive radio setup, the purpose of the relay is to allow the resources of a primary downlink to be efficiently utilised by a secondary system. While conventionally the relay aims to completely remove the interference, we investigate a strategy of making use of interference energy when the cross-interference between the two systems is mutually constructive. In this way, the interference that already exists in the communication medium provides a source of green signal energy that mutually enhances the received signal power of primary and secondary users without the need to raise the transmitted power. In this direction two adaptive linear precoding techniques are proposed for the cognitive relay and compared to conventional precoding. The effect of green interference on the received signal to noise ratio (SNR) of the primary and secondary users is studied through theoretical analysis and used to predict the resulting error probability and outage performance. The results show that by exploiting free interference power, the secondary downlink can access the primary resources without deteriorating the primary users' performance.",
    "actual_venue": "Ieice Transactions On Communications"
  },
  {
    "abstract": "In this paper, a simple design method is proposed for the construction of biorthogonal f ilter banks with perfect reconstruction property. Starting from an analysis filter bank that meets certain design specifications regarding bandwidth and energy concentration, a complete biorthogonal f ilter bank system is constructed. Then, through applying the lifting scheme to the other analysis filter bank, its response can be greatly improved to meet the desired design specifications without impairing either the perfect construction property or the other analysis filter bank. Illustrative example is given to demonstrate the simplicity and efficiency of the proposed design approach.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "This study investigates the role of extensible mark-up language (XML) business forms and agent-based computing to support negotiation, management and implementation of inter-organization business processes. We explore recently proposed standards for representing artificial agent exchanges through rule-based inference. An XML representational syntax for standard business forms is developed and applied to augment rule-based agents. A prototype investigates ways to combine knowledge-based agents and business forms to assist business and customers in Web-based conference registration. The implementation helps guide customer decisions and the combination of business forms and rules promotes understandability, testability and adaptability of the registration system.",
    "actual_venue": "Expert Systems"
  },
  {
    "abstract": "Dynamic textures (or temporal textures) are a kind of visual motion pattern, and their dynamics are not trackable. Dynamic textures such as flames, smoke, and water are statistical stationary and exhibit spatial self-similarity and temporal self-similarity. Recently, description of dynamic textures has attracted growing attention. In this paper, a novel method for analyzing and synthesizing dynamic textures is proposed. First, the dynamic textures are modeled by combination of dynamic texture units. Second, the segment parameters and dynamic texture units' parameters are learnt at the same time. To reduce computational complexity, we discard Fourier coefficients that are under contribution ratio threshold. Our algorithm is general and automatic, and it works well on various types of textures. Experimental results demonstrate that our approach can reconstruct dynamic texture sequences with promising visual quality and fewer coefficients.",
    "actual_venue": "CSO (2)"
  },
  {
    "abstract": "In this paper we introduce a technique for applying textual labels to 3D surfaces. An effective labeling must balance the conflicting goals of conveying the shape of the surface while being legible from a range of viewing directions. Shape can be conveyed by placing the text as a texture directly on the surface, providing shape cues, meaningful landmarks and minimally obstructing the rest of the model. But rendering such surface text is problematic both in regions of high curvature, where text would be warped, and in highly occluded regions, where it would be hidden. Our approach achieves both labeling goals by applying surface labels to a 'text scaffold', a surface explicitly constructed to hold the labels. Text scaffolds conform to the underlying surface whenever possible, but can also float above problem regions, allowing them to be smooth while still conveying the overall shape. This paper provides methods for constructing scaffolds from a variety of input sources, including meshes, constructive solid geometry, and scalar fields. These sources are first mapped into a distance transform, which is then filtered and used to construct a new mesh on which labels are either manually or automatically placed. In the latter case, annotated regions of the input surface are associated with proximal regions on the new mesh, and labels placed using cartographic principles.",
    "actual_venue": "Ieee Trans Vis Comput Graph"
  },
  {
    "abstract": "We have created and tested Tahuti, a dual-view sketch recognition environment for class diagrams in UML. The system is based on a multi-layer recognition framework which recognizes multi-stroke objects by their geometrical properties allowing users the freedom to draw naturally as they would on paper rather than requiring the user to draw the objects in a pre-defined manner. Users can draw and edit while viewing either their original strokes or the interpreted version of their strokes engendering user-autonomy in sketching. The experiments showed that users preferred Tahuti to a paint program and to Rational Rose™ because it combined the ease of drawing found in a paint program with the ease of editing available in a UML editor.",
    "actual_venue": "Acm Siggraph Courses"
  },
  {
    "abstract": "We present the FORTRAN-code HPOLY.f for the numerical calculation of harmonic polylogarithms up to w = 8 at an absolute accuracy of ∼10−15 or better. Using algebraic and argument relations the numerical representation can be limited to the range x∈[0,2−1]. We provide replacement files to map all harmonic polylogarithms to a basis and the usual range of arguments x∈]−∞,+∞[ to the above interval analytically. We also briefly comment on a numerical implementation of real valued cyclotomic harmonic polylogarithms.",
    "actual_venue": "Computer Physics Communications"
  },
  {
    "abstract": "Based on the theoretical results of (Li & Micchelli, 2000) on approximation by radial basis functions, a rich class of new radial basis functions has been explicitly constructed for the purpose of implementing the simultaneous approximation of functions and their derivatives, which 2D and 3D numerical examples are conducted to show the efficiency of the computation.",
    "actual_venue": "Neural Parallel And Scientific Comp"
  },
  {
    "abstract": "Suppose there is a need to swiftly navigate through a spatial arrangement of possibly forbidden regions, with each region marked with the probability that it is, indeed, forbidden. In close proximity to any of these regions, you have the dynamic capability of disambiguating the region and learning for certain whether or not the region is forbidden - only in the latter case may you proceed through that region. The central issue is how to most effectively exploit this disambiguation capability to minimize the expected length of the traversal. Regions are never entered while they are possibly forbidden, and thus, no risk is ever actually incurred. Nonetheless, for the sole purpose of deciding where to disambiguate, it may be advantageous to simulate risk, temporarily pretending that possibly forbidden regions are riskily traversable, and each potential traversal is weighted with its level of undesirability, which is a function of its traversal length and traversal risk. In this paper, the simulated risk disambiguation protocol is introduced, which has you follow along a shortest traversal - in this undesirability sense - until an ambiguous region is about to be entered; at that location, a disambiguation is performed on this ambiguous region. (The process is then repeated from the current location, until the destination is reached.) We introduce the tangent arc graph as a means of simplifying the implementation of simulated risk disambiguation protocols, and we show how to efficiently implement the simulated risk disambiguation protocols that are based on linear undesirability functions. The effectiveness of these disambiguation protocols is illustrated with examples, including an example that involves mine countermeasures path planning.",
    "actual_venue": "Ieee Transactions On Systems Man And Cybernetics"
  },
  {
    "abstract": "Lift-and-project cuts for mixed integer programs (MIP), derived from a disjunction on an integer-constrained fractional variable,\n were originally (Balas et al. in Math program 58:295–324, 1993) generated by solving a higher-dimensional cut generating linear\n program (CGLP). Later, a correspondence established (Balas and Perregaard in Math program 94:221–245, 2003) between basic\n feasible solutions to the CGLP and basic (not necessarily feasible) solutions to the linear programming relaxation LP of the\n MIP, has made it possible to mimic the process of solving the CGLP through certain pivots in the LP tableau guaranteed to\n improve the CGLP objective function. This has also led to an alternative interpretation of lift-and-project (L&P) cuts, as\n mixed integer Gomory cuts from various (in general neither primal nor dual feasible) LP tableaus, guaranteed to be stronger\n than the one from the optimal tableau. In this paper we analyze the relationship between a pivot in the LP tableau and the\n (unique) corresponding block pivot (sequence of pivots) in the CGLP tableau. Namely, we show how a single pivot in the LP\n defines a sequence (potentially as long as the number of variables) of pivots in the CGLP, and we identify this sequence.\n Also, we give a new procedure for finding in a given LP tableau a pivot that produces the maximum improvement in the CGLP\n objective (which measures the amount of violation of the resulting cut by the current LP solution). Further, we introduce\n a procedure called iterative disjunctive modularization. In the standard procedure, pivoting in the LP tableau optimizes the\n multipliers with which the inequalities on each side of the disjunction are weighted in the resulting cut. Once this solution\n has been obtained, a strengthening step is applied that uses the integrality constraints (if any) on the variables on each\n side of the disjunction to improve the cut coefficients by choosing optimal values for the elements of a certain monoid. Iterative\n disjunctive modularization is a procedure for approximating the simultaneous optimization of both the continuous multipliers\n and the integer elements of the monoid. All this is discussed in the context of a CGLP with a more general normalization constraint\n than the standard one used in (Balas and Perregaard in Math program 94:221–245, 2003), and the expressions that describe the\n above mentioned correspondence are accordingly generalized. Finally, we summarize our extensive computational experience with\n the above procedures.",
    "actual_venue": "Math Program Comput"
  },
  {
    "abstract": "Generic programming is a key paradigm for developing reusable software components. The inherent support for generic constructs is therefore important in programming languages. As for C++, the generic construct, templates, has been supported since the language was first released. However, little is currently known about how C++ templates are actually used in developing real software. In this study, we conduct an experiment to investigate the use of templates in practice. We analyze 1,267 historical revisions of 50 open source systems, consisting of 566 million lines of C++ code, to collect the data of the practical use of templates. We perform statistical analyses on the collected data and produce many interesting results. We uncover the following important findings: (1) templates are practically used to prevent code duplication, but this benefit is largely confined to a few highly used templates; (2) function templates do not effectively replace C-style generics, and developers with a C background do not show significant preference between the two language constructs; (3) developers seldom convert dynamic polymorphism to static polymorphism by using CRTP (Curiously Recursive Template Pattern); (4) the use of templates follows a power-law distribution in most cases, and C++ developers who prefer using templates are those without other language background; (5) C developer background seems to override C++ project guidelines. These findings are helpful not only for researchers to understand the tendency of template use but also for tool builders to implement better tools to support generic programming.",
    "actual_venue": "Acm Transactions On Software Engineering And Methodology"
  },
  {
    "abstract": "D-Cloud is a software testing environment for dependable parallel and distributed systems using cloud computing technology. We use Eucalyptus as cloud management software to manage virtual machines designed based on QEMU, called FaultVM, which have a fault injection mechanism. D-Cloud enables the test procedures to be automated using a large amount of computing resources in the cloud by interpreting the system configuration and the test scenario written in XML in D-Cloud front end and enables tests including hardware faults by emulating hardware faults by FaultVM flexibly. In the present paper, we describe the customization facility of FaultVM used to add new device models. We use SpecC, which is a system description language, to describe the behavior of devices, and a simulator generated from the description by SpecC is linked and integrated into FaultVM. This also makes the definition and injection of faults flexible without the modification of the original QEMU source codes. This facility allows D-Cloud to be used to test distributed systems with customized devices.",
    "actual_venue": "Dependable Computing"
  },
  {
    "abstract": "The operation of electric vehicles in cold weather is a concern, but there is not a lot of literature available regarding the precise nature of impacts on travel range. Two types of commercial battery electric vehicles, namely, the Nissan Leaf and the Mitsubishi i-MiEV, were driven to depletion across a broad range of temperatures that occur naturally in Winnipeg, MB, Canada, due to its climate. A...",
    "actual_venue": "Ieee Transactions On Vehicular Technology"
  },
  {
    "abstract": "A family of gossiping algorithms depending on a combinatorial parameter is introduced, formalized, and discussed. Three members are analyzed. It is shown that, depending on the pattern of the parameter, gossiping can use from O(N2) to O(N) time, N being the number of communicating members. The last and best-performing algorithm, whose activity follows the execution pattern of pipelined hardware processors, is shown to exhibit high throughput and efficiency that are constantwith respect to N. This translates in unlimited scalability for the corresponding gossiping service provided by this algorithm",
    "actual_venue": "Hpcn Europe"
  },
  {
    "abstract": "Recent research shows that significant energy saving can be achieved in wireless sensor networks with a mobile base station that collects data from sensor nodes via short-range communications. However, a major performance bottleneck of such WSNs is the significantly increased latency in data collection due to the low movement speed of mobile base stations. To address this issue, we propose a rendezvous-based data collection approach in which a subset of nodes serve as the rendezvous points that buffer and aggregate data originated from sources and transfer to the base station when it arrives. This approach combines the advantages of controlled mobility and in-network data caching and can achieve a desirable balance between network energy saving and data collection delay. We propose two efficient rendezvous design algorithms with provable performance bounds for mobile base stations with variable and fixed tracks, respectively. The effectiveness of our approach is validated through both theoretical analysis and extensive simulations.",
    "actual_venue": "Mobihoc"
  },
  {
    "abstract": "Motivation: DataPfex is aMATLAB-based application that facilitates the manipulation and visualization of multidimensional datasets. The strength of DataPflex lies in the intuitive graphical user interface for the efficient incorporation, manipulation and visualization of high-dimensional data that can be generated by multiplexed protein measurement platforms including, but not limited to Luminex or Meso-Scale Discovery. Such data can generally be represented in the form of multidimensional datasets [for example ( time x stimulation x inhibitor x inhibitor concentration x cell type x measurement)]. For cases where measurements are made in a combinational fashion across multiple dimensions, there is a need for a tool to efficiently manipulate and reorganize such data for visualization. DataPflex accepts data consisting of up to five arbitrary dimensions in addition to a measurement dimension. Data are imported from a simple. xls format and can be exported to MATLAB or. xls. Data dimensions can be reordered, subdivided, merged, normalized and visualized in the form of collections of line graphs, bar graphs, surface plots, heatmaps, IC50's and other custom plots. Open source implementation in MATLAB enables easy extension for custom plotting routines and integration with more sophisticated analysis tools.",
    "actual_venue": "Bioinformatics"
  },
  {
    "abstract": "This paper presents a Neural Network based model that can be used to predict pointing target for both physical and situational impairment. The model takes different trajectory profiles like velocity, acceleration and bearing of movement as input parameters and based on that predicts next pointing target. We reported three user studies -- one involving users with physical and age-related impairment using a mouse and the other two involved able-bodied users using head and eye-gaze tracking based systems. We found that the model can accurately predict target in all cases. Finally we proposed an adaptation system using the target prediction model that can statistically significantly reduce pointing times.",
    "actual_venue": "Chi Extended Abstracts"
  },
  {
    "abstract": "We introduce a new type of multi-resolution image pyramid for high-resolution images called sparse pdf maps (sPDF-maps). Each pyramid level consists of a sparse encoding of continuous probability density functions (pdfs) of pixel neighborhoods in the original image. The encoded pdfs enable the accurate computation of non-linear image operations directly in any pyramid level with proper pre-filtering for anti-aliasing, without accessing higher or lower resolutions. The sparsity of sPDF-maps makes them feasible for gigapixel images, while enabling direct evaluation of a variety of non-linear operators from the same representation. We illustrate this versatility for antialiased color mapping, O(n) local Laplacian filters, smoothed local histogram filters (e.g., median or mode filters), and bilateral filters.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "mCrypton, which is a mini-version of Crypton, is a 64-bit block cipher with three key size options (64 bits, 96 bits, 128 bits). It was designed for use in low-cost ubiquitous wireless devices and resource-constrained tiny devices such as low-cost Radio-Frequency Identification tags and sensors in Ubiquitous Sensor Network. In this paper we show that 8-round mCrypton with 128-bit key is vulnerable to related-key rectangle attack. We first describe how to construct two related-key truncated differentials on which 7-round related-key rectangle distinguisher is based and then we exploit it to attack 8-round mCrypton. This attack requires 246 dada and 246 time complexities, which is faster than exhaustive search. This is the first known cryptanalytic result on mCrypton. Copyright © 2009 John Wiley & Sons, Ltd. In this paper, we show that 8-round mCrypton with 128-bit key is vulnerable to related-key rectangle attack. We first describe how to construct two related-key truncated differentials on which 7-round related-key rectangle distinguisher is based and then we exploit it to attack 8-round mCrypton. This attack requires 246 dada and 246 time complexities which is faster than exhaustive search. This is the first known cryptanalytic result on mCrypton. Copyright © 2009 John Wiley & Sons, Ltd.",
    "actual_venue": "Int J Communication Systems"
  },
  {
    "abstract": "Search engines process queries conjunctively to restrict the size of the answer set. Further, it is not rare to observe a mismatch between the vocabulary used in the text of Web pages and the terms used to compose the Web queries. The combination of these two features might lead to irrelevant query results, particularly in the case of more specific queries composed of three or more terms. To deal with this problem we propose a new technique for automatically structuring Web queries as a set of smaller subqueries. To select representative subqueries we use information on their distributions in the document collection. This can be adequately modeled using the concept of maximal termsets derived from the formalism of association rules theory. Experimentation shows that our technique leads to improved results. For the TREC-8 test collection, for instance, our technique led to gains in average precision of roughly 28% with regard to a BM25 ranking formula.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "This paper is concerned with modeling the dependence structure of two (or more) time series in the presence of a (possibly multivariate) covariate which may include past values of the time series. We assume that the covariate influences only the conditional mean and the conditional variance of each of the time series but the distribution of the standardized innovations is not influenced by the covariate and is stable in time. The joint distribution of the time series is then determined by the conditional means, the conditional variances and the marginal distributions of the innovations, which we estimate nonparametrically, and the copula of the innovations, which represents the dependency structure. We consider a nonparametric and a semiparametric estimator based on the estimated residuals. We show that under suitable assumptions, these copula estimators are asymptotically equivalent to estimators that would be based on the unobserved innovations. The theoretical results are illustrated by simulations and a real data example.",
    "actual_venue": "Journal Of Multivariate Analysis"
  },
  {
    "abstract": "With the increasing use of audio sensors in user generated content (UGC) collections, semantic concept annotation from video soundtracks has become an important research problem. In this paper, we investigate reducing the semantic gap of the traditional data-driven bag-of-audio-words based audio annotation approach by utilizing the large-amount of wild audio data and their rich user tags, from which we propose a new feature representation based on semantic class model distance. We conduct experiments on the data collection from HUAWEI Accurate and Fast Mobile Video Annotation Grand Challenge 2014. We also fuse the audio-only annotation system with a visual-only system. The experimental results show that our audio-only concept annotation system can detect semantic concepts significantly better than does random guessing. The new feature representation achieves comparable annotation performance with the bag-of-audio-words feature. In addition, it can provide more semantic interpretation in the output. The experimental results also prove that the audio-only system can provide significant complementary information to the visual-only concept annotation system for performance boost and for better interpretation of semantic concepts both visually and acoustically.",
    "actual_venue": "Icmr"
  },
  {
    "abstract": "We present some arguments why existing methods for representing agents fall short in applications crucial to artificial life. Using a thought experiment involving a fictitious dynamical systems model of the biosphere we argue that the metabolism, motility, and the concept of counterfactual variation should be compatible with any agent representation in dynamical systems. We then propose an information-theoretic notion of integrated spatiotemporal patterns which we believe can serve as the basic building block of an agent definition. We argue that these patterns are capable of solving the problems mentioned before. We also test this in some preliminary experiments.",
    "actual_venue": "Alife : The Fourteenth International Conference On The Synthesis And Simulation Of Living Systems"
  },
  {
    "abstract": "Process variability and dynamic domains increase the uncertainty of embedded systems and force designers to apply pessimistic designs, which become unnecessarily conservative and have a tremendous impact on both performance and energy consumption. In this context, developing uncertainty-aware design methodologies that take both variation at platform and at application level into account becomes a must. These methodologies should mitigate the effects derived from uncertainty, avoiding worst-case assumptions. In this article we propose a comprehensive methodology to tackle two forms of uncertainty: (1) process variation on the memory system, (2) application dynamism. A statistical model has been developed to deal with variability derived from fabrication process, whereas system scenarios are selected to cope with dynamic domains. Both sources of uncertainty are firstly tackled in combination at design time, to be refined later, at setup. As a result, at run time the platform can be successfully adapted to the current application behaviour as well as the current variations. Our simulations show that this methodology provides significant energy savings while still meeting strict timing constraints.",
    "actual_venue": "Acm Trans Embedded Comput Syst"
  },
  {
    "abstract": "In this paper we formalize a novel multirate folding transformation which is a tool used to systematically synthesize control circuits for pipelined VLSI architectures which implement multirate algorithms. Although multirate algorithms contain decimators and expanders which change the effective sample rate of a discrete-time signal, multirate folding time-multiplexes the multirate algorithm to hardware in such a manner that the resulting synchronous architecture requires only a single-clock signal. Multirate folding equations are derived and these equations are used to address two related issues. The first issue is memory requirements in folded architectures. We derive expressions for the minimum number of registers required by a folded architecture which implements a multirate algorithm. The second issue is retiming. Based on the noble identities of multirate signal processing, we derive retiming for folding constraints which indicate how a multirate data-flow graph must be retimed for a given schedule to be feasible. The techniques introduced in this paper can be used to synthesize architectures for a wide variety of digital signal processing applications which are based on multirate algorithms, such as signal analysis and coding based on subband decompositions and wavelet transforms.",
    "actual_venue": "Ieee Trans Vlsi Syst"
  },
  {
    "abstract": "THIS ARTICLE EXAMINES DISCOURSES IN THE academic and information science literature that attempt to justify and promote, to criticize and resist, or to explain and interpret transformational social change. These discussions represent one face of a much larger wave of popular and technical discourse that has arisen in response to pressures put on currently dominant institutions by the processes of post-industrialization. The nature of these institutions and the pressures they face is explicated in terms of Western civilization's modernization project, whose internal cultural contradictions and conflicting foundational metaphors have generated a variety of unanticipated social consequences. The resulting cultural disjunctions provide an invitation to rhetoric. Modern organizations, with their complex division of labor designed to accomplish unified corporate purposes, have become primary sites for the application of managerial ideologies aimed at creating identity among divisions. Modern academic libraries, as organizations devoted to the preservation and production of cultural knowledge through the efficient collection and processing of information, stand directly astride the cultural fissures that generate transformational discourse. This article surveys the resulting corpus of library and information science (LIS) literature about organizational change in academic libraries and uses multiple methods to build a syncretic interpretation that may be able to overcome some of the traditional problems of quantitative research. To accomplish this, multiple interpretative frameworks were applied by means of an especially flexible and powerful qualitative analysis software program to identify overlapping discourse features and to begin generating theories that can be used to explain these features. The unique contribution of this research derives fr-om its attempt to identify basic formal linguistic patterns in a representative corpus of discourse that can be linked to larger discourse systems and whose organization, in turn, can be interpreted in terms of broader social theories. Patterns discovered so far suggest that current LIS rhetorical strategies continue to operate within a modern grammar of organizational motives that reproduces existing forms of organizational life rather than radically transforming them.",
    "actual_venue": "Library Trends"
  },
  {
    "abstract": "Directional antennas are largely employed in wireless networks due to their capability of increasing spatial reuse of the wireless channel with respect to the classical omnidirectional systems. In this context. the use of Smart Antenna Systems (SAS) are providing a strong increasing impact in digital wireless communication systems. Because of their many benefits, this kind of systems are extremely used especially in pervasive network environments such as Mobile Ad hoc Networks (MANET). However, almost of the whole of existing network simulators use omnidirectional antennas on nodes, and in many cases, do not provide a support for asymmetrical and directional communications. In view of this, we intend to enhance Omnet++ network simulator features by designing a new switched beam smart antenna model for providing a support for directional communications. Our proposed model simulates the basic operation of a Phased Array System also exploiting a very simple algorithm for modeling power management in the channel in order to provide an asymmetrical communication behavior.",
    "actual_venue": "International Symposium On Performance Evaluation Of Computer And Telecommunication Systems"
  },
  {
    "abstract": "IEEE 802.16j mobile multi-hop relay (MMR) is an attractive solution for coverage extension and throughput enhancement on IEEE 802.16e networks. IEEE 802.16j MMR requires a method for finding the relay station (RS) route that minimises latency and maximises throughput. In this paper, we propose such a path selection method called optimal path relay association (OPRA). In this method, each RS in the MMR network is capable of finding its optimal path with path metrics which are available link bandwidth, signal-to-noise ratio (SNR), and hop count. Simulation results show that OPRA achieves performance gains over other path selection methods. Copyright (C) 2010 John Wiley & Sons, Ltd.",
    "actual_venue": "European Transactions On Telecommunications"
  },
  {
    "abstract": "We propose an expert finding method based on assumption of sequential dependence between a candidate expert and the query terms in the scope of a document. We assume that the strength of relation of a candidate to the document's content depends on its position in this document with respect to the positions of the query terms. The experiments on the official Enterprise TREC data demonstrate the advantage of our method over the method based on independence of query terms and persons in a document.",
    "actual_venue": "Sigir"
  },
  {
    "abstract": "To construct a conceptual model of a device, the user must conceptualize the device's representation of the task domain. This knowledge can be represented by three components: a device-based problem space, which specifies the ontology of the device in terms of the objects that can be manipulated and their interrelations, plus the operators that perform the manipulations; a goal space, which represents the objects in terms of which user's goals are expressed; and a semantic mapping, which determines how goal space objects are represented in the device space. The yoked state space (YSS) model allows an important distinction concerning the mental representation of procedures. If a step in a procedure specifies a transformation of the user's device space, then it has an autonomous meaning for the user, independent of its role in the sequence or method. The device space provides a figurative account of the operator. However, some operators do not affect the minimal device space, and their only meaning for the user derives from their role in a method: The method affords an operational account of the operator. Figurative accounts can be constructed from operational accounts only by elaborating the device space with new concepts. The YSS is illustrated through a simple description of a device model for a cut-and-paste text editor. Three experiments addressed the claims of this model. The first experiment used a sorting paradigm to show that users do acquire the novel device space concept of a string of adjacent characters (including space and return). The second and third experiments asked novices to make inferences about text editor behavior on the basis of simple demonstrations. They showed that (a) the availability of the string concept is critically dependent on the details of interface design, (b) figurative accounts of the copy operation afford more efficient methods and may be promoted by appropriate names for procedure steps, and (c) a conceptual model may transfer from one device to another. Together, the three experiments supported the YSS hypothsis.",
    "actual_venue": "Human-Computer Interaction"
  },
  {
    "abstract": "This paper presents a dependency injection based, unit testing methodology for unit testing components, or actors, involved in discrete event based computer network simulation via an xUnit testing framework. The fundamental purpose of discrete event based computer network simulation is verification of networking protocols used in physical--not simulated--networks. Thus, use of rigorous unit testing and test driven development methodologies mitigates risk of modeling the wrong system. We validate the methodology through the design and implementation of OPNET-Unit, an xUnit style unit testing application for an actor oriented discrete event based network simulation environment, OPNET Modeler.",
    "actual_venue": "Winter Simulation Conference"
  },
  {
    "abstract": "We demonstrate the utility of a new methodological tool, neural-network word embedding models, for large-scale text analysis, revealing how these models produce richer insights into cultural associations and categories than possible with prior methods. Word embeddings represent semantic relations between words as geometric relationships between vectors in a  space, operationalizing a relational model of meaning consistent with contemporary theories of identity and culture. We show that dimensions induced by word differences (e.g. man - woman, rich - poor, black - white, liberal - conservative) in these vector spaces closely correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared cultural connotations when compared to surveyed responses and labeled historical data. We pilot a method for testing the stability of these associations, then demonstrate applications of word embeddings for macro-cultural investigation with a longitudinal analysis of the coevolution of gender and class associations in the United States over the 20th century and a comparative analysis of historic distinctions between markers of gender and class in the U.S. and Britain. We argue that the success of these  models motivates a move towards high-dimensional theorizing of meanings, identities and cultural processes.",
    "actual_venue": "Arxiv: Computation And Language"
  },
  {
    "abstract": "Route caching strategy is important in on-demand routing protocols in wireless ad hoc networks. While high routing overhead usually has a significant performance impact in low bandwidth wireless networks, a good route caching strategy can reduce routing overheads by making use of the available route information more efficiently. In this paper, we first study the effects of two cache schemes, \"link cache\" and \"path cache\", on the performance of on-demand routing protocols through simulations based on the Dynamic Source Routing (DSR) protocol. Since the \"path cache\" DSR has been extensively studied, we focus in this paper on the \"link cache\" DSR in combination with timer-based stale link expiry mechanisms. The effects of different link lifetime values on the performance of routing protocol in terms of routing overhead, packet delivery ratio and packet latency are investigated. A caching strategy incorporating adaptive link timeout is then proposed, which aims at tracking the \"optimal\" link lifetime under various node mobility levels by adaptively adjusting the link lifetime based on the real link lifetime statistics. The performance of the proposed strategy is then compared with the conventional \"path cache\" DSR. The results show that without a timeout mechanism, a link cache scheme may suffer severe performance degradation due to the use of broken routes, while the proposed adaptive \"link cache\" strategy achieves significantly improved performance by reducing the routing overhead when the network traffic load is high.",
    "actual_venue": "Wireless Networks"
  },
  {
    "abstract": "•A new methodology to create the user’s health profile has been defined through dynamic adaptive questionnaires.•Adaptive questionnaires have been prepared and validated by medical doctors.•The resulting health profile extends the typical user profile of adaptive web system, including health information.•A database of typical Calabrian foods annotated by nutritional facts and indication with respect to main diseases.•A recommendation methodology suggests foods to the user according to his/her health conditions and eventual chronic diseases.",
    "actual_venue": "Computer Methods And Programs In Biomedicine"
  },
  {
    "abstract": "The cutting plane proof system for proving the unsatisfiability of propositional formulas in conjunctive normalform is based on a natural representation of formulas as systems of integer inequalities. We show: Frege proof systems p-simulate the cutting plane proof system. This strengthens a result in [5], that extended Frege proof systems (which are believed to be stronger than Frege proof systems) p-simulate the cutting plane proof system. Our proof is based on the techniques introduced in [2].",
    "actual_venue": "CSL"
  },
  {
    "abstract": "Fuzzy ontologies are deemed as useful formalisms for dealing with vagueness in the Semantic Web community. Description logics (DLs) are the logical foundations of standard web ontology languages. Conjunctive queries are deemed as an expressive reasoning service for DLs. DL reasoners can be enriched by a conjunctive query service. In this study, we focus on fuzzy (threshold) conjunctive queries over knowledge bases encoding in fuzzy DL ALC(G), the well known fuzzy DL with customized fuzzy data type support. We provide a tableau-based algorithm for deciding query entailment of ALC(G). Our algorithm is applicable to more expressive DLs and arbitrary conforming fuzzy data type group.",
    "actual_venue": "Web Intelligence"
  },
  {
    "abstract": "Cloud computing is concept of computing technology in which user uses remote server for maintain their data and application. Resources in cloud computing are demand driven utilized in forms of virtual machines to facilitate the execution of complicated tasks. Virtual machine placement is the process of mapping virtual machines to physical machines. This is an active research topic and different strategies have been adopted in literature for this problem. In this paper, the problem of virtual machine placement is formulated as a multiobjective optimization problem aiming to simultaneously optimize total processing resource wastage and total memory resource wastage. After that ant colony optimization algorithm is proposed for solving the formulated problem. The main goal of the proposed algorithm is to search the solution space more efficiently and obtain a set of non-dominated solutions called the Pareto set. The proposed algorithm has been compared with the well-known algorithms for virtual machine placement problem existing in the literature. The comparison results elucidate that the proposed algorithm is more efficient and significantly outperforms the compared methods on the basis of CPU resource wastage and memory resource wastage.",
    "actual_venue": "Advanced Machine Learning Technologies And Applications"
  },
  {
    "abstract": "This paper is dedicated to the problem of using case-based reasoning AI in a commercial mobile game of lawn tennis. We discuss the unavoidable manual game analysis stage, aimed to represent user intentions accurately and supply them to the machine learning procedure. We show how the right combination of machine learning and manual effort helps to construct a solid game AI system, able to play in human-like manner. Our experience shows that the key factor of the successful decision making and reasonable resource consumption in mobile tennis is careful representation of context-aware behavior and anticipation of opponents' actions, exhibited by real players.",
    "actual_venue": "Ieee International Conference On Systems, Man And Cybernetics"
  },
  {
    "abstract": "This paper presents an efficient local features boosting strategy for interactive objects retrieval tasks such as on-line supervised learning or relevance feedback. The prediction time complexity of most existing methods is indeed usually linear in dataset size since the retrieval works by applying a trained classifier on the images of the dataset one by one. In our method, the trained classifier can be computed directly on the whole dataset in sublinear time thanks to distance-based weak classifiers. The idea is to speed-up drastically the prediction of each weak classifier on the whole dataset by performing approximate range queries with an efficient similarity search structure. Experiments on Caltech 256 dataset show that the technique is up to 250 times faster than the naive exhaustive method. Thanks to this efficiency improvement, we developed a relevance feedback mechanism on image regions freely selected by the user and we show how it improves the effectiveness of the retrieval.",
    "actual_venue": "Acm Multimedia"
  },
  {
    "abstract": "This letter presents a case study addressing the comparison between different synthetic aperture radar (SAR) partial polarimetric options for tropical-vegetation cartography. These options include compact polarization (CP), dual polarization (DP), and alternating polarization (AP). They are all derived from fully polarimetric (FP) SAR data acquired by the airborne SAR (AIRSAR) sensor over the French Polynesian Tubuai Island. The classification approach is based on the support vector machine algorithm and is further validated by several ground surveys. For a single frequency band, FP data give significantly better results than any other partial polarimetric configuration. Among the partial polarimetric architectures, the CP mode performs best. In addition, the DP mode shows better performance than the AP mode, highlighting the value of the polarimetric differential phase. The combination of different frequency bands (P-, L-, and C-bands) holds the most significant improvement: The multifrequency diversity adds generally more information than the multipolarization diversity. A noticeable result is the major contribution of the C-band at VV polarization (the only polarization available at C-band with the AIRSAR data set used in this letter) to the classification performance, due to its ability to discriminate between Pinus and Falcata.",
    "actual_venue": "Geoscience And Remote Sensing Letters, Ieee"
  },
  {
    "abstract": "The disruption of operations due to IS failure becomes more important as IS has become an increasingly essential component of the organization's operations and can affect its strategic objectives. Nevertheless, traditional IS risk analysis methods do not adequately reflect the loss from disruption of operations in determining the value of IS assets. Quantitative methods do not measure the loss from disruption of operations. Qualitative methods consider the loss, but their results are subjective and not suitable for cost-benefit decision support. There is a lack of systematic methods to measure the value of IS assets from the viewpoint of operational continuity.This study presents an IS risk analysis method based on a business model. The method uses a systematic quantitative approach dealing with operational continuity: the importance of various business functions and the necessity level of various assets are first determined. The value of each asset is then determined based on these two levels.The proposed method adds the first stage, organizational investigation, to traditional risk analysis. The process of the method utilizes various methodologies such as paired comparison, asset-function assignment tables, and asset dependency diagrams.",
    "actual_venue": "Information And Management"
  },
  {
    "abstract": "This paper describes an intelligent home healthcare system characterized by a wireless sensor network (WSN) and a reasoning component. The aim of the system is to allow constant and unobtrusive monitoring of a patient in order to enhance autonomy and increase quality of life. Data collected by the sensor network are used to support a reasoning component, which is based on answer set programming (ASP), in performing three main reasoning tasks: (i) continuous contextualization of the physical, mental and social state of a patient, (ii) prediction of possibly risky situations and (iii) identification of plausible causes for the worsening of a patient's health. Starting from different data sources (sensor data, test results, inference results) the reasoning component applies expressive logic rules aimed at correct interpretation of incomplete or inconsistent contextual information, and evaluates correlation rules expressed by clinicians. The expressive power of ASP allows efficient enough reasoning to support prevention, while declarativity simplifies rule-specification and allows automatic encoding of knowledge. Preliminary evaluations show that the combination of an ASP-based reasoning component and a WSN is a good solution for creating a home-based healthcare system.",
    "actual_venue": "Comput J"
  },
  {
    "abstract": "Face sketch recognition has great practical value in the criminal detection, security and other fields. Especially, it can help the police narrow down potential suspects in criminal detection effectively. Face sketch represents the original photos in a simple and recognizable form, so sketch and photo are images of two different modes. In order to identify the corresponding sketch face image in a lot of photo face images, this paper presents an improved sketch-photo transformation algorithm, and it uses the effective characteristics of the photo image more reasonably during transforming a photo image into sketch. In this way, it can reduce the difference between the sketch and photo image to improve the matching effect, and save the recognition time. Many experiments on CUHK Face Sketch database including 188 sketchphotos prove the effectiveness of the method in this paper.",
    "actual_venue": "International Journal Of Pattern Recognition And Artificial Intelligence"
  },
  {
    "abstract": "This paper presents several industrial applications of ML in the context of their effort to solve the \"KAML problem\", i.e., the problem of merging knowledge acquisition and machine learning techniques. Case-based reasoning is a possible alternative to the problem of acquiring highly compiled expert knowledge, but it raises also many new problems that must be solved before really efficient implementations are available.",
    "actual_venue": "Ecml"
  },
  {
    "abstract": "In studies of information users' cognitive behaviors, it is widely recognized that users' perceptions of their information problem situations play a major role, Time-line interviewing and inductive content analysis are two research methods that, used together, have proven extremely useful for exploring and describing users' perceptions in various situational contexts. This article describes advantages and disadvantages of the methods using examples from a study of users' criteria for evaluation in a multimedia context.",
    "actual_venue": "Jasis"
  },
  {
    "abstract": "An improved penalty immersed boundary (pIB) method has been proposed for simulation of fluid-flexible body interaction problems. In the proposed method, the fluid motion is defined on the Eulerian domain, while the solid motion is described by the Lagrangian variables. To account for the interaction, the flexible body is assumed to be composed of two parts: massive material points and massless material points, which are assumed to be linked closely by a stiff spring with damping. The massive material points are subjected to the elastic force of solid deformation but do not interact with the fluid directly, while the massless material points interact with the fluid by moving with the local fluid velocity. The flow solver and the solid solver are coupled in this framework and are developed separately by different methods. The fractional step method is adopted to solve the incompressible fluid motion on a staggered Cartesian grid, while the finite element method is developed to simulate the solid motion using an unstructured triangular mesh. The interaction force is just the restoring force of the stiff spring with damping, and is spread from the Lagrangian coordinates to the Eulerian grids by a smoothed approximation of the Dirac delta function. In the numerical simulations, we first validate the solid solver by using a vibrating circular ring in vacuum, and a second-order spatial accuracy is observed. Then both two- and three-dimensional simulations of fluid-flexible body interaction are carried out, including a circular disk in a linear shear flow, an elastic circular disk moving through a constricted channel, a spherical capsule in a linear shear flow, and a windsock in a uniform flow. The spatial accuracy is shown to be between first-order and second-order for both the fluid velocities and the solid positions. Comparisons between the numerical results and the theoretical solutions are also presented.",
    "actual_venue": "J Comput Physics"
  },
  {
    "abstract": "This paper presents a systematic approach to proving temporal properties of arbitrary Z specifications. The approach involves (i) transforming the Z specification to an abstract temporal structure (or state transition system), (ii) applying a model checker to the temporal structure, (iii) determining whether the temporal structure is too based on the model checking result and (iv) refining the temporal structure where necessary. The approach is based on existing work from the model checking literature, adapting it to Z.",
    "actual_venue": "ZB"
  },
  {
    "abstract": "Satellite-derived land surface phenology (LSP) serves as a valuable input source for many environmental applications such as land cover classifications and global change studies. Commonly, LSP is derived from coarse-resolution (CR) sensors due to their well-suited temporal resolution. However, LSP is increasingly demanded at medium resolution (MR), but inferring LSP directly from MR imagery remain...",
    "actual_venue": "Ieee Transactions On Geoscience And Remote Sensing"
  },
  {
    "abstract": "This paper is motivated by the fact that mixed integer nonlinear programming is an important and difficult area for which there is a need for developing new methods and software for solving large-scale problems. Moreover, both fundamental building blocks, namely mixed integer linear programming and nonlinear programming, have seen considerable and steady progress in recent years. Wishing to exploit expertise in these areas as well as on previous work in mixed integer nonlinear programming, this work represents the first step in an ongoing and ambitious project within an open-source environment. COIN-OR is our chosen environment for the development of the optimization software. A class of hybrid algorithms, of which branch-and-bound and polyhedral outer approximation are the two extreme cases, are proposed and implemented. Computational results that demonstrate the effectiveness of this framework are reported. Both the library of mixed integer nonlinear problems that exhibit convex continuous relaxations, on which the experiments are carried out, and a version of the software used are publicly available.",
    "actual_venue": "Discrete Optimization"
  },
  {
    "abstract": "Three new image-based visual-impedance control laws are proposed in this letter allowing physical interaction of a dual-arm unmanned aerial manipulator equipped with a camera and a force/torque sensor. Namely, two first-order impedance behaviors are designed based on the transpose and the inverse of the system Jacobian matrix, respectively, while a second-order impedance behavior is carried out as...",
    "actual_venue": "Ieee Robotics And Automation Letters"
  },
  {
    "abstract": "In this paper, we present a student model for rule based e-tutoring systems. This model describes both properties of rewrite rules (difficulty and discriminativity) and of students (start competence and learning speed). The model is an extension of the two-parameter logistic ogive function of Item Response Theory. We show that the model can be applied even to relatively small datasets. We gather data from students working on problems in the logic domain, and show that the model estimates of rule difficulty correspond well to expert opinions. We also show that the estimated start competence corresponds well to our expectations based on the previous experience of the students in the logic domain. We point out that this model can be used to inform students about their competence and learning, and teachers about the students and the difficulty and discriminativity of the rules.",
    "actual_venue": "LAK"
  },
  {
    "abstract": "This work presents two fast and iterative methods that integrate segmentation by watershed and three-dimensional visualization, while the classical approach is to separate these two processes. The user-aided segmentation is based on iterative watershed, efficiently implemented using the image foresting transform (IFT). The first proposed algorithm consists of extracting segmented structures' borders during the segmentation and updating them at each iteration. Structure visualization is achieved by border projection. The second algorithm updates the image directly from the changes of the segmented scene occurred in each step. The two methods were implemented in C and tested with various magnetic resonance images. The results are satisfactory as compared to the classical approaches. The incremental image algorithm is a little faster than the classical one while the incremental border algorithm, although a little slower in segmenting, allows a very fast rendering, very suitable for object manipulation. Thus, the proposed methods provide a good feedback to user.",
    "actual_venue": "Sibgrapi"
  },
  {
    "abstract": "In this paper, we consider the problem of biomedical sensor node localization and propose a new particle filter (PF) which adjusts variance and gradient data for Kullback-Leibler distance (KLD)-resampling algorithm in wireless biomedical sensor networks (WBSN). This method eliminates the bad effect of the received signal strength (RSS) variety and also reduces the number of particles used by generating sample set near the high likelihood region. A number of simulations are conducted to evaluate sample size and the effect of different parameters such as root mean square error (RMSE) or estimation error, operation time, etc. Simulation results show that this technique enhances the localization accuracy of biomedical sensor node and decreases the number of particles used compared with the traditional methods.",
    "actual_venue": "Imcom"
  },
  {
    "abstract": "Single pass fuzzy c-means and Online fuzzy c-means are two scalable versions of the widely used fuzzy c-means clustering algorithm. They both facilitate scaling to very large numbers of examples while providing partitions that very closely approximate those one would obtain using fuzzy c-means. They have been successfully applied to a number of data sets, most notably magnetic resonance image volumes of the human brain. In practice, the algorithms have converged on the data sets to which they been applied. Computers are of finite precision, which will allow real values to be converted to integers with minor loss of information. In this paper, we show that they will converge to local minima or saddle points of the modified objective function for any data set when weights are integers.",
    "actual_venue": "Fuzzy Systems"
  },
  {
    "abstract": "How to improve search accuracy for difficult topics is an under-addressed, yet important research question. In this paper, we consider a scenario when the search results are so poor that none of the top-ranked documents is relevant to a user's query, and propose to exploit negative feedback to improve retrieval accuracy for such difficult queries. Specifically, we propose to learn from a certain number of top-ranked non-relevant documents to rerank the rest unseen documents. We propose several approaches to penalizing the documents that are similar to the known non-relevant documents in the language modeling framework. To evaluate the proposed methods, we adapt standard TREC collections to construct a test collection containing only difficult queries. Experiment results show that the proposed approaches are effective for improving retrieval accuracy of difficult queries.",
    "actual_venue": "Cikm"
  },
  {
    "abstract": "Skinning algorithms that work across a broad range of character designs and poses are crucial to creating compelling animations. Currently, linear blend skinning (LBS) and dual quaternion skinning (DQS) are the most widely used, especially for real-time applications. Both techniques are efficient to compute and are effective for many purposes. However, they also have many well-known artifacts, such as collapsing elbows, candy wrapper twists, and bulging around the joints. Due to the popularity of LBS and DQS, it would be of great benefit to reduce these artifacts without changing the animation pipeline or increasing the computational cost significantly. In this paper, we introduce a new direct skinning method that addresses this problem. Our key idea is to pre-compute the optimized center of rotation for each vertex from the rest pose and skinning weights. At runtime, these centers of rotation are used to interpolate the rigid transformation for each vertex. Compared to other direct skinning methods, our method significantly reduces the artifacts of LBS and DQS while maintaining real-time performance and backwards compatibility with the animation pipeline.",
    "actual_venue": "Acm Trans Graph"
  },
  {
    "abstract": "The accuracy of 3D measurements of objects is highly affected by the errors originated from camera calibration. Therefore, camera calibration has been one of the most challenging research fields in the computer vision and photogrammetry recently. In this paper, an Artificial Neural Network Based Camera Calibration Method, NBM, is proposed. The NBM is especially useful for back-projection in the applications that do not require internal and external camera calibration parameters in addition to the expert knowledge. The NBM offers solutions to various camera calibration problems such as calibrating cameras with automated active lenses that are often encountered in computer vision applications. The difference of the NBM from the other artificial neural network based back-projection algorithms used in intelligent photogrammetry (photogrammetron) is its ability to support the multiple view geometry. In this paper, a comparison of the proposed method has been made with the Bundle Block Adjustment based back-projection algorithm, BBA. The performance of accuracy and validity of the NBM have been tested and verified over real images by extensive simulations.",
    "actual_venue": "International Conference On Neural Information Processing"
  },
  {
    "abstract": "A framework for representing and reasoning about uncertain temporal information is presented. This framework relies on a non-recursive representation of probabilities as functions of time. The representation is compatible with survival analysis techniques, and allows the use of regressive survival models to represent common interaction patterns. In another departure from previous temporal probabilistic knowledge representations, surprise measures are used to detect and correct poor probability estimates. The present framework relies mainly on a theory of causality, while still allowing the representation of phenomena without requiring a complete understanding of causes.",
    "actual_venue": "Journal Of Experimental And Theoretical Artificial Intelligence"
  },
  {
    "abstract": "Sequences with low autocorrelation levels are highly desirable in digital communication. Conventional sidelobe reduction techniques have suffered from performance degradations in range resolutions and signal-to-noise ratio (SNR) gains. In this paper we propose a way to synthesize amplitude weight correlators for polyphase codes. These weighted codes generate sidelobes that are lower than those found in Barker codes. The sidelobe level is kept uniformly flat over all time delays while the range resolution loss is prevented. Unlike the Barker codes, the proposed technique is not limited by the code length. The technique is easy to implement and incurs a minimal SNR loss.",
    "actual_venue": "Ieee Communications Letters"
  },
  {
    "abstract": "This paper includes security constraints in OPF calculations for combined HVAC and HVDC grids. Two formulations of the problem are considered: a preventive and a corrective formulation. The outages of lines, generators and terminal stations are included. The preventive control method assumes that no control actions are taken after a contingency happens, meaning that all actions must be performed in advance. Linear sensitivity factors are used to calculate the influence of the contingency. Under the corrective control method, control actions can be performed both before and after a contingency, using fast controllable HVDC terminals. The two methods are applied to the IEEE14 and the RTS96 test cases, both expanded with an HVDC grid. The corrective control is computationally more intensive, but, in general, gives more economical results.",
    "actual_venue": "Pscc"
  },
  {
    "abstract": "This paper introduces a computational framework which allows to assess a-priori the control performance of a nonlinear networked control system (NCS) for which actuator signal communication is time triggered. The plant system is nonlinear and continuous-time. A discrete nonlinear controller has been designed without the knowledge of the communication network. The integration of the control system under the assumption of time-triggered actuator signal communication requires the choice of a feasible communication policy. It is shown that a continuous-time quadratic infinite horizon cost can be evaluated for the NCS via a sum-of-squares (SOS) approach. The equivalent linear cost computation for the linearized closed-loop is used to minimize computational effort by reducing the number of variables for the cost computation and avoiding SOS-computations where possible. A numerical example proves the feasibility of the approach.",
    "actual_venue": "Ieee International Symposium On Industrial Electronics"
  },
  {
    "abstract": "In today's data centers, many application services share the same physical/virtual devices and affect each other. Therefore, application service managers need to spend a lot of time monitoring the application services by investigating a wide range of historical data about the shared devices. In this paper, integrated monitoring software for a wide range of historical data, which shortens the transition time (i.e., the time to switch from one historical data to another) by collecting and processing the historical data, is proposed and evaluated. Five basic historical data formats for automatically creating both well-organized historical data and relation data for the historical data are also proposed. The formats help application service managers by eliminating the need for the additional software development, which was otherwise required for each application service. Surveys on application service managers in a SaaS provider for over 50 000 companies show that about 98.6% of monitoring tasks can be covered by the five basic data formats and that the integrated monitoring software reduces the transition times for switching between historical data by about 54.7% compared to those with conventional monitoring software. These results suggest that the proposed integrated monitoring software is effective not only for reducing the time required for monitoring application services, but also for enhancing the overall service availability of SaaS providers' systems.",
    "actual_venue": "Ieee Transactions On Network And Service Management"
  },
  {
    "abstract": "In this paper we consider the problem of distributed fault diagnosis in Wireless Sensor Networks (WSNs). The proposed Fault Diagnosis Algorithm (FDA) aims to handle both permanent and intermittent faults. The sensor nodes with permanent communication faults can be diagnosed by using the conventional time-out mechanism. In contrast, it is difficult to detect intermittent faults due to their inherent unpredictable behavior. The FDA is based on the comparison of sensor measurements and residual energy values of neighboring sensor nodes, exploiting their spatial correlations. To handle intermittent faults, the comparisons are made for $$r$$r rounds. Two special cases of intermittent faults are considered: one, when an intermittently faulty node sends similar sensor measurement and similar residual energy value to some of its neighbors in all $$r$$r rounds; another, when it sends these values, either or both of which deviates significantly from that of some neighbors in all $$r$$r rounds. Through extensive simulation and analysis, the proposed scheme is proved to be correct, complete, and efficient to handle intermittent faults and hence, well suited for WSNs.",
    "actual_venue": "Wireless Personal Communications"
  },
  {
    "abstract": "Spectral Fine Granular Scalability (SFGS), a variation of MPEG-4 FGS, is proposed in this paper as a scalable coding technique for video streaming. SFGS re-arranges enhancement-layer bit-plane data according to spectral frequency orderings before they are further processed with traditional FGS bit-plane coding technique. Based on this data reordering within each bit-plane, spectral bands of lower frequency or higher rate-distortion properties are transmitted with priorities. In spite of this modification, SFGS retains similar properties as FGS, such as the coding efficiency, error resilience, and adaptation to channel bandwidth variation. However, SFGS is promising in yielding evener image quality and smoother video perception at the receiver side when the channel bandwidth is limited. Traditional FGS rate control techniques lack a systematic approach to making tradeoffs between image quality, motion smoothness (i.e., the frame rate), and video smoothness (PSNR difference between consecutive frames), according to limitations on system resources and users’ preferences. In view of this drawback, we propose here a unified rate allocation scheme, based on rate-distortion-smoothness optimization criterion and multi-stage dynamic programming (DP) technique, to solve the above problems. Experiments show that our algorithm is capable of achieving a smoother video quality, and simultaneously guaranteeing no buffer overflow and underflow at encoder/decoder, under a target bit-rate constraint. Though our proposed algorithm was designed for SFGS, it can be also applied to other FGS variations.",
    "actual_venue": "Journal Of Visual Communication And Image Representation"
  },
  {
    "abstract": "A core comprises of a group of central and densely connected nodes which governs the overall behaviour of a network. It is recognised as one of the key meso-scale structures in complex networks. Profiling this meso-scale structure currently relies on a limited number of methods which are often complex and parameter dependent or require a null model. As a result, scalability issues are likely to arise when dealing with very large networks together with the need for subjective adjustment of parameters. The notion of a rich-club describes nodes which are essentially the hub of a network, as they play a dominating role in structural and functional properties. The definition of a rich-club naturally emphasises high degree nodes and divides a network into two subgroups. Here, we develop a method to characterise a rich-core in networks by theoretically coupling the underlying principle of a rich-club with the escape time of a random walker. The method is fast, scalable to large networks and completely parameter free. In particular, we show that the evolution of the core in World Trade and C. elegans networks correspond to responses to historical events and key stages in their physical development, respectively.",
    "actual_venue": "Plos One"
  },
  {
    "abstract": "A new family of speed-sensorless sliding-mode ob- servers for induction motor drives has been developed. Three topologies are investigated in order to determine their feasibil- ity, parameter sensitivity, and practical applicability. The most significant feature of all schemes is that they do not require the rotor speed adaptation, i.e., they are inherently sensorless observers. The most versatile and robust is a dual-reference-frame full-order flux observer. The other two schemes are flux observers implemented in stator frame and rotor frame, respectively. These are simpler than the first one and make use of the sliding-mode invariance over a specified range of modeling uncertainties and disturbances. Main theoretical aspects, results of parameter sensi- tivity analysis, and implementation details are given for each ob- server in order to allow the comparison. Experimental results with the dual-reference-frame observer, considered the most adequate for practical applications, are presented and discussed. Sensorless operation with a sliding-mode direct-torque-controlled drive at very low speeds is demonstrated. It is concluded that the new proposed observers represent a feasible alternative to the classical speed-adaptive flux observers. Index Terms—Sliding-mode observers (SMOs), state observers, variable-speed drives, variable-structure systems.",
    "actual_venue": "Ieee Transactions On Industrial Electronics"
  },
  {
    "abstract": "Zero frequency filter (ZFF) is a marginally stable infinite impulse response resonant filter at 0 Hz that is used to extract the epoch locations reliably from speech signals. However, the output of such an ideal resonator is an exponentially increasing/decreasing function of time. The trend is removed from the filtered output by subtracting the average over 1–2 pitch periods to obtain zero frequency filtered signal. Alternatively in this paper, a bounded input bounded output stable realization of ZFF is proposed for epoch extraction, where the output of such a filter is not an increasing/decreasing function of time. The advantages of using such a stable filter is that the filter output is bounded and has no precision related problem associated with the output for lengthy speech files, also, the method does not require remove trend procedure that needs initial pitch estimation. The proposed approach is evaluated using CMU-Arctic database for clean and degraded conditions. Furthermore, the method is also validated in cases of singing voice and emotional speech to demonstrate the robustness for varying pitch scenarios. The proposed method is found to be robust for wide range of chosen parameters.",
    "actual_venue": "Circuits Systems And Signal Processing"
  },
  {
    "abstract": "The vast number of biomedical literature is an important source of biomedical interaction information discovery. However, it is complicated to obtain interaction information from them because most of them are not easily readable by machine. In this paper, we present a method for extracting biomedical interaction information assuming that the biomedical Named Entities (NEs) are already identified. The proposed method labels all possible pairs of given biomedical NEs as INTERACTION or NO-INTERACTION by using a Maximum Entropy (ME) classifier. The features used for the classifier are obtained by applying various NLP techniques such as POS tagging, base phrase recognition, parsing and predicate-argument recognition. Especially, specific verb predicates (activate, inhibit, diminish and etc.) and their biomedical NE arguments are very useful features for identifying interactive NE pairs. Based on this, we devised a two-step method: 1) an interaction verb extraction step to find biomedically salient verbs, and 2) an argument relation identification step to generate partial predicate-argument structures between extracted interaction verbs and their NE arguments. In the experiments, we analyzed how much each applied NLP technique improves the performance. The proposed method can be completely improved by more than 2% compared to the baseline method. The use of external contextual features, which are obtained from outside of NEs, is crucial for the performance improvement. We also compare the performance of the proposed method against the co-occurrence-based and the rule-based methods. The result demonstrates that the proposed method considerably improves the performance.",
    "actual_venue": "Journal Of Information Processing Systems"
  },
  {
    "abstract": "An algorithm for the continuous iterative solution of the inverse kinematics problem for six-degree-of-freedom manipulators of arbitrary structure is presented. The algorithm is based on path following from a known solution by the predictor-corrector method using: a new second-order predictor, a first-order Newton-method corrector, and the idea of including the end effector's position (along its trajectory) as a variable in the formulation. The importance of the proposed approach is that it follows for the rapid generation of joint trajectories that include regular positions as well as singular positions. The algorithms for both the predictor and the corrector are derived from Taylor series expansion of the matrix equation of closure and require only the solution of linear systems of equations. The approach is illustrated with examples for the Puma manipulator and a general manipulator",
    "actual_venue": "Icra"
  },
  {
    "abstract": "We propose a model for coupling that considers substrate contacts between through silicon vias (TSVs) in bulk-CMOS technologies. The proposed model is compact but has reasonable accuracy for the dense substrate contacts in large-scale three dimensional integrated circuits (3D ICs). We describe the modeling for substrate contacts with the equivalent electrical circuit, discuss the impact of substrate contacts on the electrical parasitic parameters, and clarify the effect of substrate contacts on reducing crosstalk noise and increasing delay. Results of analysis show that if substrate contacts are not considered, crosstalk noise becomes the overestimate of 5 to 700 times and delay becomes the underestimate of 1.4 to 2.4 times.",
    "actual_venue": "Circuits And Systems"
  },
  {
    "abstract": "We propose a prototypical Split Inverse Problem (SIP) and a new variational problem, called the Split Variational Inequality Problem (SVIP), which is a SIP. It entails finding a solution of one inverse problem (e.g., a Variational Inequality Problem (VIP)), the image of which under a given bounded linear transformation is a solution of another inverse problem such as a VIP. We construct iterative algorithms that solve such problems, under reasonable conditions, in Hilbert space and then discuss special cases, some of which are new even in Euclidean space.",
    "actual_venue": "Numerical Algorithms"
  },
  {
    "abstract": "The general purpose of this paper is to show a practical instance of how philosophy can benefit from some ideas, methods and techniques developed in the field of Artificial Intelligence (AI). It has to do with some recent claims [4] that some of the most traditional philosophical problems have been raised and, in some sense, solved by AI researchers. The philosophical problem we will deal with here is the representation of non-deductive intra-theoretic scientific inferences. We start by showing the flaws with the most traditional solution for this problem found in philosophy: Hempel's Inductive-Statistical (I-S) model [5]. After we present a new formal model based on previous works motivated by reasoning needs in Artificial Intelligence [11] and show that since it does not suffer from the problems identified in the I-S model, it has great chances to be successful in the task of satisfactorily representing the non-deductive intra-theoretic scientific inferences.",
    "actual_venue": "Advances In Artificial Intelligence - Iberamia"
  },
  {
    "abstract": "In human vision, it has been well understood that the red, green, and blue (RGB) trichromatic system, whose colors can be well expressed by an RGB hue ring with complementary color relations, is the most accessible and extensible color representation. However, such color relations rarely play a role in color image/video processing tools such as wavelets. In this paper, the gap between wavelets and...",
    "actual_venue": "Ieee Transactions On Circuits And Systems For Video Technology"
  },
  {
    "abstract": "Deforestation in the world's tropics is an urgent international issue. One response has been the development of satellite based monitoring initiatives largely focused on the carbon rich forests of western Indonesia. In contrast this study focuses on one eastern Indonesian district, Kabupaten Kupang, which has some of the largest and least studied tracts of remaining forest in West Timor. A combination of remote sensing, GIS and social science methods were used to describe the state of forests in Kabupaten Kupang, how and why they are changing. Using satellite imagery, case studies and on-ground interviews, this study explores the proposition that transdisciplinary local social, cultural and biophysical knowledge is important for effectively using remotely sensed data as a tool to inform local management policies. When compared to some other parts of Indonesia, the rate and extent of deforestation in West Timor was found to be relatively small and a satellite based assessment alone could conclude that it is not a critical issue. However this study showed that when on-ground social data are coupled with (such) satellite-based data a more complex picture emerges, related to key livelihood issues. The causes of forest cover change were found to be multivariate and location specific, requiring management approaches tailored to local social issues. This study suggests that integrative research can maximise the utility of satellite data for understanding causation and thus informing management strategies. In addition, the satellite based assessment found that at the time of the study less than 4% of forested land was within national parks and nature reserves and less than a third of the protected catchment forest zone was forested. These data suggest considerable scope for upland re-forestation activities or the redrawing of protected forest boundaries. (C) 2011 Elsevier B.V. All rights reserved.",
    "actual_venue": "International Journal Of Applied Earth Observation And Geoinformation"
  },
  {
    "abstract": "User-generated content (UGC) is emerging as one of the dominate forms in global media industry. However, the efficient delivery of UGC faces with massive technical challenges due to its long-tail nature. Content delivery networks (CDN) based systems are considered as the potential solutions to deliver UGC. But none of the existing CDN based solutions can support all the required features in UGC delivery. This paper proposes content-delivery-as-a-service (CoDaaS), an innovative idea to enable on-demand virtual content delivery service (vCDS) overlays for UGC providers to deliver their contents to a group of designated consumers. The proposed CoDaaS solution is built on a hybrid media cloud, and offers elastic private virtual content delivery service with an agreed Quality of Service (QoS) to UGC providers. In this paper, we also implement a simulation to CoDaaS. The preliminary results validate all the required features for UGC delivery and verify its comparative performance advantages. We are working on optimizing the system performance with different algorithms (e.g., collaborative caching, context-aware streaming, etc), and ultimately characterizing the fundamental trade-off between the cost and the quality-of-service in UGC delivery.",
    "actual_venue": "Icnc"
  },
  {
    "abstract": "Clinical notes contain information that is crucial for the diagnosis process. However, it is usually not properly manually analyzed due to the tremendous efforts and time it takes. Hence, an automated approach is eagerly needed to maximize clinical knowledge management and reduce cost. In this paper, we propose a framework SESARF: a Semantic Extractor to identify hidden risk factors in clinical notes and a Sentimental Analyzer to assess the severity levels associated with the identified Risk Factors. This tool can be customized to any disease using Linked Open Data (LOD) by selecting a specific disease and collecting its risk factors list from medical ontologies. The extracted knowledge can serve two purposes: 1) a feature vector is prepared, for any classifier in machine learning, containing risk factors and their weights based on our semantic enrichment and sentimental analyzer and 2) a proper comparison of the extracted information with wearable body sensors that can alert any major changes in a patient's health status to personalize treatment.",
    "actual_venue": "Ieee Annual Computer Software And Applications Conference"
  },
  {
    "abstract": "An adaptive beamforming algorithm for the global system for mobile communications (GSM) is presented where the weights are updated using re-encoded data and a semi-blind technique. The receiver consists of several stages operating on the synchronization channel (SCH) in an iterative manner to improve the reliability of the cochannel signal decisions. An extension of the constant modulus algorithm (CMA) with a least-squares (LS) formulation is used to implement the semi-blind technique. Iterative semi-blind processing is introduced to further refine the beamformer weights. The performance of the enhanced GSM receiver is evaluated using real and simulated data.",
    "actual_venue": "Ciss"
  },
  {
    "abstract": "Nonlinear normalization (NLN) based on line density equalization has been widely used in handwritten Chinese character recognition (HCCR). Our previous results showed that global transformation methods, including moment normalization and a newly proposed bi-moment method, generate smooth normalized shapes at lower computation effort while yielding comparable recognition accuracies. This paper proposes a new global transformation method, named modified centroid-boundary alignment (MCBA) method, for HCCR. The previous CBA method can efficiently correct the skewness of centroid by quadratic curve fitting but fails to adjust the inner density. The MCBA method adds a simple trigonometric (sine) function onto quadratic function to adjust the inner density. The amplitude of the sine wave is estimated from the centroids of half images. Experiments on the ETL9B and JEITA-HP databases show that the MCBA method yields comparably high accuracies to the NLN and bi-moment methods and shows complementariness.",
    "actual_venue": "Iwfhr"
  },
  {
    "abstract": "In Social Internet of Things (SIoT) environments, a large number of users and Internet of Things (IoT) based devices are connected to each other, so that they can share SIoT-based services. IoT-based devices establish social relations with each other according to the social relations of their owners in Online Social Networks (OSNs). In such an environment, a big challenge is how to provide trustworthy service evaluation. Currently, the prevalent trust management mechanisms consider QoS-based trust and social-relation based trust mechanisms in evaluating the trustworthiness of service providers. However, the existing trust management mechanisms in SIoT environments do not consider the different contexts of trust. Therefore, dishonest SIoT devices, based on their owners’ social relations, can succeed in advertising low-quality services or exploiting maliciously provided services. In this paper, we first propose three contexts of trust in SIoT environments including the status and environment (time and location) of devices, and the types of tasks. Then, we propose a novel Mutual Context-aware Trustworthy Service Evaluation (MCTSE) model. The experiments demonstrate that our proposed contextual trust evaluation model can effectively differentiate honest and dishonest devices and provide a high success rate in selecting the most trustworthy services and providing high resilience against different attacks from dishonest devices.",
    "actual_venue": "Icsoc"
  },
  {
    "abstract": "We propose a classification method that automatically classifies annotated resources under the concepts of a classification system represented by an ontology. We use two well known systems used to classify web pages, del.icio.us for the folksonomy information and DMOZ for an existing ontology, to validate the method. Results obtained provide a correct classification rate of resources of 78%, rising to 93% when using an adequate threshold.",
    "actual_venue": "K-Cap"
  },
  {
    "abstract": "The recent development of information and communication technology has made computer software able to create highly realistic multimedia contents that can be, for human, impossible to distinguish from the natural ones. This fact leads to the need for tools and techniques that can reliably discriminate between natural and computer generated multimedia data in forensics applications. In this paper, we focus on the specific class of images containing faces, since we consider critical to be able to discriminate between photographic faces and the photorealistic ones. To this aim, we present a new geometric-based approach relying on face asymmetry information. Experimental results show that asymmetry information could be used as a hint to tackle this problem without requiring classification tools and training or combined with state-of-the-art approaches to improve their performances.",
    "actual_venue": "Eusipco"
  },
  {
    "abstract": "Discrete functions are now commonly represented by binary (BDD) and multiple-valued (MDD) decision diagrams. Sifting is an effective heuristic technique which applies adjacent variable interchanges to find a good variable ordering to reduce the size of a BDD or MDD. Linear sifting is an extension of BDD sifting where XOR operations involving adjacent variable pairs augment adjacent variable interchange leading to further reduction in the node count. In this paper, we consider the extension of this approach to MDDs. In particular, we show that the XOR operation of linear sifting can be extended to a variety of operations. We term the resulting approach augmented sifting. Experimental results are presented showing sifting and augmented sifting can be quite effective in reducing the size of MDDs for certain types of functions.",
    "actual_venue": "Ismvl"
  },
  {
    "abstract": "As more and more software vulnerabilities are exposed, shellcode has become very popular in recent years. It is widely used by attackers to exploit vulnerabilities and then hijack program's execution. Previous solutions suffer from limitations in that: 1) Some methods based on static analysis may fail to detect the shellcode using obfuscation techniques. 2) Other methods based on dynamic analysis could impose considerable performance overhead. In this paper, we propose Lemo, an efficient shellcode detection system. Our system is compatible with commodity hardware and operating systems, which enables deployment. To improve the performance of our system, we make use of the multi-core technology. The experiments show that our system can detect shellcode efficiently.",
    "actual_venue": "The Ieice Transactions On Information And Systems"
  },
  {
    "abstract": "Coordinating procurement decisions for a family of products that share a constrained resource, such as an ocean shipping container, is an important managerial problem. However due to the problem’s difficult mathematical properties, efficient and effective solution procedures for the problem have eluded researchers. This paper proposes two heuristics, for the capacitated, coordinated dynamic demand lot-size problem with deterministic but time-varying demand. In addition to inventory holding costs, the problem assumes a joint setup cost each time any member of the product family is replenished and an individual item setup cost for each item type replenished. The objective is to meet all customer demand without backorders at minimum total cost. We propose a six-phase heuristic (SPH) and a simulated annealing meta-heuristic (SAM). The SPH begins by assuming that each customer demand is met by a unique replenishment and then it seeks to iteratively maximize the net savings associated with order consolidation. Using SPH to find a starting solution, the SAM orchestrates escaping local solutions and exploring other areas of the solution state space that are randomly generated in an annealing search process. The results of extensive computational experiments document the effectiveness and efficiency of the heuristics. Over a wide range of problem parameter values, the SPH and SAM find solutions with an average optimality gap of 1.53% and 0.47% in an average time of 0.023CPUseconds and 0.32CPUseconds, respectively. The heuristics are strong candidates for application as stand alone solvers or as an upper bounding procedure within an optimization based algorithm. The procedures are currently being tested as a solver in the procurement software suite of a nationally recognized procurement software provider.",
    "actual_venue": "European Journal Of Operational Research"
  },
  {
    "abstract": "Contextual location prediction is an important topic in the field of personalized location recommendation in LBS (location-based services). With the advancement of mobile positioning techniques and various sensors embedded in smartphones, it is convenient to obtain massive human mobile trajectories and to derive a large amount of valuable information from geospatial big data. Extracting and recognizing personally interesting places and predicting next semantic location become a research hot spot in LBS. In this paper, we proposed an approach to predict next personally semantic place with historical visiting patterns derived from mobile device logs. To address the problems of location imprecision and lack of semantic information, a modified trip-identify method is employed to extract key visit points from GPS trajectories to a more accurate extent while semantic information are added through stay point detection and semantic places recognition. At last, a decision tree model is adopted to explore the spatial, temporal, and sequential features in contextual location prediction. To validate the effectiveness of our approach, experiments were conducted based on a trajectory collection in Guangzhou downtown area. The results verified the feasibility of our approach on contextual location prediction from continuous mobile devices logs.",
    "actual_venue": "Mobile Information Systems"
  },
  {
    "abstract": "How to manage and analyze interconnected, multidimensional and heterogeneous information network data has become the focus of current research. Graph On-Line Analytical Processing (GraphOLAP) can process a quick online analysis and query operation of graph data. With the existing achievement of GraphOLAP we propose a new graph cube framework according to the multidimensional heterogeneous informational network. We introduce the concept of relation path which is the guidance of the relation path aggregate network. We propose the concept of derived dimension to support more analysis with clustering algorithm. We also propose some traditional operations and new operations based on our model. Then we discuss the materialization strategies and implement the framework in Spark. The result of experiments has proved the efficiency and effectiveness of our framework.",
    "actual_venue": "Asonam : Advances In Social Networks Analysis And Mining Sydney Australia July"
  },
  {
    "abstract": "Maximum margin clustering (MMC), which borrows the large margin heuristic from support vector machine (SVM), has achieved more accurate results than traditional clustering methods. The intuition is that, for a good clustering, when labels are assigned to different clusters, SVM can achieve a large minimum margin on this data. Recent studies, however, disclosed that maximizing the minimum margin does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In this paper, we propose a novel approach ODMC (Optimal margin Distribution Machine for Clustering), which tries to cluster the data and achieve optimal margin distribution simultaneously. Specifically, we characterize the margin distribution by the firstand second-order statistics, i.e., the margin mean and variance, and extend a stochastic mirror descent method to solve the resultant minimax problem. Moreover, we prove theoretically that ODMC has the same convergence rate with state-of-the-art cutting plane based algorithms but involves much less computation cost per iteration, so our method is much more scalable than existing approaches. Extensive experiments on UCI data sets show that ODMC is significantly better than compared methods, which verifies the superiority of optimal margin distribution learning.",
    "actual_venue": "Thirty-Second Aaai Conference On Artificial Intelligence / Thirtieth Innovative Applications Of Artificial Intelligence Conference / Eighth Aaai Symposium On Educational Advances In Artificial Intelligence"
  },
  {
    "abstract": "Proponents of Approval Voting argue that this electoral rule leads to more centrist outcomes compared to Plurality Voting.\n This claim has been substantiated by scholarly work using spatial models of political competition. We revisit this issue in\n the context of a model of political competition in which (1) candidates are policy-motivated; (2) candidacy decisions are\n endogenous; and (3) candidates can credibly commit to implementing any policy. Under these assumptions we find the opposite\n to be true – Plurality Voting yields convergence to the median voter’s ideal policy but Approval Voting may not. We argue\n that this result is driven by the differential incentives for candidate entry under the two voting rules. Our results suggest\n that whether Approval Voting yields more centrist outcomes vis-�-vis Plurality Voting depends on the possibility of policy\n commitment on the part of the candidates.",
    "actual_venue": "Social Choice And Welfare"
  },
  {
    "abstract": "Hosts participating in overlay multicast applications have a wide range of heterogeneity in bandwidth and participa- tion characteristics. In this paper, we highlight and show the need to systematically consider prioritization as a key criterion in the design of protocols for overlay multicast. We identify trade-offs in the design of prioritization heuristics in two important contexts. The first part of the paper considers prioritization strategies in the context of heterogeneity in node outgoing bandwidth and node stay time durations, and a lack of correlation between the two dimensions. The second part of the paper considers bandwidth allocation and prioritization policies with multi-tree data delivery in environments with heterogeneity in outgoing bandwidth and a certain degree of altruistic behavior. We conduct a systematic study of the trade-offs using both real trace data, and sensitivity studies using synthetic workloads. To the best of our knowledge, this is the first work to identify and study these trade-offs, and t o demonstrate the potential benefits of the resulting prioritization heuristics. In this paper, we argue that such heterogeneity in node characteristics makes it important to carefully consider p ri- oritization in protocol design. In particular, overlay tre es must be constructed in a manner as to enable nodes that are more critical to the system (for example, nodes that contribute more bandwidth or are more likely to yield stable performance) to be placed at higher locations in the overlay tree. We believe such carefully constructed prioritization policies can se rve two goals. First, they can improve the performance of the entire set of hosts. Second, they can ensure nodes that contribute more to the system can receive better performance, which in turn could provide an incentive for them to contribute more. These observations motivate us to conduct a systematic study of various prioritization strategies for constructi ng over- lays in the presence of heterogeneity. These strategies ass ume the same base protocol for overlay construction, however they differ with regard to how nodes are prioritized relative to e ach other. Nodes with higher priority occupy higher positions in the tree. The first part of this paper focuses on data delivery using single trees. Given that the performance of a node depends on its depth in the tree and the stability of its ancestors, we consider prioritization heuristics based on the outgoin g bandwidth and stay time duration of nodes. Our study is set in the context of a large and interesting class of fixed durati on broadcasts, where evidence from several measurement studies have indicated nodes with a higher age tend to stay longer in the group (18), (15). Further, given indications from real d ata that outgoing bandwidth and stay times are not correlated (for example, the correlation coefficient is -0.01 for the Slashd ot trace), we systematically evaluate a family of heuristics t hat choose different operating points to trade off node outgoing bandwidth and node stay time. Our results indicate clear-cut advantages for degree-based prioritization and show potential benefits for age-based prioritization, but indicate that co mbin- ing degree- and age-based prioritization does not significa ntly improve performance over degree-based prioritization alone. The second part of the paper considers data delivery us- ing multiple trees (7), (11). Here, the multimedia stream is encoded into many sub-streams, and each sub-stream is distributed along a particular overlay tree. The quality ex pe- rienced by a receiver depends on the number of sub-streams that it receives. The performance with multi-tree data delivery depends both on how the outgoing bandwidth of a node is allocated across various trees, as well as the prioritizati on",
    "actual_venue": "Infocom"
  },
  {
    "abstract": "In this paper, the authors will briefly describe the conceptual graphs based tools that was developed by various members of the Knowledge Engineering Group at Deakin University, Australia, in the mid-1980's. Using these tools, we have experimented in developing many different types of knowledge based systems.",
    "actual_venue": "Iccs"
  },
  {
    "abstract": "Stanley associated to a simple graph G a symmetric function X G which generalizes the chromatic polynomial of G . He conjectured that X G is Schur positive when G is clawfree. This is equivalent to the minors of a certain matrix with polynomial entries being polynomials with nonnegative coefficients. We prove this for the 2×2 minors, which extends a result of Krattenthaler (J. Combin. Theory Ser. A 74(2) (1996) 351–354). We also give a characterization of clawfree graphs in terms of cardinalities of stable sets.",
    "actual_venue": "Discrete Mathematics"
  },
  {
    "abstract": "We show that unsupervised training of latent capsule layers using only the reconstruction loss, without masking to select the correct output class, causes a loss of equivariances and other desirable capsule qualities. This implies that supervised capsules networks canu0027t be very deep. Unsupervised sparsening of latent capsule layer activity both restores these qualities and appears to generalize better than supervised masking, while potentially enabling deeper capsules networks. We train a sparse, unsupervised capsules network of similar geometry to Sabour et al (2017) on MNIST, and then test classification accuracy on affNIST using an SVM layer. Accuracy is improved from benchmark 79% to 90%.",
    "actual_venue": "Arxiv: Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "Mobile computing systems are increasingly difficult to configure, operate, and manage. To reduce operation and maintenance cost plus meet user's expectation with respect to QoS, the computing system and its building blocks should be self-managed. When addressing the challenges associated with architecting self-managed mobile computing systems, one must take a holistic view on QoS management and the heterogonous entities in the mobile environment. This paper presents a novel model that combines resources and context elements. It helps us in modelling the environment and design resource and context managers that support functions for adapting the application to changes in the environment. The model is applied on a video streaming application for mobile terminals: i) resource and context elements are classified, ii) their QoS characteristics and context properties are modelled, and iii) weakly integrated resource and context managers are presented and validated.",
    "actual_venue": "Arcs"
  },
  {
    "abstract": "Change-based code review, e.g., in the form of pull requests, is the dominant style of code review in practice. An important option to improve review's efficiency is cognitive support for the reviewer. Nevertheless, review tools present the change parts under review sorted in alphabetical order of file path, thus leaving the effort of understanding the construction, connections, and logic of the changes on the reviewer. This leads to the question: How should a code review tool order the parts of a code change to best support the reviewer? We answer this question with a middle-range theory, which we generated inductively in a mixed methods study, based on interviews, an online survey, and existing findings from related areas. Our results indicate that an optimal order is mainly an optimal grouping of the change parts by relatedness. We present our findings as a collection of principles and formalize them as a partial order relation among review orders.",
    "actual_venue": "Ieee International Conference On Software Maintenance And Evolution"
  },
  {
    "abstract": "Self-assembly of DNA is considered a fundamental operation in realization of molecular logic circuits. We propose a new approach to implementation of data flow logical operations based on manipulating DNA strands. In our method the logic gates, input, and output signals are represented by DNA molecules. Each logical operation is carried out as soon as the operands are ready. This technique employs standard operations of genetic engineering including radioactive labeling as well as digestion by the second class restriction nuclease and polymerase chain reaction (PCR). To check practical utility of the method a series of genetic engineering experiments have been performed. The obtained information confirms interesting properties of the DNA-based molecular data flow logic gates. Some experimental results demonstrating implementation of a single logic NAND gate and only in one vessel calculation of a tree-like Boolean function with the help of the PCR are provided. These techniques may be utilized in massively parallel computers and on DNA chips.",
    "actual_venue": "Future Generation Comp Syst"
  },
  {
    "abstract": "Despite the well documented and emerging insider threat to information systems, there is currently no substantial effort devoted to addressing the problem of internal IT misuse. In fact, the great majority of misuse counter measures address forms of abuse originating from external factors (i.e. the perceived threat from unauthorized users). This paper suggests a new and innovative approach of dealing with insiders that abuse IT systems. The proposed solution estimates the level of threat that is likely to originate from a particular insider by introducing a threat evaluation system based on certain profiles of user behaviour. However, a substantial amount of work is required, in order to materialize and validate the proposed solutions.",
    "actual_venue": "Computers And Security"
  },
  {
    "abstract": "A virtual unit, tool group, is used to represent the group of tools in semiconductor foundry. Hence, a tool may belong to several tool groups because it may be handle several kinds of recipes. The coupling relation between the tools and the tool groups is a complex many-to-many relation. The paper aims to decide the tool-dispatching rule in tool capacity allocation. The term, \"priority-based\", is two-fold. One is to prioritize the lots in the fab, the other is to prioritize the tools in the correlated tool groups. The priority-based algorithm is used to estimate the tool utilization, bottleneck, tool-sharing in the tool groups, throughput of the tool groups and WIP (work in process) in a specific day. The proposed algorithm is applied to a real foundry fab and the results show that maximum fuzzy candidative ratio is better than other rules. Besides, a demonstration shows that the supervisor can find the bottleneck tools by the proposed algorithm and release the bottleneck by adding the right tools.",
    "actual_venue": "Icra"
  },
  {
    "abstract": "Deep Learning methods have been extensively used to analyze video data to extract valuable information by classifying image frames and detecting objects. We describe a unique approach for using video feed from a moving Locomotive to continuously monitor the Railway Track and detect significant assets like Switches on the Track. The technique used here is called Siamese Networks, which uses 2 identical networks to learn the similarity between of 2 images. Here we will use a Siamese network to continuously compare Track images and detect any significant difference in the Track. Switch will be one of those images that will be different and we will find a mapping that clearly distinguishes the Switch from other possible Track anomalies. The same method will then be extended to detect any abnormalities on the Railway Track. Railway Transportation is unique in the sense that is has wheeled vehicles, Trains pulled by Locomotives, running on guided Rails at very high speeds nearing 200 mph. Multiple Tracks on the Rail network are connected to each other using an equipment called Switch or a Turnout. Switch is either operated manually or automatically through command from a Control center and it governs the movement of Trains on different Tracks of the network. Accurate location of these Switches is very important for the railroad and getting a true picture of their state in field is important. Modern trains use high definition video cameras facing the Track that continuously record video from track. Using a Siamese network and comparing to benchmark images, we describe a method to monitor the Track and highlight anomalies.",
    "actual_venue": "Arxiv: Computer Vision And Pattern Recognition"
  },
  {
    "abstract": "Interference management has been recognized by the industry as a key enabler for 4G systems. Emerging technologies include multicarrier systems such as LTE and WiMAX for which effective management of intercell interference is of utmost importance in order to improve the Quality of Service (QoS) at cell edges. Static Intercell Interference Coordination (ICIC) techniques such as Soft Frequency Reuse (SFR) are aimed at alleviating this problem; however the usage of baseline SFR designs (schemes without optimization) only offers tradeoffs between cell edge performance and spectral efficiency and performance is indeed far from optimal as results herein confirm. Thus, this paper presents a novel multiobjective algorithm in order to address this problem and achieve effective optimization of SFR implementations. Results show that the proposed algorithm succeeds in finding good-quality SFR configurations enhancing simultaneously network capacity and cell edge performance while reducing energy consumption with respect to baseline designs and previous proposals.",
    "actual_venue": "Wireless Communications, Ieee Transactions"
  },
  {
    "abstract": "Product design is an evolving process which is characterized with creativity and uncertainty. The task management in this process is different from traditional ones because it involves more specific factors such as user specifications, product design specifications, reengineering, design process, and design data. In this article, we aim to analyze product design process with the viewpoint of project management and task management, and propose an integrated approach to interconnect those constraints, functional and nonfunctional factors coming from both product design and task management processes. In the presented approach, utilizing workflow technique, the design process and task management are directly and closely interconnected at process and data level by using the shared model and the shared database. Therefore, we can easily obtain the work breakdown structure (WBS) for project management. Implemented as part of PDM (Product data management) systems, the proposed method has been tested that it can facilitate the product design.",
    "actual_venue": "Research And Practical Issues Of Enterprise Information Systems"
  },
  {
    "abstract": "We propose a method of sampling regular and irregular-grid volume data for visualization. The method is based on the Metropolis algorithm that is a type of Monte Carlo technique. Our method enables \"importance sampling\" of local regions of interest in the visualization by generating sample points intensively in regions where a user-specified transfer function takes the peak values. The generated sample-point distribution is independent of the grid structure of the given volume data. Therefore, our method is applicable to irregular grids as well as regular grids. We demonstrate the effectiveness of our method by applying it to regular cubic grids and irregular tetrahedral grids with adaptive cell sizes. We visualize volume data by projecting the generated sample points onto the 2D image plane. We tested our sampling with three rendering models: an X-ray model, a simple illuminant particle model, and an illuminant particle model with light-attenuation effects. The grid-independency and the efficiency in the parallel processing mean that our method is suitable for visualizing large-scale volume data. The former means that the required number of sample points is proportional to the number of 2D pixels, not the number of 3D voxels. The latter means that our method can be easily accelerated on the multiple-CPU and/or GPU platforms. We also show that our method can work with adaptive space partitioning of volume data, which also enables us to treat large-scale/complex volume data easily.",
    "actual_venue": "International Journal Of Modeling Simulation And Scientific Computing"
  },
  {
    "abstract": "In this paper, we introduce some methods for finding mutually corresponding dependent components from two different but related data sets in an unsupervised (blind) manner. The basic idea is to generalize cross-correlation analysis by taking into account higher-order statistics. We propose independent component analysis (ICA) type extensions for the singular value decomposition of the cross-correlation matrix. They extend cross-correlation analysis in a similar manner as ICA extends standard principal component analysis for covariance matrices. We present experimental results demonstrating the usefulness of the proposed methods both for artificially generated data and for a cryptographic problem.",
    "actual_venue": "Neurocomputing"
  },
  {
    "abstract": "Nowadays, the requirements in the high quality managing and mathematical modeling of diabetes are increasing. Due to the several types of this chronic disease the problem represents a challenging task that was mostly covered in the literature for type 1 diabetes while examining algorithms for artificial pancreas. The aim of the current paper is to analyze a recently developed novel time-delay diabetes model elaborated for both type 1 and type 2 diabetes mellitus; hence, to handle mixed state of these diseases (double diabetes) as well. Control theoretical characteristics are investigated followed by parametric sensitivity analysis for both type 1 and type 2 cases. Finally, the model is compared with the well-known and widely used Cambridge (Hovorka)-model under different simulation scenarios.",
    "actual_venue": "European Control Conference"
  },
  {
    "abstract": "Many biological phenomena are inherently multiscale, i.e. they are characterised by interactions involving different scales at the same time. This is the case of bone remodelling, where macroscopic behaviour (at organ and tissue scale) and microstructure (at cell scale) strongly influence each other. Consequently, several approaches have been defined to model such a process at different spatial and temporal levels and, in particular, in terms of continuum properties, abstracting in this way from a realistic - and more complex - cellular scenario. While a large amount of information is available to validate such models separately, more work is needed to integrate all levels fully in a faithful multiscale model. In this scenario, we propose the use of BioShape, a 3D particle-based, scale-independent, geometry and space oriented simulator. It is used to define and integrate a cell and tissue scale model for bone remodelling in terms of shapes equipped with perception, interaction and movement capabilities. Their in-silico simulation allows for tuning continuum-based tissutal and cellular models, as well as for better understanding - both in qualitative and in quantitative terms - the blurry synergy between mechanical and metabolic factors triggering bone remodelling.",
    "actual_venue": "Electr Notes Theor Comput Sci"
  },
  {
    "abstract": "As part of the GMES system, Sentinel-1 is designed to provide an independent and operational information capacity to the European Union to warrant environment and security policies and to support sustainable economic growth. Sentinel-1 is a satellite system designed to operate a ground segment for 20 years supporting a system of up to three satellites. One satellite is built to perform at least seven years in the reference orbit and to operate a SAR instrument in C- band. Product quality is of paramount importance. Hence, the success or failure of the mission is essentially dependent on the calibration of the Sentinel-1 system ensuring the product quality and the correct in-orbit operation of the entire SAR system. The essential task of calibrating Sentinel-1 is to estimate and correct systematic error contributions throughout the complete SAR system and to tie-down image information (magnitude and phase) to reference units in geophysical terms. The quality of this calibration process is dependent on the inherent stability of the radar system and the capability to determine and monitor the radiometric and geometric characteristics.",
    "actual_venue": "Igarss"
  },
  {
    "abstract": "FIE (5'-end Information Extraction) is a web-based program designed primarily to extract the sequence of the regions around the 5'-end and around the translation initiation sites for a particular gene, based on information provided by LocusLink.",
    "actual_venue": "In Silico Biology"
  },
  {
    "abstract": "Time Related Association rule mining is a kind of sequence pattern mining for sequential databases. In this paper, we introduce a method of Generalized Association Rule Mining using Genetic Network Programming (GNP) with time series processing mechanism in order to find time related sequential rules efficiently. GNP represents solutions as directed graph structures, thus has compact structure and implicit memory function. The inherent features of GNP make it possible for GNP to work well especially in dynamic environments. GNP has been applied to generate time related candidate association rules as a tool using the database consisting of a large number of time related attributes. The aim of this algorithm is to better handle association rule extraction from the databases in a variety of time-related applications, especially in the traffic volume prediction problems. The generalized algorithm which can find the important time related association rules is described and experimental results are presented considering a traffic prediction problem.",
    "actual_venue": "Ieee Congress On Evolutionary Computation"
  },
  {
    "abstract": "The use of Computer Aided Software Engineering (CASE) tools for teaching object-oriented systems analysis and design (OOSAD) and the Unified Modelling Language (UML) has many potential benefits, but there are several problems associated with the usability and learnability of these tools. This paper describes a study undertaken to determine if computing students from a linguistically and technologically diverse community experience problems with learning to use a CASE tool, and to determine if there is a relationship between two user characteristics of the students and the learnability of CASE tools.",
    "actual_venue": "Human-Computer Interaction Symposium"
  },
  {
    "abstract": "This paper describes local area network (LAN) access using public wide area data networks and problems that arise when using integrated services digital network (ISDN) technology (Stallings 90) (Thachenkary 93). To date mainly modem connections at serial lines with a terminal port have been the standard remote access technique. With ISDN it is foreseen that these modem lines will be replaced rather soon. This is mainly due to the fact that ISDN offers a more adequate bandwidth and is much more consistent from the point of view of access and embedding. This paper demonstrates in the main section a router-based solution for enhanced call management. One of the main advantages is the separation of the strategic module which defines the behavior and thus allows for a number of active connections exceeding the number of ports. It also addresses traffic and access control in the network environment.",
    "actual_venue": "J Ucs"
  },
  {
    "abstract": "For a given regression model, each individual prediction may be more or less accurate. The average accuracy of the system cannot provide the error estimate for a single particular prediction, which could be used to correct the prediction to a more accurate value. We propose a method for correction of the regression predictions that is based on the sensitivity analysis approach. Using predictions, gained in sensitivity analysis procedure, we build a secondary regression predictor whose task is to predict the signed error of the prediction which was made using the original regression model. We test the proposed methodology using four regression models: locally weighted regression, linear regression, regression trees and neural networks. The results of our experiments indicate significant increase of prediction accuracy in more than 20% of experiments. The favorable results prevale especially with the regression trees and neural networks, where locally weighted regression was used as a model for predicting the prediction error. In these experiments the prediction accuracy increased in 60% of experiments with regression trees and in 50% of experiments with neural networks, while the increase of the prediction error did not occur in any experiment.",
    "actual_venue": "Computing And Informatics"
  },
  {
    "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.",
    "actual_venue": "Proceedings Of The Annual Meeting Of The Association For Computational Linguistics And The International Joint Conference On Natural Language Processing"
  },
  {
    "abstract": "The authors propose a ring-based injection-locked frequency divider (ILFD) incorporating a novel process and temperature compensation technique. The core of the ILFD consists of a process and temperature compensated ring oscillator based on modified symmetric load delay elements. Measurement results show that the natural frequency of oscillation of the ring oscillator varies only 4.4% across six different chips and for a temperature range of 0%80%C. The ILFD possesses a wide locking range over process corners as well as a wide temperature range because of the proposed compensation technique and the incorporation of the delay cell architecture in the design. A calibration circuitry can be used to further enhance the locking range. Measurement results show that the proposed ILFD functions as a divide-by-4 for an input frequency range of 1.8%3.2%GHz for an input power level as low as %3%dBm. The worst-case power consumption was approximately 2%mW from a 1.8%V power supply. The proposed ILFD can be used as a low-power prescaler for multi-band applications.",
    "actual_venue": "Circuits, Devices And Systems, Iet"
  },
  {
    "abstract": "Unsupervisedmethodsforautomaticvesselsegmentationfrom retinal images are attractive when only small datasets, with associated ground truth markings, are available. We present an unsupervised, curvature-based method for segmenting the complete vessel tree from colour retinal images. The vessels are modeled as trenches and the medial lines of the trenches are extracted using the curvature information derived from a novel curvature estimate. The complete vessel structure is then extracted using a modified region growingmethod. Test- results of the algorithm using the DRIVE dataset are superior to previously reported unsupervised methods and comparable to those obtained with the supervised methods in (1),(2).",
    "actual_venue": "Isbi"
  },
  {
    "abstract": "Human activity recognition (HAR) based on inertial sensors has been investigated for many industrial informatics applications, such as healthcare and ubiquitous computing. Existing methods mainly rely on supervised learning schemes, which require large labeled training data. However, labeled data are sometimes difficult to acquire, while unlabeled data are readily available. Thus, we intend to make use of both labeled and unlabeled data with semisupervised learning for accurate HAR. In this paper, we propose a semisupervised deep learning approach, using temporal ensembling of deep long short-term memory, to recognize human activities with smartphone inertial sensors. With the deep neural network processing, features are extracted for local dependencies in the recurrent framework. Besides, with an ensemble approach based on both labeled and unlabeled data, we can combine together the supervised and unsupervised losses, so as to make good use of unlabeled data that the supervised learning method cannot leverage. Experimental results indicate the effectiveness of our proposed semisupervised learning scheme, when compared to several state-of-the-art semisupervised learning approaches.",
    "actual_venue": "Ieee Transactions Industrial Informatics"
  },
  {
    "abstract": "RNA-seq technology offers the promise of rapid comprehensive discovery of long intervening noncoding RNAs (lincRNAs). Basic tools such as Tophat and Cufflinks have been widely used for RNA-seq assembly. However, advanced bioinformatics methodologies that allow in-depth analysis of lincRNAs are lacking. Here, we describe a computational protocol that is especially designed for the identification of novel lincRNAs and the prediction of the function. The protocol mainly includes two open-access tools, CNCI and ncFANs. CNCI allows users to distinguish noncoding from protein-coding transcripts and to retrieve novel lincRNAs. ncFANs integrates expression profiles of protein-coding and lincRNA genes to construct co-expression networks. Such networks are subsequently used to perform function predictions of unknown lincRNAs. This protocol will allow users to apply these procedures without the need of additional training. All the tools in current protocol are available http://www.bioinfo.org/np/.",
    "actual_venue": "Briefings In Bioinformatics"
  },
  {
    "abstract": "The Caernarvon operating system was developed to demonstrate that a high assurance system for smart cards was technically feasible and commercially viable. The entire system has been designed to be evaluated under the Common Criteria at EAL7, the highest defined level of assurance. Historically, smart card processors have not supported the hardware protection features necessary to separate the OS from the applications, and one application from another. The Caernarvon OS has taken advantage of the first smart card processors with such features to be the first smart card OS to provide this kind of protection. Even when compared with conventional systems where the hardware protection is routine, the Caernarvon OS is noteworthy, because of the EAL7 assurance. This approach facilitated implementation of a formally specified, mandatory security policy providing multi-level security (MLS) suitable for both government agencies and commercial users. The mandatory security policy requires effective authentication of its users that is independent of applications. For this reason, the Caernarvon OS also contains a privacy-preserving, two-way authentication protocol integrated with the Mandatory Security Policy. The Caernarvon OS includes a strong cryptographic library that has been separately certified under the Common Criteria at EAL5+ for use with other systems. The Caernarvon OS implements a secure method for downloading trusted and untrusted application software and data in the field, with the assumption that all applications are potentially hostile. While the initial platform for the operating system was smart cards, the design could also be used in other embedded devices, such as USB tokens, PDAs, cell phones, etc.",
    "actual_venue": "Operating Systems Review"
  },
  {
    "abstract": "In order to have comparable results for several algorithms designed for Virtual Synchronous Generators (VSG), a set of performance indicators have been derived from the test field operation. The uncertainties budget of the VSG control algorithm is also derived from measurement data and grid parameters. The frequency measurement and rate of change of frequency estimation determine the VSG algorithm performance. However, in grid connected mode, it is not possible to highlight the VSG contribution to the grid inertia, due to system size differences. Therefore, in this paper, results from island mode operation are also included.",
    "actual_venue": "Amps"
  },
  {
    "abstract": "In this paper, we present a new high bit-rate digital subscriber line (HDSL) echo canceler scheme. It uses an optimized orthogonal recursive prefilter reducing the computational cost if compared to a conventional FIR filter, a new analog precancellation technique which reduces the dynamic range of the A/D converter, and simple digital correction techniques that reduce the linearity requirements on the data converters. With these features, we expect the hybrid echo canceler to be suitable for a possible one-pair HDSL system",
    "actual_venue": "Circuits And Systems, Iscas , Ieee International Symposium"
  },
  {
    "abstract": "Membrane properties such as potentials (intracellular, extracellular, electrotonic) and axonal excitability indices (strength–duration and charge–duration curves, strength–duration time constants, rheobasic currents, recovery cycles) can now be measured in healthy subjects and patients with demyelinating neuropathies. They are regarded here in two cases of simultaneously reduced paranodal seal resistance and myelin lamellae in one to three consecutive internodes of human motor nerve fiber. The investigations are performed for 70 and 96% myelin reduction values. The first value is not sufficient to develop a conduction block, but the second leads to a block and the corresponding demyelinations are regarded as mild and severe. For both the mild and severe demyelinations, the paranodally internodally focally demyelinated cases (termed as PIFD1, PIFD2, and PIFD3, respectively, with one, two, and three demyelinated internodes) are simulated using our previous double-cable model of the fiber. The axon model consists of 30 nodes and 29 internodes. The membrane property abnormalities obtained can be observed in vivo in patients with demyelinating forms of Guillain-Barré syndrome (GBS) and multifocal motor neuropathy (MMN). The study confirms that focal demyelinations are specific indicators for acquired demyelinating neuropathies. Moreover, the following changes have been calculated in our previous papers: (1) uniform reduction of myelin thickness in all internodes (Stephanova et al. in Clin Neurophysiol 116: 1153–1158, 2005); (2) demyelination of all paranodal regions (Stephanova and Daskalova in Clin Neurophysiol 116: 1159–1166, 2005a); (3) simultaneous reduction of myelin thickness and paranodal demyelination in all internodes (Stephanova and Daskalova in Clin Neurophysiol 116: 2334–2341, 2005b); and (4) reduction of myelin thickness of up to three internodes (Stephanova et al., in J Biol Phys, 2006a,b, DOI: 10.1007/s10867-005-9001-9; DOI: 10.1007/s10867-006-9008-x). The mem- brane property abnormalities obtained in the homogenously demyelinated cases are quite different and abnormally greater than those in the case investigated here of simultaneous reduction in myelin thickness and paranodal demyelination of up to three internodes. Our previous and present results show that unless focal demyelination is severe enough to cause outright conduction block, changes are so slight as to be essentially indistinguishable from normal values. Consequently, the excitability-based approaches that have shown strong potential as diagnostic tools in systematically demyelinated conditions may not be useful in detecting mild focal demyelinations, independently of whether they are internodal, paranodal, or paranodal internodal.",
    "actual_venue": "Biological Cybernetics"
  },
  {
    "abstract": "We propose a new generalized thresholding algorithm useful for inverse problems with sparsity constraints. The algorithm uses a thresholding function with a parameter p, first mentioned in [1]. When p = 1, the thresholding function is equivalent to classical soft thresholding. For values of p below 1, the thresholding penalizes small coefficients over a wider range and applies less bias to the larger coefficients, much like hard thresholding but without discontinuities. The functional that the new thresholding minimizes is non-convex for p <; 1. We state an algorithm similar to the Iterative Soft Thresholding Algorithm (ISTA) [2].We show that the new thresholding performs better in numerical examples than soft thresholding.",
    "actual_venue": "Acoustics Speech And Signal Processing"
  },
  {
    "abstract": "This paper addresses the motion planning problem for dextrous manipulation by an artificial multi-fingered ed hand. We focus on the task of re-orienting polyhedral objects and present a practical motion planner for achieving quasi-static manipulations by a four-fingered hand. The planner makes use of a simple manipulation strategy in which a single finger is moved and the other three are fixed. We describe how this scenario is exploited for reducing the space of trajectory solutions and used within a two-level incremental process to search for global re-orientation motions. The planner has been implemented and used for achieving several complex re-orientation task with frictional and frictionless contacts that show the practicality of our approach.",
    "actual_venue": "Iros - Proceedings Of The Ieee/Rsj International Conference On Intelligent Robot And Systems: Innovative Robotics For Real-World Applications, Vols"
  },
  {
    "abstract": "The C programming language is an important piece of many undergraduate CS programs, as it provides an environment for interacting directly with memory and exploring systems-programming concepts. However, while many common introductory languages have rich tools that support instruction, C has received relatively little attention [2, 1]. To provide students with rapid feedback and tools for understanding C, we have extended PCRS, a web-based platform for deploying programming exercises and content such as videos. Students submit C code to solve programming exercises and receive immediate feedback generated by running the submission against a set of instructor-defined testcases. Students also have access to graphical traces of execution, so they can explore how their code manipulates memory. The system has been deployed to two second-year systems-programming courses with a total enrollment over 600, and a set of modules, consisting of videos and exercises, is being developed for use by the community.",
    "actual_venue": "Annual Joint Conference Integrating Technology Into Computer Science Education"
  },
  {
    "abstract": "This paper presents a distributed control algorithm for multi-target surveillance by multiple robots. Robots equipped with sensors and communication devices discover and track as many evasive targets as possible in an open region. The algorithm utilizes information from sensors, communication, and a mechanism to predict the minimum time before a robot loses a target. Workload is shared locally between robots using a greedy assignment of targets. Across long distances robots cooperate through explicit communication. The approach is coined Behavioral Cooperative Multi-robot Observation of Multiple Moving Targets. A formal representation of the proposed algorithm as well as proofs of performance guarantee are provided. Extensive simulations confirm the theoretical results in practice.",
    "actual_venue": "J Robotics Res"
  },
  {
    "abstract": "Microarray has been widely used to measure the relative amounts of every mRNA transcript from the genome in a single scan. Biologists have been accustomed to reading their experimental data directly from tables. However, microarray data are quite large and are stored in a series of files in a machine-readable format, so direct reading of the full data set is not feasible. The challenge is to design a user interface that allows biologists to usefully view large tables of raw microarray-based gene expression data. This paper presents one such interface--an electronic table (E-table) that uses fisheye distortion technology.The Fisheye Viewer for microarray-based gene expression data has been successfully developed to view MIAME data stored in the MAGE-ML format. The viewer can be downloaded from the project web site http://polaris.imt.uwm.edu:7777/fisheye/. The fisheye viewer was implemented in Java so that it could run on multiple platforms. We implemented the E-table by adapting JTable, a default table implementation in the Java Swing user interface library. Fisheye views use variable magnification to balance magnification for easy viewing and compression for maximizing the amount of data on the screen.This Fisheye Viewer is a lightweight but useful tool for biologists to quickly overview the raw microarray-based gene expression data in an E-table.",
    "actual_venue": "Bmc Bioinformatics"
  },
  {
    "abstract": "We investigate the use of an auxiliary network of sensors to locate mobiles in a cellular system, based on the received signal strength at the sensor receivers from a mobile's transmission. The investigation uses a generic path loss model incorporating distance effects and spatially correlated shadow fading. We describe four simple localization schemes and show that they all meet E-911 requirements in most environments. Performance can be further improved by implementing the MMSE algorithm, which ideally reaches the Cramer-Rao bound. We compare the MMSE algorithm and the four simple schemes when the model parameters are estimated via inter-sensor measurements.",
    "actual_venue": "Ieee Transactions On Wireless Communications"
  },
  {
    "abstract": "The performance of many communication systems could be improved if the transmission channel was estimated blindly, i.e. without training sequences. As an example, we investigate whether, on GSM conditions, the blind channel estimation method EVI (eigen vector approach to blind identification) can compete with the non-blind least squares scheme based on the cross-correlation. For Gaussian stationary uncorrelated scattering channels, we give simulated bit error rates (BER) after Viterbi detection in terms of the mean signal-to-noise ratio (S¯N¯R¯) for blind, non-blind, and ideal channel estimation. Averaged over three COST-207 propagation environments, EVI leads to an S¯N¯R¯ loss of 1.1 dB only, which is quite remarkable for an approach based on higher order statistics, as just 142 samples can be used for blind channel estimation",
    "actual_venue": "Communications, Icc Montreal, Towards The Knowledge Millennium Ieee International Conference"
  },
  {
    "abstract": "In previous work, the author introduced the B-treap, a uniquely represented\nB-tree analogue, and proved strong performance guarantees for it. However, the\nB-treap maintains complex invariants and is very complex to implement. In this\npaper we introduce the B-skip-list, which has most of the guarantees of the\nB-treap, but is vastly simpler and easier to implement. Like the B-treap, the\nB-skip-list may be used to construct strongly history-independent index\nstructures and filesystems; such constructions reveal no information about the\nhistorical sequence of operations that led to the current logical state. For\nexample, a uniquely represented filesystem would support the deletion of a file\nin a way that, in a strong information-theoretic sense, provably removes all\nevidence that the file ever existed. Like the B-tree, the B-skip-list has depth\nO(log_B (n)) where B is the block transfer size of the external memory, uses\nlinear space with high probability, and supports efficient one-dimensional\nrange queries.",
    "actual_venue": "Clinical Orthopaedics And Related Research"
  },
  {
    "abstract": "We present NetMesh, a new algorithm that produces a conforming Delaunay mesh for point sets in any fixed dimension with guaranteed optimal mesh size and quality. Our comparison-based algorithm runs in O(n log n + m) time, where n is the input size and m is the output size, and with constants depending only on the dimension and the desired element quality. It can terminate early in O(n log n) time returning a O(n) size Voronoi diagram of a superset of P, which again matches the known lower bounds. The previous best results in the comparison model depended on the log of the spread of the input, the ratio of the largest to smallest pairwise distance. We reduce this dependence to O(log n) by using a sequence of ε-nets to determine input insertion order into a incremental Voronoi diagram. We generate a hierarchy of well-spaced meshes and use these to show that the complexity of the Voronoi diagram stays linear in the number of points throughout the construction.",
    "actual_venue": "Symposium On Computational Geometry"
  },
  {
    "abstract": "Abstract We consider a timetabling and rostering problem involving periodic retraining of large numbers of employees at an Australian electricity distributor. This problem is different from traditional high school and university timetabling problems studied in the literature in several aspects. We propose a three-stage heuristic consisting of timetable generation, timetable improvement, and trainer rostering. Large-scale integer linear programming models for both the timetabling and the rostering components are proposed, and several unique operational constraints are discussed. We show that this solution approach is able to produce good solutions in practically acceptable time.",
    "actual_venue": "Annals Of Operations Research"
  }
]